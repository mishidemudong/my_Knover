{
  "is_distributed": true,
  "save_path": "./output",
  "train_file": "data/train.txt",
  "valid_file": "data/valid.txt",
  "start_step": 0,
  "num_epochs": 20,
  "log_steps": 100,
  "validation_steps": 1000,
  "save_steps": 1000,
  "Model": {
    "model": "UnifiedTransformer",
    "config_path": "./config/12L.json",
    "init_checkpoint": "",
    "init_pretraining_params": "12L",
    "learning_rate": 1e-05,
    "warmup_steps": 1000,
    "weight_decay": 0.01,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": true,
    "amp_loss_scaling": 12800,
    "max_seq_len": 512,
    "weight_sharing": true,
    "mem_efficient": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": null,
    "topk": 10,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "DialogGeneration",
    "do_generation": false,
    "is_cn": false,
    "nsp_inference_model_path": null,
    "nsp_attention_style": "bidirectional",
    "ranking_score": "decode_score"
  },
  "Reader": {
    "max_src_len": 384,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": true,
    "batch_size": 8192,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  }
}


API is deprecated since 2.0.0 Please use FleetAPI instead.
WIKI: https://github.com/PaddlePaddle/Fleet/blob/develop/markdown_doc/transpiler

        
W1023 11:27:58.465924  4109 device_context.cc:252] Please NOTE: device: 0, CUDA Capability: 70, Driver API Version: 11.0, Runtime API Version: 9.0
W1023 11:27:58.470854  4109 device_context.cc:260] device: 0, cuDNN Version: 7.6.
Traceback (most recent call last):
  File "./train.py", line 175, in <module>
    train(args)
  File "./train.py", line 78, in train
    model = models.create_model(args, place)
  File "/home/aistudio/ldk/Knover/models/__init__.py", line 49, in create_model
    return MODEL_REGISTRY[args.model](args, place)
  File "/home/aistudio/ldk/Knover/models/unified_transformer.py", line 93, in __init__
    super(UnifiedTransformer, self).__init__(args, place)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 74, in __init__
    self._build_programs()
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 140, in _build_programs
    init_pretraining_params(self.exe, self.init_pretraining_params, self.program)
  File "/home/aistudio/ldk/Knover/utils/__init__.py", line 131, in init_pretraining_params
    ), "[%s] cann't be found." % pretraining_params_path
AssertionError: [12L] cann't be found.
{
  "is_distributed": true,
  "save_path": "./output",
  "train_file": "data/train.txt",
  "valid_file": "data/valid.txt",
  "start_step": 0,
  "num_epochs": 20,
  "log_steps": 100,
  "validation_steps": 1000,
  "save_steps": 1000,
  "Model": {
    "model": "UnifiedTransformer",
    "config_path": "./config/12L.json",
    "init_checkpoint": "",
    "init_pretraining_params": "12L",
    "learning_rate": 1e-05,
    "warmup_steps": 1000,
    "weight_decay": 0.01,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": true,
    "amp_loss_scaling": 12800,
    "max_seq_len": 512,
    "weight_sharing": true,
    "mem_efficient": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": null,
    "topk": 10,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "DialogGeneration",
    "do_generation": false,
    "is_cn": false,
    "nsp_inference_model_path": null,
    "nsp_attention_style": "bidirectional",
    "ranking_score": "decode_score"
  },
  "Reader": {
    "max_src_len": 384,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": true,
    "batch_size": 8192,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  }
}


API is deprecated since 2.0.0 Please use FleetAPI instead.
WIKI: https://github.com/PaddlePaddle/Fleet/blob/develop/markdown_doc/transpiler

        
W1023 11:29:59.346154  4604 device_context.cc:252] Please NOTE: device: 0, CUDA Capability: 70, Driver API Version: 11.0, Runtime API Version: 9.0
W1023 11:29:59.351296  4604 device_context.cc:260] device: 0, cuDNN Version: 7.6.
Traceback (most recent call last):
  File "./train.py", line 175, in <module>
    train(args)
  File "./train.py", line 78, in train
    model = models.create_model(args, place)
  File "/home/aistudio/ldk/Knover/models/__init__.py", line 49, in create_model
    return MODEL_REGISTRY[args.model](args, place)
  File "/home/aistudio/ldk/Knover/models/unified_transformer.py", line 93, in __init__
    super(UnifiedTransformer, self).__init__(args, place)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 74, in __init__
    self._build_programs()
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 140, in _build_programs
    init_pretraining_params(self.exe, self.init_pretraining_params, self.program)
  File "/home/aistudio/ldk/Knover/utils/__init__.py", line 131, in init_pretraining_params
    ), "[%s] cann't be found." % pretraining_params_path
AssertionError: [12L] cann't be found.
{
  "is_distributed": true,
  "save_path": "./output",
  "train_file": "data/train.txt",
  "valid_file": "data/valid.txt",
  "start_step": 0,
  "num_epochs": 20,
  "log_steps": 100,
  "validation_steps": 1000,
  "save_steps": 1000,
  "Model": {
    "model": "UnifiedTransformer",
    "config_path": "./config/12L.json",
    "init_checkpoint": "",
    "init_pretraining_params": "12L",
    "learning_rate": 1e-05,
    "warmup_steps": 1000,
    "weight_decay": 0.01,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": true,
    "amp_loss_scaling": 12800,
    "max_seq_len": 512,
    "weight_sharing": true,
    "mem_efficient": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": null,
    "topk": 10,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "DialogGeneration",
    "do_generation": false,
    "is_cn": false,
    "nsp_inference_model_path": null,
    "nsp_attention_style": "bidirectional",
    "ranking_score": "decode_score"
  },
  "Reader": {
    "max_src_len": 384,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": true,
    "batch_size": 8192,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  }
}


API is deprecated since 2.0.0 Please use FleetAPI instead.
WIKI: https://github.com/PaddlePaddle/Fleet/blob/develop/markdown_doc/transpiler

        
W1023 12:51:57.896736 93680 device_context.cc:252] Please NOTE: device: 0, CUDA Capability: 70, Driver API Version: 11.0, Runtime API Version: 9.0
W1023 12:51:57.950273 93680 device_context.cc:260] device: 0, cuDNN Version: 7.6.
Load pretraining parameters from 12L.
Traceback (most recent call last):
  File "./train.py", line 175, in <module>
    train(args)
  File "./train.py", line 99, in train
    outputs = task.train_step(model, data)
  File "/home/aistudio/ldk/Knover/tasks/task_base.py", line 31, in train_step
    outputs = model.train_step(inputs)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 278, in train_step
    use_program_cache=True)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 266, in _execute
    fetch_vars = self.exe.run(program, feed, fetch_list, **kwargs)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1066, in run
    return_merged=return_merged)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1167, in _run_impl
    return_merged=return_merged)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 879, in _run_parallel
    tensors = exe.run(fetch_var_names, return_merged)._move_to_list()
KeyboardInterrupt
{
  "is_distributed": false,
  "save_path": "./output",
  "train_file": "data/train.txt",
  "valid_file": "data/valid.txt",
  "start_step": 0,
  "num_epochs": 20,
  "log_steps": 100,
  "validation_steps": 1000,
  "save_steps": 1000,
  "Model": {
    "model": "UnifiedTransformer",
    "config_path": "./config/12L.json",
    "init_checkpoint": "",
    "init_pretraining_params": "12L",
    "learning_rate": 1e-05,
    "warmup_steps": 1000,
    "weight_decay": 0.01,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": true,
    "amp_loss_scaling": 12800,
    "max_seq_len": 512,
    "weight_sharing": true,
    "mem_efficient": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": null,
    "topk": 10,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "DialogGeneration",
    "do_generation": false,
    "is_cn": false,
    "nsp_inference_model_path": null,
    "nsp_attention_style": "bidirectional",
    "ranking_score": "decode_score"
  },
  "Reader": {
    "max_src_len": 384,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": true,
    "batch_size": 32,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  }
}
W1023 15:16:08.963105   551 device_context.cc:252] Please NOTE: device: 0, CUDA Capability: 70, Driver API Version: 11.0, Runtime API Version: 9.0
W1023 15:16:08.967741   551 device_context.cc:260] device: 0, cuDNN Version: 7.6.
Load pretraining parameters from 12L.
[train][1] progress: 1/1 step: 100, time: 6.095, speed: 16.406 steps/s
	current lr: 0.0000010
	lm_loss: 3.4719, ppl: 32.1986, loss: 3.4719
[train][1] progress: 1/1 step: 200, time: 3.791, speed: 26.378 steps/s
	current lr: 0.0000020
	lm_loss: 5.3314, ppl: 206.7245, loss: 5.3314
[train][1] progress: 1/1 step: 300, time: 3.689, speed: 27.105 steps/s
	current lr: 0.0000030
	lm_loss: 5.0572, ppl: 157.1482, loss: 5.0572
[train][1] progress: 1/1 step: 400, time: 3.295, speed: 30.353 steps/s
	current lr: 0.0000040
	lm_loss: 6.5387, ppl: 691.4001, loss: 6.5387
[train][1] progress: 1/1 step: 500, time: 3.406, speed: 29.361 steps/s
	current lr: 0.0000050
	lm_loss: 2.4119, ppl: 11.1554, loss: 2.4119
[train][1] progress: 1/1 step: 600, time: 3.353, speed: 29.820 steps/s
	current lr: 0.0000060
	lm_loss: 4.3914, ppl: 80.7560, loss: 4.3914
[train][1] progress: 1/1 step: 700, time: 3.728, speed: 26.821 steps/s
	current lr: 0.0000070
	lm_loss: 4.2710, ppl: 71.5911, loss: 4.2710
[train][1] progress: 1/1 step: 800, time: 3.839, speed: 26.052 steps/s
	current lr: 0.0000080
	lm_loss: 3.8050, ppl: 44.9271, loss: 3.8050
[train][1] progress: 1/1 step: 900, time: 3.280, speed: 30.492 steps/s
	current lr: 0.0000090
	lm_loss: 5.2665, ppl: 193.7304, loss: 5.2665
[train][1] progress: 1/1 step: 1000, time: 3.667, speed: 27.272 steps/s
	current lr: 0.0000100
	lm_loss: 4.5077, ppl: 90.7127, loss: 4.5077
================================================================================
Evaluation:
	step 100:lm_loss: 4.6425, ppl: 103.8064, loss: 4.7189
	step 200:lm_loss: 4.5978, ppl: 99.2660, loss: 4.6775
	step 300:lm_loss: 4.6364, ppl: 103.1717, loss: 4.6785
	step 400:lm_loss: 4.6608, ppl: 105.7213, loss: 4.7132
	step 500:lm_loss: 4.6661, ppl: 106.2797, loss: 4.7202
	step 600:lm_loss: 4.6387, ppl: 103.4049, loss: 4.6875
	step 700:lm_loss: 4.5893, ppl: 98.4209, loss: 4.6469
	step 800:lm_loss: 4.5955, ppl: 99.0419, loss: 4.6419
	step 900:lm_loss: 4.5983, ppl: 99.3194, loss: 4.6432
	step 1000:lm_loss: 4.6181, ppl: 101.3006, loss: 4.6524
	step 1100:lm_loss: 4.5940, ppl: 98.8859, loss: 4.6248
	step 1200:lm_loss: 4.5886, ppl: 98.3590, loss: 4.6228
	step 1300:lm_loss: 4.5943, ppl: 98.9189, loss: 4.6302
	step 1400:lm_loss: 4.6036, ppl: 99.8414, loss: 4.6375
	step 1500:lm_loss: 4.6157, ppl: 101.0615, loss: 4.6460
	step 1600:lm_loss: 4.6129, ppl: 100.7766, loss: 4.6510
	step 1700:lm_loss: 4.6157, ppl: 101.0549, loss: 4.6548
	step 1800:lm_loss: 4.6157, ppl: 101.0569, loss: 4.6520
	step 1900:lm_loss: 4.6158, ppl: 101.0680, loss: 4.6570
	step 2000:lm_loss: 4.6185, ppl: 101.3394, loss: 4.6574
	step 2100:lm_loss: 4.6121, ppl: 100.6906, loss: 4.6521
	step 2200:lm_loss: 4.6169, ppl: 101.1812, loss: 4.6584
	step 2300:lm_loss: 4.6266, ppl: 102.1658, loss: 4.6666
	step 2400:lm_loss: 4.6232, ppl: 101.8196, loss: 4.6623
	step 2500:lm_loss: 4.6174, ppl: 101.2263, loss: 4.6637
	step 2600:lm_loss: 4.6175, ppl: 101.2431, loss: 4.6646
	step 2700:lm_loss: 4.6157, ppl: 101.0627, loss: 4.6672
	step 2800:lm_loss: 4.6106, ppl: 100.5420, loss: 4.6593
	step 2900:lm_loss: 4.6106, ppl: 100.5476, loss: 4.6587
	step 3000:lm_loss: 4.6141, ppl: 100.8995, loss: 4.6597
	step 3100:lm_loss: 4.6186, ppl: 101.3560, loss: 4.6634
	step 3200:lm_loss: 4.6143, ppl: 100.9159, loss: 4.6614
	step 3300:lm_loss: 4.6143, ppl: 100.9133, loss: 4.6599
	step 3400:lm_loss: 4.6189, ppl: 101.3778, loss: 4.6609
	step 3500:lm_loss: 4.6243, ppl: 101.9359, loss: 4.6654
	step 3600:lm_loss: 4.6244, ppl: 101.9401, loss: 4.6667
	step 3700:lm_loss: 4.6236, ppl: 101.8650, loss: 4.6660
	step 3800:lm_loss: 4.6263, ppl: 102.1327, loss: 4.6679
	step 3900:lm_loss: 4.6297, ppl: 102.4807, loss: 4.6707
	step 4000:lm_loss: 4.6319, ppl: 102.7054, loss: 4.6719
	step 4100:lm_loss: 4.6287, ppl: 102.3806, loss: 4.6665
	step 4200:lm_loss: 4.6285, ppl: 102.3603, loss: 4.6653
Traceback (most recent call last):
  File "./train.py", line 175, in <module>
    train(args)
  File "./train.py", line 113, in train
    evaluate(task, model, valid_generator, args, dev_count, gpu_id, step)
  File "./train.py", line 131, in evaluate
    part_outputs = task.eval_step(model, data)
  File "/home/aistudio/ldk/Knover/tasks/task_base.py", line 37, in eval_step
    outputs = model.eval_step(inputs)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 288, in eval_step
    self.eval_fetch_dict)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 266, in _execute
    fetch_vars = self.exe.run(program, feed, fetch_list, **kwargs)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1066, in run
    return_merged=return_merged)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1154, in _run_impl
    use_program_cache=use_program_cache)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1224, in _run_program
    fetch_var_name=fetch_var_name)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 580, in _add_feed_fetch_ops
    tmp_program = program.clone()
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/framework.py", line 4149, in clone
    p._sync_with_cpp()
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/framework.py", line 4561, in _sync_with_cpp
    block._sync_with_cpp()
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/framework.py", line 2692, in _sync_with_cpp
    self.create_var(name=var.name(), desc=var, type=var.type())
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/framework.py", line 2447, in create_var
    var = Variable(block=self, *args, **kwargs)
KeyboardInterrupt
{
  "is_distributed": false,
  "save_path": "./output",
  "train_file": "data/train.txt",
  "valid_file": "data/valid.txt",
  "start_step": 0,
  "num_epochs": 20,
  "log_steps": 100,
  "validation_steps": 1000,
  "save_steps": 1000,
  "Model": {
    "model": "UnifiedTransformer",
    "config_path": "./config/12L.json",
    "init_checkpoint": "",
    "init_pretraining_params": "12L",
    "learning_rate": 1e-05,
    "warmup_steps": 1000,
    "weight_decay": 0.01,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": true,
    "amp_loss_scaling": 12800,
    "max_seq_len": 512,
    "weight_sharing": true,
    "mem_efficient": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": null,
    "topk": 10,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "DialogGeneration",
    "do_generation": false,
    "is_cn": false,
    "nsp_inference_model_path": null,
    "nsp_attention_style": "bidirectional",
    "ranking_score": "decode_score"
  },
  "Reader": {
    "max_src_len": 384,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": true,
    "batch_size": 8192,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  }
}
W1023 15:23:33.244757  1956 device_context.cc:252] Please NOTE: device: 0, CUDA Capability: 70, Driver API Version: 11.0, Runtime API Version: 9.0
W1023 15:23:33.249191  1956 device_context.cc:260] device: 0, cuDNN Version: 7.6.
Load pretraining parameters from 12L.
[train][1] progress: 1/1 step: 100, time: 49.693, speed: 2.012 steps/s
	current lr: 0.0000010
	lm_loss: 5.2926, ppl: 198.8580, loss: 5.2926
[train][1] progress: 1/1 step: 200, time: 47.064, speed: 2.125 steps/s
	current lr: 0.0000020
	lm_loss: 5.1180, ppl: 166.9956, loss: 5.1180
[train][1] progress: 1/1 step: 300, time: 47.689, speed: 2.097 steps/s
	current lr: 0.0000030
	lm_loss: 4.9271, ppl: 137.9782, loss: 4.9271
[train][1] progress: 1/1 step: 400, time: 47.667, speed: 2.098 steps/s
	current lr: 0.0000040
	lm_loss: 4.6561, ppl: 105.2204, loss: 4.6561
[train][1] progress: 1/1 step: 500, time: 47.765, speed: 2.094 steps/s
	current lr: 0.0000050
	lm_loss: 4.4682, ppl: 87.2021, loss: 4.4682
[train][1] progress: 1/1 step: 600, time: 47.353, speed: 2.112 steps/s
	current lr: 0.0000060
	lm_loss: 4.6823, ppl: 108.0175, loss: 4.6823
[train][1] progress: 1/1 step: 700, time: 47.692, speed: 2.097 steps/s
	current lr: 0.0000070
	lm_loss: 4.4639, ppl: 86.8211, loss: 4.4639
[train][1] progress: 1/1 step: 800, time: 47.759, speed: 2.094 steps/s
	current lr: 0.0000080
	lm_loss: 4.2531, ppl: 70.3218, loss: 4.2531
[train][1] progress: 1/1 step: 900, time: 47.573, speed: 2.102 steps/s
	current lr: 0.0000090
	lm_loss: 4.5006, ppl: 90.0706, loss: 4.5006
[train][1] progress: 1/1 step: 1000, time: 47.662, speed: 2.098 steps/s
	current lr: 0.0000100
	lm_loss: 4.2821, ppl: 72.3931, loss: 4.2821
================================================================================
Evaluation:
	step 100:lm_loss: 4.1322, ppl: 62.3161, loss: 4.1556
	step 200:lm_loss: 4.1382, ppl: 62.6880, loss: 4.1623
	step 300:lm_loss: 4.1314, ppl: 62.2624, loss: 4.1562
	step 400:lm_loss: 4.1287, ppl: 62.0955, loss: 4.1525
	step 500:lm_loss: 4.1300, ppl: 62.1799, loss: 4.1515
	step 600:lm_loss: 4.1310, ppl: 62.2431, loss: 4.1513
	step 700:lm_loss: 4.1285, ppl: 62.0832, loss: 4.1489
	step 800:lm_loss: 4.1267, ppl: 61.9725, loss: 4.1476
	step 900:lm_loss: 4.1256, ppl: 61.9076, loss: 4.1471
	step 1000:lm_loss: 4.1268, ppl: 61.9792, loss: 4.1479
	step 1100:lm_loss: 4.1248, ppl: 61.8575, loss: 4.1465
	step 1200:lm_loss: 4.1226, ppl: 61.7219, loss: 4.1452
	step 1300:lm_loss: 4.1252, ppl: 61.8831, loss: 4.1472
	step 1400:lm_loss: 4.1276, ppl: 62.0316, loss: 4.1495
	step 1500:lm_loss: 4.1280, ppl: 62.0512, loss: 4.1502
	step 1600:lm_loss: 4.1298, ppl: 62.1666, loss: 4.1514
	step 1700:lm_loss: 4.1348, ppl: 62.4764, loss: 4.1545
	step 1800:lm_loss: 4.1490, ppl: 63.3698, loss: 4.1639
	step 1900:lm_loss: 4.1641, ppl: 64.3331, loss: 4.1741
	step 2000:lm_loss: 4.1769, ppl: 65.1653, loss: 4.1829
	step 2100:lm_loss: 4.1881, ppl: 65.8964, loss: 4.1904
	step 2200:lm_loss: 4.1929, ppl: 66.2177, loss: 4.1925
	step 2300:lm_loss: 4.1954, ppl: 66.3784, loss: 4.1934
	step 2400:lm_loss: 4.1989, ppl: 66.6116, loss: 4.1942
	step 2500:lm_loss: 4.2034, ppl: 66.9163, loss: 4.1961
	step 2600:lm_loss: 4.2032, ppl: 66.8995, loss: 4.1957
Traceback (most recent call last):
  File "./train.py", line 175, in <module>
    train(args)
  File "./train.py", line 113, in train
    evaluate(task, model, valid_generator, args, dev_count, gpu_id, step)
  File "./train.py", line 131, in evaluate
    part_outputs = task.eval_step(model, data)
  File "/home/aistudio/ldk/Knover/tasks/task_base.py", line 37, in eval_step
    outputs = model.eval_step(inputs)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 288, in eval_step
    self.eval_fetch_dict)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 266, in _execute
    fetch_vars = self.exe.run(program, feed, fetch_list, **kwargs)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1066, in run
    return_merged=return_merged)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1154, in _run_impl
    use_program_cache=use_program_cache)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1229, in _run_program
    fetch_var_name)
KeyboardInterrupt
{
  "is_distributed": false,
  "save_path": "./output",
  "train_file": "data/train.txt",
  "valid_file": "data/valid.txt",
  "start_step": 0,
  "num_epochs": 20,
  "log_steps": 1,
  "validation_steps": 1000,
  "save_steps": 1000,
  "Model": {
    "model": "UnifiedTransformer",
    "config_path": "./config/12L.json",
    "init_checkpoint": "",
    "init_pretraining_params": "12L",
    "learning_rate": 1e-05,
    "warmup_steps": 1000,
    "weight_decay": 0.01,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": true,
    "amp_loss_scaling": 12800,
    "max_seq_len": 512,
    "weight_sharing": true,
    "mem_efficient": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": null,
    "topk": 10,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "DialogGeneration",
    "do_generation": false,
    "is_cn": false,
    "nsp_inference_model_path": null,
    "nsp_attention_style": "bidirectional",
    "ranking_score": "decode_score"
  },
  "Reader": {
    "max_src_len": 384,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": true,
    "batch_size": 128,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  }
}
W1023 15:41:37.818339  5732 device_context.cc:252] Please NOTE: device: 0, CUDA Capability: 70, Driver API Version: 11.0, Runtime API Version: 9.0
W1023 15:41:37.823026  5732 device_context.cc:260] device: 0, cuDNN Version: 7.6.
Load pretraining parameters from 12L.
[train][1] progress: 1/1 step: 1, time: 2.493, speed: 0.401 steps/s
	current lr: 0.0000000
	lm_loss: 6.8532, ppl: 946.9486, loss: 6.8532
[train][1] progress: 1/1 step: 2, time: 0.035, speed: 28.324 steps/s
	current lr: 0.0000000
	lm_loss: 6.1066, ppl: 448.8126, loss: 6.1066
[train][1] progress: 1/1 step: 3, time: 0.034, speed: 29.272 steps/s
	current lr: 0.0000000
	lm_loss: 6.1882, ppl: 486.9604, loss: 6.1882
[train][1] progress: 1/1 step: 4, time: 0.033, speed: 30.516 steps/s
	current lr: 0.0000000
	lm_loss: 4.2813, ppl: 72.3361, loss: 4.2813
[train][1] progress: 1/1 step: 5, time: 0.032, speed: 30.946 steps/s
	current lr: 0.0000001
	lm_loss: 5.6648, ppl: 288.5321, loss: 5.6648
[train][1] progress: 1/1 step: 6, time: 0.034, speed: 29.345 steps/s
	current lr: 0.0000001
	lm_loss: 7.4164, ppl: 1663.0278, loss: 7.4164
[train][1] progress: 1/1 step: 7, time: 0.033, speed: 30.515 steps/s
	current lr: 0.0000001
	lm_loss: 5.8611, ppl: 351.1139, loss: 5.8611
[train][1] progress: 1/1 step: 8, time: 0.032, speed: 31.047 steps/s
	current lr: 0.0000001
	lm_loss: 5.6357, ppl: 280.2673, loss: 5.6357
[train][1] progress: 1/1 step: 9, time: 0.033, speed: 30.540 steps/s
	current lr: 0.0000001
	lm_loss: 6.2666, ppl: 526.6982, loss: 6.2666
[train][1] progress: 1/1 step: 10, time: 0.032, speed: 30.977 steps/s
	current lr: 0.0000001
	lm_loss: 5.8591, ppl: 350.4241, loss: 5.8591
[train][1] progress: 1/1 step: 11, time: 0.033, speed: 30.732 steps/s
	current lr: 0.0000001
	lm_loss: 6.0177, ppl: 410.6398, loss: 6.0177
[train][1] progress: 1/1 step: 12, time: 0.032, speed: 31.214 steps/s
	current lr: 0.0000001
	lm_loss: 5.8854, ppl: 359.7531, loss: 5.8854
[train][1] progress: 1/1 step: 13, time: 0.032, speed: 31.415 steps/s
	current lr: 0.0000001
	lm_loss: 6.0027, ppl: 404.5245, loss: 6.0027
[train][1] progress: 1/1 step: 14, time: 0.032, speed: 31.528 steps/s
	current lr: 0.0000001
	lm_loss: 5.2178, ppl: 184.5354, loss: 5.2178
[train][1] progress: 1/1 step: 15, time: 0.033, speed: 30.560 steps/s
	current lr: 0.0000002
	lm_loss: 6.1680, ppl: 477.2485, loss: 6.1680
[train][1] progress: 1/1 step: 16, time: 0.032, speed: 31.090 steps/s
	current lr: 0.0000002
	lm_loss: 5.3775, ppl: 216.4866, loss: 5.3775
[train][1] progress: 1/1 step: 17, time: 0.032, speed: 31.315 steps/s
	current lr: 0.0000002
	lm_loss: 4.7577, ppl: 116.4739, loss: 4.7577
[train][1] progress: 1/1 step: 18, time: 0.032, speed: 31.149 steps/s
	current lr: 0.0000002
	lm_loss: 6.5241, ppl: 681.3645, loss: 6.5241
[train][1] progress: 1/1 step: 19, time: 0.033, speed: 30.691 steps/s
	current lr: 0.0000002
	lm_loss: 4.6266, ppl: 102.1640, loss: 4.6266
[train][1] progress: 1/1 step: 20, time: 0.032, speed: 31.195 steps/s
	current lr: 0.0000002
	lm_loss: 6.5476, ppl: 697.5772, loss: 6.5476
[train][1] progress: 1/1 step: 21, time: 0.032, speed: 31.033 steps/s
	current lr: 0.0000002
	lm_loss: 6.0423, ppl: 420.8602, loss: 6.0423
[train][1] progress: 1/1 step: 22, time: 0.032, speed: 31.253 steps/s
	current lr: 0.0000002
	lm_loss: 6.4606, ppl: 639.4305, loss: 6.4606
[train][1] progress: 1/1 step: 23, time: 0.031, speed: 32.006 steps/s
	current lr: 0.0000002
	lm_loss: 5.6600, ppl: 287.1567, loss: 5.6600
[train][1] progress: 1/1 step: 24, time: 0.033, speed: 30.694 steps/s
	current lr: 0.0000002
	lm_loss: 6.1642, ppl: 475.4321, loss: 6.1642
[train][1] progress: 1/1 step: 25, time: 0.032, speed: 31.697 steps/s
	current lr: 0.0000002
	lm_loss: 5.4179, ppl: 225.3993, loss: 5.4179
[train][1] progress: 1/1 step: 26, time: 0.030, speed: 32.825 steps/s
	current lr: 0.0000003
	lm_loss: 5.7325, ppl: 308.7346, loss: 5.7325
[train][1] progress: 1/1 step: 27, time: 0.030, speed: 32.898 steps/s
	current lr: 0.0000003
	lm_loss: 6.1482, ppl: 467.8542, loss: 6.1482
[train][1] progress: 1/1 step: 28, time: 0.030, speed: 33.072 steps/s
	current lr: 0.0000003
	lm_loss: 6.5652, ppl: 709.9693, loss: 6.5652
[train][1] progress: 1/1 step: 29, time: 0.031, speed: 32.253 steps/s
	current lr: 0.0000003
	lm_loss: 2.9785, ppl: 19.6591, loss: 2.9785
[train][1] progress: 1/1 step: 30, time: 0.031, speed: 32.724 steps/s
	current lr: 0.0000003
	lm_loss: 6.0913, ppl: 441.9958, loss: 6.0913
[train][1] progress: 1/1 step: 31, time: 0.030, speed: 32.864 steps/s
	current lr: 0.0000003
	lm_loss: 5.4070, ppl: 222.9540, loss: 5.4070
[train][1] progress: 1/1 step: 32, time: 0.030, speed: 33.058 steps/s
	current lr: 0.0000003
	lm_loss: 4.5246, ppl: 92.2621, loss: 4.5246
[train][1] progress: 1/1 step: 33, time: 0.030, speed: 33.005 steps/s
	current lr: 0.0000003
	lm_loss: 6.6368, ppl: 762.6301, loss: 6.6368
[train][1] progress: 1/1 step: 34, time: 0.030, speed: 33.008 steps/s
	current lr: 0.0000003
	lm_loss: 5.5891, ppl: 267.5007, loss: 5.5891
[train][1] progress: 1/1 step: 35, time: 0.030, speed: 33.298 steps/s
	current lr: 0.0000004
	lm_loss: 5.1585, ppl: 173.8977, loss: 5.1585
[train][1] progress: 1/1 step: 36, time: 0.030, speed: 33.085 steps/s
	current lr: 0.0000004
	lm_loss: 6.5035, ppl: 667.4661, loss: 6.5035
[train][1] progress: 1/1 step: 37, time: 0.030, speed: 33.091 steps/s
	current lr: 0.0000004
	lm_loss: 4.9224, ppl: 137.3362, loss: 4.9224
[train][1] progress: 1/1 step: 38, time: 0.030, speed: 33.044 steps/s
	current lr: 0.0000004
	lm_loss: 5.0234, ppl: 151.9301, loss: 5.0234
[train][1] progress: 1/1 step: 39, time: 0.031, speed: 32.246 steps/s
	current lr: 0.0000004
	lm_loss: 4.7504, ppl: 115.6327, loss: 4.7504
[train][1] progress: 1/1 step: 40, time: 0.030, speed: 32.809 steps/s
	current lr: 0.0000004
	lm_loss: 5.1892, ppl: 179.3309, loss: 5.1892
[train][1] progress: 1/1 step: 41, time: 0.030, speed: 33.035 steps/s
	current lr: 0.0000004
	lm_loss: 7.7764, ppl: 2383.5635, loss: 7.7764
[train][1] progress: 1/1 step: 42, time: 0.030, speed: 33.333 steps/s
	current lr: 0.0000004
	lm_loss: 4.6981, ppl: 109.7366, loss: 4.6981
[train][1] progress: 1/1 step: 43, time: 0.030, speed: 32.977 steps/s
	current lr: 0.0000004
	lm_loss: 6.0651, ppl: 430.5726, loss: 6.0651
[train][1] progress: 1/1 step: 44, time: 0.030, speed: 33.281 steps/s
	current lr: 0.0000004
	lm_loss: 5.2832, ppl: 196.9941, loss: 5.2832
[train][1] progress: 1/1 step: 45, time: 0.030, speed: 33.116 steps/s
	current lr: 0.0000005
	lm_loss: 5.9127, ppl: 369.7066, loss: 5.9127
[train][1] progress: 1/1 step: 46, time: 0.030, speed: 33.246 steps/s
	current lr: 0.0000005
	lm_loss: 5.2300, ppl: 186.7935, loss: 5.2300
[train][1] progress: 1/1 step: 47, time: 0.030, speed: 33.178 steps/s
	current lr: 0.0000005
	lm_loss: 6.6345, ppl: 760.9359, loss: 6.6345
[train][1] progress: 1/1 step: 48, time: 0.033, speed: 30.305 steps/s
	current lr: 0.0000005
	lm_loss: 5.3246, ppl: 205.3315, loss: 5.3246
[train][1] progress: 1/1 step: 49, time: 0.034, speed: 29.567 steps/s
	current lr: 0.0000005
	lm_loss: 6.0918, ppl: 442.2301, loss: 6.0918
[train][1] progress: 1/1 step: 50, time: 0.032, speed: 31.746 steps/s
	current lr: 0.0000005
	lm_loss: 6.2168, ppl: 501.1190, loss: 6.2168
[train][1] progress: 1/1 step: 51, time: 0.032, speed: 31.518 steps/s
	current lr: 0.0000005
	lm_loss: 6.0323, ppl: 416.6574, loss: 6.0323
[train][1] progress: 1/1 step: 52, time: 0.033, speed: 30.336 steps/s
	current lr: 0.0000005
	lm_loss: 6.2024, ppl: 493.9558, loss: 6.2024
[train][1] progress: 1/1 step: 53, time: 0.032, speed: 31.199 steps/s
	current lr: 0.0000005
	lm_loss: 5.2923, ppl: 198.8036, loss: 5.2923
[train][1] progress: 1/1 step: 54, time: 0.032, speed: 31.611 steps/s
	current lr: 0.0000005
	lm_loss: 5.5289, ppl: 251.8631, loss: 5.5289
[train][1] progress: 1/1 step: 55, time: 0.033, speed: 30.526 steps/s
	current lr: 0.0000006
	lm_loss: 5.0162, ppl: 150.8358, loss: 5.0162
[train][1] progress: 1/1 step: 56, time: 0.032, speed: 31.031 steps/s
	current lr: 0.0000006
	lm_loss: 5.8342, ppl: 341.7791, loss: 5.8342
[train][1] progress: 1/1 step: 57, time: 0.031, speed: 31.758 steps/s
	current lr: 0.0000006
	lm_loss: 5.5237, ppl: 250.5654, loss: 5.5237
[train][1] progress: 1/1 step: 58, time: 0.031, speed: 32.191 steps/s
	current lr: 0.0000006
	lm_loss: 6.1843, ppl: 485.0860, loss: 6.1843
[train][1] progress: 1/1 step: 59, time: 0.031, speed: 32.665 steps/s
	current lr: 0.0000006
	lm_loss: 6.1654, ppl: 476.0115, loss: 6.1654
[train][1] progress: 1/1 step: 60, time: 0.032, speed: 31.154 steps/s
	current lr: 0.0000006
	lm_loss: 4.9099, ppl: 135.6305, loss: 4.9099
[train][1] progress: 1/1 step: 61, time: 0.032, speed: 31.259 steps/s
	current lr: 0.0000006
	lm_loss: 5.4347, ppl: 229.2345, loss: 5.4347
[train][1] progress: 1/1 step: 62, time: 0.033, speed: 30.040 steps/s
	current lr: 0.0000006
	lm_loss: 5.5154, ppl: 248.4874, loss: 5.5154
[train][1] progress: 1/1 step: 63, time: 0.033, speed: 30.486 steps/s
	current lr: 0.0000006
	lm_loss: 6.1719, ppl: 479.0719, loss: 6.1719
[train][1] progress: 1/1 step: 64, time: 0.032, speed: 30.921 steps/s
	current lr: 0.0000006
	lm_loss: 5.1509, ppl: 172.5831, loss: 5.1509
[train][1] progress: 1/1 step: 65, time: 0.032, speed: 31.132 steps/s
	current lr: 0.0000007
	lm_loss: 6.3281, ppl: 560.0798, loss: 6.3281
[train][1] progress: 1/1 step: 66, time: 0.031, speed: 32.672 steps/s
	current lr: 0.0000007
	lm_loss: 4.6536, ppl: 104.9663, loss: 4.6536
[train][1] progress: 1/1 step: 67, time: 0.030, speed: 32.899 steps/s
	current lr: 0.0000007
	lm_loss: 6.1452, ppl: 466.4515, loss: 6.1452
[train][1] progress: 1/1 step: 68, time: 0.031, speed: 32.535 steps/s
	current lr: 0.0000007
	lm_loss: 5.3331, ppl: 207.0736, loss: 5.3331
[train][1] progress: 1/1 step: 69, time: 0.030, speed: 32.859 steps/s
	current lr: 0.0000007
	lm_loss: 4.1185, ppl: 61.4663, loss: 4.1185
[train][1] progress: 1/1 step: 70, time: 0.031, speed: 32.693 steps/s
	current lr: 0.0000007
	lm_loss: 5.6821, ppl: 293.5666, loss: 5.6821
[train][1] progress: 1/1 step: 71, time: 0.030, speed: 32.924 steps/s
	current lr: 0.0000007
	lm_loss: 3.6832, ppl: 39.7744, loss: 3.6832
[train][1] progress: 1/1 step: 72, time: 0.030, speed: 32.990 steps/s
	current lr: 0.0000007
	lm_loss: 4.6772, ppl: 107.4691, loss: 4.6772
[train][1] progress: 1/1 step: 73, time: 0.031, speed: 32.566 steps/s
	current lr: 0.0000007
	lm_loss: 6.0316, ppl: 416.3605, loss: 6.0316
[train][1] progress: 1/1 step: 74, time: 0.031, speed: 32.394 steps/s
	current lr: 0.0000007
	lm_loss: 6.3206, ppl: 555.8945, loss: 6.3206
[train][1] progress: 1/1 step: 75, time: 0.030, speed: 32.914 steps/s
	current lr: 0.0000008
	lm_loss: 5.5264, ppl: 251.2340, loss: 5.5264
[train][1] progress: 1/1 step: 76, time: 0.030, speed: 33.022 steps/s
	current lr: 0.0000008
	lm_loss: 4.9790, ppl: 145.3356, loss: 4.9790
[train][1] progress: 1/1 step: 77, time: 0.030, speed: 33.334 steps/s
	current lr: 0.0000008
	lm_loss: 5.8113, ppl: 334.0540, loss: 5.8113
[train][1] progress: 1/1 step: 78, time: 0.030, speed: 32.919 steps/s
	current lr: 0.0000008
	lm_loss: 6.1506, ppl: 469.0150, loss: 6.1506
[train][1] progress: 1/1 step: 79, time: 0.030, speed: 33.076 steps/s
	current lr: 0.0000008
	lm_loss: 5.8110, ppl: 333.9559, loss: 5.8110
[train][1] progress: 1/1 step: 80, time: 0.031, speed: 32.299 steps/s
	current lr: 0.0000008
	lm_loss: 5.1530, ppl: 172.9559, loss: 5.1530
[train][1] progress: 1/1 step: 81, time: 0.031, speed: 32.649 steps/s
	current lr: 0.0000008
	lm_loss: 6.5167, ppl: 676.3371, loss: 6.5167
[train][1] progress: 1/1 step: 82, time: 0.030, speed: 33.096 steps/s
	current lr: 0.0000008
	lm_loss: 7.1938, ppl: 1331.1761, loss: 7.1938
[train][1] progress: 1/1 step: 83, time: 0.031, speed: 32.064 steps/s
	current lr: 0.0000008
	lm_loss: 4.6877, ppl: 108.6070, loss: 4.6877
[train][1] progress: 1/1 step: 84, time: 0.033, speed: 30.168 steps/s
	current lr: 0.0000008
	lm_loss: 6.0992, ppl: 445.5156, loss: 6.0992
[train][1] progress: 1/1 step: 85, time: 0.032, speed: 31.024 steps/s
	current lr: 0.0000009
	lm_loss: 5.5289, ppl: 251.8790, loss: 5.5289
[train][1] progress: 1/1 step: 86, time: 0.031, speed: 32.580 steps/s
	current lr: 0.0000009
	lm_loss: 6.4561, ppl: 636.5842, loss: 6.4561
[train][1] progress: 1/1 step: 87, time: 0.031, speed: 32.728 steps/s
	current lr: 0.0000009
	lm_loss: 4.2538, ppl: 70.3711, loss: 4.2538
[train][1] progress: 1/1 step: 88, time: 0.030, speed: 33.023 steps/s
	current lr: 0.0000009
	lm_loss: 6.2202, ppl: 502.8072, loss: 6.2202
[train][1] progress: 1/1 step: 89, time: 0.030, speed: 32.938 steps/s
	current lr: 0.0000009
	lm_loss: 4.4614, ppl: 86.6091, loss: 4.4614
[train][1] progress: 1/1 step: 90, time: 0.030, speed: 32.904 steps/s
	current lr: 0.0000009
	lm_loss: 5.9562, ppl: 386.1498, loss: 5.9562
[train][1] progress: 1/1 step: 91, time: 0.031, speed: 32.734 steps/s
	current lr: 0.0000009
	lm_loss: 6.3721, ppl: 585.2911, loss: 6.3721
[train][1] progress: 1/1 step: 92, time: 0.030, speed: 32.958 steps/s
	current lr: 0.0000009
	lm_loss: 5.1944, ppl: 180.2522, loss: 5.1944
[train][1] progress: 1/1 step: 93, time: 0.031, speed: 32.279 steps/s
	current lr: 0.0000009
	lm_loss: 6.8779, ppl: 970.5843, loss: 6.8779
[train][1] progress: 1/1 step: 94, time: 0.031, speed: 32.667 steps/s
	current lr: 0.0000009
	lm_loss: 5.7328, ppl: 308.8212, loss: 5.7328
[train][1] progress: 1/1 step: 95, time: 0.031, speed: 32.655 steps/s
	current lr: 0.0000010
	lm_loss: 5.2071, ppl: 182.5652, loss: 5.2071
[train][1] progress: 1/1 step: 96, time: 0.031, speed: 32.022 steps/s
	current lr: 0.0000010
	lm_loss: 5.3432, ppl: 209.1902, loss: 5.3432
[train][1] progress: 1/1 step: 97, time: 0.031, speed: 32.700 steps/s
	current lr: 0.0000010
	lm_loss: 5.8500, ppl: 347.2213, loss: 5.8500
[train][1] progress: 1/1 step: 98, time: 0.031, speed: 32.148 steps/s
	current lr: 0.0000010
	lm_loss: 6.0247, ppl: 413.4975, loss: 6.0247
[train][1] progress: 1/1 step: 99, time: 0.031, speed: 31.888 steps/s
	current lr: 0.0000010
	lm_loss: 6.3368, ppl: 564.9801, loss: 6.3368
[train][1] progress: 1/1 step: 100, time: 0.032, speed: 31.544 steps/s
	current lr: 0.0000010
	lm_loss: 4.5374, ppl: 93.4431, loss: 4.5374
[train][1] progress: 1/1 step: 101, time: 0.033, speed: 30.219 steps/s
	current lr: 0.0000010
	lm_loss: 5.0685, ppl: 158.9416, loss: 5.0685
[train][1] progress: 1/1 step: 102, time: 0.035, speed: 28.852 steps/s
	current lr: 0.0000010
	lm_loss: 5.9778, ppl: 394.5788, loss: 5.9778
[train][1] progress: 1/1 step: 103, time: 0.034, speed: 29.790 steps/s
	current lr: 0.0000010
	lm_loss: 3.8968, ppl: 49.2456, loss: 3.8968
[train][1] progress: 1/1 step: 104, time: 0.031, speed: 31.987 steps/s
	current lr: 0.0000010
	lm_loss: 6.5273, ppl: 683.5540, loss: 6.5273
[train][1] progress: 1/1 step: 105, time: 0.031, speed: 32.226 steps/s
	current lr: 0.0000011
	lm_loss: 6.7798, ppl: 879.8868, loss: 6.7798
[train][1] progress: 1/1 step: 106, time: 0.031, speed: 31.754 steps/s
	current lr: 0.0000011
	lm_loss: 5.2937, ppl: 199.0766, loss: 5.2937
[train][1] progress: 1/1 step: 107, time: 0.032, speed: 31.587 steps/s
	current lr: 0.0000011
	lm_loss: 6.1927, ppl: 489.1597, loss: 6.1927
[train][1] progress: 1/1 step: 108, time: 0.031, speed: 31.905 steps/s
	current lr: 0.0000011
	lm_loss: 6.3802, ppl: 590.0532, loss: 6.3802
[train][1] progress: 1/1 step: 109, time: 0.032, speed: 31.661 steps/s
	current lr: 0.0000011
	lm_loss: 6.7433, ppl: 848.3140, loss: 6.7433
[train][1] progress: 1/1 step: 110, time: 0.031, speed: 32.570 steps/s
	current lr: 0.0000011
	lm_loss: 6.0683, ppl: 431.9545, loss: 6.0683
[train][1] progress: 1/1 step: 111, time: 0.031, speed: 32.633 steps/s
	current lr: 0.0000011
	lm_loss: 5.3383, ppl: 208.1649, loss: 5.3383
[train][1] progress: 1/1 step: 112, time: 0.031, speed: 32.619 steps/s
	current lr: 0.0000011
	lm_loss: 5.6295, ppl: 278.5244, loss: 5.6295
[train][1] progress: 1/1 step: 113, time: 0.030, speed: 32.948 steps/s
	current lr: 0.0000011
	lm_loss: 6.2859, ppl: 536.9259, loss: 6.2859
[train][1] progress: 1/1 step: 114, time: 0.031, speed: 32.749 steps/s
	current lr: 0.0000011
	lm_loss: 5.3895, ppl: 219.0846, loss: 5.3895
[train][1] progress: 1/1 step: 115, time: 0.031, speed: 32.545 steps/s
	current lr: 0.0000012
	lm_loss: 4.9000, ppl: 134.2955, loss: 4.9000
[train][1] progress: 1/1 step: 116, time: 0.032, speed: 31.434 steps/s
	current lr: 0.0000012
	lm_loss: 6.4055, ppl: 605.1611, loss: 6.4055
[train][1] progress: 1/1 step: 117, time: 0.032, speed: 31.223 steps/s
	current lr: 0.0000012
	lm_loss: 5.2923, ppl: 198.8100, loss: 5.2923
[train][1] progress: 1/1 step: 118, time: 0.031, speed: 31.821 steps/s
	current lr: 0.0000012
	lm_loss: 3.6361, ppl: 37.9428, loss: 3.6361
[train][1] progress: 1/1 step: 119, time: 0.031, speed: 32.080 steps/s
	current lr: 0.0000012
	lm_loss: 5.9890, ppl: 399.0323, loss: 5.9890
[train][1] progress: 1/1 step: 120, time: 0.031, speed: 32.531 steps/s
	current lr: 0.0000012
	lm_loss: 5.7324, ppl: 308.7078, loss: 5.7324
[train][1] progress: 1/1 step: 121, time: 0.031, speed: 32.206 steps/s
	current lr: 0.0000012
	lm_loss: 4.0097, ppl: 55.1324, loss: 4.0097
[train][1] progress: 1/1 step: 122, time: 0.031, speed: 31.866 steps/s
	current lr: 0.0000012
	lm_loss: 5.1273, ppl: 168.5649, loss: 5.1273
[train][1] progress: 1/1 step: 123, time: 0.032, speed: 30.775 steps/s
	current lr: 0.0000012
	lm_loss: 4.2313, ppl: 68.8064, loss: 4.2313
[train][1] progress: 1/1 step: 124, time: 0.032, speed: 30.807 steps/s
	current lr: 0.0000012
	lm_loss: 5.9381, ppl: 379.2018, loss: 5.9381
[train][1] progress: 1/1 step: 125, time: 0.033, speed: 30.711 steps/s
	current lr: 0.0000013
	lm_loss: 4.9239, ppl: 137.5421, loss: 4.9239
[train][1] progress: 1/1 step: 126, time: 0.031, speed: 32.131 steps/s
	current lr: 0.0000013
	lm_loss: 4.8374, ppl: 126.1417, loss: 4.8374
[train][1] progress: 1/1 step: 127, time: 0.031, speed: 32.102 steps/s
	current lr: 0.0000013
	lm_loss: 3.4822, ppl: 32.5309, loss: 3.4822
[train][1] progress: 1/1 step: 128, time: 0.031, speed: 32.772 steps/s
	current lr: 0.0000013
	lm_loss: 5.4613, ppl: 235.4127, loss: 5.4613
[train][1] progress: 1/1 step: 129, time: 0.031, speed: 32.339 steps/s
	current lr: 0.0000013
	lm_loss: 6.1320, ppl: 460.3412, loss: 6.1320
[train][1] progress: 1/1 step: 130, time: 0.030, speed: 33.181 steps/s
	current lr: 0.0000013
	lm_loss: 6.3240, ppl: 557.7768, loss: 6.3240
[train][1] progress: 1/1 step: 131, time: 0.031, speed: 32.692 steps/s
	current lr: 0.0000013
	lm_loss: 6.1149, ppl: 452.5480, loss: 6.1149
[train][1] progress: 1/1 step: 132, time: 0.031, speed: 32.128 steps/s
	current lr: 0.0000013
	lm_loss: 3.7516, ppl: 42.5871, loss: 3.7516
[train][1] progress: 1/1 step: 133, time: 0.032, speed: 31.642 steps/s
	current lr: 0.0000013
	lm_loss: 3.7418, ppl: 42.1730, loss: 3.7418
[train][1] progress: 1/1 step: 134, time: 0.031, speed: 32.251 steps/s
	current lr: 0.0000013
	lm_loss: 5.1313, ppl: 169.2431, loss: 5.1313
[train][1] progress: 1/1 step: 135, time: 0.031, speed: 31.907 steps/s
	current lr: 0.0000014
	lm_loss: 4.4597, ppl: 86.4647, loss: 4.4597
[train][1] progress: 1/1 step: 136, time: 0.031, speed: 32.255 steps/s
	current lr: 0.0000014
	lm_loss: 5.5806, ppl: 265.2371, loss: 5.5806
[train][1] progress: 1/1 step: 137, time: 0.030, speed: 32.902 steps/s
	current lr: 0.0000014
	lm_loss: 7.4213, ppl: 1671.1854, loss: 7.4213
[train][1] progress: 1/1 step: 138, time: 0.031, speed: 32.189 steps/s
	current lr: 0.0000014
	lm_loss: 4.9719, ppl: 144.3019, loss: 4.9719
[train][1] progress: 1/1 step: 139, time: 0.031, speed: 32.565 steps/s
	current lr: 0.0000014
	lm_loss: 5.6117, ppl: 273.6220, loss: 5.6117
[train][1] progress: 1/1 step: 140, time: 0.030, speed: 32.862 steps/s
	current lr: 0.0000014
	lm_loss: 4.8242, ppl: 124.4920, loss: 4.8242
[train][1] progress: 1/1 step: 141, time: 0.031, speed: 32.213 steps/s
	current lr: 0.0000014
	lm_loss: 5.6153, ppl: 274.5933, loss: 5.6153
[train][1] progress: 1/1 step: 142, time: 0.034, speed: 29.293 steps/s
	current lr: 0.0000014
	lm_loss: 7.2149, ppl: 1359.4774, loss: 7.2149
[train][1] progress: 1/1 step: 143, time: 0.035, speed: 28.955 steps/s
	current lr: 0.0000014
	lm_loss: 4.4791, ppl: 88.1540, loss: 4.4791
[train][1] progress: 1/1 step: 144, time: 0.035, speed: 28.717 steps/s
	current lr: 0.0000014
	lm_loss: 6.5050, ppl: 668.4799, loss: 6.5050
[train][1] progress: 1/1 step: 145, time: 0.035, speed: 28.766 steps/s
	current lr: 0.0000015
	lm_loss: 4.8759, ppl: 131.0939, loss: 4.8759
[train][1] progress: 1/1 step: 146, time: 0.033, speed: 30.218 steps/s
	current lr: 0.0000015
	lm_loss: 5.4653, ppl: 236.3468, loss: 5.4653
[train][1] progress: 1/1 step: 147, time: 0.032, speed: 31.500 steps/s
	current lr: 0.0000015
	lm_loss: 4.7281, ppl: 113.0764, loss: 4.7281
[train][1] progress: 1/1 step: 148, time: 0.031, speed: 31.878 steps/s
	current lr: 0.0000015
	lm_loss: 6.4234, ppl: 616.1171, loss: 6.4234
[train][1] progress: 1/1 step: 149, time: 0.031, speed: 32.295 steps/s
	current lr: 0.0000015
	lm_loss: 6.3525, ppl: 573.9038, loss: 6.3525
[train][1] progress: 1/1 step: 150, time: 0.031, speed: 32.175 steps/s
	current lr: 0.0000015
	lm_loss: 7.6606, ppl: 2122.9817, loss: 7.6606
[train][1] progress: 1/1 step: 151, time: 0.032, speed: 31.429 steps/s
	current lr: 0.0000015
	lm_loss: 6.2574, ppl: 521.8830, loss: 6.2574
[train][1] progress: 1/1 step: 152, time: 0.031, speed: 32.013 steps/s
	current lr: 0.0000015
	lm_loss: 6.5284, ppl: 684.2721, loss: 6.5284
[train][1] progress: 1/1 step: 153, time: 0.031, speed: 32.262 steps/s
	current lr: 0.0000015
	lm_loss: 4.3648, ppl: 78.6316, loss: 4.3648
[train][1] progress: 1/1 step: 154, time: 0.031, speed: 32.435 steps/s
	current lr: 0.0000015
	lm_loss: 5.2071, ppl: 182.5729, loss: 5.2071
[train][1] progress: 1/1 step: 155, time: 0.030, speed: 32.843 steps/s
	current lr: 0.0000016
	lm_loss: 5.5447, ppl: 255.8870, loss: 5.5447
[train][1] progress: 1/1 step: 156, time: 0.031, speed: 32.582 steps/s
	current lr: 0.0000016
	lm_loss: 5.0434, ppl: 155.0002, loss: 5.0434
[train][1] progress: 1/1 step: 157, time: 0.037, speed: 27.334 steps/s
	current lr: 0.0000016
	lm_loss: 6.9752, ppl: 1069.7708, loss: 6.9752
[train][1] progress: 1/1 step: 158, time: 0.037, speed: 27.026 steps/s
	current lr: 0.0000016
	lm_loss: 6.6544, ppl: 776.2234, loss: 6.6544
[train][1] progress: 1/1 step: 159, time: 0.037, speed: 26.675 steps/s
	current lr: 0.0000016
	lm_loss: 5.7036, ppl: 299.9598, loss: 5.7036
[train][1] progress: 1/1 step: 160, time: 0.037, speed: 27.313 steps/s
	current lr: 0.0000016
	lm_loss: 5.5641, ppl: 260.8948, loss: 5.5641
[train][1] progress: 1/1 step: 161, time: 0.035, speed: 28.642 steps/s
	current lr: 0.0000016
	lm_loss: 5.8324, ppl: 341.1815, loss: 5.8324
[train][1] progress: 1/1 step: 162, time: 0.033, speed: 30.033 steps/s
	current lr: 0.0000016
	lm_loss: 7.0971, ppl: 1208.4359, loss: 7.0971
[train][1] progress: 1/1 step: 163, time: 0.032, speed: 30.791 steps/s
	current lr: 0.0000016
	lm_loss: 4.3066, ppl: 74.1847, loss: 4.3066
[train][1] progress: 1/1 step: 164, time: 0.033, speed: 30.754 steps/s
	current lr: 0.0000016
	lm_loss: 4.2608, ppl: 70.8661, loss: 4.2608
[train][1] progress: 1/1 step: 165, time: 0.032, speed: 31.231 steps/s
	current lr: 0.0000017
	lm_loss: 5.5901, ppl: 267.7625, loss: 5.5901
[train][1] progress: 1/1 step: 166, time: 0.032, speed: 31.287 steps/s
	current lr: 0.0000017
	lm_loss: 4.9018, ppl: 134.5285, loss: 4.9018
[train][1] progress: 1/1 step: 167, time: 0.031, speed: 32.048 steps/s
	current lr: 0.0000017
	lm_loss: 6.3925, ppl: 597.3338, loss: 6.3925
[train][1] progress: 1/1 step: 168, time: 0.031, speed: 32.097 steps/s
	current lr: 0.0000017
	lm_loss: 5.6119, ppl: 273.6540, loss: 5.6119
[train][1] progress: 1/1 step: 169, time: 0.032, speed: 31.547 steps/s
	current lr: 0.0000017
	lm_loss: 6.1846, ppl: 485.2035, loss: 6.1846
[train][1] progress: 1/1 step: 170, time: 0.031, speed: 32.154 steps/s
	current lr: 0.0000017
	lm_loss: 4.6403, ppl: 103.5794, loss: 4.6403
[train][1] progress: 1/1 step: 171, time: 0.032, speed: 31.318 steps/s
	current lr: 0.0000017
	lm_loss: 5.7598, ppl: 317.2791, loss: 5.7598
[train][1] progress: 1/1 step: 172, time: 0.031, speed: 31.780 steps/s
	current lr: 0.0000017
	lm_loss: 4.8211, ppl: 124.1022, loss: 4.8211
[train][1] progress: 1/1 step: 173, time: 0.031, speed: 31.761 steps/s
	current lr: 0.0000017
	lm_loss: 5.5853, ppl: 266.4874, loss: 5.5853
[train][1] progress: 1/1 step: 174, time: 0.031, speed: 32.778 steps/s
	current lr: 0.0000017
	lm_loss: 5.3796, ppl: 216.9450, loss: 5.3796
[train][1] progress: 1/1 step: 175, time: 0.031, speed: 32.509 steps/s
	current lr: 0.0000018
	lm_loss: 4.6642, ppl: 106.0833, loss: 4.6642
[train][1] progress: 1/1 step: 176, time: 0.031, speed: 32.010 steps/s
	current lr: 0.0000018
	lm_loss: 6.5411, ppl: 693.0518, loss: 6.5411
[train][1] progress: 1/1 step: 177, time: 0.031, speed: 32.599 steps/s
	current lr: 0.0000018
	lm_loss: 5.7373, ppl: 310.2261, loss: 5.7373
[train][1] progress: 1/1 step: 178, time: 0.031, speed: 32.644 steps/s
	current lr: 0.0000018
	lm_loss: 5.2127, ppl: 183.5936, loss: 5.2127
[train][1] progress: 1/1 step: 179, time: 0.031, speed: 32.583 steps/s
	current lr: 0.0000018
	lm_loss: 5.8038, ppl: 331.5440, loss: 5.8038
[train][1] progress: 1/1 step: 180, time: 0.030, speed: 32.792 steps/s
	current lr: 0.0000018
	lm_loss: 6.9416, ppl: 1034.4722, loss: 6.9416
[train][1] progress: 1/1 step: 181, time: 0.031, speed: 32.582 steps/s
	current lr: 0.0000018
	lm_loss: 4.9323, ppl: 138.6944, loss: 4.9323
[train][1] progress: 1/1 step: 182, time: 0.032, speed: 31.581 steps/s
	current lr: 0.0000018
	lm_loss: 4.5995, ppl: 99.4357, loss: 4.5995
[train][1] progress: 1/1 step: 183, time: 0.031, speed: 31.773 steps/s
	current lr: 0.0000018
	lm_loss: 4.4255, ppl: 83.5564, loss: 4.4255
[train][1] progress: 1/1 step: 184, time: 0.031, speed: 32.152 steps/s
	current lr: 0.0000018
	lm_loss: 5.9069, ppl: 367.5791, loss: 5.9069
[train][1] progress: 1/1 step: 185, time: 0.034, speed: 29.626 steps/s
	current lr: 0.0000019
	lm_loss: 3.8247, ppl: 45.8187, loss: 3.8247
[train][1] progress: 1/1 step: 186, time: 0.033, speed: 30.066 steps/s
	current lr: 0.0000019
	lm_loss: 5.3902, ppl: 219.2406, loss: 5.3902
[train][1] progress: 1/1 step: 187, time: 0.033, speed: 30.028 steps/s
	current lr: 0.0000019
	lm_loss: 4.7373, ppl: 114.1212, loss: 4.7373
[train][1] progress: 1/1 step: 188, time: 0.033, speed: 30.093 steps/s
	current lr: 0.0000019
	lm_loss: 4.7732, ppl: 118.2918, loss: 4.7732
[train][1] progress: 1/1 step: 189, time: 0.032, speed: 31.100 steps/s
	current lr: 0.0000019
	lm_loss: 5.6155, ppl: 274.6537, loss: 5.6155
[train][1] progress: 1/1 step: 190, time: 0.031, speed: 32.006 steps/s
	current lr: 0.0000019
	lm_loss: 6.0291, ppl: 415.3454, loss: 6.0291
[train][1] progress: 1/1 step: 191, time: 0.030, speed: 32.931 steps/s
	current lr: 0.0000019
	lm_loss: 5.9824, ppl: 396.3968, loss: 5.9824
[train][1] progress: 1/1 step: 192, time: 0.030, speed: 33.031 steps/s
	current lr: 0.0000019
	lm_loss: 6.0396, ppl: 419.7297, loss: 6.0396
[train][1] progress: 1/1 step: 193, time: 0.031, speed: 32.640 steps/s
	current lr: 0.0000019
	lm_loss: 4.5948, ppl: 98.9665, loss: 4.5948
[train][1] progress: 1/1 step: 194, time: 0.030, speed: 32.949 steps/s
	current lr: 0.0000019
	lm_loss: 5.5776, ppl: 264.4342, loss: 5.5776
[train][1] progress: 1/1 step: 195, time: 0.031, speed: 32.665 steps/s
	current lr: 0.0000020
	lm_loss: 5.5501, ppl: 257.2547, loss: 5.5501
[train][1] progress: 1/1 step: 196, time: 0.031, speed: 32.270 steps/s
	current lr: 0.0000020
	lm_loss: 5.3685, ppl: 214.5426, loss: 5.3685
[train][1] progress: 1/1 step: 197, time: 0.031, speed: 32.094 steps/s
	current lr: 0.0000020
	lm_loss: 6.2945, ppl: 541.5637, loss: 6.2945
[train][1] progress: 1/1 step: 198, time: 0.032, speed: 31.269 steps/s
	current lr: 0.0000020
	lm_loss: 5.5169, ppl: 248.8553, loss: 5.5169
[train][1] progress: 1/1 step: 199, time: 0.031, speed: 31.935 steps/s
	current lr: 0.0000020
	lm_loss: 5.8303, ppl: 340.4771, loss: 5.8303
[train][1] progress: 1/1 step: 200, time: 0.031, speed: 32.487 steps/s
	current lr: 0.0000020
	lm_loss: 7.6187, ppl: 2035.9885, loss: 7.6187
[train][1] progress: 1/1 step: 201, time: 0.031, speed: 32.251 steps/s
	current lr: 0.0000020
	lm_loss: 6.0673, ppl: 431.5144, loss: 6.0673
[train][1] progress: 1/1 step: 202, time: 0.031, speed: 32.587 steps/s
	current lr: 0.0000020
	lm_loss: 6.2569, ppl: 521.6068, loss: 6.2569
[train][1] progress: 1/1 step: 203, time: 0.031, speed: 32.667 steps/s
	current lr: 0.0000020
	lm_loss: 5.1669, ppl: 175.3734, loss: 5.1669
[train][1] progress: 1/1 step: 204, time: 0.031, speed: 32.770 steps/s
	current lr: 0.0000020
	lm_loss: 6.1816, ppl: 483.7559, loss: 6.1816
[train][1] progress: 1/1 step: 205, time: 0.030, speed: 33.029 steps/s
	current lr: 0.0000021
	lm_loss: 5.0986, ppl: 163.7977, loss: 5.0986
[train][1] progress: 1/1 step: 206, time: 0.031, speed: 32.337 steps/s
	current lr: 0.0000021
	lm_loss: 4.4476, ppl: 85.4201, loss: 4.4476
[train][1] progress: 1/1 step: 207, time: 0.030, speed: 32.945 steps/s
	current lr: 0.0000021
	lm_loss: 5.4567, ppl: 234.3281, loss: 5.4567
[train][1] progress: 1/1 step: 208, time: 0.031, speed: 32.059 steps/s
	current lr: 0.0000021
	lm_loss: 5.4632, ppl: 235.8535, loss: 5.4632
[train][1] progress: 1/1 step: 209, time: 0.031, speed: 32.196 steps/s
	current lr: 0.0000021
	lm_loss: 4.7195, ppl: 112.1134, loss: 4.7195
[train][1] progress: 1/1 step: 210, time: 0.031, speed: 32.653 steps/s
	current lr: 0.0000021
	lm_loss: 5.4770, ppl: 239.1226, loss: 5.4770
[train][1] progress: 1/1 step: 211, time: 0.031, speed: 32.770 steps/s
	current lr: 0.0000021
	lm_loss: 4.8520, ppl: 127.9995, loss: 4.8520
[train][1] progress: 1/1 step: 212, time: 0.030, speed: 33.081 steps/s
	current lr: 0.0000021
	lm_loss: 4.8211, ppl: 124.1031, loss: 4.8211
[train][1] progress: 1/1 step: 213, time: 0.030, speed: 33.210 steps/s
	current lr: 0.0000021
	lm_loss: 5.4375, ppl: 229.8607, loss: 5.4375
[train][1] progress: 1/1 step: 214, time: 0.031, speed: 32.703 steps/s
	current lr: 0.0000021
	lm_loss: 6.1707, ppl: 478.5194, loss: 6.1707
[train][1] progress: 1/1 step: 215, time: 0.033, speed: 29.937 steps/s
	current lr: 0.0000022
	lm_loss: 5.6098, ppl: 273.0973, loss: 5.6098
[train][1] progress: 1/1 step: 216, time: 0.035, speed: 28.530 steps/s
	current lr: 0.0000022
	lm_loss: 5.3390, ppl: 208.3142, loss: 5.3390
[train][1] progress: 1/1 step: 217, time: 0.035, speed: 28.818 steps/s
	current lr: 0.0000022
	lm_loss: 5.3680, ppl: 214.4335, loss: 5.3680
[train][1] progress: 1/1 step: 218, time: 0.035, speed: 28.967 steps/s
	current lr: 0.0000022
	lm_loss: 5.8161, ppl: 335.6768, loss: 5.8161
[train][1] progress: 1/1 step: 219, time: 0.034, speed: 29.464 steps/s
	current lr: 0.0000022
	lm_loss: 4.9066, ppl: 135.1746, loss: 4.9066
[train][1] progress: 1/1 step: 220, time: 0.034, speed: 29.072 steps/s
	current lr: 0.0000022
	lm_loss: 5.3327, ppl: 207.0012, loss: 5.3327
[train][1] progress: 1/1 step: 221, time: 0.035, speed: 28.936 steps/s
	current lr: 0.0000022
	lm_loss: 4.8915, ppl: 133.1503, loss: 4.8915
[train][1] progress: 1/1 step: 222, time: 0.034, speed: 29.263 steps/s
	current lr: 0.0000022
	lm_loss: 5.6123, ppl: 273.7640, loss: 5.6123
[train][1] progress: 1/1 step: 223, time: 0.035, speed: 28.356 steps/s
	current lr: 0.0000022
	lm_loss: 4.5854, ppl: 98.0398, loss: 4.5854
[train][1] progress: 1/1 step: 224, time: 0.037, speed: 27.297 steps/s
	current lr: 0.0000022
	lm_loss: 5.7641, ppl: 318.6591, loss: 5.7641
[train][1] progress: 1/1 step: 225, time: 0.036, speed: 28.137 steps/s
	current lr: 0.0000023
	lm_loss: 6.8608, ppl: 954.0926, loss: 6.8608
[train][1] progress: 1/1 step: 226, time: 0.035, speed: 28.917 steps/s
	current lr: 0.0000023
	lm_loss: 5.6588, ppl: 286.8133, loss: 5.6588
[train][1] progress: 1/1 step: 227, time: 0.035, speed: 28.566 steps/s
	current lr: 0.0000023
	lm_loss: 4.2140, ppl: 67.6296, loss: 4.2140
[train][1] progress: 1/1 step: 228, time: 0.034, speed: 29.433 steps/s
	current lr: 0.0000023
	lm_loss: 4.5894, ppl: 98.4358, loss: 4.5894
[train][1] progress: 1/1 step: 229, time: 0.034, speed: 29.014 steps/s
	current lr: 0.0000023
	lm_loss: 6.5116, ppl: 672.9224, loss: 6.5116
[train][1] progress: 1/1 step: 230, time: 0.035, speed: 28.725 steps/s
	current lr: 0.0000023
	lm_loss: 5.7264, ppl: 306.8507, loss: 5.7264
[train][1] progress: 1/1 step: 231, time: 0.035, speed: 28.824 steps/s
	current lr: 0.0000023
	lm_loss: 6.0725, ppl: 433.7829, loss: 6.0725
[train][1] progress: 1/1 step: 232, time: 0.035, speed: 28.885 steps/s
	current lr: 0.0000023
	lm_loss: 5.5699, ppl: 262.4111, loss: 5.5699
[train][1] progress: 1/1 step: 233, time: 0.036, speed: 27.873 steps/s
	current lr: 0.0000023
	lm_loss: 5.1595, ppl: 174.0822, loss: 5.1595
[train][1] progress: 1/1 step: 234, time: 0.036, speed: 28.035 steps/s
	current lr: 0.0000023
	lm_loss: 5.6811, ppl: 293.2679, loss: 5.6811
[train][1] progress: 1/1 step: 235, time: 0.035, speed: 28.619 steps/s
	current lr: 0.0000024
	lm_loss: 5.6735, ppl: 291.0513, loss: 5.6735
[train][1] progress: 1/1 step: 236, time: 0.035, speed: 28.903 steps/s
	current lr: 0.0000024
	lm_loss: 6.0914, ppl: 442.0454, loss: 6.0914
[train][1] progress: 1/1 step: 237, time: 0.034, speed: 29.531 steps/s
	current lr: 0.0000024
	lm_loss: 6.1090, ppl: 449.8715, loss: 6.1090
[train][1] progress: 1/1 step: 238, time: 0.033, speed: 29.905 steps/s
	current lr: 0.0000024
	lm_loss: 6.3725, ppl: 585.4972, loss: 6.3725
[train][1] progress: 1/1 step: 239, time: 0.034, speed: 29.475 steps/s
	current lr: 0.0000024
	lm_loss: 5.0054, ppl: 149.2154, loss: 5.0054
[train][1] progress: 1/1 step: 240, time: 0.034, speed: 29.814 steps/s
	current lr: 0.0000024
	lm_loss: 5.4160, ppl: 224.9882, loss: 5.4160
[train][1] progress: 1/1 step: 241, time: 0.034, speed: 29.705 steps/s
	current lr: 0.0000024
	lm_loss: 4.7650, ppl: 117.3286, loss: 4.7650
[train][1] progress: 1/1 step: 242, time: 0.034, speed: 28.988 steps/s
	current lr: 0.0000024
	lm_loss: 5.7659, ppl: 319.2147, loss: 5.7659
[train][1] progress: 1/1 step: 243, time: 0.034, speed: 29.017 steps/s
	current lr: 0.0000024
	lm_loss: 4.8422, ppl: 126.7448, loss: 4.8422
[train][1] progress: 1/1 step: 244, time: 0.034, speed: 29.317 steps/s
	current lr: 0.0000024
	lm_loss: 5.6010, ppl: 270.6856, loss: 5.6010
[train][1] progress: 1/1 step: 245, time: 0.033, speed: 29.890 steps/s
	current lr: 0.0000025
	lm_loss: 5.0186, ppl: 151.2010, loss: 5.0186
[train][1] progress: 1/1 step: 246, time: 0.033, speed: 30.119 steps/s
	current lr: 0.0000025
	lm_loss: 5.5681, ppl: 261.9472, loss: 5.5681
[train][1] progress: 1/1 step: 247, time: 0.032, speed: 30.772 steps/s
	current lr: 0.0000025
	lm_loss: 5.8976, ppl: 364.1670, loss: 5.8976
[train][1] progress: 1/1 step: 248, time: 0.032, speed: 30.856 steps/s
	current lr: 0.0000025
	lm_loss: 4.3410, ppl: 76.7828, loss: 4.3410
[train][1] progress: 1/1 step: 249, time: 0.033, speed: 30.410 steps/s
	current lr: 0.0000025
	lm_loss: 5.4562, ppl: 234.2113, loss: 5.4562
[train][1] progress: 1/1 step: 250, time: 0.033, speed: 30.757 steps/s
	current lr: 0.0000025
	lm_loss: 4.8290, ppl: 125.0856, loss: 4.8290
[train][1] progress: 1/1 step: 251, time: 0.033, speed: 30.580 steps/s
	current lr: 0.0000025
	lm_loss: 4.9564, ppl: 142.0850, loss: 4.9564
[train][1] progress: 1/1 step: 252, time: 0.033, speed: 30.597 steps/s
	current lr: 0.0000025
	lm_loss: 4.0710, ppl: 58.6148, loss: 4.0710
[train][1] progress: 1/1 step: 253, time: 0.032, speed: 30.863 steps/s
	current lr: 0.0000025
	lm_loss: 6.6658, ppl: 785.0719, loss: 6.6658
[train][1] progress: 1/1 step: 254, time: 0.033, speed: 30.740 steps/s
	current lr: 0.0000025
	lm_loss: 5.3495, ppl: 210.5024, loss: 5.3495
[train][1] progress: 1/1 step: 255, time: 0.032, speed: 30.931 steps/s
	current lr: 0.0000025
	lm_loss: 5.6621, ppl: 287.7426, loss: 5.6621
[train][1] progress: 1/1 step: 256, time: 0.032, speed: 31.000 steps/s
	current lr: 0.0000026
	lm_loss: 5.8688, ppl: 353.8336, loss: 5.8688
[train][1] progress: 1/1 step: 257, time: 0.033, speed: 30.191 steps/s
	current lr: 0.0000026
	lm_loss: 6.2093, ppl: 497.3739, loss: 6.2093
[train][1] progress: 1/1 step: 258, time: 0.033, speed: 30.598 steps/s
	current lr: 0.0000026
	lm_loss: 5.1426, ppl: 171.1558, loss: 5.1426
[train][1] progress: 1/1 step: 259, time: 0.033, speed: 30.505 steps/s
	current lr: 0.0000026
	lm_loss: 3.6732, ppl: 39.3788, loss: 3.6732
[train][1] progress: 1/1 step: 260, time: 0.033, speed: 30.588 steps/s
	current lr: 0.0000026
	lm_loss: 5.1033, ppl: 164.5706, loss: 5.1033
[train][1] progress: 1/1 step: 261, time: 0.032, speed: 30.799 steps/s
	current lr: 0.0000026
	lm_loss: 5.6832, ppl: 293.8904, loss: 5.6832
[train][1] progress: 1/1 step: 262, time: 0.034, speed: 29.524 steps/s
	current lr: 0.0000026
	lm_loss: 5.6825, ppl: 293.6943, loss: 5.6825
[train][1] progress: 1/1 step: 263, time: 0.034, speed: 29.671 steps/s
	current lr: 0.0000026
	lm_loss: 5.8242, ppl: 338.3789, loss: 5.8242
[train][1] progress: 1/1 step: 264, time: 0.033, speed: 30.077 steps/s
	current lr: 0.0000026
	lm_loss: 5.9057, ppl: 367.1090, loss: 5.9057
[train][1] progress: 1/1 step: 265, time: 0.033, speed: 30.721 steps/s
	current lr: 0.0000026
	lm_loss: 5.0129, ppl: 150.3447, loss: 5.0129
[train][1] progress: 1/1 step: 266, time: 0.033, speed: 30.267 steps/s
	current lr: 0.0000027
	lm_loss: 4.9065, ppl: 135.1674, loss: 4.9065
[train][1] progress: 1/1 step: 267, time: 0.032, speed: 31.076 steps/s
	current lr: 0.0000027
	lm_loss: 6.1013, ppl: 446.4511, loss: 6.1013
[train][1] progress: 1/1 step: 268, time: 0.033, speed: 30.713 steps/s
	current lr: 0.0000027
	lm_loss: 2.8556, ppl: 17.3856, loss: 2.8556
[train][1] progress: 1/1 step: 269, time: 0.033, speed: 30.485 steps/s
	current lr: 0.0000027
	lm_loss: 4.7394, ppl: 114.3669, loss: 4.7394
[train][1] progress: 1/1 step: 270, time: 0.032, speed: 30.978 steps/s
	current lr: 0.0000027
	lm_loss: 6.2312, ppl: 508.3628, loss: 6.2312
[train][1] progress: 1/1 step: 271, time: 0.032, speed: 31.182 steps/s
	current lr: 0.0000027
	lm_loss: 4.9469, ppl: 140.7366, loss: 4.9469
[train][1] progress: 1/1 step: 272, time: 0.032, speed: 31.387 steps/s
	current lr: 0.0000027
	lm_loss: 5.7126, ppl: 302.6458, loss: 5.7126
[train][1] progress: 1/1 step: 273, time: 0.032, speed: 31.262 steps/s
	current lr: 0.0000027
	lm_loss: 6.1676, ppl: 477.0501, loss: 6.1676
[train][1] progress: 1/1 step: 274, time: 0.032, speed: 30.829 steps/s
	current lr: 0.0000027
	lm_loss: 6.2507, ppl: 518.3958, loss: 6.2507
[train][1] progress: 1/1 step: 275, time: 0.033, speed: 30.549 steps/s
	current lr: 0.0000028
	lm_loss: 5.9258, ppl: 374.5958, loss: 5.9258
[train][1] progress: 1/1 step: 276, time: 0.032, speed: 31.125 steps/s
	current lr: 0.0000028
	lm_loss: 5.3348, ppl: 207.4326, loss: 5.3348
[train][1] progress: 1/1 step: 277, time: 0.033, speed: 30.665 steps/s
	current lr: 0.0000028
	lm_loss: 4.9311, ppl: 138.5375, loss: 4.9311
[train][1] progress: 1/1 step: 278, time: 0.032, speed: 30.981 steps/s
	current lr: 0.0000028
	lm_loss: 5.3919, ppl: 219.6176, loss: 5.3919
[train][1] progress: 1/1 step: 279, time: 0.033, speed: 30.388 steps/s
	current lr: 0.0000028
	lm_loss: 2.8564, ppl: 17.3982, loss: 2.8564
[train][1] progress: 1/1 step: 280, time: 0.034, speed: 29.662 steps/s
	current lr: 0.0000028
	lm_loss: 4.5927, ppl: 98.7568, loss: 4.5927
[train][1] progress: 1/1 step: 281, time: 0.033, speed: 30.265 steps/s
	current lr: 0.0000028
	lm_loss: 2.7742, ppl: 16.0261, loss: 2.7742
[train][1] progress: 1/1 step: 282, time: 0.032, speed: 30.987 steps/s
	current lr: 0.0000028
	lm_loss: 4.6776, ppl: 107.5151, loss: 4.6776
[train][1] progress: 1/1 step: 283, time: 0.032, speed: 30.965 steps/s
	current lr: 0.0000028
	lm_loss: 5.7230, ppl: 305.8282, loss: 5.7230
[train][1] progress: 1/1 step: 284, time: 0.032, speed: 30.908 steps/s
	current lr: 0.0000028
	lm_loss: 5.0460, ppl: 155.4027, loss: 5.0460
[train][1] progress: 1/1 step: 285, time: 0.033, speed: 30.586 steps/s
	current lr: 0.0000028
	lm_loss: 5.4815, ppl: 240.2166, loss: 5.4815
[train][1] progress: 1/1 step: 286, time: 0.033, speed: 30.576 steps/s
	current lr: 0.0000029
	lm_loss: 6.1896, ppl: 487.6652, loss: 6.1896
[train][1] progress: 1/1 step: 287, time: 0.033, speed: 30.531 steps/s
	current lr: 0.0000029
	lm_loss: 5.6900, ppl: 295.9020, loss: 5.6900
[train][1] progress: 1/1 step: 288, time: 0.032, speed: 31.494 steps/s
	current lr: 0.0000029
	lm_loss: 3.2425, ppl: 25.5975, loss: 3.2425
[train][1] progress: 1/1 step: 289, time: 0.033, speed: 30.761 steps/s
	current lr: 0.0000029
	lm_loss: 6.5336, ppl: 687.8438, loss: 6.5336
[train][1] progress: 1/1 step: 290, time: 0.033, speed: 30.661 steps/s
	current lr: 0.0000029
	lm_loss: 5.3308, ppl: 206.6118, loss: 5.3308
[train][1] progress: 1/1 step: 291, time: 0.034, speed: 29.233 steps/s
	current lr: 0.0000029
	lm_loss: 5.7264, ppl: 306.8650, loss: 5.7264
[train][1] progress: 1/1 step: 292, time: 0.035, speed: 28.982 steps/s
	current lr: 0.0000029
	lm_loss: 6.6720, ppl: 789.9671, loss: 6.6720
[train][1] progress: 1/1 step: 293, time: 0.035, speed: 28.872 steps/s
	current lr: 0.0000029
	lm_loss: 5.3557, ppl: 211.8093, loss: 5.3557
[train][1] progress: 1/1 step: 294, time: 0.035, speed: 28.621 steps/s
	current lr: 0.0000029
	lm_loss: 4.4112, ppl: 82.3661, loss: 4.4112
[train][1] progress: 1/1 step: 295, time: 0.035, speed: 28.720 steps/s
	current lr: 0.0000029
	lm_loss: 6.2972, ppl: 543.0514, loss: 6.2972
[train][1] progress: 1/1 step: 296, time: 0.034, speed: 29.020 steps/s
	current lr: 0.0000030
	lm_loss: 5.9173, ppl: 371.4215, loss: 5.9173
[train][1] progress: 1/1 step: 297, time: 0.035, speed: 28.936 steps/s
	current lr: 0.0000030
	lm_loss: 4.8707, ppl: 130.4117, loss: 4.8707
[train][1] progress: 1/1 step: 298, time: 0.034, speed: 29.325 steps/s
	current lr: 0.0000030
	lm_loss: 5.5836, ppl: 266.0353, loss: 5.5836
[train][1] progress: 1/1 step: 299, time: 0.034, speed: 29.542 steps/s
	current lr: 0.0000030
	lm_loss: 6.2276, ppl: 506.5543, loss: 6.2276
[train][1] progress: 1/1 step: 300, time: 0.034, speed: 29.848 steps/s
	current lr: 0.0000030
	lm_loss: 6.7612, ppl: 863.6574, loss: 6.7612
[train][1] progress: 1/1 step: 301, time: 0.033, speed: 29.902 steps/s
	current lr: 0.0000030
	lm_loss: 3.6604, ppl: 38.8761, loss: 3.6604
[train][1] progress: 1/1 step: 302, time: 0.034, speed: 29.348 steps/s
	current lr: 0.0000030
	lm_loss: 4.7354, ppl: 113.9049, loss: 4.7354
[train][1] progress: 1/1 step: 303, time: 0.035, speed: 28.839 steps/s
	current lr: 0.0000030
	lm_loss: 6.3741, ppl: 586.4578, loss: 6.3741
[train][1] progress: 1/1 step: 304, time: 0.035, speed: 28.674 steps/s
	current lr: 0.0000030
	lm_loss: 5.7459, ppl: 312.9162, loss: 5.7459
[train][1] progress: 1/1 step: 305, time: 0.035, speed: 28.738 steps/s
	current lr: 0.0000031
	lm_loss: 5.7240, ppl: 306.1395, loss: 5.7240
[train][1] progress: 1/1 step: 306, time: 0.034, speed: 29.600 steps/s
	current lr: 0.0000031
	lm_loss: 4.8789, ppl: 131.4827, loss: 4.8789
[train][1] progress: 1/1 step: 307, time: 0.034, speed: 29.408 steps/s
	current lr: 0.0000031
	lm_loss: 4.8192, ppl: 123.8643, loss: 4.8192
[train][1] progress: 1/1 step: 308, time: 0.034, speed: 29.476 steps/s
	current lr: 0.0000031
	lm_loss: 5.9576, ppl: 386.6794, loss: 5.9576
[train][1] progress: 1/1 step: 309, time: 0.034, speed: 29.720 steps/s
	current lr: 0.0000031
	lm_loss: 5.0476, ppl: 155.6507, loss: 5.0476
[train][1] progress: 1/1 step: 310, time: 0.033, speed: 29.877 steps/s
	current lr: 0.0000031
	lm_loss: 6.2002, ppl: 492.8561, loss: 6.2002
[train][1] progress: 1/1 step: 311, time: 0.034, speed: 29.368 steps/s
	current lr: 0.0000031
	lm_loss: 4.7423, ppl: 114.6984, loss: 4.7423
[train][1] progress: 1/1 step: 312, time: 0.034, speed: 29.813 steps/s
	current lr: 0.0000031
	lm_loss: 5.2733, ppl: 195.0504, loss: 5.2733
[train][1] progress: 1/1 step: 313, time: 0.033, speed: 30.029 steps/s
	current lr: 0.0000031
	lm_loss: 5.8185, ppl: 336.4779, loss: 5.8185
[train][1] progress: 1/1 step: 314, time: 0.033, speed: 30.069 steps/s
	current lr: 0.0000031
	lm_loss: 5.1972, ppl: 180.7598, loss: 5.1972
[train][1] progress: 1/1 step: 315, time: 0.033, speed: 30.247 steps/s
	current lr: 0.0000031
	lm_loss: 4.6746, ppl: 107.1905, loss: 4.6746
[train][1] progress: 1/1 step: 316, time: 0.033, speed: 30.403 steps/s
	current lr: 0.0000032
	lm_loss: 4.6361, ppl: 103.1453, loss: 4.6361
[train][1] progress: 1/1 step: 317, time: 0.033, speed: 30.082 steps/s
	current lr: 0.0000032
	lm_loss: 6.9589, ppl: 1052.5203, loss: 6.9589
[train][1] progress: 1/1 step: 318, time: 0.034, speed: 29.699 steps/s
	current lr: 0.0000032
	lm_loss: 4.8749, ppl: 130.9663, loss: 4.8749
[train][1] progress: 1/1 step: 319, time: 0.033, speed: 29.892 steps/s
	current lr: 0.0000032
	lm_loss: 6.4195, ppl: 613.6777, loss: 6.4195
[train][1] progress: 1/1 step: 320, time: 0.034, speed: 29.496 steps/s
	current lr: 0.0000032
	lm_loss: 5.3122, ppl: 202.8046, loss: 5.3122
[train][1] progress: 1/1 step: 321, time: 0.033, speed: 30.553 steps/s
	current lr: 0.0000032
	lm_loss: 5.7581, ppl: 316.7405, loss: 5.7581
[train][1] progress: 1/1 step: 322, time: 0.033, speed: 30.304 steps/s
	current lr: 0.0000032
	lm_loss: 5.5642, ppl: 260.9157, loss: 5.5642
[train][1] progress: 1/1 step: 323, time: 0.033, speed: 29.945 steps/s
	current lr: 0.0000032
	lm_loss: 5.4550, ppl: 233.9186, loss: 5.4550
[train][1] progress: 1/1 step: 324, time: 0.033, speed: 29.960 steps/s
	current lr: 0.0000032
	lm_loss: 5.0879, ppl: 162.0559, loss: 5.0879
[train][1] progress: 1/1 step: 325, time: 0.034, speed: 29.137 steps/s
	current lr: 0.0000032
	lm_loss: 4.9509, ppl: 141.3046, loss: 4.9509
[train][1] progress: 1/1 step: 326, time: 0.035, speed: 28.272 steps/s
	current lr: 0.0000033
	lm_loss: 5.3217, ppl: 204.7406, loss: 5.3217
[train][1] progress: 1/1 step: 327, time: 0.034, speed: 29.439 steps/s
	current lr: 0.0000033
	lm_loss: 5.2792, ppl: 196.2032, loss: 5.2792
[train][1] progress: 1/1 step: 328, time: 0.034, speed: 29.809 steps/s
	current lr: 0.0000033
	lm_loss: 5.5441, ppl: 255.7333, loss: 5.5441
[train][1] progress: 1/1 step: 329, time: 0.034, speed: 29.472 steps/s
	current lr: 0.0000033
	lm_loss: 4.8704, ppl: 130.3729, loss: 4.8704
[train][1] progress: 1/1 step: 330, time: 0.034, speed: 29.574 steps/s
	current lr: 0.0000033
	lm_loss: 5.1201, ppl: 167.3551, loss: 5.1201
[train][1] progress: 1/1 step: 331, time: 0.034, speed: 29.580 steps/s
	current lr: 0.0000033
	lm_loss: 4.7583, ppl: 116.5518, loss: 4.7583
[train][1] progress: 1/1 step: 332, time: 0.034, speed: 29.631 steps/s
	current lr: 0.0000033
	lm_loss: 3.8819, ppl: 48.5140, loss: 3.8819
[train][1] progress: 1/1 step: 333, time: 0.033, speed: 30.332 steps/s
	current lr: 0.0000033
	lm_loss: 4.8593, ppl: 128.9318, loss: 4.8593
[train][1] progress: 1/1 step: 334, time: 0.034, speed: 29.637 steps/s
	current lr: 0.0000033
	lm_loss: 4.8182, ppl: 123.7431, loss: 4.8182
[train][1] progress: 1/1 step: 335, time: 0.034, speed: 29.472 steps/s
	current lr: 0.0000034
	lm_loss: 6.1401, ppl: 464.0962, loss: 6.1401
[train][1] progress: 1/1 step: 336, time: 0.034, speed: 29.451 steps/s
	current lr: 0.0000034
	lm_loss: 4.7572, ppl: 116.4212, loss: 4.7572
[train][1] progress: 1/1 step: 337, time: 0.035, speed: 28.386 steps/s
	current lr: 0.0000034
	lm_loss: 4.8980, ppl: 134.0184, loss: 4.8980
[train][1] progress: 1/1 step: 338, time: 0.034, speed: 29.428 steps/s
	current lr: 0.0000034
	lm_loss: 5.9732, ppl: 392.7676, loss: 5.9732
[train][1] progress: 1/1 step: 339, time: 0.033, speed: 30.237 steps/s
	current lr: 0.0000034
	lm_loss: 5.6483, ppl: 283.7983, loss: 5.6483
[train][1] progress: 1/1 step: 340, time: 0.033, speed: 30.568 steps/s
	current lr: 0.0000034
	lm_loss: 5.6179, ppl: 275.3077, loss: 5.6179
[train][1] progress: 1/1 step: 341, time: 0.033, speed: 30.407 steps/s
	current lr: 0.0000034
	lm_loss: 5.5879, ppl: 267.1833, loss: 5.5879
[train][1] progress: 1/1 step: 342, time: 0.033, speed: 30.260 steps/s
	current lr: 0.0000034
	lm_loss: 5.7238, ppl: 306.0803, loss: 5.7238
[train][1] progress: 1/1 step: 343, time: 0.032, speed: 30.908 steps/s
	current lr: 0.0000034
	lm_loss: 5.4104, ppl: 223.7152, loss: 5.4104
[train][1] progress: 1/1 step: 344, time: 0.032, speed: 31.073 steps/s
	current lr: 0.0000034
	lm_loss: 4.4079, ppl: 82.0948, loss: 4.4079
[train][1] progress: 1/1 step: 345, time: 0.034, speed: 29.757 steps/s
	current lr: 0.0000034
	lm_loss: 4.8039, ppl: 121.9825, loss: 4.8039
[train][1] progress: 1/1 step: 346, time: 0.033, speed: 30.072 steps/s
	current lr: 0.0000035
	lm_loss: 6.8245, ppl: 920.1058, loss: 6.8245
[train][1] progress: 1/1 step: 347, time: 0.033, speed: 30.134 steps/s
	current lr: 0.0000035
	lm_loss: 5.6789, ppl: 292.6387, loss: 5.6789
[train][1] progress: 1/1 step: 348, time: 0.033, speed: 30.029 steps/s
	current lr: 0.0000035
	lm_loss: 4.9807, ppl: 145.5799, loss: 4.9807
[train][1] progress: 1/1 step: 349, time: 0.033, speed: 30.369 steps/s
	current lr: 0.0000035
	lm_loss: 5.7629, ppl: 318.2576, loss: 5.7629
[train][1] progress: 1/1 step: 350, time: 0.033, speed: 30.500 steps/s
	current lr: 0.0000035
	lm_loss: 5.0956, ppl: 163.2993, loss: 5.0956
[train][1] progress: 1/1 step: 351, time: 0.033, speed: 30.421 steps/s
	current lr: 0.0000035
	lm_loss: 5.8123, ppl: 334.3895, loss: 5.8123
[train][1] progress: 1/1 step: 352, time: 0.032, speed: 31.071 steps/s
	current lr: 0.0000035
	lm_loss: 4.3777, ppl: 79.6547, loss: 4.3777
[train][1] progress: 1/1 step: 353, time: 0.034, speed: 29.440 steps/s
	current lr: 0.0000035
	lm_loss: 5.3537, ppl: 211.3915, loss: 5.3537
[train][1] progress: 1/1 step: 354, time: 0.033, speed: 29.859 steps/s
	current lr: 0.0000035
	lm_loss: 5.9577, ppl: 386.7376, loss: 5.9577
[train][1] progress: 1/1 step: 355, time: 0.033, speed: 30.487 steps/s
	current lr: 0.0000035
	lm_loss: 4.0494, ppl: 57.3620, loss: 4.0494
[train][1] progress: 1/1 step: 356, time: 0.033, speed: 30.448 steps/s
	current lr: 0.0000036
	lm_loss: 5.6957, ppl: 297.5866, loss: 5.6957
[train][1] progress: 1/1 step: 357, time: 0.034, speed: 29.747 steps/s
	current lr: 0.0000036
	lm_loss: 6.3820, ppl: 591.1186, loss: 6.3820
[train][1] progress: 1/1 step: 358, time: 0.034, speed: 29.642 steps/s
	current lr: 0.0000036
	lm_loss: 4.7359, ppl: 113.9636, loss: 4.7359
[train][1] progress: 1/1 step: 359, time: 0.033, speed: 29.996 steps/s
	current lr: 0.0000036
	lm_loss: 5.8254, ppl: 338.8119, loss: 5.8254
[train][1] progress: 1/1 step: 360, time: 0.033, speed: 30.328 steps/s
	current lr: 0.0000036
	lm_loss: 5.2143, ppl: 183.8856, loss: 5.2143
[train][1] progress: 1/1 step: 361, time: 0.033, speed: 30.565 steps/s
	current lr: 0.0000036
	lm_loss: 5.3842, ppl: 217.9436, loss: 5.3842
[train][1] progress: 1/1 step: 362, time: 0.034, speed: 29.342 steps/s
	current lr: 0.0000036
	lm_loss: 5.3637, ppl: 213.5210, loss: 5.3637
[train][1] progress: 1/1 step: 363, time: 0.037, speed: 27.052 steps/s
	current lr: 0.0000036
	lm_loss: 6.3095, ppl: 549.7449, loss: 6.3095
[train][1] progress: 1/1 step: 364, time: 0.037, speed: 26.711 steps/s
	current lr: 0.0000036
	lm_loss: 2.4677, ppl: 11.7949, loss: 2.4677
[train][1] progress: 1/1 step: 365, time: 0.037, speed: 27.186 steps/s
	current lr: 0.0000037
	lm_loss: 5.7669, ppl: 319.5356, loss: 5.7669
[train][1] progress: 1/1 step: 366, time: 0.036, speed: 27.693 steps/s
	current lr: 0.0000037
	lm_loss: 6.5613, ppl: 707.2122, loss: 6.5613
[train][1] progress: 1/1 step: 367, time: 0.036, speed: 27.566 steps/s
	current lr: 0.0000037
	lm_loss: 5.8863, ppl: 360.0704, loss: 5.8863
[train][1] progress: 1/1 step: 368, time: 0.036, speed: 27.956 steps/s
	current lr: 0.0000037
	lm_loss: 5.0265, ppl: 152.4045, loss: 5.0265
[train][1] progress: 1/1 step: 369, time: 0.034, speed: 29.130 steps/s
	current lr: 0.0000037
	lm_loss: 5.5233, ppl: 250.4492, loss: 5.5233
[train][1] progress: 1/1 step: 370, time: 0.034, speed: 29.344 steps/s
	current lr: 0.0000037
	lm_loss: 6.7568, ppl: 859.8752, loss: 6.7568
[train][1] progress: 1/1 step: 371, time: 0.033, speed: 30.496 steps/s
	current lr: 0.0000037
	lm_loss: 4.3026, ppl: 73.8908, loss: 4.3026
[train][1] progress: 1/1 step: 372, time: 0.032, speed: 31.056 steps/s
	current lr: 0.0000037
	lm_loss: 5.4433, ppl: 231.2041, loss: 5.4433
[train][1] progress: 1/1 step: 373, time: 0.032, speed: 31.053 steps/s
	current lr: 0.0000037
	lm_loss: 4.7207, ppl: 112.2439, loss: 4.7207
[train][1] progress: 1/1 step: 374, time: 0.032, speed: 30.779 steps/s
	current lr: 0.0000037
	lm_loss: 4.0780, ppl: 59.0245, loss: 4.0780
[train][1] progress: 1/1 step: 375, time: 0.033, speed: 30.031 steps/s
	current lr: 0.0000037
	lm_loss: 4.6715, ppl: 106.8572, loss: 4.6715
[train][1] progress: 1/1 step: 376, time: 0.034, speed: 29.621 steps/s
	current lr: 0.0000038
	lm_loss: 4.2278, ppl: 68.5682, loss: 4.2278
[train][1] progress: 1/1 step: 377, time: 0.034, speed: 29.493 steps/s
	current lr: 0.0000038
	lm_loss: 5.1974, ppl: 180.8033, loss: 5.1974
[train][1] progress: 1/1 step: 378, time: 0.034, speed: 29.637 steps/s
	current lr: 0.0000038
	lm_loss: 6.0600, ppl: 428.3617, loss: 6.0600
[train][1] progress: 1/1 step: 379, time: 0.033, speed: 30.204 steps/s
	current lr: 0.0000038
	lm_loss: 4.1451, ppl: 63.1235, loss: 4.1451
[train][1] progress: 1/1 step: 380, time: 0.033, speed: 30.409 steps/s
	current lr: 0.0000038
	lm_loss: 3.5125, ppl: 33.5314, loss: 3.5125
[train][1] progress: 1/1 step: 381, time: 0.033, speed: 30.605 steps/s
	current lr: 0.0000038
	lm_loss: 3.8566, ppl: 47.3062, loss: 3.8566
[train][1] progress: 1/1 step: 382, time: 0.034, speed: 29.589 steps/s
	current lr: 0.0000038
	lm_loss: 5.4180, ppl: 225.4206, loss: 5.4180
[train][1] progress: 1/1 step: 383, time: 0.033, speed: 30.443 steps/s
	current lr: 0.0000038
	lm_loss: 5.1344, ppl: 169.7566, loss: 5.1344
[train][1] progress: 1/1 step: 384, time: 0.032, speed: 30.927 steps/s
	current lr: 0.0000038
	lm_loss: 5.1717, ppl: 176.2083, loss: 5.1717
[train][1] progress: 1/1 step: 385, time: 0.032, speed: 30.789 steps/s
	current lr: 0.0000039
	lm_loss: 5.9290, ppl: 375.7730, loss: 5.9290
[train][1] progress: 1/1 step: 386, time: 0.033, speed: 30.025 steps/s
	current lr: 0.0000039
	lm_loss: 4.1388, ppl: 62.7293, loss: 4.1388
[train][1] progress: 1/1 step: 387, time: 0.033, speed: 29.896 steps/s
	current lr: 0.0000039
	lm_loss: 5.9712, ppl: 391.9724, loss: 5.9712
[train][1] progress: 1/1 step: 388, time: 0.034, speed: 29.672 steps/s
	current lr: 0.0000039
	lm_loss: 5.9270, ppl: 375.0383, loss: 5.9270
[train][1] progress: 1/1 step: 389, time: 0.034, speed: 29.740 steps/s
	current lr: 0.0000039
	lm_loss: 5.2694, ppl: 194.2929, loss: 5.2694
[train][1] progress: 1/1 step: 390, time: 0.034, speed: 29.028 steps/s
	current lr: 0.0000039
	lm_loss: 5.4814, ppl: 240.1754, loss: 5.4814
[train][1] progress: 1/1 step: 391, time: 0.037, speed: 27.395 steps/s
	current lr: 0.0000039
	lm_loss: 4.6451, ppl: 104.0687, loss: 4.6451
[train][1] progress: 1/1 step: 392, time: 0.039, speed: 25.585 steps/s
	current lr: 0.0000039
	lm_loss: 4.0288, ppl: 56.1924, loss: 4.0288
[train][1] progress: 1/1 step: 393, time: 0.040, speed: 25.166 steps/s
	current lr: 0.0000039
	lm_loss: 4.6921, ppl: 109.0772, loss: 4.6921
[train][1] progress: 1/1 step: 394, time: 0.039, speed: 25.457 steps/s
	current lr: 0.0000039
	lm_loss: 5.4025, ppl: 221.9536, loss: 5.4025
[train][1] progress: 1/1 step: 395, time: 0.040, speed: 24.850 steps/s
	current lr: 0.0000040
	lm_loss: 5.9303, ppl: 376.2671, loss: 5.9303
[train][1] progress: 1/1 step: 396, time: 0.040, speed: 25.067 steps/s
	current lr: 0.0000040
	lm_loss: 5.3762, ppl: 216.1946, loss: 5.3762
[train][1] progress: 1/1 step: 397, time: 0.039, speed: 25.875 steps/s
	current lr: 0.0000040
	lm_loss: 5.2002, ppl: 181.3121, loss: 5.2002
[train][1] progress: 1/1 step: 398, time: 0.037, speed: 26.880 steps/s
	current lr: 0.0000040
	lm_loss: 4.1445, ppl: 63.0889, loss: 4.1445
[train][1] progress: 1/1 step: 399, time: 0.037, speed: 27.100 steps/s
	current lr: 0.0000040
	lm_loss: 4.8319, ppl: 125.4447, loss: 4.8319
[train][1] progress: 1/1 step: 400, time: 0.037, speed: 27.271 steps/s
	current lr: 0.0000040
	lm_loss: 5.8339, ppl: 341.7022, loss: 5.8339
[train][1] progress: 1/1 step: 401, time: 0.037, speed: 27.279 steps/s
	current lr: 0.0000040
	lm_loss: 6.1099, ppl: 450.3113, loss: 6.1099
[train][1] progress: 1/1 step: 402, time: 0.036, speed: 27.694 steps/s
	current lr: 0.0000040
	lm_loss: 4.9911, ppl: 147.0995, loss: 4.9911
[train][1] progress: 1/1 step: 403, time: 0.036, speed: 27.402 steps/s
	current lr: 0.0000040
	lm_loss: 4.8568, ppl: 128.6159, loss: 4.8568
[train][1] progress: 1/1 step: 404, time: 0.037, speed: 27.330 steps/s
	current lr: 0.0000040
	lm_loss: 3.8732, ppl: 48.0958, loss: 3.8732
[train][1] progress: 1/1 step: 405, time: 0.038, speed: 26.017 steps/s
	current lr: 0.0000040
	lm_loss: 5.2398, ppl: 188.6366, loss: 5.2398
[train][1] progress: 1/1 step: 406, time: 0.040, speed: 25.099 steps/s
	current lr: 0.0000041
	lm_loss: 3.9918, ppl: 54.1515, loss: 3.9918
[train][1] progress: 1/1 step: 407, time: 0.038, speed: 26.290 steps/s
	current lr: 0.0000041
	lm_loss: 5.6632, ppl: 288.0779, loss: 5.6632
[train][1] progress: 1/1 step: 408, time: 0.037, speed: 26.872 steps/s
	current lr: 0.0000041
	lm_loss: 4.5897, ppl: 98.4616, loss: 4.5897
[train][1] progress: 1/1 step: 409, time: 0.037, speed: 26.896 steps/s
	current lr: 0.0000041
	lm_loss: 4.7576, ppl: 116.4608, loss: 4.7576
[train][1] progress: 1/1 step: 410, time: 0.037, speed: 27.090 steps/s
	current lr: 0.0000041
	lm_loss: 5.0361, ppl: 153.8740, loss: 5.0361
[train][1] progress: 1/1 step: 411, time: 0.037, speed: 27.057 steps/s
	current lr: 0.0000041
	lm_loss: 4.9894, ppl: 146.8477, loss: 4.9894
[train][1] progress: 1/1 step: 412, time: 0.037, speed: 26.890 steps/s
	current lr: 0.0000041
	lm_loss: 4.4891, ppl: 89.0445, loss: 4.4891
[train][1] progress: 1/1 step: 413, time: 0.038, speed: 26.406 steps/s
	current lr: 0.0000041
	lm_loss: 4.9180, ppl: 136.7343, loss: 4.9180
[train][1] progress: 1/1 step: 414, time: 0.039, speed: 25.566 steps/s
	current lr: 0.0000041
	lm_loss: 5.1847, ppl: 178.5221, loss: 5.1847
[train][1] progress: 1/1 step: 415, time: 0.041, speed: 24.619 steps/s
	current lr: 0.0000042
	lm_loss: 4.6940, ppl: 109.2862, loss: 4.6940
[train][1] progress: 1/1 step: 416, time: 0.039, speed: 25.540 steps/s
	current lr: 0.0000042
	lm_loss: 5.1309, ppl: 169.1700, loss: 5.1309
[train][1] progress: 1/1 step: 417, time: 0.039, speed: 25.765 steps/s
	current lr: 0.0000042
	lm_loss: 5.7201, ppl: 304.9360, loss: 5.7201
[train][1] progress: 1/1 step: 418, time: 0.037, speed: 26.837 steps/s
	current lr: 0.0000042
	lm_loss: 5.8356, ppl: 342.2723, loss: 5.8356
[train][1] progress: 1/1 step: 419, time: 0.038, speed: 26.604 steps/s
	current lr: 0.0000042
	lm_loss: 5.1835, ppl: 178.3065, loss: 5.1835
[train][1] progress: 1/1 step: 420, time: 0.039, speed: 25.822 steps/s
	current lr: 0.0000042
	lm_loss: 5.6036, ppl: 271.4083, loss: 5.6036
[train][1] progress: 1/1 step: 421, time: 0.038, speed: 26.450 steps/s
	current lr: 0.0000042
	lm_loss: 5.2651, ppl: 193.4720, loss: 5.2651
[train][1] progress: 1/1 step: 422, time: 0.039, speed: 25.768 steps/s
	current lr: 0.0000042
	lm_loss: 4.6312, ppl: 102.6340, loss: 4.6312
[train][1] progress: 1/1 step: 423, time: 0.040, speed: 24.834 steps/s
	current lr: 0.0000042
	lm_loss: 4.0939, ppl: 59.9754, loss: 4.0939
[train][1] progress: 1/1 step: 424, time: 0.041, speed: 24.485 steps/s
	current lr: 0.0000042
	lm_loss: 5.1076, ppl: 165.2810, loss: 5.1076
[train][1] progress: 1/1 step: 425, time: 0.038, speed: 26.174 steps/s
	current lr: 0.0000043
	lm_loss: 5.9823, ppl: 396.3416, loss: 5.9823
[train][1] progress: 1/1 step: 426, time: 0.039, speed: 25.675 steps/s
	current lr: 0.0000043
	lm_loss: 6.4045, ppl: 604.5822, loss: 6.4045
[train][1] progress: 1/1 step: 427, time: 0.037, speed: 26.836 steps/s
	current lr: 0.0000043
	lm_loss: 6.8088, ppl: 905.7674, loss: 6.8088
[train][1] progress: 1/1 step: 428, time: 0.036, speed: 27.457 steps/s
	current lr: 0.0000043
	lm_loss: 5.4575, ppl: 234.5213, loss: 5.4575
[train][1] progress: 1/1 step: 429, time: 0.038, speed: 26.453 steps/s
	current lr: 0.0000043
	lm_loss: 5.7491, ppl: 313.9145, loss: 5.7491
[train][1] progress: 1/1 step: 430, time: 0.038, speed: 26.626 steps/s
	current lr: 0.0000043
	lm_loss: 2.7617, ppl: 15.8271, loss: 2.7617
[train][1] progress: 1/1 step: 431, time: 0.037, speed: 27.251 steps/s
	current lr: 0.0000043
	lm_loss: 6.6591, ppl: 779.8856, loss: 6.6591
[train][1] progress: 1/1 step: 432, time: 0.044, speed: 22.526 steps/s
	current lr: 0.0000043
	lm_loss: 4.3248, ppl: 75.5507, loss: 4.3248
[train][1] progress: 1/1 step: 433, time: 0.041, speed: 24.619 steps/s
	current lr: 0.0000043
	lm_loss: 5.2744, ppl: 195.2651, loss: 5.2744
[train][1] progress: 1/1 step: 434, time: 0.041, speed: 24.647 steps/s
	current lr: 0.0000043
	lm_loss: 5.1999, ppl: 181.2534, loss: 5.1999
[train][1] progress: 1/1 step: 435, time: 0.039, speed: 25.325 steps/s
	current lr: 0.0000043
	lm_loss: 5.3732, ppl: 215.5447, loss: 5.3732
[train][1] progress: 1/1 step: 436, time: 0.040, speed: 24.830 steps/s
	current lr: 0.0000044
	lm_loss: 5.7808, ppl: 324.0105, loss: 5.7808
[train][1] progress: 1/1 step: 437, time: 0.042, speed: 24.023 steps/s
	current lr: 0.0000044
	lm_loss: 4.2753, ppl: 71.9026, loss: 4.2753
[train][1] progress: 1/1 step: 438, time: 0.042, speed: 24.086 steps/s
	current lr: 0.0000044
	lm_loss: 5.3629, ppl: 213.3454, loss: 5.3629
[train][1] progress: 1/1 step: 439, time: 0.039, speed: 25.886 steps/s
	current lr: 0.0000044
	lm_loss: 5.7894, ppl: 326.8135, loss: 5.7894
[train][1] progress: 1/1 step: 440, time: 0.040, speed: 25.216 steps/s
	current lr: 0.0000044
	lm_loss: 4.9801, ppl: 145.4844, loss: 4.9801
[train][1] progress: 1/1 step: 441, time: 0.041, speed: 24.367 steps/s
	current lr: 0.0000044
	lm_loss: 4.5791, ppl: 97.4250, loss: 4.5791
[train][1] progress: 1/1 step: 442, time: 0.041, speed: 24.494 steps/s
	current lr: 0.0000044
	lm_loss: 2.4035, ppl: 11.0614, loss: 2.4035
[train][1] progress: 1/1 step: 443, time: 0.040, speed: 24.693 steps/s
	current lr: 0.0000044
	lm_loss: 5.7159, ppl: 303.6598, loss: 5.7159
[train][1] progress: 1/1 step: 444, time: 0.039, speed: 25.702 steps/s
	current lr: 0.0000044
	lm_loss: 5.5393, ppl: 254.4929, loss: 5.5393
[train][1] progress: 1/1 step: 445, time: 0.038, speed: 26.631 steps/s
	current lr: 0.0000045
	lm_loss: 4.7958, ppl: 121.0039, loss: 4.7958
[train][1] progress: 1/1 step: 446, time: 0.038, speed: 26.310 steps/s
	current lr: 0.0000045
	lm_loss: 4.6056, ppl: 100.0463, loss: 4.6056
[train][1] progress: 1/1 step: 447, time: 0.037, speed: 26.838 steps/s
	current lr: 0.0000045
	lm_loss: 1.8586, ppl: 6.4145, loss: 1.8586
[train][1] progress: 1/1 step: 448, time: 0.037, speed: 27.074 steps/s
	current lr: 0.0000045
	lm_loss: 6.3657, ppl: 581.5314, loss: 6.3657
[train][1] progress: 1/1 step: 449, time: 0.037, speed: 26.682 steps/s
	current lr: 0.0000045
	lm_loss: 4.9659, ppl: 143.4359, loss: 4.9659
[train][1] progress: 1/1 step: 450, time: 0.038, speed: 26.375 steps/s
	current lr: 0.0000045
	lm_loss: 5.7394, ppl: 310.8763, loss: 5.7394
[train][1] progress: 1/1 step: 451, time: 0.039, speed: 25.578 steps/s
	current lr: 0.0000045
	lm_loss: 6.0495, ppl: 423.9036, loss: 6.0495
[train][1] progress: 1/1 step: 452, time: 0.040, speed: 25.173 steps/s
	current lr: 0.0000045
	lm_loss: 4.4549, ppl: 86.0495, loss: 4.4549
[train][1] progress: 1/1 step: 453, time: 0.040, speed: 25.312 steps/s
	current lr: 0.0000045
	lm_loss: 3.5894, ppl: 36.2120, loss: 3.5894
[train][1] progress: 1/1 step: 454, time: 0.038, speed: 26.419 steps/s
	current lr: 0.0000045
	lm_loss: 5.0236, ppl: 151.9559, loss: 5.0236
[train][1] progress: 1/1 step: 455, time: 0.038, speed: 26.499 steps/s
	current lr: 0.0000046
	lm_loss: 5.2362, ppl: 187.9576, loss: 5.2362
[train][1] progress: 1/1 step: 456, time: 0.038, speed: 26.100 steps/s
	current lr: 0.0000046
	lm_loss: 4.8351, ppl: 125.8527, loss: 4.8351
[train][1] progress: 1/1 step: 457, time: 0.038, speed: 26.467 steps/s
	current lr: 0.0000046
	lm_loss: 5.2693, ppl: 194.2867, loss: 5.2693
[train][1] progress: 1/1 step: 458, time: 0.040, speed: 25.123 steps/s
	current lr: 0.0000046
	lm_loss: 3.9015, ppl: 49.4788, loss: 3.9015
[train][1] progress: 1/1 step: 459, time: 0.038, speed: 26.141 steps/s
	current lr: 0.0000046
	lm_loss: 4.8016, ppl: 121.7041, loss: 4.8016
[train][1] progress: 1/1 step: 460, time: 0.039, speed: 25.319 steps/s
	current lr: 0.0000046
	lm_loss: 5.2741, ppl: 195.2069, loss: 5.2741
[train][1] progress: 1/1 step: 461, time: 0.057, speed: 17.419 steps/s
	current lr: 0.0000046
	lm_loss: 6.5146, ppl: 674.9032, loss: 6.5146
[train][1] progress: 1/1 step: 462, time: 0.045, speed: 22.188 steps/s
	current lr: 0.0000046
	lm_loss: 4.1330, ppl: 62.3657, loss: 4.1330
[train][1] progress: 1/1 step: 463, time: 0.039, speed: 25.790 steps/s
	current lr: 0.0000046
	lm_loss: 5.3958, ppl: 220.4681, loss: 5.3958
[train][1] progress: 1/1 step: 464, time: 0.039, speed: 25.821 steps/s
	current lr: 0.0000046
	lm_loss: 5.4731, ppl: 238.1868, loss: 5.4731
[train][1] progress: 1/1 step: 465, time: 0.039, speed: 25.617 steps/s
	current lr: 0.0000046
	lm_loss: 5.4978, ppl: 244.1637, loss: 5.4978
[train][1] progress: 1/1 step: 466, time: 0.038, speed: 26.496 steps/s
	current lr: 0.0000047
	lm_loss: 7.4648, ppl: 1745.4296, loss: 7.4648
[train][1] progress: 1/1 step: 467, time: 0.037, speed: 26.766 steps/s
	current lr: 0.0000047
	lm_loss: 5.0417, ppl: 154.7358, loss: 5.0417
[train][1] progress: 1/1 step: 468, time: 0.037, speed: 27.038 steps/s
	current lr: 0.0000047
	lm_loss: 4.8765, ppl: 131.1755, loss: 4.8765
[train][1] progress: 1/1 step: 469, time: 0.038, speed: 26.039 steps/s
	current lr: 0.0000047
	lm_loss: 4.9426, ppl: 140.1279, loss: 4.9426
[train][1] progress: 1/1 step: 470, time: 0.039, speed: 25.655 steps/s
	current lr: 0.0000047
	lm_loss: 5.6037, ppl: 271.4184, loss: 5.6037
[train][1] progress: 1/1 step: 471, time: 0.038, speed: 26.030 steps/s
	current lr: 0.0000047
	lm_loss: 5.6585, ppl: 286.7182, loss: 5.6585
[train][1] progress: 1/1 step: 472, time: 0.038, speed: 26.266 steps/s
	current lr: 0.0000047
	lm_loss: 0.7592, ppl: 2.1365, loss: 0.7592
[train][1] progress: 1/1 step: 473, time: 0.037, speed: 27.014 steps/s
	current lr: 0.0000047
	lm_loss: 5.4759, ppl: 238.8715, loss: 5.4759
[train][1] progress: 1/1 step: 474, time: 0.038, speed: 26.486 steps/s
	current lr: 0.0000047
	lm_loss: 5.4369, ppl: 229.7190, loss: 5.4369
[train][1] progress: 1/1 step: 475, time: 0.040, speed: 25.312 steps/s
	current lr: 0.0000048
	lm_loss: 5.4068, ppl: 222.9082, loss: 5.4068
[train][1] progress: 1/1 step: 476, time: 0.037, speed: 26.788 steps/s
	current lr: 0.0000048
	lm_loss: 5.8750, ppl: 356.0196, loss: 5.8750
[train][1] progress: 1/1 step: 477, time: 0.037, speed: 26.791 steps/s
	current lr: 0.0000048
	lm_loss: 5.1427, ppl: 171.1818, loss: 5.1427
[train][1] progress: 1/1 step: 478, time: 0.038, speed: 26.081 steps/s
	current lr: 0.0000048
	lm_loss: 4.6383, ppl: 103.3707, loss: 4.6383
[train][1] progress: 1/1 step: 479, time: 0.040, speed: 25.163 steps/s
	current lr: 0.0000048
	lm_loss: 4.3225, ppl: 75.3746, loss: 4.3225
[train][1] progress: 1/1 step: 480, time: 0.038, speed: 26.074 steps/s
	current lr: 0.0000048
	lm_loss: 5.8897, ppl: 361.3118, loss: 5.8897
[train][1] progress: 1/1 step: 481, time: 0.037, speed: 27.044 steps/s
	current lr: 0.0000048
	lm_loss: 5.0809, ppl: 160.9116, loss: 5.0809
[train][1] progress: 1/1 step: 482, time: 0.038, speed: 26.184 steps/s
	current lr: 0.0000048
	lm_loss: 2.9661, ppl: 19.4165, loss: 2.9661
[train][1] progress: 1/1 step: 483, time: 0.039, speed: 25.771 steps/s
	current lr: 0.0000048
	lm_loss: 6.9071, ppl: 999.3918, loss: 6.9071
[train][1] progress: 1/1 step: 484, time: 0.038, speed: 26.067 steps/s
	current lr: 0.0000048
	lm_loss: 4.5183, ppl: 91.6826, loss: 4.5183
[train][1] progress: 1/1 step: 485, time: 0.037, speed: 27.090 steps/s
	current lr: 0.0000049
	lm_loss: 4.0598, ppl: 57.9644, loss: 4.0598
[train][1] progress: 1/1 step: 486, time: 0.038, speed: 26.574 steps/s
	current lr: 0.0000049
	lm_loss: 5.8108, ppl: 333.8924, loss: 5.8108
[train][1] progress: 1/1 step: 487, time: 0.037, speed: 26.780 steps/s
	current lr: 0.0000049
	lm_loss: 5.6861, ppl: 294.7274, loss: 5.6861
[train][1] progress: 1/1 step: 488, time: 0.040, speed: 25.070 steps/s
	current lr: 0.0000049
	lm_loss: 4.8754, ppl: 131.0282, loss: 4.8754
[train][1] progress: 1/1 step: 489, time: 0.038, speed: 26.090 steps/s
	current lr: 0.0000049
	lm_loss: 6.3556, ppl: 575.7276, loss: 6.3556
[train][1] progress: 1/1 step: 490, time: 0.038, speed: 26.238 steps/s
	current lr: 0.0000049
	lm_loss: 5.4953, ppl: 243.5501, loss: 5.4953
[train][1] progress: 1/1 step: 491, time: 0.037, speed: 26.890 steps/s
	current lr: 0.0000049
	lm_loss: 4.4606, ppl: 86.5406, loss: 4.4606
[train][1] progress: 1/1 step: 492, time: 0.038, speed: 26.474 steps/s
	current lr: 0.0000049
	lm_loss: 4.9129, ppl: 136.0306, loss: 4.9129
[train][1] progress: 1/1 step: 493, time: 0.038, speed: 26.364 steps/s
	current lr: 0.0000049
	lm_loss: 5.2667, ppl: 193.7728, loss: 5.2667
[train][1] progress: 1/1 step: 494, time: 0.038, speed: 26.478 steps/s
	current lr: 0.0000049
	lm_loss: 4.8788, ppl: 131.4768, loss: 4.8788
[train][1] progress: 1/1 step: 495, time: 0.039, speed: 25.542 steps/s
	current lr: 0.0000049
	lm_loss: 5.3444, ppl: 209.4257, loss: 5.3444
[train][1] progress: 1/1 step: 496, time: 0.040, speed: 25.232 steps/s
	current lr: 0.0000050
	lm_loss: 5.0638, ppl: 158.1878, loss: 5.0638
[train][1] progress: 1/1 step: 497, time: 0.040, speed: 24.791 steps/s
	current lr: 0.0000050
	lm_loss: 3.7595, ppl: 42.9254, loss: 3.7595
[train][1] progress: 1/1 step: 498, time: 0.042, speed: 24.050 steps/s
	current lr: 0.0000050
	lm_loss: 5.4535, ppl: 233.5736, loss: 5.4535
[train][1] progress: 1/1 step: 499, time: 0.039, speed: 25.620 steps/s
	current lr: 0.0000050
	lm_loss: 5.9704, ppl: 391.6604, loss: 5.9704
[train][1] progress: 1/1 step: 500, time: 0.038, speed: 26.356 steps/s
	current lr: 0.0000050
	lm_loss: 4.9194, ppl: 136.9248, loss: 4.9194
[train][1] progress: 1/1 step: 501, time: 0.037, speed: 26.763 steps/s
	current lr: 0.0000050
	lm_loss: 4.7340, ppl: 113.7465, loss: 4.7340
[train][1] progress: 1/1 step: 502, time: 0.038, speed: 26.298 steps/s
	current lr: 0.0000050
	lm_loss: 5.9485, ppl: 383.1789, loss: 5.9485
[train][1] progress: 1/1 step: 503, time: 0.039, speed: 25.867 steps/s
	current lr: 0.0000050
	lm_loss: 4.9605, ppl: 142.6587, loss: 4.9605
[train][1] progress: 1/1 step: 504, time: 0.039, speed: 25.935 steps/s
	current lr: 0.0000050
	lm_loss: 5.2692, ppl: 194.2689, loss: 5.2692
[train][1] progress: 1/1 step: 505, time: 0.038, speed: 26.001 steps/s
	current lr: 0.0000051
	lm_loss: 4.8883, ppl: 132.7231, loss: 4.8883
[train][1] progress: 1/1 step: 506, time: 0.039, speed: 25.823 steps/s
	current lr: 0.0000051
	lm_loss: 4.6903, ppl: 108.8854, loss: 4.6903
[train][1] progress: 1/1 step: 507, time: 0.039, speed: 25.738 steps/s
	current lr: 0.0000051
	lm_loss: 4.6477, ppl: 104.3410, loss: 4.6477
[train][1] progress: 1/1 step: 508, time: 0.038, speed: 26.002 steps/s
	current lr: 0.0000051
	lm_loss: 5.4066, ppl: 222.8726, loss: 5.4066
[train][1] progress: 1/1 step: 509, time: 0.039, speed: 25.741 steps/s
	current lr: 0.0000051
	lm_loss: 4.6561, ppl: 105.2199, loss: 4.6561
[train][1] progress: 1/1 step: 510, time: 0.039, speed: 25.852 steps/s
	current lr: 0.0000051
	lm_loss: 4.7664, ppl: 117.4974, loss: 4.7664
[train][1] progress: 1/1 step: 511, time: 0.039, speed: 25.386 steps/s
	current lr: 0.0000051
	lm_loss: 6.7701, ppl: 871.3663, loss: 6.7701
[train][1] progress: 1/1 step: 512, time: 0.039, speed: 25.537 steps/s
	current lr: 0.0000051
	lm_loss: 5.2614, ppl: 192.7477, loss: 5.2614
[train][1] progress: 1/1 step: 513, time: 0.038, speed: 26.110 steps/s
	current lr: 0.0000051
	lm_loss: 4.5016, ppl: 90.1589, loss: 4.5016
[train][1] progress: 1/1 step: 514, time: 0.038, speed: 26.140 steps/s
	current lr: 0.0000051
	lm_loss: 5.4010, ppl: 221.6281, loss: 5.4010
[train][1] progress: 1/1 step: 515, time: 0.039, speed: 25.901 steps/s
	current lr: 0.0000052
	lm_loss: 6.1817, ppl: 483.7949, loss: 6.1817
[train][1] progress: 1/1 step: 516, time: 0.041, speed: 24.403 steps/s
	current lr: 0.0000052
	lm_loss: 5.3811, ppl: 217.2554, loss: 5.3811
[train][1] progress: 1/1 step: 517, time: 0.040, speed: 25.126 steps/s
	current lr: 0.0000052
	lm_loss: 5.9685, ppl: 390.9006, loss: 5.9685
[train][1] progress: 1/1 step: 518, time: 0.039, speed: 25.657 steps/s
	current lr: 0.0000052
	lm_loss: 5.9897, ppl: 399.2869, loss: 5.9897
[train][1] progress: 1/1 step: 519, time: 0.040, speed: 25.315 steps/s
	current lr: 0.0000052
	lm_loss: 4.7226, ppl: 112.4578, loss: 4.7226
[train][1] progress: 1/1 step: 520, time: 0.039, speed: 25.392 steps/s
	current lr: 0.0000052
	lm_loss: 4.6996, ppl: 109.8998, loss: 4.6996
[train][1] progress: 1/1 step: 521, time: 0.039, speed: 25.659 steps/s
	current lr: 0.0000052
	lm_loss: 4.5103, ppl: 90.9481, loss: 4.5103
[train][1] progress: 1/1 step: 522, time: 0.039, speed: 25.453 steps/s
	current lr: 0.0000052
	lm_loss: 5.4153, ppl: 224.8254, loss: 5.4153
[train][1] progress: 1/1 step: 523, time: 0.039, speed: 25.758 steps/s
	current lr: 0.0000052
	lm_loss: 5.7516, ppl: 314.6898, loss: 5.7516
[train][1] progress: 1/1 step: 524, time: 0.040, speed: 25.250 steps/s
	current lr: 0.0000052
	lm_loss: 5.5877, ppl: 267.1197, loss: 5.5877
[train][1] progress: 1/1 step: 525, time: 0.041, speed: 24.374 steps/s
	current lr: 0.0000053
	lm_loss: 4.6374, ppl: 103.2720, loss: 4.6374
[train][1] progress: 1/1 step: 526, time: 0.039, speed: 25.584 steps/s
	current lr: 0.0000053
	lm_loss: 5.9843, ppl: 397.1280, loss: 5.9843
[train][1] progress: 1/1 step: 527, time: 0.040, speed: 24.778 steps/s
	current lr: 0.0000053
	lm_loss: 3.3119, ppl: 27.4359, loss: 3.3119
[train][1] progress: 1/1 step: 528, time: 0.039, speed: 25.458 steps/s
	current lr: 0.0000053
	lm_loss: 3.5557, ppl: 35.0124, loss: 3.5557
[train][1] progress: 1/1 step: 529, time: 0.039, speed: 25.474 steps/s
	current lr: 0.0000053
	lm_loss: 5.2650, ppl: 193.4368, loss: 5.2650
[train][1] progress: 1/1 step: 530, time: 0.038, speed: 26.442 steps/s
	current lr: 0.0000053
	lm_loss: 4.5022, ppl: 90.2135, loss: 4.5022
[train][1] progress: 1/1 step: 531, time: 0.038, speed: 26.396 steps/s
	current lr: 0.0000053
	lm_loss: 4.0962, ppl: 60.1104, loss: 4.0962
[train][1] progress: 1/1 step: 532, time: 0.039, speed: 25.760 steps/s
	current lr: 0.0000053
	lm_loss: 4.9759, ppl: 144.8739, loss: 4.9759
[train][1] progress: 1/1 step: 533, time: 0.039, speed: 25.970 steps/s
	current lr: 0.0000053
	lm_loss: 5.5082, ppl: 246.7127, loss: 5.5082
[train][1] progress: 1/1 step: 534, time: 0.040, speed: 24.850 steps/s
	current lr: 0.0000053
	lm_loss: 5.6372, ppl: 280.6729, loss: 5.6372
[train][1] progress: 1/1 step: 535, time: 0.041, speed: 24.519 steps/s
	current lr: 0.0000054
	lm_loss: 4.5568, ppl: 95.2824, loss: 4.5568
[train][1] progress: 1/1 step: 536, time: 0.039, speed: 25.855 steps/s
	current lr: 0.0000054
	lm_loss: 5.1703, ppl: 175.9632, loss: 5.1703
[train][1] progress: 1/1 step: 537, time: 0.043, speed: 23.197 steps/s
	current lr: 0.0000054
	lm_loss: 4.0415, ppl: 56.9125, loss: 4.0415
[train][1] progress: 1/1 step: 538, time: 0.040, speed: 24.943 steps/s
	current lr: 0.0000054
	lm_loss: 3.6203, ppl: 37.3499, loss: 3.6203
[train][1] progress: 1/1 step: 539, time: 0.040, speed: 24.859 steps/s
	current lr: 0.0000054
	lm_loss: 5.6418, ppl: 281.9702, loss: 5.6418
[train][1] progress: 1/1 step: 540, time: 0.043, speed: 23.287 steps/s
	current lr: 0.0000054
	lm_loss: 4.4271, ppl: 83.6851, loss: 4.4271
[train][1] progress: 1/1 step: 541, time: 0.043, speed: 23.378 steps/s
	current lr: 0.0000054
	lm_loss: 5.0273, ppl: 152.5198, loss: 5.0273
[train][1] progress: 1/1 step: 542, time: 0.040, speed: 24.982 steps/s
	current lr: 0.0000054
	lm_loss: 6.0699, ppl: 432.6199, loss: 6.0699
[train][1] progress: 1/1 step: 543, time: 0.058, speed: 17.308 steps/s
	current lr: 0.0000054
	lm_loss: 5.1655, ppl: 175.1320, loss: 5.1655
[train][1] progress: 1/1 step: 544, time: 0.059, speed: 16.954 steps/s
	current lr: 0.0000054
	lm_loss: 4.9574, ppl: 142.2266, loss: 4.9574
[train][1] progress: 1/1 step: 545, time: 0.052, speed: 19.088 steps/s
	current lr: 0.0000055
	lm_loss: 5.7772, ppl: 322.8475, loss: 5.7772
[train][1] progress: 1/1 step: 546, time: 0.042, speed: 24.045 steps/s
	current lr: 0.0000055
	lm_loss: 4.6163, ppl: 101.1221, loss: 4.6163
[train][1] progress: 1/1 step: 547, time: 0.039, speed: 25.834 steps/s
	current lr: 0.0000055
	lm_loss: 3.9451, ppl: 51.6816, loss: 3.9451
[train][1] progress: 1/1 step: 548, time: 0.037, speed: 26.996 steps/s
	current lr: 0.0000055
	lm_loss: 4.7992, ppl: 121.4136, loss: 4.7992
[train][1] progress: 1/1 step: 549, time: 0.038, speed: 26.574 steps/s
	current lr: 0.0000055
	lm_loss: 5.4751, ppl: 238.6778, loss: 5.4751
[train][1] progress: 1/1 step: 550, time: 0.039, speed: 25.755 steps/s
	current lr: 0.0000055
	lm_loss: 5.8345, ppl: 341.8886, loss: 5.8345
[train][1] progress: 1/1 step: 551, time: 0.038, speed: 26.315 steps/s
	current lr: 0.0000055
	lm_loss: 5.2868, ppl: 197.7082, loss: 5.2868
[train][1] progress: 1/1 step: 552, time: 0.039, speed: 25.953 steps/s
	current lr: 0.0000055
	lm_loss: 5.2376, ppl: 188.2219, loss: 5.2376
[train][1] progress: 1/1 step: 553, time: 0.037, speed: 26.706 steps/s
	current lr: 0.0000055
	lm_loss: 4.8108, ppl: 122.8326, loss: 4.8108
[train][1] progress: 1/1 step: 554, time: 0.038, speed: 26.173 steps/s
	current lr: 0.0000055
	lm_loss: 4.1931, ppl: 66.2298, loss: 4.1931
[train][1] progress: 1/1 step: 555, time: 0.038, speed: 26.147 steps/s
	current lr: 0.0000055
	lm_loss: 5.5402, ppl: 254.7262, loss: 5.5402
[train][1] progress: 1/1 step: 556, time: 0.037, speed: 26.703 steps/s
	current lr: 0.0000056
	lm_loss: 6.0480, ppl: 423.2476, loss: 6.0480
[train][1] progress: 1/1 step: 557, time: 0.037, speed: 26.819 steps/s
	current lr: 0.0000056
	lm_loss: 4.8582, ppl: 128.7876, loss: 4.8582
[train][1] progress: 1/1 step: 558, time: 0.036, speed: 27.751 steps/s
	current lr: 0.0000056
	lm_loss: 4.5011, ppl: 90.1145, loss: 4.5011
[train][1] progress: 1/1 step: 559, time: 0.037, speed: 27.377 steps/s
	current lr: 0.0000056
	lm_loss: 5.0782, ppl: 160.4880, loss: 5.0782
[train][1] progress: 1/1 step: 560, time: 0.037, speed: 26.831 steps/s
	current lr: 0.0000056
	lm_loss: 4.7700, ppl: 117.9223, loss: 4.7700
[train][1] progress: 1/1 step: 561, time: 0.036, speed: 27.644 steps/s
	current lr: 0.0000056
	lm_loss: 3.4471, ppl: 31.4086, loss: 3.4471
[train][1] progress: 1/1 step: 562, time: 0.037, speed: 27.011 steps/s
	current lr: 0.0000056
	lm_loss: 4.1340, ppl: 62.4292, loss: 4.1340
[train][1] progress: 1/1 step: 563, time: 0.036, speed: 27.627 steps/s
	current lr: 0.0000056
	lm_loss: 3.2483, ppl: 25.7456, loss: 3.2483
[train][1] progress: 1/1 step: 564, time: 0.036, speed: 27.733 steps/s
	current lr: 0.0000056
	lm_loss: 5.0385, ppl: 154.2340, loss: 5.0385
[train][1] progress: 1/1 step: 565, time: 0.035, speed: 28.637 steps/s
	current lr: 0.0000057
	lm_loss: 6.5542, ppl: 702.1967, loss: 6.5542
[train][1] progress: 1/1 step: 566, time: 0.035, speed: 28.359 steps/s
	current lr: 0.0000057
	lm_loss: 5.7743, ppl: 321.9081, loss: 5.7743
[train][1] progress: 1/1 step: 567, time: 0.035, speed: 28.537 steps/s
	current lr: 0.0000057
	lm_loss: 4.8475, ppl: 127.4257, loss: 4.8475
[train][1] progress: 1/1 step: 568, time: 0.036, speed: 28.083 steps/s
	current lr: 0.0000057
	lm_loss: 3.7700, ppl: 43.3782, loss: 3.7700
[train][1] progress: 1/1 step: 569, time: 0.036, speed: 27.729 steps/s
	current lr: 0.0000057
	lm_loss: 6.5150, ppl: 675.2261, loss: 6.5150
[train][1] progress: 1/1 step: 570, time: 0.036, speed: 27.582 steps/s
	current lr: 0.0000057
	lm_loss: 5.3130, ppl: 202.9512, loss: 5.3130
[train][1] progress: 1/1 step: 571, time: 0.037, speed: 27.181 steps/s
	current lr: 0.0000057
	lm_loss: 5.8883, ppl: 360.8029, loss: 5.8883
[train][1] progress: 1/1 step: 572, time: 0.036, speed: 27.402 steps/s
	current lr: 0.0000057
	lm_loss: 5.1583, ppl: 173.8682, loss: 5.1583
[train][1] progress: 1/1 step: 573, time: 0.037, speed: 27.227 steps/s
	current lr: 0.0000057
	lm_loss: 5.1059, ppl: 164.9877, loss: 5.1059
[train][1] progress: 1/1 step: 574, time: 0.036, speed: 27.836 steps/s
	current lr: 0.0000057
	lm_loss: 6.1531, ppl: 470.1946, loss: 6.1531
[train][1] progress: 1/1 step: 575, time: 0.035, speed: 28.876 steps/s
	current lr: 0.0000058
	lm_loss: 4.6406, ppl: 103.6067, loss: 4.6406
[train][1] progress: 1/1 step: 576, time: 0.034, speed: 29.282 steps/s
	current lr: 0.0000058
	lm_loss: 6.0437, ppl: 421.4671, loss: 6.0437
[train][1] progress: 1/1 step: 577, time: 0.034, speed: 29.392 steps/s
	current lr: 0.0000058
	lm_loss: 5.9006, ppl: 365.2558, loss: 5.9006
[train][1] progress: 1/1 step: 578, time: 0.034, speed: 29.574 steps/s
	current lr: 0.0000058
	lm_loss: 5.6370, ppl: 280.6206, loss: 5.6370
[train][1] progress: 1/1 step: 579, time: 0.034, speed: 29.249 steps/s
	current lr: 0.0000058
	lm_loss: 5.0041, ppl: 149.0246, loss: 5.0041
[train][1] progress: 1/1 step: 580, time: 0.034, speed: 29.772 steps/s
	current lr: 0.0000058
	lm_loss: 5.6838, ppl: 294.0546, loss: 5.6838
[train][1] progress: 1/1 step: 581, time: 0.032, speed: 31.045 steps/s
	current lr: 0.0000058
	lm_loss: 3.5893, ppl: 36.2071, loss: 3.5893
[train][1] progress: 1/1 step: 582, time: 0.031, speed: 31.782 steps/s
	current lr: 0.0000058
	lm_loss: 6.1928, ppl: 489.2317, loss: 6.1928
[train][1] progress: 1/1 step: 583, time: 0.032, speed: 31.573 steps/s
	current lr: 0.0000058
	lm_loss: 5.4003, ppl: 221.4667, loss: 5.4003
[train][1] progress: 1/1 step: 584, time: 0.032, speed: 31.453 steps/s
	current lr: 0.0000058
	lm_loss: 4.8606, ppl: 129.1035, loss: 4.8606
[train][1] progress: 1/1 step: 585, time: 0.031, speed: 31.883 steps/s
	current lr: 0.0000059
	lm_loss: 5.3594, ppl: 212.5891, loss: 5.3594
[train][1] progress: 1/1 step: 586, time: 0.031, speed: 31.905 steps/s
	current lr: 0.0000059
	lm_loss: 5.1878, ppl: 179.0805, loss: 5.1878
[train][1] progress: 1/1 step: 587, time: 0.032, speed: 30.890 steps/s
	current lr: 0.0000059
	lm_loss: 5.2157, ppl: 184.1386, loss: 5.2157
[train][1] progress: 1/1 step: 588, time: 0.033, speed: 30.423 steps/s
	current lr: 0.0000059
	lm_loss: 4.8967, ppl: 133.8448, loss: 4.8967
[train][1] progress: 1/1 step: 589, time: 0.034, speed: 29.457 steps/s
	current lr: 0.0000059
	lm_loss: 5.3614, ppl: 213.0123, loss: 5.3614
[train][1] progress: 1/1 step: 590, time: 0.034, speed: 29.655 steps/s
	current lr: 0.0000059
	lm_loss: 4.6852, ppl: 108.3308, loss: 4.6852
[train][1] progress: 1/1 step: 591, time: 0.033, speed: 30.247 steps/s
	current lr: 0.0000059
	lm_loss: 4.8951, ppl: 133.6373, loss: 4.8951
[train][1] progress: 1/1 step: 592, time: 0.033, speed: 30.712 steps/s
	current lr: 0.0000059
	lm_loss: 4.8555, ppl: 128.4386, loss: 4.8555
[train][1] progress: 1/1 step: 593, time: 0.032, speed: 31.047 steps/s
	current lr: 0.0000059
	lm_loss: 3.3002, ppl: 27.1180, loss: 3.3002
[train][1] progress: 1/1 step: 594, time: 0.032, speed: 31.017 steps/s
	current lr: 0.0000059
	lm_loss: 4.4326, ppl: 84.1533, loss: 4.4326
[train][1] progress: 1/1 step: 595, time: 0.032, speed: 31.475 steps/s
	current lr: 0.0000060
	lm_loss: 4.7826, ppl: 119.4098, loss: 4.7826
[train][1] progress: 1/1 step: 596, time: 0.032, speed: 31.088 steps/s
	current lr: 0.0000060
	lm_loss: 5.6029, ppl: 271.2179, loss: 5.6029
[train][1] progress: 1/1 step: 597, time: 0.032, speed: 31.634 steps/s
	current lr: 0.0000060
	lm_loss: 6.1028, ppl: 447.1030, loss: 6.1028
[train][1] progress: 1/1 step: 598, time: 0.033, speed: 30.226 steps/s
	current lr: 0.0000060
	lm_loss: 5.6598, ppl: 287.0938, loss: 5.6598
[train][1] progress: 1/1 step: 599, time: 0.033, speed: 30.396 steps/s
	current lr: 0.0000060
	lm_loss: 6.8668, ppl: 959.8788, loss: 6.8668
[train][1] progress: 1/1 step: 600, time: 0.032, speed: 30.998 steps/s
	current lr: 0.0000060
	lm_loss: 4.6785, ppl: 107.6058, loss: 4.6785
[train][1] progress: 1/1 step: 601, time: 0.032, speed: 30.792 steps/s
	current lr: 0.0000060
	lm_loss: 4.2369, ppl: 69.1954, loss: 4.2369
[train][1] progress: 1/1 step: 602, time: 0.033, speed: 30.678 steps/s
	current lr: 0.0000060
	lm_loss: 4.9548, ppl: 141.8500, loss: 4.9548
[train][1] progress: 1/1 step: 603, time: 0.033, speed: 30.631 steps/s
	current lr: 0.0000060
	lm_loss: 5.7679, ppl: 319.8652, loss: 5.7679
[train][1] progress: 1/1 step: 604, time: 0.032, speed: 30.848 steps/s
	current lr: 0.0000060
	lm_loss: 6.5482, ppl: 697.9711, loss: 6.5482
[train][1] progress: 1/1 step: 605, time: 0.032, speed: 31.325 steps/s
	current lr: 0.0000061
	lm_loss: 4.3142, ppl: 74.7560, loss: 4.3142
[train][1] progress: 1/1 step: 606, time: 0.032, speed: 31.368 steps/s
	current lr: 0.0000061
	lm_loss: 6.0018, ppl: 404.1531, loss: 6.0018
[train][1] progress: 1/1 step: 607, time: 0.031, speed: 32.193 steps/s
	current lr: 0.0000061
	lm_loss: 3.4276, ppl: 30.8038, loss: 3.4276
[train][1] progress: 1/1 step: 608, time: 0.031, speed: 32.084 steps/s
	current lr: 0.0000061
	lm_loss: 4.5818, ppl: 97.6905, loss: 4.5818
[train][1] progress: 1/1 step: 609, time: 0.030, speed: 32.984 steps/s
	current lr: 0.0000061
	lm_loss: 5.1833, ppl: 178.2663, loss: 5.1833
[train][1] progress: 1/1 step: 610, time: 0.031, speed: 32.538 steps/s
	current lr: 0.0000061
	lm_loss: 5.0342, ppl: 153.5736, loss: 5.0342
[train][1] progress: 1/1 step: 611, time: 0.032, speed: 31.250 steps/s
	current lr: 0.0000061
	lm_loss: 5.1582, ppl: 173.8457, loss: 5.1582
[train][1] progress: 1/1 step: 612, time: 0.031, speed: 31.810 steps/s
	current lr: 0.0000061
	lm_loss: 5.4947, ppl: 243.4073, loss: 5.4947
[train][1] progress: 1/1 step: 613, time: 0.031, speed: 31.830 steps/s
	current lr: 0.0000061
	lm_loss: 4.3682, ppl: 78.9030, loss: 4.3682
[train][1] progress: 1/1 step: 614, time: 0.031, speed: 32.572 steps/s
	current lr: 0.0000061
	lm_loss: 5.1940, ppl: 180.1945, loss: 5.1940
[train][1] progress: 1/1 step: 615, time: 0.030, speed: 32.857 steps/s
	current lr: 0.0000061
	lm_loss: 4.2876, ppl: 72.7903, loss: 4.2876
[train][1] progress: 1/1 step: 616, time: 0.030, speed: 33.060 steps/s
	current lr: 0.0000062
	lm_loss: 5.0510, ppl: 156.1860, loss: 5.0510
[train][1] progress: 1/1 step: 617, time: 0.030, speed: 32.974 steps/s
	current lr: 0.0000062
	lm_loss: 4.8084, ppl: 122.5323, loss: 4.8084
[train][1] progress: 1/1 step: 618, time: 0.031, speed: 32.357 steps/s
	current lr: 0.0000062
	lm_loss: 5.1365, ppl: 170.1187, loss: 5.1365
[train][1] progress: 1/1 step: 619, time: 0.030, speed: 32.928 steps/s
	current lr: 0.0000062
	lm_loss: 5.4157, ppl: 224.9007, loss: 5.4157
[train][1] progress: 1/1 step: 620, time: 0.030, speed: 33.058 steps/s
	current lr: 0.0000062
	lm_loss: 4.6798, ppl: 107.7476, loss: 4.6798
[train][1] progress: 1/1 step: 621, time: 0.031, speed: 32.500 steps/s
	current lr: 0.0000062
	lm_loss: 5.3682, ppl: 214.4773, loss: 5.3682
[train][1] progress: 1/1 step: 622, time: 0.030, speed: 32.883 steps/s
	current lr: 0.0000062
	lm_loss: 4.1390, ppl: 62.7421, loss: 4.1390
[train][1] progress: 1/1 step: 623, time: 0.030, speed: 33.149 steps/s
	current lr: 0.0000062
	lm_loss: 5.2660, ppl: 193.6428, loss: 5.2660
[train][1] progress: 1/1 step: 624, time: 0.030, speed: 33.325 steps/s
	current lr: 0.0000062
	lm_loss: 5.2179, ppl: 184.5392, loss: 5.2179
[train][1] progress: 1/1 step: 625, time: 0.030, speed: 33.341 steps/s
	current lr: 0.0000063
	lm_loss: 4.8428, ppl: 126.8190, loss: 4.8428
[train][1] progress: 1/1 step: 626, time: 0.030, speed: 32.942 steps/s
	current lr: 0.0000063
	lm_loss: 3.8712, ppl: 47.9995, loss: 3.8712
[train][1] progress: 1/1 step: 627, time: 0.030, speed: 33.333 steps/s
	current lr: 0.0000063
	lm_loss: 5.9754, ppl: 393.6326, loss: 5.9754
[train][1] progress: 1/1 step: 628, time: 0.030, speed: 33.330 steps/s
	current lr: 0.0000063
	lm_loss: 4.3069, ppl: 74.2103, loss: 4.3069
[train][1] progress: 1/1 step: 629, time: 0.030, speed: 33.055 steps/s
	current lr: 0.0000063
	lm_loss: 6.0768, ppl: 435.6352, loss: 6.0768
[train][1] progress: 1/1 step: 630, time: 0.030, speed: 33.100 steps/s
	current lr: 0.0000063
	lm_loss: 5.1446, ppl: 171.5047, loss: 5.1446
[train][1] progress: 1/1 step: 631, time: 0.030, speed: 33.419 steps/s
	current lr: 0.0000063
	lm_loss: 3.5309, ppl: 34.1548, loss: 3.5309
[train][1] progress: 1/1 step: 632, time: 0.030, speed: 33.411 steps/s
	current lr: 0.0000063
	lm_loss: 3.8704, ppl: 47.9620, loss: 3.8704
[train][1] progress: 1/1 step: 633, time: 0.030, speed: 33.144 steps/s
	current lr: 0.0000063
	lm_loss: 3.5353, ppl: 34.3066, loss: 3.5353
[train][1] progress: 1/1 step: 634, time: 0.031, speed: 32.402 steps/s
	current lr: 0.0000063
	lm_loss: 4.8878, ppl: 132.6679, loss: 4.8878
[train][1] progress: 1/1 step: 635, time: 0.033, speed: 30.765 steps/s
	current lr: 0.0000064
	lm_loss: 5.7492, ppl: 313.9409, loss: 5.7492
[train][1] progress: 1/1 step: 636, time: 0.031, speed: 31.834 steps/s
	current lr: 0.0000064
	lm_loss: 4.3628, ppl: 78.4729, loss: 4.3628
[train][1] progress: 1/1 step: 637, time: 0.031, speed: 32.247 steps/s
	current lr: 0.0000064
	lm_loss: 4.5084, ppl: 90.7762, loss: 4.5084
[train][1] progress: 1/1 step: 638, time: 0.033, speed: 29.997 steps/s
	current lr: 0.0000064
	lm_loss: 4.5650, ppl: 96.0624, loss: 4.5650
[train][1] progress: 1/1 step: 639, time: 0.033, speed: 30.533 steps/s
	current lr: 0.0000064
	lm_loss: 4.3810, ppl: 79.9159, loss: 4.3810
[train][1] progress: 1/1 step: 640, time: 0.033, speed: 29.969 steps/s
	current lr: 0.0000064
	lm_loss: 3.3419, ppl: 28.2727, loss: 3.3419
[train][1] progress: 1/1 step: 641, time: 0.034, speed: 29.785 steps/s
	current lr: 0.0000064
	lm_loss: 4.3866, ppl: 80.3651, loss: 4.3866
[train][1] progress: 1/1 step: 642, time: 0.033, speed: 30.504 steps/s
	current lr: 0.0000064
	lm_loss: 4.1754, ppl: 65.0639, loss: 4.1754
[train][1] progress: 1/1 step: 643, time: 0.032, speed: 31.392 steps/s
	current lr: 0.0000064
	lm_loss: 5.2953, ppl: 199.3964, loss: 5.2953
[train][1] progress: 1/1 step: 644, time: 0.032, speed: 31.610 steps/s
	current lr: 0.0000064
	lm_loss: 4.4105, ppl: 82.3144, loss: 4.4105
[train][1] progress: 1/1 step: 645, time: 0.032, speed: 31.315 steps/s
	current lr: 0.0000065
	lm_loss: 4.8192, ppl: 123.8664, loss: 4.8192
[train][1] progress: 1/1 step: 646, time: 0.033, speed: 30.521 steps/s
	current lr: 0.0000065
	lm_loss: 3.3927, ppl: 29.7462, loss: 3.3927
[train][1] progress: 1/1 step: 647, time: 0.033, speed: 30.654 steps/s
	current lr: 0.0000065
	lm_loss: 4.8634, ppl: 129.4678, loss: 4.8634
[train][1] progress: 1/1 step: 648, time: 0.031, speed: 31.762 steps/s
	current lr: 0.0000065
	lm_loss: 5.3604, ppl: 212.8176, loss: 5.3604
[train][1] progress: 1/1 step: 649, time: 0.032, speed: 31.017 steps/s
	current lr: 0.0000065
	lm_loss: 4.6664, ppl: 106.3109, loss: 4.6664
[train][1] progress: 1/1 step: 650, time: 0.033, speed: 30.210 steps/s
	current lr: 0.0000065
	lm_loss: 6.3722, ppl: 585.3202, loss: 6.3722
[train][1] progress: 1/1 step: 651, time: 0.033, speed: 29.871 steps/s
	current lr: 0.0000065
	lm_loss: 4.9367, ppl: 139.3107, loss: 4.9367
[train][1] progress: 1/1 step: 652, time: 0.033, speed: 29.878 steps/s
	current lr: 0.0000065
	lm_loss: 5.4625, ppl: 235.6929, loss: 5.4625
[train][1] progress: 1/1 step: 653, time: 0.034, speed: 29.143 steps/s
	current lr: 0.0000065
	lm_loss: 5.1727, ppl: 176.3930, loss: 5.1727
[train][1] progress: 1/1 step: 654, time: 0.034, speed: 29.758 steps/s
	current lr: 0.0000065
	lm_loss: 4.0447, ppl: 57.0941, loss: 4.0447
[train][1] progress: 1/1 step: 655, time: 0.032, speed: 31.209 steps/s
	current lr: 0.0000066
	lm_loss: 5.1289, ppl: 168.8314, loss: 5.1289
[train][1] progress: 1/1 step: 656, time: 0.031, speed: 32.388 steps/s
	current lr: 0.0000066
	lm_loss: 5.2912, ppl: 198.5736, loss: 5.2912
[train][1] progress: 1/1 step: 657, time: 0.031, speed: 31.911 steps/s
	current lr: 0.0000066
	lm_loss: 5.1664, ppl: 175.2894, loss: 5.1664
[train][1] progress: 1/1 step: 658, time: 0.032, speed: 31.390 steps/s
	current lr: 0.0000066
	lm_loss: 4.8925, ppl: 133.2922, loss: 4.8925
[train][1] progress: 1/1 step: 659, time: 0.032, speed: 31.611 steps/s
	current lr: 0.0000066
	lm_loss: 2.9983, ppl: 20.0505, loss: 2.9983
[train][1] progress: 1/1 step: 660, time: 0.031, speed: 32.047 steps/s
	current lr: 0.0000066
	lm_loss: 4.8643, ppl: 129.5760, loss: 4.8643
[train][1] progress: 1/1 step: 661, time: 0.031, speed: 32.579 steps/s
	current lr: 0.0000066
	lm_loss: 3.2958, ppl: 26.9986, loss: 3.2958
[train][1] progress: 1/1 step: 662, time: 0.030, speed: 32.853 steps/s
	current lr: 0.0000066
	lm_loss: 4.5416, ppl: 93.8370, loss: 4.5416
[train][1] progress: 1/1 step: 663, time: 0.030, speed: 33.224 steps/s
	current lr: 0.0000066
	lm_loss: 4.6146, ppl: 100.9451, loss: 4.6146
[train][1] progress: 1/1 step: 664, time: 0.030, speed: 32.871 steps/s
	current lr: 0.0000066
	lm_loss: 4.5247, ppl: 92.2696, loss: 4.5247
[train][1] progress: 1/1 step: 665, time: 0.030, speed: 32.859 steps/s
	current lr: 0.0000066
	lm_loss: 4.2167, ppl: 67.8076, loss: 4.2167
[train][1] progress: 1/1 step: 666, time: 0.031, speed: 32.255 steps/s
	current lr: 0.0000067
	lm_loss: 4.6052, ppl: 99.9990, loss: 4.6052
[train][1] progress: 1/1 step: 667, time: 0.033, speed: 29.859 steps/s
	current lr: 0.0000067
	lm_loss: 2.2503, ppl: 9.4902, loss: 2.2503
[train][1] progress: 1/1 step: 668, time: 0.032, speed: 31.280 steps/s
	current lr: 0.0000067
	lm_loss: 7.1975, ppl: 1336.1466, loss: 7.1975
[train][1] progress: 1/1 step: 669, time: 0.031, speed: 32.777 steps/s
	current lr: 0.0000067
	lm_loss: 4.2794, ppl: 72.1984, loss: 4.2794
[train][1] progress: 1/1 step: 670, time: 0.030, speed: 32.922 steps/s
	current lr: 0.0000067
	lm_loss: 4.7224, ppl: 112.4392, loss: 4.7224
[train][1] progress: 1/1 step: 671, time: 0.030, speed: 33.008 steps/s
	current lr: 0.0000067
	lm_loss: 4.9039, ppl: 134.8171, loss: 4.9039
[train][1] progress: 1/1 step: 672, time: 0.031, speed: 31.782 steps/s
	current lr: 0.0000067
	lm_loss: 4.1335, ppl: 62.3975, loss: 4.1335
[train][1] progress: 1/1 step: 673, time: 0.033, speed: 30.767 steps/s
	current lr: 0.0000067
	lm_loss: 3.7530, ppl: 42.6495, loss: 3.7530
[train][1] progress: 1/1 step: 674, time: 0.032, speed: 30.893 steps/s
	current lr: 0.0000067
	lm_loss: 5.1632, ppl: 174.7278, loss: 5.1632
[train][1] progress: 1/1 step: 675, time: 0.032, speed: 31.236 steps/s
	current lr: 0.0000068
	lm_loss: 4.7599, ppl: 116.7324, loss: 4.7599
[train][1] progress: 1/1 step: 676, time: 0.032, speed: 30.775 steps/s
	current lr: 0.0000068
	lm_loss: 5.8573, ppl: 349.7909, loss: 5.8573
[train][1] progress: 1/1 step: 677, time: 0.034, speed: 29.237 steps/s
	current lr: 0.0000068
	lm_loss: 5.2325, ppl: 187.2546, loss: 5.2325
[train][1] progress: 1/1 step: 678, time: 0.034, speed: 29.212 steps/s
	current lr: 0.0000068
	lm_loss: 4.9789, ppl: 145.3150, loss: 4.9789
[train][1] progress: 1/1 step: 679, time: 0.034, speed: 29.668 steps/s
	current lr: 0.0000068
	lm_loss: 4.0718, ppl: 58.6612, loss: 4.0718
[train][1] progress: 1/1 step: 680, time: 0.038, speed: 26.535 steps/s
	current lr: 0.0000068
	lm_loss: 5.1425, ppl: 171.1422, loss: 5.1425
[train][1] progress: 1/1 step: 681, time: 0.039, speed: 25.452 steps/s
	current lr: 0.0000068
	lm_loss: 5.2469, ppl: 189.9704, loss: 5.2469
[train][1] progress: 1/1 step: 682, time: 0.039, speed: 25.340 steps/s
	current lr: 0.0000068
	lm_loss: 4.4529, ppl: 85.8721, loss: 4.4529
[train][1] progress: 1/1 step: 683, time: 0.039, speed: 25.534 steps/s
	current lr: 0.0000068
	lm_loss: 5.5176, ppl: 249.0307, loss: 5.5176
[train][1] progress: 1/1 step: 684, time: 0.039, speed: 25.380 steps/s
	current lr: 0.0000068
	lm_loss: 5.9697, ppl: 391.4014, loss: 5.9697
[train][1] progress: 1/1 step: 685, time: 0.038, speed: 26.198 steps/s
	current lr: 0.0000068
	lm_loss: 5.5232, ppl: 250.4452, loss: 5.5232
[train][1] progress: 1/1 step: 686, time: 0.039, speed: 25.665 steps/s
	current lr: 0.0000069
	lm_loss: 4.9672, ppl: 143.6283, loss: 4.9672
[train][1] progress: 1/1 step: 687, time: 0.038, speed: 26.241 steps/s
	current lr: 0.0000069
	lm_loss: 5.5730, ppl: 263.2324, loss: 5.5730
[train][1] progress: 1/1 step: 688, time: 0.037, speed: 27.345 steps/s
	current lr: 0.0000069
	lm_loss: 5.2465, ppl: 189.8967, loss: 5.2465
[train][1] progress: 1/1 step: 689, time: 0.035, speed: 28.218 steps/s
	current lr: 0.0000069
	lm_loss: 4.2733, ppl: 71.7567, loss: 4.2733
[train][1] progress: 1/1 step: 690, time: 0.035, speed: 28.683 steps/s
	current lr: 0.0000069
	lm_loss: 4.1732, ppl: 64.9239, loss: 4.1732
[train][1] progress: 1/1 step: 691, time: 0.036, speed: 28.071 steps/s
	current lr: 0.0000069
	lm_loss: 4.0374, ppl: 56.6810, loss: 4.0374
[train][1] progress: 1/1 step: 692, time: 0.035, speed: 28.196 steps/s
	current lr: 0.0000069
	lm_loss: 2.4290, ppl: 11.3477, loss: 2.4290
[train][1] progress: 1/1 step: 693, time: 0.035, speed: 28.627 steps/s
	current lr: 0.0000069
	lm_loss: 4.0921, ppl: 59.8655, loss: 4.0921
[train][1] progress: 1/1 step: 694, time: 0.036, speed: 27.540 steps/s
	current lr: 0.0000069
	lm_loss: 5.0187, ppl: 151.2081, loss: 5.0187
[train][1] progress: 1/1 step: 695, time: 0.039, speed: 25.448 steps/s
	current lr: 0.0000070
	lm_loss: 4.9262, ppl: 137.8508, loss: 4.9262
[train][1] progress: 1/1 step: 696, time: 0.037, speed: 27.105 steps/s
	current lr: 0.0000070
	lm_loss: 3.9668, ppl: 52.8178, loss: 3.9668
[train][1] progress: 1/1 step: 697, time: 0.036, speed: 27.812 steps/s
	current lr: 0.0000070
	lm_loss: 4.8729, ppl: 130.7046, loss: 4.8729
[train][1] progress: 1/1 step: 698, time: 0.036, speed: 27.823 steps/s
	current lr: 0.0000070
	lm_loss: 4.9079, ppl: 135.3558, loss: 4.9079
[train][1] progress: 1/1 step: 699, time: 0.036, speed: 27.996 steps/s
	current lr: 0.0000070
	lm_loss: 4.3133, ppl: 74.6833, loss: 4.3133
[train][1] progress: 1/1 step: 700, time: 0.035, speed: 28.724 steps/s
	current lr: 0.0000070
	lm_loss: 4.3618, ppl: 78.3968, loss: 4.3618
[train][1] progress: 1/1 step: 701, time: 0.035, speed: 28.819 steps/s
	current lr: 0.0000070
	lm_loss: 4.2408, ppl: 69.4661, loss: 4.2408
[train][1] progress: 1/1 step: 702, time: 0.035, speed: 28.685 steps/s
	current lr: 0.0000070
	lm_loss: 5.3649, ppl: 213.7600, loss: 5.3649
[train][1] progress: 1/1 step: 703, time: 0.038, speed: 26.455 steps/s
	current lr: 0.0000070
	lm_loss: 3.8925, ppl: 49.0322, loss: 3.8925
[train][1] progress: 1/1 step: 704, time: 0.038, speed: 26.172 steps/s
	current lr: 0.0000070
	lm_loss: 6.1088, ppl: 449.7890, loss: 6.1088
[train][1] progress: 1/1 step: 705, time: 0.038, speed: 26.406 steps/s
	current lr: 0.0000071
	lm_loss: 5.6842, ppl: 294.1968, loss: 5.6842
[train][1] progress: 1/1 step: 706, time: 0.037, speed: 27.012 steps/s
	current lr: 0.0000071
	lm_loss: 5.5833, ppl: 265.9396, loss: 5.5833
[train][1] progress: 1/1 step: 707, time: 0.036, speed: 27.810 steps/s
	current lr: 0.0000071
	lm_loss: 4.1288, ppl: 62.1047, loss: 4.1288
[train][1] progress: 1/1 step: 708, time: 0.036, speed: 28.124 steps/s
	current lr: 0.0000071
	lm_loss: 4.7272, ppl: 112.9836, loss: 4.7272
[train][1] progress: 1/1 step: 709, time: 0.035, speed: 28.335 steps/s
	current lr: 0.0000071
	lm_loss: 3.7414, ppl: 42.1560, loss: 3.7414
[train][1] progress: 1/1 step: 710, time: 0.036, speed: 28.028 steps/s
	current lr: 0.0000071
	lm_loss: 5.3347, ppl: 207.4062, loss: 5.3347
[train][1] progress: 1/1 step: 711, time: 0.035, speed: 28.448 steps/s
	current lr: 0.0000071
	lm_loss: 4.7443, ppl: 114.9229, loss: 4.7443
[train][1] progress: 1/1 step: 712, time: 0.035, speed: 28.582 steps/s
	current lr: 0.0000071
	lm_loss: 4.8846, ppl: 132.2363, loss: 4.8846
[train][1] progress: 1/1 step: 713, time: 0.035, speed: 28.460 steps/s
	current lr: 0.0000071
	lm_loss: 4.9858, ppl: 146.3274, loss: 4.9858
[train][1] progress: 1/1 step: 714, time: 0.035, speed: 28.301 steps/s
	current lr: 0.0000071
	lm_loss: 4.5940, ppl: 98.8923, loss: 4.5940
[train][1] progress: 1/1 step: 715, time: 0.035, speed: 28.314 steps/s
	current lr: 0.0000072
	lm_loss: 4.3708, ppl: 79.1066, loss: 4.3708
[train][1] progress: 1/1 step: 716, time: 0.037, speed: 27.166 steps/s
	current lr: 0.0000072
	lm_loss: 4.6520, ppl: 104.7959, loss: 4.6520
[train][1] progress: 1/1 step: 717, time: 0.036, speed: 27.398 steps/s
	current lr: 0.0000072
	lm_loss: 5.3369, ppl: 207.8669, loss: 5.3369
[train][1] progress: 1/1 step: 718, time: 0.036, speed: 27.961 steps/s
	current lr: 0.0000072
	lm_loss: 4.9548, ppl: 141.8474, loss: 4.9548
[train][1] progress: 1/1 step: 719, time: 0.036, speed: 28.132 steps/s
	current lr: 0.0000072
	lm_loss: 1.5759, ppl: 4.8352, loss: 1.5759
[train][1] progress: 1/1 step: 720, time: 0.035, speed: 28.594 steps/s
	current lr: 0.0000072
	lm_loss: 4.9656, ppl: 143.3984, loss: 4.9656
[train][1] progress: 1/1 step: 721, time: 0.038, speed: 26.592 steps/s
	current lr: 0.0000072
	lm_loss: 4.9087, ppl: 135.4667, loss: 4.9087
[train][1] progress: 1/1 step: 722, time: 0.038, speed: 26.263 steps/s
	current lr: 0.0000072
	lm_loss: 4.0821, ppl: 59.2722, loss: 4.0821
[train][1] progress: 1/1 step: 723, time: 0.038, speed: 26.064 steps/s
	current lr: 0.0000072
	lm_loss: 4.8442, ppl: 127.0035, loss: 4.8442
[train][1] progress: 1/1 step: 724, time: 0.039, speed: 25.898 steps/s
	current lr: 0.0000072
	lm_loss: 4.3039, ppl: 73.9865, loss: 4.3039
[train][1] progress: 1/1 step: 725, time: 0.037, speed: 26.725 steps/s
	current lr: 0.0000072
	lm_loss: 4.1621, ppl: 64.2058, loss: 4.1621
[train][1] progress: 1/1 step: 726, time: 0.036, speed: 27.779 steps/s
	current lr: 0.0000073
	lm_loss: 5.0720, ppl: 159.4863, loss: 5.0720
[train][1] progress: 1/1 step: 727, time: 0.035, speed: 28.374 steps/s
	current lr: 0.0000073
	lm_loss: 5.4356, ppl: 229.4274, loss: 5.4356
[train][1] progress: 1/1 step: 728, time: 0.035, speed: 28.757 steps/s
	current lr: 0.0000073
	lm_loss: 4.5582, ppl: 95.4096, loss: 4.5582
[train][1] progress: 1/1 step: 729, time: 0.035, speed: 28.897 steps/s
	current lr: 0.0000073
	lm_loss: 5.1502, ppl: 172.4744, loss: 5.1502
[train][1] progress: 1/1 step: 730, time: 0.035, speed: 28.614 steps/s
	current lr: 0.0000073
	lm_loss: 6.1817, ppl: 483.8132, loss: 6.1817
[train][1] progress: 1/1 step: 731, time: 0.035, speed: 28.570 steps/s
	current lr: 0.0000073
	lm_loss: 4.4842, ppl: 88.6090, loss: 4.4842
[train][1] progress: 1/1 step: 732, time: 0.035, speed: 28.938 steps/s
	current lr: 0.0000073
	lm_loss: 4.5184, ppl: 91.6876, loss: 4.5184
[train][1] progress: 1/1 step: 733, time: 0.035, speed: 28.637 steps/s
	current lr: 0.0000073
	lm_loss: 4.9274, ppl: 138.0226, loss: 4.9274
[train][1] progress: 1/1 step: 734, time: 0.035, speed: 28.567 steps/s
	current lr: 0.0000073
	lm_loss: 5.4132, ppl: 224.3557, loss: 5.4132
[train][1] progress: 1/1 step: 735, time: 0.035, speed: 28.910 steps/s
	current lr: 0.0000074
	lm_loss: 5.2911, ppl: 198.5596, loss: 5.2911
[train][1] progress: 1/1 step: 736, time: 0.036, speed: 27.698 steps/s
	current lr: 0.0000074
	lm_loss: 5.0007, ppl: 148.5101, loss: 5.0007
[train][1] progress: 1/1 step: 737, time: 0.035, speed: 28.410 steps/s
	current lr: 0.0000074
	lm_loss: 4.0689, ppl: 58.4931, loss: 4.0689
[train][1] progress: 1/1 step: 738, time: 0.035, speed: 28.356 steps/s
	current lr: 0.0000074
	lm_loss: 4.4298, ppl: 83.9125, loss: 4.4298
[train][1] progress: 1/1 step: 739, time: 0.036, speed: 28.055 steps/s
	current lr: 0.0000074
	lm_loss: 5.0521, ppl: 156.3543, loss: 5.0521
[train][1] progress: 1/1 step: 740, time: 0.035, speed: 28.314 steps/s
	current lr: 0.0000074
	lm_loss: 4.6226, ppl: 101.7616, loss: 4.6226
[train][1] progress: 1/1 step: 741, time: 0.035, speed: 28.516 steps/s
	current lr: 0.0000074
	lm_loss: 3.0867, ppl: 21.9051, loss: 3.0867
[train][1] progress: 1/1 step: 742, time: 0.035, speed: 28.245 steps/s
	current lr: 0.0000074
	lm_loss: 4.9966, ppl: 147.9142, loss: 4.9966
[train][1] progress: 1/1 step: 743, time: 0.035, speed: 28.196 steps/s
	current lr: 0.0000074
	lm_loss: 5.1748, ppl: 176.7613, loss: 5.1748
[train][1] progress: 1/1 step: 744, time: 0.035, speed: 28.369 steps/s
	current lr: 0.0000074
	lm_loss: 4.7508, ppl: 115.6775, loss: 4.7508
[train][1] progress: 1/1 step: 745, time: 0.035, speed: 28.467 steps/s
	current lr: 0.0000074
	lm_loss: 5.1089, ppl: 165.4837, loss: 5.1089
[train][1] progress: 1/1 step: 746, time: 0.035, speed: 28.508 steps/s
	current lr: 0.0000075
	lm_loss: 4.3926, ppl: 80.8528, loss: 4.3926
[train][1] progress: 1/1 step: 747, time: 0.030, speed: 32.814 steps/s
	current lr: 0.0000075
	lm_loss: 5.0492, ppl: 155.8915, loss: 5.0492
[train][1] progress: 1/1 step: 748, time: 0.031, speed: 32.072 steps/s
	current lr: 0.0000075
	lm_loss: 4.6428, ppl: 103.8299, loss: 4.6428
[train][1] progress: 1/1 step: 749, time: 0.031, speed: 32.166 steps/s
	current lr: 0.0000075
	lm_loss: 3.9186, ppl: 50.3305, loss: 3.9186
[train][1] progress: 1/1 step: 750, time: 0.031, speed: 32.033 steps/s
	current lr: 0.0000075
	lm_loss: 5.0097, ppl: 149.8647, loss: 5.0097
[train][1] progress: 1/1 step: 751, time: 0.031, speed: 31.755 steps/s
	current lr: 0.0000075
	lm_loss: 5.3861, ppl: 218.3602, loss: 5.3861
[train][1] progress: 1/1 step: 752, time: 0.032, speed: 31.652 steps/s
	current lr: 0.0000075
	lm_loss: 4.5394, ppl: 93.6330, loss: 4.5394
[train][1] progress: 1/1 step: 753, time: 0.031, speed: 31.856 steps/s
	current lr: 0.0000075
	lm_loss: 4.6011, ppl: 99.5935, loss: 4.6011
[train][1] progress: 1/1 step: 754, time: 0.031, speed: 31.865 steps/s
	current lr: 0.0000075
	lm_loss: 2.3887, ppl: 10.8991, loss: 2.3887
[train][1] progress: 1/1 step: 755, time: 0.031, speed: 32.320 steps/s
	current lr: 0.0000076
	lm_loss: 4.6564, ppl: 105.2615, loss: 4.6564
[train][1] progress: 1/1 step: 756, time: 0.031, speed: 32.114 steps/s
	current lr: 0.0000076
	lm_loss: 4.2569, ppl: 70.5904, loss: 4.2569
[train][1] progress: 1/1 step: 757, time: 0.036, speed: 27.846 steps/s
	current lr: 0.0000076
	lm_loss: 5.0805, ppl: 160.8473, loss: 5.0805
[train][1] progress: 1/1 step: 758, time: 0.039, speed: 25.414 steps/s
	current lr: 0.0000076
	lm_loss: 3.4707, ppl: 32.1597, loss: 3.4707
[train][1] progress: 1/1 step: 759, time: 0.037, speed: 27.230 steps/s
	current lr: 0.0000076
	lm_loss: 3.5507, ppl: 34.8366, loss: 3.5507
[train][1] progress: 1/1 step: 760, time: 0.034, speed: 29.448 steps/s
	current lr: 0.0000076
	lm_loss: 4.0097, ppl: 55.1311, loss: 4.0097
[train][1] progress: 1/1 step: 761, time: 0.032, speed: 31.360 steps/s
	current lr: 0.0000076
	lm_loss: 4.6663, ppl: 106.3082, loss: 4.6663
[train][1] progress: 1/1 step: 762, time: 0.031, speed: 32.366 steps/s
	current lr: 0.0000076
	lm_loss: 5.4694, ppl: 237.3064, loss: 5.4694
[train][1] progress: 1/1 step: 763, time: 0.031, speed: 32.164 steps/s
	current lr: 0.0000076
	lm_loss: 5.5661, ppl: 261.4181, loss: 5.5661
[train][1] progress: 1/1 step: 764, time: 0.031, speed: 32.456 steps/s
	current lr: 0.0000076
	lm_loss: 5.7000, ppl: 298.8789, loss: 5.7000
[train][1] progress: 1/1 step: 765, time: 0.031, speed: 32.352 steps/s
	current lr: 0.0000077
	lm_loss: 5.2279, ppl: 186.4069, loss: 5.2279
[train][1] progress: 1/1 step: 766, time: 0.032, speed: 31.564 steps/s
	current lr: 0.0000077
	lm_loss: 4.4854, ppl: 88.7114, loss: 4.4854
[train][1] progress: 1/1 step: 767, time: 0.031, speed: 32.262 steps/s
	current lr: 0.0000077
	lm_loss: 3.4847, ppl: 32.6115, loss: 3.4847
[train][1] progress: 1/1 step: 768, time: 0.032, speed: 31.008 steps/s
	current lr: 0.0000077
	lm_loss: 5.8853, ppl: 359.6963, loss: 5.8853
[train][1] progress: 1/1 step: 769, time: 0.035, speed: 28.893 steps/s
	current lr: 0.0000077
	lm_loss: 3.4147, ppl: 30.4082, loss: 3.4147
[train][1] progress: 1/1 step: 770, time: 0.033, speed: 30.736 steps/s
	current lr: 0.0000077
	lm_loss: 4.2649, ppl: 71.1567, loss: 4.2649
[train][1] progress: 1/1 step: 771, time: 0.032, speed: 31.545 steps/s
	current lr: 0.0000077
	lm_loss: 4.5418, ppl: 93.8623, loss: 4.5418
[train][1] progress: 1/1 step: 772, time: 0.031, speed: 32.239 steps/s
	current lr: 0.0000077
	lm_loss: 4.7215, ppl: 112.3421, loss: 4.7215
[train][1] progress: 1/1 step: 773, time: 0.031, speed: 32.383 steps/s
	current lr: 0.0000077
	lm_loss: 4.6451, ppl: 104.0728, loss: 4.6451
[train][1] progress: 1/1 step: 774, time: 0.031, speed: 32.063 steps/s
	current lr: 0.0000077
	lm_loss: 5.2156, ppl: 184.1196, loss: 5.2156
[train][1] progress: 1/1 step: 775, time: 0.031, speed: 32.107 steps/s
	current lr: 0.0000077
	lm_loss: 3.8976, ppl: 49.2861, loss: 3.8976
[train][1] progress: 1/1 step: 776, time: 0.032, speed: 31.513 steps/s
	current lr: 0.0000078
	lm_loss: 4.1024, ppl: 60.4877, loss: 4.1024
[train][1] progress: 1/1 step: 777, time: 0.032, speed: 31.214 steps/s
	current lr: 0.0000078
	lm_loss: 4.9033, ppl: 134.7306, loss: 4.9033
[train][1] progress: 1/1 step: 778, time: 0.032, speed: 31.708 steps/s
	current lr: 0.0000078
	lm_loss: 5.8243, ppl: 338.4094, loss: 5.8243
[train][1] progress: 1/1 step: 779, time: 0.031, speed: 32.220 steps/s
	current lr: 0.0000078
	lm_loss: 5.4161, ppl: 224.9965, loss: 5.4161
[train][1] progress: 1/1 step: 780, time: 0.031, speed: 32.748 steps/s
	current lr: 0.0000078
	lm_loss: 5.1757, ppl: 176.9132, loss: 5.1757
[train][1] progress: 1/1 step: 781, time: 0.032, speed: 30.934 steps/s
	current lr: 0.0000078
	lm_loss: 3.7708, ppl: 43.4127, loss: 3.7708
[train][1] progress: 1/1 step: 782, time: 0.037, speed: 27.034 steps/s
	current lr: 0.0000078
	lm_loss: 3.9050, ppl: 49.6495, loss: 3.9050
[train][1] progress: 1/1 step: 783, time: 0.035, speed: 28.205 steps/s
	current lr: 0.0000078
	lm_loss: 4.5388, ppl: 93.5798, loss: 4.5388
[train][1] progress: 1/1 step: 784, time: 0.036, speed: 27.492 steps/s
	current lr: 0.0000078
	lm_loss: 3.7456, ppl: 42.3346, loss: 3.7456
[train][1] progress: 1/1 step: 785, time: 0.036, speed: 27.874 steps/s
	current lr: 0.0000078
	lm_loss: 4.8361, ppl: 125.9742, loss: 4.8361
[train][1] progress: 1/1 step: 786, time: 0.035, speed: 28.416 steps/s
	current lr: 0.0000079
	lm_loss: 5.1186, ppl: 167.1048, loss: 5.1186
[train][1] progress: 1/1 step: 787, time: 0.036, speed: 27.433 steps/s
	current lr: 0.0000079
	lm_loss: 4.7109, ppl: 111.1497, loss: 4.7109
[train][1] progress: 1/1 step: 788, time: 0.036, speed: 27.919 steps/s
	current lr: 0.0000079
	lm_loss: 5.3147, ppl: 203.3057, loss: 5.3147
[train][1] progress: 1/1 step: 789, time: 0.037, speed: 26.953 steps/s
	current lr: 0.0000079
	lm_loss: 4.4615, ppl: 86.6166, loss: 4.4615
[train][1] progress: 1/1 step: 790, time: 0.038, speed: 26.234 steps/s
	current lr: 0.0000079
	lm_loss: 5.0018, ppl: 148.6847, loss: 5.0018
[train][1] progress: 1/1 step: 791, time: 0.039, speed: 25.924 steps/s
	current lr: 0.0000079
	lm_loss: 3.9904, ppl: 54.0771, loss: 3.9904
[train][1] progress: 1/1 step: 792, time: 0.036, speed: 27.421 steps/s
	current lr: 0.0000079
	lm_loss: 3.8065, ppl: 44.9939, loss: 3.8065
[train][1] progress: 1/1 step: 793, time: 0.036, speed: 28.045 steps/s
	current lr: 0.0000079
	lm_loss: 4.9593, ppl: 142.4913, loss: 4.9593
[train][1] progress: 1/1 step: 794, time: 0.034, speed: 28.999 steps/s
	current lr: 0.0000079
	lm_loss: 4.7677, ppl: 117.6540, loss: 4.7677
[train][1] progress: 1/1 step: 795, time: 0.035, speed: 28.970 steps/s
	current lr: 0.0000080
	lm_loss: 3.9874, ppl: 53.9141, loss: 3.9874
[train][1] progress: 1/1 step: 796, time: 0.036, speed: 27.809 steps/s
	current lr: 0.0000080
	lm_loss: 4.3557, ppl: 77.9199, loss: 4.3557
[train][1] progress: 1/1 step: 797, time: 0.035, speed: 28.598 steps/s
	current lr: 0.0000080
	lm_loss: 4.0937, ppl: 59.9615, loss: 4.0937
[train][1] progress: 1/1 step: 798, time: 0.037, speed: 27.314 steps/s
	current lr: 0.0000080
	lm_loss: 4.3813, ppl: 79.9451, loss: 4.3813
[train][1] progress: 1/1 step: 799, time: 0.039, speed: 25.905 steps/s
	current lr: 0.0000080
	lm_loss: 4.7072, ppl: 110.7385, loss: 4.7072
[train][1] progress: 1/1 step: 800, time: 0.036, speed: 28.002 steps/s
	current lr: 0.0000080
	lm_loss: 4.5601, ppl: 95.5939, loss: 4.5601
[train][1] progress: 1/1 step: 801, time: 0.037, speed: 27.364 steps/s
	current lr: 0.0000080
	lm_loss: 5.4600, ppl: 235.0899, loss: 5.4600
[train][1] progress: 1/1 step: 802, time: 0.036, speed: 28.012 steps/s
	current lr: 0.0000080
	lm_loss: 5.5001, ppl: 244.7099, loss: 5.5001
[train][1] progress: 1/1 step: 803, time: 0.036, speed: 27.980 steps/s
	current lr: 0.0000080
	lm_loss: 5.4455, ppl: 231.7035, loss: 5.4455
[train][1] progress: 1/1 step: 804, time: 0.036, speed: 27.670 steps/s
	current lr: 0.0000080
	lm_loss: 4.1647, ppl: 64.3729, loss: 4.1647
[train][1] progress: 1/1 step: 805, time: 0.036, speed: 28.083 steps/s
	current lr: 0.0000080
	lm_loss: 4.5644, ppl: 96.0074, loss: 4.5644
[train][1] progress: 1/1 step: 806, time: 0.035, speed: 28.347 steps/s
	current lr: 0.0000081
	lm_loss: 5.3878, ppl: 218.7225, loss: 5.3878
[train][1] progress: 1/1 step: 807, time: 0.036, speed: 28.030 steps/s
	current lr: 0.0000081
	lm_loss: 5.6292, ppl: 278.4373, loss: 5.6292
[train][1] progress: 1/1 step: 808, time: 0.035, speed: 28.251 steps/s
	current lr: 0.0000081
	lm_loss: 5.2327, ppl: 187.3022, loss: 5.2327
[train][1] progress: 1/1 step: 809, time: 0.037, speed: 27.304 steps/s
	current lr: 0.0000081
	lm_loss: 5.5146, ppl: 248.2799, loss: 5.5146
[train][1] progress: 1/1 step: 810, time: 0.035, speed: 28.477 steps/s
	current lr: 0.0000081
	lm_loss: 4.1345, ppl: 62.4597, loss: 4.1345
[train][1] progress: 1/1 step: 811, time: 0.037, speed: 26.950 steps/s
	current lr: 0.0000081
	lm_loss: 4.4592, ppl: 86.4193, loss: 4.4592
[train][1] progress: 1/1 step: 812, time: 0.036, speed: 28.150 steps/s
	current lr: 0.0000081
	lm_loss: 6.1541, ppl: 470.6424, loss: 6.1541
[train][1] progress: 1/1 step: 813, time: 0.035, speed: 28.305 steps/s
	current lr: 0.0000081
	lm_loss: 3.5514, ppl: 34.8628, loss: 3.5514
[train][1] progress: 1/1 step: 814, time: 0.037, speed: 27.383 steps/s
	current lr: 0.0000081
	lm_loss: 6.0953, ppl: 443.7556, loss: 6.0953
[train][1] progress: 1/1 step: 815, time: 0.035, speed: 28.274 steps/s
	current lr: 0.0000082
	lm_loss: 5.3420, ppl: 208.9386, loss: 5.3420
[train][1] progress: 1/1 step: 816, time: 0.035, speed: 28.209 steps/s
	current lr: 0.0000082
	lm_loss: 4.0779, ppl: 59.0235, loss: 4.0779
[train][1] progress: 1/1 step: 817, time: 0.036, speed: 27.761 steps/s
	current lr: 0.0000082
	lm_loss: 5.0447, ppl: 155.1929, loss: 5.0447
[train][1] progress: 1/1 step: 818, time: 0.037, speed: 27.111 steps/s
	current lr: 0.0000082
	lm_loss: 4.3699, ppl: 79.0381, loss: 4.3699
[train][1] progress: 1/1 step: 819, time: 0.037, speed: 27.043 steps/s
	current lr: 0.0000082
	lm_loss: 4.0214, ppl: 55.7786, loss: 4.0214
[train][1] progress: 1/1 step: 820, time: 0.036, speed: 27.419 steps/s
	current lr: 0.0000082
	lm_loss: 5.0846, ppl: 161.5222, loss: 5.0846
[train][1] progress: 1/1 step: 821, time: 0.036, speed: 27.876 steps/s
	current lr: 0.0000082
	lm_loss: 4.9150, ppl: 136.3204, loss: 4.9150
[train][1] progress: 1/1 step: 822, time: 0.036, speed: 27.721 steps/s
	current lr: 0.0000082
	lm_loss: 5.4965, ppl: 243.8443, loss: 5.4965
[train][1] progress: 1/1 step: 823, time: 0.037, speed: 27.244 steps/s
	current lr: 0.0000082
	lm_loss: 4.3345, ppl: 76.2863, loss: 4.3345
[train][1] progress: 1/1 step: 824, time: 0.037, speed: 26.884 steps/s
	current lr: 0.0000082
	lm_loss: 4.4813, ppl: 88.3513, loss: 4.4813
[train][1] progress: 1/1 step: 825, time: 0.037, speed: 27.302 steps/s
	current lr: 0.0000083
	lm_loss: 5.5070, ppl: 246.4169, loss: 5.5070
[train][1] progress: 1/1 step: 826, time: 0.038, speed: 26.017 steps/s
	current lr: 0.0000083
	lm_loss: 4.0716, ppl: 58.6489, loss: 4.0716
[train][1] progress: 1/1 step: 827, time: 0.037, speed: 26.879 steps/s
	current lr: 0.0000083
	lm_loss: 4.8924, ppl: 133.2721, loss: 4.8924
[train][1] progress: 1/1 step: 828, time: 0.038, speed: 26.663 steps/s
	current lr: 0.0000083
	lm_loss: 4.4512, ppl: 85.7256, loss: 4.4512
[train][1] progress: 1/1 step: 829, time: 0.042, speed: 24.036 steps/s
	current lr: 0.0000083
	lm_loss: 4.9858, ppl: 146.3164, loss: 4.9858
[train][1] progress: 1/1 step: 830, time: 0.036, speed: 27.547 steps/s
	current lr: 0.0000083
	lm_loss: 5.1841, ppl: 178.4070, loss: 5.1841
[train][1] progress: 1/1 step: 831, time: 0.035, speed: 28.831 steps/s
	current lr: 0.0000083
	lm_loss: 4.4192, ppl: 83.0311, loss: 4.4192
[train][1] progress: 1/1 step: 832, time: 0.034, speed: 29.153 steps/s
	current lr: 0.0000083
	lm_loss: 4.9889, ppl: 146.7709, loss: 4.9889
[train][1] progress: 1/1 step: 833, time: 0.034, speed: 29.096 steps/s
	current lr: 0.0000083
	lm_loss: 2.0989, ppl: 8.1570, loss: 2.0989
[train][1] progress: 1/1 step: 834, time: 0.034, speed: 29.011 steps/s
	current lr: 0.0000083
	lm_loss: 4.4997, ppl: 89.9938, loss: 4.4997
[train][1] progress: 1/1 step: 835, time: 0.034, speed: 29.428 steps/s
	current lr: 0.0000083
	lm_loss: 5.7716, ppl: 321.0653, loss: 5.7716
[train][1] progress: 1/1 step: 836, time: 0.035, speed: 28.880 steps/s
	current lr: 0.0000084
	lm_loss: 3.7244, ppl: 41.4451, loss: 3.7244
[train][1] progress: 1/1 step: 837, time: 0.035, speed: 28.245 steps/s
	current lr: 0.0000084
	lm_loss: 4.3001, ppl: 73.7103, loss: 4.3001
[train][1] progress: 1/1 step: 838, time: 0.036, speed: 28.105 steps/s
	current lr: 0.0000084
	lm_loss: 4.5175, ppl: 91.6046, loss: 4.5175
[train][1] progress: 1/1 step: 839, time: 0.036, speed: 28.146 steps/s
	current lr: 0.0000084
	lm_loss: 4.6058, ppl: 100.0643, loss: 4.6058
[train][1] progress: 1/1 step: 840, time: 0.035, speed: 28.451 steps/s
	current lr: 0.0000084
	lm_loss: 4.3608, ppl: 78.3192, loss: 4.3608
[train][1] progress: 1/1 step: 841, time: 0.036, speed: 27.799 steps/s
	current lr: 0.0000084
	lm_loss: 5.3053, ppl: 201.3981, loss: 5.3053
[train][1] progress: 1/1 step: 842, time: 0.035, speed: 28.734 steps/s
	current lr: 0.0000084
	lm_loss: 5.1853, ppl: 178.6279, loss: 5.1853
[train][1] progress: 1/1 step: 843, time: 0.035, speed: 28.783 steps/s
	current lr: 0.0000084
	lm_loss: 3.2471, ppl: 25.7168, loss: 3.2471
[train][1] progress: 1/1 step: 844, time: 0.035, speed: 28.834 steps/s
	current lr: 0.0000084
	lm_loss: 4.3918, ppl: 80.7835, loss: 4.3918
[train][1] progress: 1/1 step: 845, time: 0.035, speed: 28.922 steps/s
	current lr: 0.0000084
	lm_loss: 5.6447, ppl: 282.7747, loss: 5.6447
[train][1] progress: 1/1 step: 846, time: 0.035, speed: 28.191 steps/s
	current lr: 0.0000085
	lm_loss: 5.0924, ppl: 162.7847, loss: 5.0924
[train][1] progress: 1/1 step: 847, time: 0.035, speed: 28.524 steps/s
	current lr: 0.0000085
	lm_loss: 5.1927, ppl: 179.9521, loss: 5.1927
[train][1] progress: 1/1 step: 848, time: 0.035, speed: 28.918 steps/s
	current lr: 0.0000085
	lm_loss: 5.4709, ppl: 237.6754, loss: 5.4709
[train][1] progress: 1/1 step: 849, time: 0.035, speed: 28.244 steps/s
	current lr: 0.0000085
	lm_loss: 5.0642, ppl: 158.2483, loss: 5.0642
[train][1] progress: 1/1 step: 850, time: 0.034, speed: 29.229 steps/s
	current lr: 0.0000085
	lm_loss: 4.8103, ppl: 122.7683, loss: 4.8103
[train][1] progress: 1/1 step: 851, time: 0.034, speed: 29.135 steps/s
	current lr: 0.0000085
	lm_loss: 4.0365, ppl: 56.6257, loss: 4.0365
[train][1] progress: 1/1 step: 852, time: 0.034, speed: 29.004 steps/s
	current lr: 0.0000085
	lm_loss: 4.9158, ppl: 136.4340, loss: 4.9158
[train][1] progress: 1/1 step: 853, time: 0.034, speed: 29.090 steps/s
	current lr: 0.0000085
	lm_loss: 4.7131, ppl: 111.4007, loss: 4.7131
[train][1] progress: 1/1 step: 854, time: 0.037, speed: 27.238 steps/s
	current lr: 0.0000085
	lm_loss: 5.4196, ppl: 225.7928, loss: 5.4196
[train][1] progress: 1/1 step: 855, time: 0.035, speed: 28.205 steps/s
	current lr: 0.0000086
	lm_loss: 4.9942, ppl: 147.5607, loss: 4.9942
[train][1] progress: 1/1 step: 856, time: 0.037, speed: 27.384 steps/s
	current lr: 0.0000086
	lm_loss: 5.0257, ppl: 152.2701, loss: 5.0257
[train][1] progress: 1/1 step: 857, time: 0.036, speed: 27.778 steps/s
	current lr: 0.0000086
	lm_loss: 5.0743, ppl: 159.8561, loss: 5.0743
[train][1] progress: 1/1 step: 858, time: 0.036, speed: 27.750 steps/s
	current lr: 0.0000086
	lm_loss: 4.3821, ppl: 80.0060, loss: 4.3821
[train][1] progress: 1/1 step: 859, time: 0.037, speed: 27.310 steps/s
	current lr: 0.0000086
	lm_loss: 4.2368, ppl: 69.1849, loss: 4.2368
[train][1] progress: 1/1 step: 860, time: 0.035, speed: 28.481 steps/s
	current lr: 0.0000086
	lm_loss: 3.5385, ppl: 34.4163, loss: 3.5385
[train][1] progress: 1/1 step: 861, time: 0.035, speed: 28.645 steps/s
	current lr: 0.0000086
	lm_loss: 3.3696, ppl: 29.0674, loss: 3.3696
[train][1] progress: 1/1 step: 862, time: 0.035, speed: 28.782 steps/s
	current lr: 0.0000086
	lm_loss: 4.9097, ppl: 135.5992, loss: 4.9097
[train][1] progress: 1/1 step: 863, time: 0.035, speed: 28.916 steps/s
	current lr: 0.0000086
	lm_loss: 4.8300, ppl: 125.2155, loss: 4.8300
[train][1] progress: 1/1 step: 864, time: 0.035, speed: 28.628 steps/s
	current lr: 0.0000086
	lm_loss: 4.0143, ppl: 55.3822, loss: 4.0143
[train][1] progress: 1/1 step: 865, time: 0.035, speed: 28.958 steps/s
	current lr: 0.0000086
	lm_loss: 4.4618, ppl: 86.6403, loss: 4.4618
[train][1] progress: 1/1 step: 866, time: 0.034, speed: 28.986 steps/s
	current lr: 0.0000087
	lm_loss: 4.3710, ppl: 79.1229, loss: 4.3710
[train][1] progress: 1/1 step: 867, time: 0.034, speed: 29.235 steps/s
	current lr: 0.0000087
	lm_loss: 4.3363, ppl: 76.4240, loss: 4.3363
[train][1] progress: 1/1 step: 868, time: 0.035, speed: 28.532 steps/s
	current lr: 0.0000087
	lm_loss: 4.8357, ppl: 125.9250, loss: 4.8357
[train][1] progress: 1/1 step: 869, time: 0.035, speed: 28.715 steps/s
	current lr: 0.0000087
	lm_loss: 6.1946, ppl: 490.0900, loss: 6.1946
[train][1] progress: 1/1 step: 870, time: 0.035, speed: 28.305 steps/s
	current lr: 0.0000087
	lm_loss: 2.3373, ppl: 10.3531, loss: 2.3373
[train][1] progress: 1/1 step: 871, time: 0.037, speed: 27.187 steps/s
	current lr: 0.0000087
	lm_loss: 4.5834, ppl: 97.8482, loss: 4.5834
[train][1] progress: 1/1 step: 872, time: 0.035, speed: 28.409 steps/s
	current lr: 0.0000087
	lm_loss: 3.9072, ppl: 49.7616, loss: 3.9072
[train][1] progress: 1/1 step: 873, time: 0.035, speed: 28.661 steps/s
	current lr: 0.0000087
	lm_loss: 4.9041, ppl: 134.8362, loss: 4.9041
[train][1] progress: 1/1 step: 874, time: 0.035, speed: 28.749 steps/s
	current lr: 0.0000087
	lm_loss: 5.5435, ppl: 255.5608, loss: 5.5435
[train][1] progress: 1/1 step: 875, time: 0.034, speed: 29.204 steps/s
	current lr: 0.0000088
	lm_loss: 5.7606, ppl: 317.5279, loss: 5.7606
[train][1] progress: 1/1 step: 876, time: 0.034, speed: 29.212 steps/s
	current lr: 0.0000088
	lm_loss: 4.4735, ppl: 87.6624, loss: 4.4735
[train][1] progress: 1/1 step: 877, time: 0.034, speed: 29.208 steps/s
	current lr: 0.0000088
	lm_loss: 4.4586, ppl: 86.3650, loss: 4.4586
[train][1] progress: 1/1 step: 878, time: 0.033, speed: 30.335 steps/s
	current lr: 0.0000088
	lm_loss: 4.3854, ppl: 80.2673, loss: 4.3854
[train][1] progress: 1/1 step: 879, time: 0.033, speed: 30.048 steps/s
	current lr: 0.0000088
	lm_loss: 3.9826, ppl: 53.6558, loss: 3.9826
[train][1] progress: 1/1 step: 880, time: 0.034, speed: 29.732 steps/s
	current lr: 0.0000088
	lm_loss: 3.3160, ppl: 27.5491, loss: 3.3160
[train][1] progress: 1/1 step: 881, time: 0.033, speed: 29.855 steps/s
	current lr: 0.0000088
	lm_loss: 4.7195, ppl: 112.1090, loss: 4.7195
[train][1] progress: 1/1 step: 882, time: 0.034, speed: 29.518 steps/s
	current lr: 0.0000088
	lm_loss: 4.8242, ppl: 124.4861, loss: 4.8242
[train][1] progress: 1/1 step: 883, time: 0.035, speed: 28.953 steps/s
	current lr: 0.0000088
	lm_loss: 5.0140, ppl: 150.5014, loss: 5.0140
[train][1] progress: 1/1 step: 884, time: 0.034, speed: 29.079 steps/s
	current lr: 0.0000088
	lm_loss: 5.8059, ppl: 332.2636, loss: 5.8059
[train][1] progress: 1/1 step: 885, time: 0.034, speed: 29.051 steps/s
	current lr: 0.0000089
	lm_loss: 4.4710, ppl: 87.4460, loss: 4.4710
[train][1] progress: 1/1 step: 886, time: 0.034, speed: 29.170 steps/s
	current lr: 0.0000089
	lm_loss: 4.5317, ppl: 92.9138, loss: 4.5317
[train][1] progress: 1/1 step: 887, time: 0.034, speed: 29.683 steps/s
	current lr: 0.0000089
	lm_loss: 4.0431, ppl: 57.0005, loss: 4.0431
[train][1] progress: 1/1 step: 888, time: 0.035, speed: 28.332 steps/s
	current lr: 0.0000089
	lm_loss: 5.2144, ppl: 183.9044, loss: 5.2144
[train][1] progress: 1/1 step: 889, time: 0.035, speed: 28.471 steps/s
	current lr: 0.0000089
	lm_loss: 4.9247, ppl: 137.6497, loss: 4.9247
[train][1] progress: 1/1 step: 890, time: 0.035, speed: 28.548 steps/s
	current lr: 0.0000089
	lm_loss: 5.7944, ppl: 328.4536, loss: 5.7944
[train][1] progress: 1/1 step: 891, time: 0.034, speed: 29.616 steps/s
	current lr: 0.0000089
	lm_loss: 5.0934, ppl: 162.9385, loss: 5.0934
[train][1] progress: 1/1 step: 892, time: 0.034, speed: 29.283 steps/s
	current lr: 0.0000089
	lm_loss: 2.9251, ppl: 18.6354, loss: 2.9251
[train][1] progress: 1/1 step: 893, time: 0.034, speed: 29.335 steps/s
	current lr: 0.0000089
	lm_loss: 4.8918, ppl: 133.1970, loss: 4.8918
[train][1] progress: 1/1 step: 894, time: 0.034, speed: 29.600 steps/s
	current lr: 0.0000089
	lm_loss: 4.8382, ppl: 126.2472, loss: 4.8382
[train][1] progress: 1/1 step: 895, time: 0.034, speed: 29.187 steps/s
	current lr: 0.0000089
	lm_loss: 3.8132, ppl: 45.2937, loss: 3.8132
[train][1] progress: 1/1 step: 896, time: 0.034, speed: 29.547 steps/s
	current lr: 0.0000090
	lm_loss: 4.7603, ppl: 116.7820, loss: 4.7603
[train][1] progress: 1/1 step: 897, time: 0.034, speed: 29.657 steps/s
	current lr: 0.0000090
	lm_loss: 4.7945, ppl: 120.8380, loss: 4.7945
[train][1] progress: 1/1 step: 898, time: 0.034, speed: 29.455 steps/s
	current lr: 0.0000090
	lm_loss: 5.0671, ppl: 158.7126, loss: 5.0671
[train][1] progress: 1/1 step: 899, time: 0.034, speed: 29.399 steps/s
	current lr: 0.0000090
	lm_loss: 5.1467, ppl: 171.8691, loss: 5.1467
[train][1] progress: 1/1 step: 900, time: 0.033, speed: 29.873 steps/s
	current lr: 0.0000090
	lm_loss: 4.1548, ppl: 63.7390, loss: 4.1548
[train][1] progress: 1/1 step: 901, time: 0.034, speed: 29.495 steps/s
	current lr: 0.0000090
	lm_loss: 5.5241, ppl: 250.6633, loss: 5.5241
[train][1] progress: 1/1 step: 902, time: 0.034, speed: 29.074 steps/s
	current lr: 0.0000090
	lm_loss: 5.0966, ppl: 163.4614, loss: 5.0966
[train][1] progress: 1/1 step: 903, time: 0.034, speed: 29.628 steps/s
	current lr: 0.0000090
	lm_loss: 4.8833, ppl: 132.0722, loss: 4.8833
[train][1] progress: 1/1 step: 904, time: 0.034, speed: 29.364 steps/s
	current lr: 0.0000090
	lm_loss: 6.0460, ppl: 422.4077, loss: 6.0460
[train][1] progress: 1/1 step: 905, time: 0.034, speed: 29.523 steps/s
	current lr: 0.0000090
	lm_loss: 4.4460, ppl: 85.2875, loss: 4.4460
[train][1] progress: 1/1 step: 906, time: 0.034, speed: 29.803 steps/s
	current lr: 0.0000091
	lm_loss: 4.8747, ppl: 130.9413, loss: 4.8747
[train][1] progress: 1/1 step: 907, time: 0.034, speed: 29.724 steps/s
	current lr: 0.0000091
	lm_loss: 5.5994, ppl: 270.2591, loss: 5.5994
[train][1] progress: 1/1 step: 908, time: 0.035, speed: 28.964 steps/s
	current lr: 0.0000091
	lm_loss: 3.6530, ppl: 38.5891, loss: 3.6530
[train][1] progress: 1/1 step: 909, time: 0.034, speed: 29.272 steps/s
	current lr: 0.0000091
	lm_loss: 4.6945, ppl: 109.3435, loss: 4.6945
[train][1] progress: 1/1 step: 910, time: 0.034, speed: 29.276 steps/s
	current lr: 0.0000091
	lm_loss: 5.4988, ppl: 244.3864, loss: 5.4988
[train][1] progress: 1/1 step: 911, time: 0.033, speed: 29.938 steps/s
	current lr: 0.0000091
	lm_loss: 5.6538, ppl: 285.3779, loss: 5.6538
[train][1] progress: 1/1 step: 912, time: 0.033, speed: 29.901 steps/s
	current lr: 0.0000091
	lm_loss: 5.3602, ppl: 212.7689, loss: 5.3602
[train][1] progress: 1/1 step: 913, time: 0.034, speed: 29.326 steps/s
	current lr: 0.0000091
	lm_loss: 3.4761, ppl: 32.3323, loss: 3.4761
[train][1] progress: 1/1 step: 914, time: 0.034, speed: 29.797 steps/s
	current lr: 0.0000091
	lm_loss: 4.5721, ppl: 96.7514, loss: 4.5721
[train][1] progress: 1/1 step: 915, time: 0.033, speed: 30.041 steps/s
	current lr: 0.0000092
	lm_loss: 4.8738, ppl: 130.8195, loss: 4.8738
[train][1] progress: 1/1 step: 916, time: 0.034, speed: 29.371 steps/s
	current lr: 0.0000092
	lm_loss: 5.8842, ppl: 359.2991, loss: 5.8842
[train][1] progress: 1/1 step: 917, time: 0.033, speed: 29.890 steps/s
	current lr: 0.0000092
	lm_loss: 5.0059, ppl: 149.2902, loss: 5.0059
[train][1] progress: 1/1 step: 918, time: 0.034, speed: 29.229 steps/s
	current lr: 0.0000092
	lm_loss: 4.2136, ppl: 67.5991, loss: 4.2136
[train][1] progress: 1/1 step: 919, time: 0.035, speed: 28.584 steps/s
	current lr: 0.0000092
	lm_loss: 4.4231, ppl: 83.3513, loss: 4.4231
[train][1] progress: 1/1 step: 920, time: 0.035, speed: 28.815 steps/s
	current lr: 0.0000092
	lm_loss: 4.2400, ppl: 69.4055, loss: 4.2400
[train][1] progress: 1/1 step: 921, time: 0.034, speed: 29.008 steps/s
	current lr: 0.0000092
	lm_loss: 4.6840, ppl: 108.2025, loss: 4.6840
[train][1] progress: 1/1 step: 922, time: 0.034, speed: 29.569 steps/s
	current lr: 0.0000092
	lm_loss: 5.4319, ppl: 228.5849, loss: 5.4319
[train][1] progress: 1/1 step: 923, time: 0.033, speed: 30.040 steps/s
	current lr: 0.0000092
	lm_loss: 5.0624, ppl: 157.9627, loss: 5.0624
[train][1] progress: 1/1 step: 924, time: 0.034, speed: 29.617 steps/s
	current lr: 0.0000092
	lm_loss: 4.1937, ppl: 66.2705, loss: 4.1937
[train][1] progress: 1/1 step: 925, time: 0.033, speed: 30.269 steps/s
	current lr: 0.0000092
	lm_loss: 4.2177, ppl: 67.8769, loss: 4.2177
[train][1] progress: 1/1 step: 926, time: 0.033, speed: 29.861 steps/s
	current lr: 0.0000093
	lm_loss: 5.3402, ppl: 208.5524, loss: 5.3402
[train][1] progress: 1/1 step: 927, time: 0.034, speed: 29.548 steps/s
	current lr: 0.0000093
	lm_loss: 4.7653, ppl: 117.3703, loss: 4.7653
[train][1] progress: 1/1 step: 928, time: 0.034, speed: 29.156 steps/s
	current lr: 0.0000093
	lm_loss: 4.5792, ppl: 97.4319, loss: 4.5792
[train][1] progress: 1/1 step: 929, time: 0.035, speed: 28.532 steps/s
	current lr: 0.0000093
	lm_loss: 4.9284, ppl: 138.1645, loss: 4.9284
[train][1] progress: 1/1 step: 930, time: 0.036, speed: 27.546 steps/s
	current lr: 0.0000093
	lm_loss: 4.4747, ppl: 87.7688, loss: 4.4747
[train][1] progress: 1/1 step: 931, time: 0.036, speed: 28.106 steps/s
	current lr: 0.0000093
	lm_loss: 4.4248, ppl: 83.5000, loss: 4.4248
[train][1] progress: 1/1 step: 932, time: 0.036, speed: 27.828 steps/s
	current lr: 0.0000093
	lm_loss: 5.1593, ppl: 174.0482, loss: 5.1593
[train][1] progress: 1/1 step: 933, time: 0.036, speed: 27.529 steps/s
	current lr: 0.0000093
	lm_loss: 5.3362, ppl: 207.7185, loss: 5.3362
[train][1] progress: 1/1 step: 934, time: 0.036, speed: 28.002 steps/s
	current lr: 0.0000093
	lm_loss: 4.6271, ppl: 102.2128, loss: 4.6271
[train][1] progress: 1/1 step: 935, time: 0.034, speed: 29.388 steps/s
	current lr: 0.0000094
	lm_loss: 4.0464, ppl: 57.1939, loss: 4.0464
[train][1] progress: 1/1 step: 936, time: 0.034, speed: 29.460 steps/s
	current lr: 0.0000094
	lm_loss: 4.3668, ppl: 78.7886, loss: 4.3668
[train][1] progress: 1/1 step: 937, time: 0.033, speed: 30.190 steps/s
	current lr: 0.0000094
	lm_loss: 5.2663, ppl: 193.6954, loss: 5.2663
[train][1] progress: 1/1 step: 938, time: 0.033, speed: 30.049 steps/s
	current lr: 0.0000094
	lm_loss: 4.6412, ppl: 103.6711, loss: 4.6412
[train][1] progress: 1/1 step: 939, time: 0.033, speed: 29.995 steps/s
	current lr: 0.0000094
	lm_loss: 2.4715, ppl: 11.8402, loss: 2.4715
[train][1] progress: 1/1 step: 940, time: 0.033, speed: 30.043 steps/s
	current lr: 0.0000094
	lm_loss: 3.8690, ppl: 47.8936, loss: 3.8690
[train][1] progress: 1/1 step: 941, time: 0.033, speed: 30.169 steps/s
	current lr: 0.0000094
	lm_loss: 5.1340, ppl: 169.6903, loss: 5.1340
[train][1] progress: 1/1 step: 942, time: 0.033, speed: 30.128 steps/s
	current lr: 0.0000094
	lm_loss: 4.7208, ppl: 112.2575, loss: 4.7208
[train][1] progress: 1/1 step: 943, time: 0.034, speed: 29.462 steps/s
	current lr: 0.0000094
	lm_loss: 5.1793, ppl: 177.5562, loss: 5.1793
[train][1] progress: 1/1 step: 944, time: 0.033, speed: 29.989 steps/s
	current lr: 0.0000094
	lm_loss: 5.8817, ppl: 358.4086, loss: 5.8817
[train][1] progress: 1/1 step: 945, time: 0.033, speed: 30.105 steps/s
	current lr: 0.0000095
	lm_loss: 4.9634, ppl: 143.0751, loss: 4.9634
[train][1] progress: 1/1 step: 946, time: 0.033, speed: 30.438 steps/s
	current lr: 0.0000095
	lm_loss: 3.1716, ppl: 23.8449, loss: 3.1716
[train][1] progress: 1/1 step: 947, time: 0.033, speed: 30.067 steps/s
	current lr: 0.0000095
	lm_loss: 5.1624, ppl: 174.5860, loss: 5.1624
[train][1] progress: 1/1 step: 948, time: 0.034, speed: 29.698 steps/s
	current lr: 0.0000095
	lm_loss: 4.6434, ppl: 103.8999, loss: 4.6434
[train][1] progress: 1/1 step: 949, time: 0.033, speed: 30.140 steps/s
	current lr: 0.0000095
	lm_loss: 4.2112, ppl: 67.4369, loss: 4.2112
[train][1] progress: 1/1 step: 950, time: 0.034, speed: 29.319 steps/s
	current lr: 0.0000095
	lm_loss: 4.7493, ppl: 115.5026, loss: 4.7493
[train][1] progress: 1/1 step: 951, time: 0.035, speed: 28.772 steps/s
	current lr: 0.0000095
	lm_loss: 3.8512, ppl: 47.0499, loss: 3.8512
[train][1] progress: 1/1 step: 952, time: 0.034, speed: 29.503 steps/s
	current lr: 0.0000095
	lm_loss: 4.1691, ppl: 64.6549, loss: 4.1691
[train][1] progress: 1/1 step: 953, time: 0.034, speed: 29.784 steps/s
	current lr: 0.0000095
	lm_loss: 3.8973, ppl: 49.2716, loss: 3.8973
[train][1] progress: 1/1 step: 954, time: 0.033, speed: 30.121 steps/s
	current lr: 0.0000095
	lm_loss: 6.3197, ppl: 555.4150, loss: 6.3197
[train][1] progress: 1/1 step: 955, time: 0.033, speed: 30.123 steps/s
	current lr: 0.0000095
	lm_loss: 3.8649, ppl: 47.6980, loss: 3.8649
[train][1] progress: 1/1 step: 956, time: 0.033, speed: 30.404 steps/s
	current lr: 0.0000096
	lm_loss: 3.8247, ppl: 45.8204, loss: 3.8247
[train][1] progress: 1/1 step: 957, time: 0.034, speed: 29.663 steps/s
	current lr: 0.0000096
	lm_loss: 5.5529, ppl: 257.9832, loss: 5.5529
[train][1] progress: 1/1 step: 958, time: 0.034, speed: 29.763 steps/s
	current lr: 0.0000096
	lm_loss: 4.9832, ppl: 145.9342, loss: 4.9832
[train][1] progress: 1/1 step: 959, time: 0.034, speed: 29.772 steps/s
	current lr: 0.0000096
	lm_loss: 5.3190, ppl: 204.1696, loss: 5.3190
[train][1] progress: 1/1 step: 960, time: 0.033, speed: 29.949 steps/s
	current lr: 0.0000096
	lm_loss: 4.6663, ppl: 106.3028, loss: 4.6663
[train][1] progress: 1/1 step: 961, time: 0.033, speed: 29.945 steps/s
	current lr: 0.0000096
	lm_loss: 4.5054, ppl: 90.5047, loss: 4.5054
[train][1] progress: 1/1 step: 962, time: 0.033, speed: 30.283 steps/s
	current lr: 0.0000096
	lm_loss: 4.2986, ppl: 73.5963, loss: 4.2986
[train][1] progress: 1/1 step: 963, time: 0.033, speed: 30.240 steps/s
	current lr: 0.0000096
	lm_loss: 4.6852, ppl: 108.3268, loss: 4.6852
[train][1] progress: 1/1 step: 964, time: 0.034, speed: 29.759 steps/s
	current lr: 0.0000096
	lm_loss: 4.1490, ppl: 63.3700, loss: 4.1490
[train][1] progress: 1/1 step: 965, time: 0.033, speed: 30.392 steps/s
	current lr: 0.0000096
	lm_loss: 3.9330, ppl: 51.0579, loss: 3.9330
[train][1] progress: 1/1 step: 966, time: 0.033, speed: 30.189 steps/s
	current lr: 0.0000097
	lm_loss: 5.4272, ppl: 227.5146, loss: 5.4272
[train][1] progress: 1/1 step: 967, time: 0.034, speed: 29.654 steps/s
	current lr: 0.0000097
	lm_loss: 4.8958, ppl: 133.7316, loss: 4.8958
[train][1] progress: 1/1 step: 968, time: 0.034, speed: 29.828 steps/s
	current lr: 0.0000097
	lm_loss: 4.7281, ppl: 113.0765, loss: 4.7281
[train][1] progress: 1/1 step: 969, time: 0.033, speed: 29.852 steps/s
	current lr: 0.0000097
	lm_loss: 4.4964, ppl: 89.6926, loss: 4.4964
[train][1] progress: 1/1 step: 970, time: 0.034, speed: 29.271 steps/s
	current lr: 0.0000097
	lm_loss: 4.7137, ppl: 111.4676, loss: 4.7137
[train][1] progress: 1/1 step: 971, time: 0.034, speed: 29.793 steps/s
	current lr: 0.0000097
	lm_loss: 2.4285, ppl: 11.3414, loss: 2.4285
[train][1] progress: 1/1 step: 972, time: 0.034, speed: 29.783 steps/s
	current lr: 0.0000097
	lm_loss: 4.9781, ppl: 145.1988, loss: 4.9781
[train][1] progress: 1/1 step: 973, time: 0.034, speed: 29.645 steps/s
	current lr: 0.0000097
	lm_loss: 4.1964, ppl: 66.4484, loss: 4.1964
[train][1] progress: 1/1 step: 974, time: 0.034, speed: 29.830 steps/s
	current lr: 0.0000097
	lm_loss: 3.9527, ppl: 52.0739, loss: 3.9527
[train][1] progress: 1/1 step: 975, time: 0.036, speed: 27.903 steps/s
	current lr: 0.0000098
	lm_loss: 3.5088, ppl: 33.4070, loss: 3.5088
[train][1] progress: 1/1 step: 976, time: 0.034, speed: 29.334 steps/s
	current lr: 0.0000098
	lm_loss: 5.7526, ppl: 315.0048, loss: 5.7526
[train][1] progress: 1/1 step: 977, time: 0.034, speed: 29.789 steps/s
	current lr: 0.0000098
	lm_loss: 3.3071, ppl: 27.3046, loss: 3.3071
[train][1] progress: 1/1 step: 978, time: 0.033, speed: 29.902 steps/s
	current lr: 0.0000098
	lm_loss: 4.8613, ppl: 129.1941, loss: 4.8613
[train][1] progress: 1/1 step: 979, time: 0.032, speed: 30.880 steps/s
	current lr: 0.0000098
	lm_loss: 5.2728, ppl: 194.9534, loss: 5.2728
[train][1] progress: 1/1 step: 980, time: 0.033, speed: 30.129 steps/s
	current lr: 0.0000098
	lm_loss: 4.2951, ppl: 73.3383, loss: 4.2951
[train][1] progress: 1/1 step: 981, time: 0.034, speed: 29.817 steps/s
	current lr: 0.0000098
	lm_loss: 3.2401, ppl: 25.5362, loss: 3.2401
[train][1] progress: 1/1 step: 982, time: 0.034, speed: 29.293 steps/s
	current lr: 0.0000098
	lm_loss: 5.0119, ppl: 150.1967, loss: 5.0119
[train][1] progress: 1/1 step: 983, time: 0.035, speed: 28.878 steps/s
	current lr: 0.0000098
	lm_loss: 4.6807, ppl: 107.8465, loss: 4.6807
[train][1] progress: 1/1 step: 984, time: 0.034, speed: 29.369 steps/s
	current lr: 0.0000098
	lm_loss: 4.4361, ppl: 84.4475, loss: 4.4361
[train][1] progress: 1/1 step: 985, time: 0.033, speed: 29.966 steps/s
	current lr: 0.0000098
	lm_loss: 4.2512, ppl: 70.1869, loss: 4.2512
[train][1] progress: 1/1 step: 986, time: 0.033, speed: 30.120 steps/s
	current lr: 0.0000099
	lm_loss: 4.9343, ppl: 138.9744, loss: 4.9343
[train][1] progress: 1/1 step: 987, time: 0.034, speed: 29.785 steps/s
	current lr: 0.0000099
	lm_loss: 4.8614, ppl: 129.2050, loss: 4.8614
[train][1] progress: 1/1 step: 988, time: 0.034, speed: 29.390 steps/s
	current lr: 0.0000099
	lm_loss: 5.2627, ppl: 192.9951, loss: 5.2627
[train][1] progress: 1/1 step: 989, time: 0.034, speed: 29.731 steps/s
	current lr: 0.0000099
	lm_loss: 4.2929, ppl: 73.1810, loss: 4.2929
[train][1] progress: 1/1 step: 990, time: 0.034, speed: 29.508 steps/s
	current lr: 0.0000099
	lm_loss: 2.8181, ppl: 16.7458, loss: 2.8181
[train][1] progress: 1/1 step: 991, time: 0.034, speed: 29.451 steps/s
	current lr: 0.0000099
	lm_loss: 5.1199, ppl: 167.3209, loss: 5.1199
[train][1] progress: 1/1 step: 992, time: 0.034, speed: 29.491 steps/s
	current lr: 0.0000099
	lm_loss: 4.5438, ppl: 94.0482, loss: 4.5438
[train][1] progress: 1/1 step: 993, time: 0.033, speed: 30.030 steps/s
	current lr: 0.0000099
	lm_loss: 3.8293, ppl: 46.0319, loss: 3.8293
[train][1] progress: 1/1 step: 994, time: 0.034, speed: 29.604 steps/s
	current lr: 0.0000099
	lm_loss: 4.9374, ppl: 139.4068, loss: 4.9374
[train][1] progress: 1/1 step: 995, time: 0.034, speed: 29.444 steps/s
	current lr: 0.0000099
	lm_loss: 4.6411, ppl: 103.6544, loss: 4.6411
[train][1] progress: 1/1 step: 996, time: 0.033, speed: 30.039 steps/s
	current lr: 0.0000100
	lm_loss: 4.7306, ppl: 113.3631, loss: 4.7306
[train][1] progress: 1/1 step: 997, time: 0.033, speed: 30.117 steps/s
	current lr: 0.0000100
	lm_loss: 5.0294, ppl: 152.8420, loss: 5.0294
[train][1] progress: 1/1 step: 998, time: 0.033, speed: 30.205 steps/s
	current lr: 0.0000100
	lm_loss: 5.2537, ppl: 191.2669, loss: 5.2537
[train][1] progress: 1/1 step: 999, time: 0.033, speed: 30.112 steps/s
	current lr: 0.0000100
	lm_loss: 4.2979, ppl: 73.5437, loss: 4.2979
[train][1] progress: 1/1 step: 1000, time: 0.032, speed: 30.792 steps/s
	current lr: 0.0000100
	lm_loss: 4.7893, ppl: 120.2188, loss: 4.7893
================================================================================
Evaluation:
	step 1:lm_loss: 4.0102, ppl: 55.1562, loss: 4.0102
	step 2:lm_loss: 4.6084, ppl: 100.3219, loss: 4.7459
	step 3:lm_loss: 4.4524, ppl: 85.8293, loss: 4.6076
	step 4:lm_loss: 4.4601, ppl: 86.4918, loss: 4.5771
	step 5:lm_loss: 4.5479, ppl: 94.4294, loss: 4.6507
	step 6:lm_loss: 4.5539, ppl: 94.9998, loss: 4.6408
	step 7:lm_loss: 4.6110, ppl: 100.5832, loss: 4.6959
	step 8:lm_loss: 4.6287, ppl: 102.3774, loss: 4.7040
	step 9:lm_loss: 4.6534, ppl: 104.9368, loss: 4.7234
	step 10:lm_loss: 4.4692, ppl: 87.2854, loss: 4.6666
	step 11:lm_loss: 4.4383, ppl: 84.6270, loss: 4.6008
	step 12:lm_loss: 4.4407, ppl: 84.8309, loss: 4.5854
	step 13:lm_loss: 4.4749, ppl: 87.7878, loss: 4.6113
	step 14:lm_loss: 4.5068, ppl: 90.6328, loss: 4.6336
	step 15:lm_loss: 4.4997, ppl: 89.9942, loss: 4.6178
	step 16:lm_loss: 4.5092, ppl: 90.8502, loss: 4.6197
	step 17:lm_loss: 4.4687, ppl: 87.2451, loss: 4.5755
	step 18:lm_loss: 4.4738, ppl: 87.6918, loss: 4.5742
	step 19:lm_loss: 4.5076, ppl: 90.7067, loss: 4.5971
	step 20:lm_loss: 4.5283, ppl: 92.6031, loss: 4.6186
	step 21:lm_loss: 4.5275, ppl: 92.5292, loss: 4.6149
	step 22:lm_loss: 4.5593, ppl: 95.5196, loss: 4.6462
	step 23:lm_loss: 4.5296, ppl: 92.7178, loss: 4.6255
	step 24:lm_loss: 4.4670, ppl: 87.0922, loss: 4.5377
	step 25:lm_loss: 4.4940, ppl: 89.4819, loss: 4.5623
	step 26:lm_loss: 4.5386, ppl: 93.5631, loss: 4.6083
	step 27:lm_loss: 4.5336, ppl: 93.0910, loss: 4.5981
	step 28:lm_loss: 4.5324, ppl: 92.9777, loss: 4.5948
	step 29:lm_loss: 4.5519, ppl: 94.8161, loss: 4.6115
	step 30:lm_loss: 4.5686, ppl: 96.4117, loss: 4.6220
	step 31:lm_loss: 4.5680, ppl: 96.3472, loss: 4.6195
	step 32:lm_loss: 4.5716, ppl: 96.7004, loss: 4.6220
	step 33:lm_loss: 4.5814, ppl: 97.6548, loss: 4.6291
	step 34:lm_loss: 4.5924, ppl: 98.7342, loss: 4.6366
	step 35:lm_loss: 4.5551, ppl: 95.1184, loss: 4.6028
	step 36:lm_loss: 4.5335, ppl: 93.0864, loss: 4.5809
	step 37:lm_loss: 4.5336, ppl: 93.0963, loss: 4.5797
	step 38:lm_loss: 4.5426, ppl: 93.9318, loss: 4.5857
	step 39:lm_loss: 4.5376, ppl: 93.4690, loss: 4.5786
	step 40:lm_loss: 4.5411, ppl: 93.7938, loss: 4.5825
	step 41:lm_loss: 4.5503, ppl: 94.6637, loss: 4.5927
	step 42:lm_loss: 4.5636, ppl: 95.9305, loss: 4.6033
	step 43:lm_loss: 4.5908, ppl: 98.5772, loss: 4.6185
	step 44:lm_loss: 4.6210, ppl: 101.5965, loss: 4.6471
	step 45:lm_loss: 4.6351, ppl: 103.0354, loss: 4.6643
	step 46:lm_loss: 4.6396, ppl: 103.5007, loss: 4.6685
	step 47:lm_loss: 4.6630, ppl: 105.9549, loss: 4.6987
	step 48:lm_loss: 4.6465, ppl: 104.2199, loss: 4.6773
	step 49:lm_loss: 4.6670, ppl: 106.3771, loss: 4.7004
	step 50:lm_loss: 4.6609, ppl: 105.7290, loss: 4.6914
	step 51:lm_loss: 4.6432, ppl: 103.8787, loss: 4.6790
	step 52:lm_loss: 4.6415, ppl: 103.6981, loss: 4.6770
	step 53:lm_loss: 4.6407, ppl: 103.6154, loss: 4.6756
	step 54:lm_loss: 4.6411, ppl: 103.6632, loss: 4.6755
	step 55:lm_loss: 4.6483, ppl: 104.4071, loss: 4.6820
	step 56:lm_loss: 4.6500, ppl: 104.5803, loss: 4.6836
	step 57:lm_loss: 4.6406, ppl: 103.6088, loss: 4.6769
	step 58:lm_loss: 4.6232, ppl: 101.8174, loss: 4.6656
	step 59:lm_loss: 4.6293, ppl: 102.4377, loss: 4.6726
	step 60:lm_loss: 4.6400, ppl: 103.5473, loss: 4.6816
	step 61:lm_loss: 4.6562, ppl: 105.2390, loss: 4.6945
	step 62:lm_loss: 4.6419, ppl: 103.7443, loss: 4.6802
	step 63:lm_loss: 4.6405, ppl: 103.5938, loss: 4.6780
	step 64:lm_loss: 4.6394, ppl: 103.4812, loss: 4.6757
	step 65:lm_loss: 4.6442, ppl: 103.9830, loss: 4.6810
	step 66:lm_loss: 4.6118, ppl: 100.6693, loss: 4.6613
	step 67:lm_loss: 4.6177, ppl: 101.2595, loss: 4.6654
	step 68:lm_loss: 4.6049, ppl: 99.9715, loss: 4.6495
	step 69:lm_loss: 4.6108, ppl: 100.5666, loss: 4.6541
	step 70:lm_loss: 4.6027, ppl: 99.7577, loss: 4.6408
	step 71:lm_loss: 4.5989, ppl: 99.3764, loss: 4.6363
	step 72:lm_loss: 4.5954, ppl: 99.0301, loss: 4.6314
	step 73:lm_loss: 4.6001, ppl: 99.4904, loss: 4.6349
	step 74:lm_loss: 4.6009, ppl: 99.5746, loss: 4.6355
	step 75:lm_loss: 4.6019, ppl: 99.6728, loss: 4.6363
	step 76:lm_loss: 4.5993, ppl: 99.4151, loss: 4.6317
	step 77:lm_loss: 4.6016, ppl: 99.6482, loss: 4.6342
	step 78:lm_loss: 4.6003, ppl: 99.5109, loss: 4.6318
	step 79:lm_loss: 4.6051, ppl: 99.9913, loss: 4.6364
	step 80:lm_loss: 4.6037, ppl: 99.8557, loss: 4.6349
	step 81:lm_loss: 4.5923, ppl: 98.7241, loss: 4.6132
	step 82:lm_loss: 4.6170, ppl: 101.1916, loss: 4.6453
	step 83:lm_loss: 4.6268, ppl: 102.1815, loss: 4.6516
	step 84:lm_loss: 4.6148, ppl: 100.9632, loss: 4.6416
	step 85:lm_loss: 4.6117, ppl: 100.6591, loss: 4.6381
	step 86:lm_loss: 4.6042, ppl: 99.9006, loss: 4.6305
	step 87:lm_loss: 4.6217, ppl: 101.6710, loss: 4.6409
	step 88:lm_loss: 4.6215, ppl: 101.6430, loss: 4.6404
	step 89:lm_loss: 4.6197, ppl: 101.4593, loss: 4.6389
	step 90:lm_loss: 4.6194, ppl: 101.4338, loss: 4.6385
	step 91:lm_loss: 4.6067, ppl: 100.1488, loss: 4.6259
	step 92:lm_loss: 4.6099, ppl: 100.4763, loss: 4.6291
	step 93:lm_loss: 4.6168, ppl: 101.1688, loss: 4.6362
	step 94:lm_loss: 4.6280, ppl: 102.3120, loss: 4.6501
	step 95:lm_loss: 4.6225, ppl: 101.7451, loss: 4.6421
	step 96:lm_loss: 4.6232, ppl: 101.8229, loss: 4.6429
	step 97:lm_loss: 4.6180, ppl: 101.2936, loss: 4.6397
	step 98:lm_loss: 4.6235, ppl: 101.8462, loss: 4.6461
	step 99:lm_loss: 4.6196, ppl: 101.4498, loss: 4.6424
	step 100:lm_loss: 4.6233, ppl: 101.8310, loss: 4.6465
	step 101:lm_loss: 4.6252, ppl: 102.0195, loss: 4.6477
	step 102:lm_loss: 4.6348, ppl: 103.0056, loss: 4.6558
	step 103:lm_loss: 4.6372, ppl: 103.2539, loss: 4.6578
	step 104:lm_loss: 4.6316, ppl: 102.6820, loss: 4.6504
	step 105:lm_loss: 4.6360, ppl: 103.1274, loss: 4.6548
	step 106:lm_loss: 4.6304, ppl: 102.5571, loss: 4.6470
	step 107:lm_loss: 4.6350, ppl: 103.0326, loss: 4.6509
	step 108:lm_loss: 4.6309, ppl: 102.6100, loss: 4.6470
	step 109:lm_loss: 4.6321, ppl: 102.7269, loss: 4.6478
	step 110:lm_loss: 4.6292, ppl: 102.4355, loss: 4.6453
	step 111:lm_loss: 4.6234, ppl: 101.8431, loss: 4.6416
	step 112:lm_loss: 4.6226, ppl: 101.7619, loss: 4.6403
	step 113:lm_loss: 4.6254, ppl: 102.0476, loss: 4.6430
	step 114:lm_loss: 4.6192, ppl: 101.4163, loss: 4.6361
	step 115:lm_loss: 4.6197, ppl: 101.4639, loss: 4.6365
	step 116:lm_loss: 4.6216, ppl: 101.6536, loss: 4.6384
	step 117:lm_loss: 4.6247, ppl: 101.9698, loss: 4.6408
	step 118:lm_loss: 4.6237, ppl: 101.8695, loss: 4.6396
	step 119:lm_loss: 4.6299, ppl: 102.5051, loss: 4.6452
	step 120:lm_loss: 4.6319, ppl: 102.7123, loss: 4.6469
	step 121:lm_loss: 4.6320, ppl: 102.7167, loss: 4.6468
	step 122:lm_loss: 4.6340, ppl: 102.9281, loss: 4.6486
	step 123:lm_loss: 4.6359, ppl: 103.1184, loss: 4.6499
	step 124:lm_loss: 4.6347, ppl: 102.9977, loss: 4.6489
	step 125:lm_loss: 4.6297, ppl: 102.4799, loss: 4.6459
	step 126:lm_loss: 4.6352, ppl: 103.0463, loss: 4.6504
	step 127:lm_loss: 4.6366, ppl: 103.1946, loss: 4.6519
	step 128:lm_loss: 4.6269, ppl: 102.1983, loss: 4.6452
	step 129:lm_loss: 4.6263, ppl: 102.1395, loss: 4.6441
	step 130:lm_loss: 4.6237, ppl: 101.8702, loss: 4.6422
	step 131:lm_loss: 4.6254, ppl: 102.0428, loss: 4.6435
	step 132:lm_loss: 4.6259, ppl: 102.0907, loss: 4.6439
	step 133:lm_loss: 4.6281, ppl: 102.3150, loss: 4.6460
	step 134:lm_loss: 4.6333, ppl: 102.8523, loss: 4.6517
	step 135:lm_loss: 4.6485, ppl: 104.4278, loss: 4.6635
	step 136:lm_loss: 4.6460, ppl: 104.1716, loss: 4.6610
	step 137:lm_loss: 4.6444, ppl: 104.0006, loss: 4.6595
	step 138:lm_loss: 4.6433, ppl: 103.8838, loss: 4.6584
	step 139:lm_loss: 4.6432, ppl: 103.8782, loss: 4.6583
	step 140:lm_loss: 4.6400, ppl: 103.5466, loss: 4.6553
	step 141:lm_loss: 4.6343, ppl: 102.9536, loss: 4.6518
	step 142:lm_loss: 4.6314, ppl: 102.6535, loss: 4.6501
	step 143:lm_loss: 4.6258, ppl: 102.0855, loss: 4.6442
	step 144:lm_loss: 4.6243, ppl: 101.9353, loss: 4.6424
	step 145:lm_loss: 4.6111, ppl: 100.5963, loss: 4.6329
	step 146:lm_loss: 4.6053, ppl: 100.0172, loss: 4.6277
	step 147:lm_loss: 4.6042, ppl: 99.8990, loss: 4.6261
	step 148:lm_loss: 4.6014, ppl: 99.6239, loss: 4.6236
	step 149:lm_loss: 4.5981, ppl: 99.2964, loss: 4.6204
	step 150:lm_loss: 4.6063, ppl: 100.1148, loss: 4.6269
	step 151:lm_loss: 4.6052, ppl: 100.0050, loss: 4.6254
	step 152:lm_loss: 4.5984, ppl: 99.3209, loss: 4.6205
	step 153:lm_loss: 4.6009, ppl: 99.5780, loss: 4.6238
	step 154:lm_loss: 4.6027, ppl: 99.7514, loss: 4.6253
	step 155:lm_loss: 4.5986, ppl: 99.3452, loss: 4.6214
	step 156:lm_loss: 4.6022, ppl: 99.7077, loss: 4.6252
	step 157:lm_loss: 4.6021, ppl: 99.6892, loss: 4.6249
	step 158:lm_loss: 4.6013, ppl: 99.6141, loss: 4.6238
	step 159:lm_loss: 4.6039, ppl: 99.8765, loss: 4.6268
	step 160:lm_loss: 4.5995, ppl: 99.4386, loss: 4.6217
	step 161:lm_loss: 4.5896, ppl: 98.4591, loss: 4.6132
	step 162:lm_loss: 4.5930, ppl: 98.7883, loss: 4.6169
	step 163:lm_loss: 4.5967, ppl: 99.1535, loss: 4.6225
	step 164:lm_loss: 4.5898, ppl: 98.4714, loss: 4.6183
	step 165:lm_loss: 4.5919, ppl: 98.6805, loss: 4.6206
	step 166:lm_loss: 4.5813, ppl: 97.6367, loss: 4.6136
	step 167:lm_loss: 4.5883, ppl: 98.3234, loss: 4.6211
	step 168:lm_loss: 4.5900, ppl: 98.4926, loss: 4.6225
	step 169:lm_loss: 4.5982, ppl: 99.3053, loss: 4.6315
	step 170:lm_loss: 4.5995, ppl: 99.4393, loss: 4.6327
	step 171:lm_loss: 4.5998, ppl: 99.4636, loss: 4.6327
	step 172:lm_loss: 4.5959, ppl: 99.0757, loss: 4.6258
	step 173:lm_loss: 4.5999, ppl: 99.4740, loss: 4.6285
	step 174:lm_loss: 4.5988, ppl: 99.3633, loss: 4.6270
	step 175:lm_loss: 4.5988, ppl: 99.3694, loss: 4.6269
	step 176:lm_loss: 4.6001, ppl: 99.4912, loss: 4.6279
	step 177:lm_loss: 4.5995, ppl: 99.4307, loss: 4.6271
	step 178:lm_loss: 4.6003, ppl: 99.5102, loss: 4.6277
	step 179:lm_loss: 4.6027, ppl: 99.7544, loss: 4.6307
	step 180:lm_loss: 4.6013, ppl: 99.6148, loss: 4.6284
	step 181:lm_loss: 4.6042, ppl: 99.8996, loss: 4.6308
	step 182:lm_loss: 4.6003, ppl: 99.5150, loss: 4.6252
	step 183:lm_loss: 4.6047, ppl: 99.9526, loss: 4.6274
	step 184:lm_loss: 4.5942, ppl: 98.9138, loss: 4.6200
	step 185:lm_loss: 4.5980, ppl: 99.2869, loss: 4.6252
	step 186:lm_loss: 4.6014, ppl: 99.6274, loss: 4.6282
	step 187:lm_loss: 4.6015, ppl: 99.6298, loss: 4.6281
	step 188:lm_loss: 4.5977, ppl: 99.2595, loss: 4.6237
	step 189:lm_loss: 4.5980, ppl: 99.2814, loss: 4.6238
	step 190:lm_loss: 4.5929, ppl: 98.7815, loss: 4.6180
	step 191:lm_loss: 4.5961, ppl: 99.0945, loss: 4.6204
	step 192:lm_loss: 4.5990, ppl: 99.3837, loss: 4.6232
	step 193:lm_loss: 4.6029, ppl: 99.7722, loss: 4.6266
	step 194:lm_loss: 4.5943, ppl: 98.9230, loss: 4.6187
	step 195:lm_loss: 4.5921, ppl: 98.7052, loss: 4.6165
	step 196:lm_loss: 4.5864, ppl: 98.1415, loss: 4.6119
	step 197:lm_loss: 4.5847, ppl: 97.9784, loss: 4.6096
	step 198:lm_loss: 4.5801, ppl: 97.5287, loss: 4.6046
	step 199:lm_loss: 4.5797, ppl: 97.4850, loss: 4.6039
	step 200:lm_loss: 4.5807, ppl: 97.5821, loss: 4.6049
	step 201:lm_loss: 4.5794, ppl: 97.4511, loss: 4.6029
	step 202:lm_loss: 4.5795, ppl: 97.4679, loss: 4.6029
	step 203:lm_loss: 4.5781, ppl: 97.3276, loss: 4.6008
	step 204:lm_loss: 4.5763, ppl: 97.1509, loss: 4.5991
	step 205:lm_loss: 4.5752, ppl: 97.0522, loss: 4.5978
	step 206:lm_loss: 4.5776, ppl: 97.2845, loss: 4.6001
	step 207:lm_loss: 4.5769, ppl: 97.2078, loss: 4.5997
	step 208:lm_loss: 4.5780, ppl: 97.3234, loss: 4.6008
	step 209:lm_loss: 4.5767, ppl: 97.1920, loss: 4.5991
	step 210:lm_loss: 4.5835, ppl: 97.8609, loss: 4.6047
	step 211:lm_loss: 4.5820, ppl: 97.7099, loss: 4.6030
	step 212:lm_loss: 4.5850, ppl: 98.0071, loss: 4.6052
	step 213:lm_loss: 4.5847, ppl: 97.9750, loss: 4.6048
	step 214:lm_loss: 4.5858, ppl: 98.0858, loss: 4.6064
	step 215:lm_loss: 4.5825, ppl: 97.7597, loss: 4.6030
	step 216:lm_loss: 4.5746, ppl: 96.9926, loss: 4.5978
	step 217:lm_loss: 4.5783, ppl: 97.3484, loss: 4.6019
	step 218:lm_loss: 4.5826, ppl: 97.7691, loss: 4.6075
	step 219:lm_loss: 4.5787, ppl: 97.3865, loss: 4.6038
	step 220:lm_loss: 4.5778, ppl: 97.3000, loss: 4.6024
	step 221:lm_loss: 4.5783, ppl: 97.3471, loss: 4.6029
	step 222:lm_loss: 4.5798, ppl: 97.4977, loss: 4.6039
	step 223:lm_loss: 4.5793, ppl: 97.4483, loss: 4.6031
	step 224:lm_loss: 4.5802, ppl: 97.5296, loss: 4.6037
	step 225:lm_loss: 4.5830, ppl: 97.8095, loss: 4.6063
	step 226:lm_loss: 4.5819, ppl: 97.7036, loss: 4.6046
	step 227:lm_loss: 4.5768, ppl: 97.2050, loss: 4.5989
	step 228:lm_loss: 4.5741, ppl: 96.9406, loss: 4.5955
	step 229:lm_loss: 4.5778, ppl: 97.3036, loss: 4.5986
	step 230:lm_loss: 4.5828, ppl: 97.7855, loss: 4.6052
	step 231:lm_loss: 4.5813, ppl: 97.6404, loss: 4.6036
	step 232:lm_loss: 4.5869, ppl: 98.1931, loss: 4.6082
	step 233:lm_loss: 4.5854, ppl: 98.0433, loss: 4.6064
	step 234:lm_loss: 4.5850, ppl: 98.0024, loss: 4.6059
	step 235:lm_loss: 4.5867, ppl: 98.1687, loss: 4.6069
	step 236:lm_loss: 4.5899, ppl: 98.4894, loss: 4.6119
	step 237:lm_loss: 4.5894, ppl: 98.4332, loss: 4.6110
	step 238:lm_loss: 4.5906, ppl: 98.5576, loss: 4.6118
	step 239:lm_loss: 4.5820, ppl: 97.7096, loss: 4.6055
	step 240:lm_loss: 4.5830, ppl: 97.8053, loss: 4.6064
	step 241:lm_loss: 4.5845, ppl: 97.9543, loss: 4.6083
	step 242:lm_loss: 4.5866, ppl: 98.1613, loss: 4.6098
	step 243:lm_loss: 4.5886, ppl: 98.3548, loss: 4.6115
	step 244:lm_loss: 4.5903, ppl: 98.5232, loss: 4.6134
	step 245:lm_loss: 4.5905, ppl: 98.5419, loss: 4.6135
	step 246:lm_loss: 4.5923, ppl: 98.7231, loss: 4.6159
	step 247:lm_loss: 4.5923, ppl: 98.7235, loss: 4.6159
	step 248:lm_loss: 4.5931, ppl: 98.8031, loss: 4.6167
	step 249:lm_loss: 4.5933, ppl: 98.8248, loss: 4.6168
	step 250:lm_loss: 4.5918, ppl: 98.6721, loss: 4.6157
	step 251:lm_loss: 4.5874, ppl: 98.2396, loss: 4.6122
	step 252:lm_loss: 4.5868, ppl: 98.1804, loss: 4.6118
	step 253:lm_loss: 4.5877, ppl: 98.2655, loss: 4.6126
	step 254:lm_loss: 4.5915, ppl: 98.6400, loss: 4.6170
	step 255:lm_loss: 4.5865, ppl: 98.1547, loss: 4.6128
	step 256:lm_loss: 4.5863, ppl: 98.1350, loss: 4.6125
	step 257:lm_loss: 4.5850, ppl: 97.9985, loss: 4.6107
	step 258:lm_loss: 4.5729, ppl: 96.8222, loss: 4.6048
	step 259:lm_loss: 4.5757, ppl: 97.0982, loss: 4.6064
	step 260:lm_loss: 4.5741, ppl: 96.9447, loss: 4.6039
	step 261:lm_loss: 4.5724, ppl: 96.7752, loss: 4.6024
	step 262:lm_loss: 4.5727, ppl: 96.8056, loss: 4.6025
	step 263:lm_loss: 4.5729, ppl: 96.8209, loss: 4.6026
	step 264:lm_loss: 4.5732, ppl: 96.8498, loss: 4.6028
	step 265:lm_loss: 4.5755, ppl: 97.0775, loss: 4.6049
	step 266:lm_loss: 4.5792, ppl: 97.4373, loss: 4.6090
	step 267:lm_loss: 4.5808, ppl: 97.5894, loss: 4.6105
	step 268:lm_loss: 4.5814, ppl: 97.6479, loss: 4.6110
	step 269:lm_loss: 4.5853, ppl: 98.0294, loss: 4.6140
	step 270:lm_loss: 4.5852, ppl: 98.0198, loss: 4.6138
	step 271:lm_loss: 4.5815, ppl: 97.6592, loss: 4.6103
	step 272:lm_loss: 4.5805, ppl: 97.5620, loss: 4.6088
	step 273:lm_loss: 4.5794, ppl: 97.4511, loss: 4.6078
	step 274:lm_loss: 4.5851, ppl: 98.0143, loss: 4.6156
	step 275:lm_loss: 4.5883, ppl: 98.3305, loss: 4.6190
	step 276:lm_loss: 4.5869, ppl: 98.1852, loss: 4.6179
	step 277:lm_loss: 4.5891, ppl: 98.4071, loss: 4.6202
	step 278:lm_loss: 4.5908, ppl: 98.5761, loss: 4.6211
	step 279:lm_loss: 4.5894, ppl: 98.4341, loss: 4.6195
	step 280:lm_loss: 4.5905, ppl: 98.5474, loss: 4.6204
	step 281:lm_loss: 4.5898, ppl: 98.4794, loss: 4.6199
	step 282:lm_loss: 4.5889, ppl: 98.3865, loss: 4.6185
	step 283:lm_loss: 4.5897, ppl: 98.4649, loss: 4.6194
	step 284:lm_loss: 4.5885, ppl: 98.3506, loss: 4.6187
	step 285:lm_loss: 4.5876, ppl: 98.2589, loss: 4.6177
	step 286:lm_loss: 4.5847, ppl: 97.9710, loss: 4.6141
	step 287:lm_loss: 4.5842, ppl: 97.9204, loss: 4.6134
	step 288:lm_loss: 4.5833, ppl: 97.8363, loss: 4.6122
	step 289:lm_loss: 4.5815, ppl: 97.6641, loss: 4.6098
	step 290:lm_loss: 4.5841, ppl: 97.9121, loss: 4.6128
	step 291:lm_loss: 4.5845, ppl: 97.9503, loss: 4.6131
	step 292:lm_loss: 4.5844, ppl: 97.9437, loss: 4.6129
	step 293:lm_loss: 4.5886, ppl: 98.3550, loss: 4.6163
	step 294:lm_loss: 4.5869, ppl: 98.1899, loss: 4.6145
	step 295:lm_loss: 4.5860, ppl: 98.0983, loss: 4.6129
	step 296:lm_loss: 4.5825, ppl: 97.7549, loss: 4.6106
	step 297:lm_loss: 4.5849, ppl: 97.9923, loss: 4.6134
	step 298:lm_loss: 4.5875, ppl: 98.2484, loss: 4.6149
	step 299:lm_loss: 4.5903, ppl: 98.5246, loss: 4.6178
	step 300:lm_loss: 4.5928, ppl: 98.7735, loss: 4.6197
	step 301:lm_loss: 4.5933, ppl: 98.8220, loss: 4.6201
	step 302:lm_loss: 4.5928, ppl: 98.7675, loss: 4.6195
	step 303:lm_loss: 4.5952, ppl: 99.0084, loss: 4.6221
	step 304:lm_loss: 4.6005, ppl: 99.5353, loss: 4.6273
	step 305:lm_loss: 4.6001, ppl: 99.4906, loss: 4.6268
	step 306:lm_loss: 4.6026, ppl: 99.7383, loss: 4.6295
	step 307:lm_loss: 4.6029, ppl: 99.7754, loss: 4.6297
	step 308:lm_loss: 4.6020, ppl: 99.6785, loss: 4.6288
	step 309:lm_loss: 4.6032, ppl: 99.8045, loss: 4.6302
	step 310:lm_loss: 4.6038, ppl: 99.8670, loss: 4.6307
	step 311:lm_loss: 4.6008, ppl: 99.5625, loss: 4.6266
	step 312:lm_loss: 4.6014, ppl: 99.6264, loss: 4.6272
	step 313:lm_loss: 4.6011, ppl: 99.5970, loss: 4.6268
	step 314:lm_loss: 4.6000, ppl: 99.4818, loss: 4.6253
	step 315:lm_loss: 4.5982, ppl: 99.3081, loss: 4.6242
	step 316:lm_loss: 4.5969, ppl: 99.1741, loss: 4.6229
	step 317:lm_loss: 4.5913, ppl: 98.6248, loss: 4.6179
	step 318:lm_loss: 4.5938, ppl: 98.8656, loss: 4.6204
	step 319:lm_loss: 4.5936, ppl: 98.8482, loss: 4.6203
	step 320:lm_loss: 4.5935, ppl: 98.8351, loss: 4.6200
	step 321:lm_loss: 4.5941, ppl: 98.9007, loss: 4.6209
	step 322:lm_loss: 4.5949, ppl: 98.9767, loss: 4.6219
	step 323:lm_loss: 4.5934, ppl: 98.8284, loss: 4.6195
	step 324:lm_loss: 4.5974, ppl: 99.2215, loss: 4.6252
	step 325:lm_loss: 4.5980, ppl: 99.2864, loss: 4.6259
	step 326:lm_loss: 4.5963, ppl: 99.1212, loss: 4.6239
	step 327:lm_loss: 4.5951, ppl: 99.0029, loss: 4.6223
	step 328:lm_loss: 4.5975, ppl: 99.2377, loss: 4.6247
	step 329:lm_loss: 4.5974, ppl: 99.2296, loss: 4.6246
	step 330:lm_loss: 4.5982, ppl: 99.3015, loss: 4.6253
	step 331:lm_loss: 4.5973, ppl: 99.2179, loss: 4.6245
	step 332:lm_loss: 4.5911, ppl: 98.6028, loss: 4.6204
	step 333:lm_loss: 4.5900, ppl: 98.4970, loss: 4.6193
	step 334:lm_loss: 4.5902, ppl: 98.5186, loss: 4.6194
	step 335:lm_loss: 4.5908, ppl: 98.5707, loss: 4.6200
	step 336:lm_loss: 4.5908, ppl: 98.5710, loss: 4.6199
	step 337:lm_loss: 4.5876, ppl: 98.2560, loss: 4.6159
	step 338:lm_loss: 4.5887, ppl: 98.3630, loss: 4.6169
	step 339:lm_loss: 4.5898, ppl: 98.4773, loss: 4.6176
	step 340:lm_loss: 4.5886, ppl: 98.3546, loss: 4.6163
	step 341:lm_loss: 4.5907, ppl: 98.5637, loss: 4.6184
	step 342:lm_loss: 4.5869, ppl: 98.1880, loss: 4.6134
	step 343:lm_loss: 4.5864, ppl: 98.1438, loss: 4.6131
	step 344:lm_loss: 4.5888, ppl: 98.3715, loss: 4.6159
	step 345:lm_loss: 4.5906, ppl: 98.5501, loss: 4.6180
	step 346:lm_loss: 4.5918, ppl: 98.6763, loss: 4.6187
	step 347:lm_loss: 4.5917, ppl: 98.6629, loss: 4.6185
	step 348:lm_loss: 4.5924, ppl: 98.7349, loss: 4.6193
	step 349:lm_loss: 4.5942, ppl: 98.9061, loss: 4.6202
	step 350:lm_loss: 4.5926, ppl: 98.7547, loss: 4.6189
	step 351:lm_loss: 4.5872, ppl: 98.2149, loss: 4.6146
	step 352:lm_loss: 4.5890, ppl: 98.3980, loss: 4.6162
	step 353:lm_loss: 4.5880, ppl: 98.3013, loss: 4.6150
	step 354:lm_loss: 4.5899, ppl: 98.4835, loss: 4.6173
	step 355:lm_loss: 4.5894, ppl: 98.4377, loss: 4.6168
	step 356:lm_loss: 4.5917, ppl: 98.6583, loss: 4.6193
	step 357:lm_loss: 4.5871, ppl: 98.2096, loss: 4.6166
	step 358:lm_loss: 4.5870, ppl: 98.2021, loss: 4.6164
	step 359:lm_loss: 4.5859, ppl: 98.0945, loss: 4.6150
	step 360:lm_loss: 4.5879, ppl: 98.2872, loss: 4.6171
	step 361:lm_loss: 4.5818, ppl: 97.6935, loss: 4.6140
	step 362:lm_loss: 4.5835, ppl: 97.8521, loss: 4.6161
	step 363:lm_loss: 4.5822, ppl: 97.7325, loss: 4.6152
	step 364:lm_loss: 4.5810, ppl: 97.6137, loss: 4.6140
	step 365:lm_loss: 4.5842, ppl: 97.9217, loss: 4.6166
	step 366:lm_loss: 4.5870, ppl: 98.2042, loss: 4.6185
	step 367:lm_loss: 4.5894, ppl: 98.4310, loss: 4.6208
	step 368:lm_loss: 4.5850, ppl: 97.9990, loss: 4.6161
	step 369:lm_loss: 4.5808, ppl: 97.5952, loss: 4.6122
	step 370:lm_loss: 4.5801, ppl: 97.5227, loss: 4.6113
	step 371:lm_loss: 4.5793, ppl: 97.4414, loss: 4.6104
	step 372:lm_loss: 4.5798, ppl: 97.4954, loss: 4.6107
	step 373:lm_loss: 4.5809, ppl: 97.6004, loss: 4.6119
	step 374:lm_loss: 4.5809, ppl: 97.6051, loss: 4.6119
	step 375:lm_loss: 4.5797, ppl: 97.4843, loss: 4.6109
	step 376:lm_loss: 4.5782, ppl: 97.3392, loss: 4.6088
	step 377:lm_loss: 4.5776, ppl: 97.2776, loss: 4.6079
	step 378:lm_loss: 4.5771, ppl: 97.2323, loss: 4.6072
	step 379:lm_loss: 4.5773, ppl: 97.2468, loss: 4.6073
	step 380:lm_loss: 4.5775, ppl: 97.2710, loss: 4.6075
	step 381:lm_loss: 4.5802, ppl: 97.5345, loss: 4.6096
	step 382:lm_loss: 4.5800, ppl: 97.5184, loss: 4.6093
	step 383:lm_loss: 4.5822, ppl: 97.7297, loss: 4.6112
	step 384:lm_loss: 4.5806, ppl: 97.5736, loss: 4.6102
	step 385:lm_loss: 4.5784, ppl: 97.3552, loss: 4.6065
	step 386:lm_loss: 4.5823, ppl: 97.7412, loss: 4.6097
	step 387:lm_loss: 4.5818, ppl: 97.6917, loss: 4.6090
	step 388:lm_loss: 4.5806, ppl: 97.5712, loss: 4.6070
	step 389:lm_loss: 4.5800, ppl: 97.5171, loss: 4.6065
	step 390:lm_loss: 4.5798, ppl: 97.4981, loss: 4.6063
	step 391:lm_loss: 4.5798, ppl: 97.4987, loss: 4.6062
	step 392:lm_loss: 4.5798, ppl: 97.4986, loss: 4.6061
	step 393:lm_loss: 4.5783, ppl: 97.3475, loss: 4.6046
	step 394:lm_loss: 4.5783, ppl: 97.3475, loss: 4.6045
	step 395:lm_loss: 4.5740, ppl: 96.9266, loss: 4.6000
	step 396:lm_loss: 4.5732, ppl: 96.8547, loss: 4.5996
	step 397:lm_loss: 4.5760, ppl: 97.1273, loss: 4.6022
	step 398:lm_loss: 4.5801, ppl: 97.5233, loss: 4.6075
	step 399:lm_loss: 4.5802, ppl: 97.5299, loss: 4.6075
	step 400:lm_loss: 4.5789, ppl: 97.4053, loss: 4.6060
	step 401:lm_loss: 4.5815, ppl: 97.6648, loss: 4.6083
	step 402:lm_loss: 4.5813, ppl: 97.6459, loss: 4.6080
	step 403:lm_loss: 4.5812, ppl: 97.6319, loss: 4.6078
	step 404:lm_loss: 4.5819, ppl: 97.7030, loss: 4.6084
	step 405:lm_loss: 4.5831, ppl: 97.8207, loss: 4.6091
	step 406:lm_loss: 4.5837, ppl: 97.8718, loss: 4.6097
	step 407:lm_loss: 4.5862, ppl: 98.1238, loss: 4.6121
	step 408:lm_loss: 4.5866, ppl: 98.1638, loss: 4.6123
	step 409:lm_loss: 4.5862, ppl: 98.1250, loss: 4.6119
	step 410:lm_loss: 4.5886, ppl: 98.3520, loss: 4.6134
	step 411:lm_loss: 4.5900, ppl: 98.4943, loss: 4.6142
	step 412:lm_loss: 4.5882, ppl: 98.3209, loss: 4.6126
	step 413:lm_loss: 4.5874, ppl: 98.2420, loss: 4.6120
	step 414:lm_loss: 4.5869, ppl: 98.1891, loss: 4.6115
	step 415:lm_loss: 4.5794, ppl: 97.4533, loss: 4.6070
	step 416:lm_loss: 4.5788, ppl: 97.3990, loss: 4.6065
	step 417:lm_loss: 4.5791, ppl: 97.4301, loss: 4.6068
	step 418:lm_loss: 4.5796, ppl: 97.4738, loss: 4.6072
	step 419:lm_loss: 4.5765, ppl: 97.1721, loss: 4.6039
	step 420:lm_loss: 4.5775, ppl: 97.2707, loss: 4.6045
	step 421:lm_loss: 4.5816, ppl: 97.6717, loss: 4.6078
	step 422:lm_loss: 4.5819, ppl: 97.7020, loss: 4.6080
	step 423:lm_loss: 4.5828, ppl: 97.7838, loss: 4.6086
	step 424:lm_loss: 4.5848, ppl: 97.9852, loss: 4.6106
	step 425:lm_loss: 4.5854, ppl: 98.0420, loss: 4.6113
	step 426:lm_loss: 4.5857, ppl: 98.0739, loss: 4.6115
	step 427:lm_loss: 4.5867, ppl: 98.1687, loss: 4.6123
	step 428:lm_loss: 4.5895, ppl: 98.4491, loss: 4.6156
	step 429:lm_loss: 4.5893, ppl: 98.4284, loss: 4.6151
	step 430:lm_loss: 4.5928, ppl: 98.7683, loss: 4.6167
	step 431:lm_loss: 4.5918, ppl: 98.6689, loss: 4.6151
	step 432:lm_loss: 4.5896, ppl: 98.4549, loss: 4.6136
	step 433:lm_loss: 4.5883, ppl: 98.3258, loss: 4.6117
	step 434:lm_loss: 4.5851, ppl: 98.0133, loss: 4.6077
	step 435:lm_loss: 4.5834, ppl: 97.8441, loss: 4.6052
	step 436:lm_loss: 4.5838, ppl: 97.8887, loss: 4.6056
	step 437:lm_loss: 4.5845, ppl: 97.9534, loss: 4.6061
	step 438:lm_loss: 4.5845, ppl: 97.9560, loss: 4.6061
	step 439:lm_loss: 4.5861, ppl: 98.1151, loss: 4.6079
	step 440:lm_loss: 4.5862, ppl: 98.1208, loss: 4.6079
	step 441:lm_loss: 4.5861, ppl: 98.1071, loss: 4.6077
	step 442:lm_loss: 4.5806, ppl: 97.5729, loss: 4.6039
	step 443:lm_loss: 4.5821, ppl: 97.7178, loss: 4.6053
	step 444:lm_loss: 4.5817, ppl: 97.6789, loss: 4.6048
	step 445:lm_loss: 4.5808, ppl: 97.5948, loss: 4.6037
	step 446:lm_loss: 4.5814, ppl: 97.6462, loss: 4.6044
	step 447:lm_loss: 4.5835, ppl: 97.8607, loss: 4.6064
	step 448:lm_loss: 4.5832, ppl: 97.8251, loss: 4.6059
	step 449:lm_loss: 4.5846, ppl: 97.9606, loss: 4.6069
	step 450:lm_loss: 4.5851, ppl: 98.0114, loss: 4.6075
	step 451:lm_loss: 4.5858, ppl: 98.0775, loss: 4.6082
	step 452:lm_loss: 4.5846, ppl: 97.9630, loss: 4.6066
	step 453:lm_loss: 4.5847, ppl: 97.9722, loss: 4.6066
	step 454:lm_loss: 4.5848, ppl: 97.9828, loss: 4.6067
	step 455:lm_loss: 4.5843, ppl: 97.9362, loss: 4.6063
	step 456:lm_loss: 4.5842, ppl: 97.9249, loss: 4.6062
	step 457:lm_loss: 4.5828, ppl: 97.7861, loss: 4.6052
	step 458:lm_loss: 4.5790, ppl: 97.4143, loss: 4.6025
	step 459:lm_loss: 4.5801, ppl: 97.5217, loss: 4.6039
	step 460:lm_loss: 4.5785, ppl: 97.3727, loss: 4.6023
	step 461:lm_loss: 4.5750, ppl: 97.0293, loss: 4.5988
	step 462:lm_loss: 4.5753, ppl: 97.0596, loss: 4.5992
	step 463:lm_loss: 4.5730, ppl: 96.8309, loss: 4.5974
	step 464:lm_loss: 4.5711, ppl: 96.6483, loss: 4.5949
	step 465:lm_loss: 4.5705, ppl: 96.5905, loss: 4.5944
	step 466:lm_loss: 4.5709, ppl: 96.6276, loss: 4.5947
	step 467:lm_loss: 4.5720, ppl: 96.7342, loss: 4.5959
	step 468:lm_loss: 4.5701, ppl: 96.5507, loss: 4.5937
	step 469:lm_loss: 4.5703, ppl: 96.5762, loss: 4.5940
	step 470:lm_loss: 4.5710, ppl: 96.6432, loss: 4.5947
	step 471:lm_loss: 4.5711, ppl: 96.6466, loss: 4.5947
	step 472:lm_loss: 4.5715, ppl: 96.6911, loss: 4.5953
	step 473:lm_loss: 4.5728, ppl: 96.8191, loss: 4.5968
	step 474:lm_loss: 4.5725, ppl: 96.7844, loss: 4.5960
	step 475:lm_loss: 4.5719, ppl: 96.7305, loss: 4.5953
	step 476:lm_loss: 4.5722, ppl: 96.7521, loss: 4.5954
	step 477:lm_loss: 4.5706, ppl: 96.5985, loss: 4.5943
	step 478:lm_loss: 4.5700, ppl: 96.5473, loss: 4.5937
	step 479:lm_loss: 4.5704, ppl: 96.5787, loss: 4.5940
	step 480:lm_loss: 4.5680, ppl: 96.3492, loss: 4.5926
	step 481:lm_loss: 4.5666, ppl: 96.2176, loss: 4.5912
	step 482:lm_loss: 4.5660, ppl: 96.1597, loss: 4.5904
	step 483:lm_loss: 4.5665, ppl: 96.2080, loss: 4.5907
	step 484:lm_loss: 4.5670, ppl: 96.2562, loss: 4.5911
	step 485:lm_loss: 4.5661, ppl: 96.1641, loss: 4.5903
	step 486:lm_loss: 4.5675, ppl: 96.2993, loss: 4.5912
	step 487:lm_loss: 4.5668, ppl: 96.2354, loss: 4.5906
	step 488:lm_loss: 4.5652, ppl: 96.0777, loss: 4.5890
	step 489:lm_loss: 4.5671, ppl: 96.2678, loss: 4.5902
	step 490:lm_loss: 4.5661, ppl: 96.1636, loss: 4.5893
	step 491:lm_loss: 4.5654, ppl: 96.0975, loss: 4.5886
	step 492:lm_loss: 4.5664, ppl: 96.2016, loss: 4.5894
	step 493:lm_loss: 4.5672, ppl: 96.2721, loss: 4.5902
	step 494:lm_loss: 4.5684, ppl: 96.3907, loss: 4.5914
	step 495:lm_loss: 4.5687, ppl: 96.4170, loss: 4.5917
	step 496:lm_loss: 4.5686, ppl: 96.4065, loss: 4.5916
	step 497:lm_loss: 4.5690, ppl: 96.4473, loss: 4.5919
	step 498:lm_loss: 4.5712, ppl: 96.6631, loss: 4.5935
	step 499:lm_loss: 4.5687, ppl: 96.4152, loss: 4.5915
	step 500:lm_loss: 4.5673, ppl: 96.2823, loss: 4.5900
	step 501:lm_loss: 4.5675, ppl: 96.3057, loss: 4.5903
	step 502:lm_loss: 4.5678, ppl: 96.3290, loss: 4.5905
	step 503:lm_loss: 4.5665, ppl: 96.2051, loss: 4.5886
	step 504:lm_loss: 4.5656, ppl: 96.1207, loss: 4.5876
	step 505:lm_loss: 4.5672, ppl: 96.2708, loss: 4.5887
	step 506:lm_loss: 4.5678, ppl: 96.3334, loss: 4.5895
	step 507:lm_loss: 4.5666, ppl: 96.2198, loss: 4.5882
	step 508:lm_loss: 4.5681, ppl: 96.3626, loss: 4.5891
	step 509:lm_loss: 4.5680, ppl: 96.3532, loss: 4.5890
	step 510:lm_loss: 4.5679, ppl: 96.3383, loss: 4.5887
	step 511:lm_loss: 4.5660, ppl: 96.1616, loss: 4.5878
	step 512:lm_loss: 4.5662, ppl: 96.1822, loss: 4.5881
	step 513:lm_loss: 4.5664, ppl: 96.1985, loss: 4.5882
	step 514:lm_loss: 4.5589, ppl: 95.4792, loss: 4.5836
	step 515:lm_loss: 4.5601, ppl: 95.5965, loss: 4.5853
	step 516:lm_loss: 4.5590, ppl: 95.4872, loss: 4.5841
	step 517:lm_loss: 4.5593, ppl: 95.5172, loss: 4.5844
	step 518:lm_loss: 4.5559, ppl: 95.1923, loss: 4.5818
	step 519:lm_loss: 4.5555, ppl: 95.1542, loss: 4.5812
	step 520:lm_loss: 4.5562, ppl: 95.2248, loss: 4.5821
	step 521:lm_loss: 4.5562, ppl: 95.2240, loss: 4.5821
	step 522:lm_loss: 4.5501, ppl: 94.6401, loss: 4.5786
	step 523:lm_loss: 4.5510, ppl: 94.7304, loss: 4.5797
	step 524:lm_loss: 4.5532, ppl: 94.9403, loss: 4.5819
	step 525:lm_loss: 4.5536, ppl: 94.9730, loss: 4.5821
	step 526:lm_loss: 4.5519, ppl: 94.8141, loss: 4.5807
	step 527:lm_loss: 4.5525, ppl: 94.8729, loss: 4.5814
	step 528:lm_loss: 4.5529, ppl: 94.9082, loss: 4.5817
	step 529:lm_loss: 4.5530, ppl: 94.9199, loss: 4.5818
	step 530:lm_loss: 4.5529, ppl: 94.9097, loss: 4.5816
	step 531:lm_loss: 4.5527, ppl: 94.8846, loss: 4.5813
	step 532:lm_loss: 4.5528, ppl: 94.9025, loss: 4.5814
	step 533:lm_loss: 4.5508, ppl: 94.7082, loss: 4.5796
	step 534:lm_loss: 4.5521, ppl: 94.8317, loss: 4.5809
	step 535:lm_loss: 4.5527, ppl: 94.8836, loss: 4.5814
	step 536:lm_loss: 4.5523, ppl: 94.8492, loss: 4.5810
	step 537:lm_loss: 4.5511, ppl: 94.7337, loss: 4.5795
	step 538:lm_loss: 4.5514, ppl: 94.7674, loss: 4.5797
	step 539:lm_loss: 4.5514, ppl: 94.7629, loss: 4.5795
	step 540:lm_loss: 4.5503, ppl: 94.6563, loss: 4.5787
	step 541:lm_loss: 4.5510, ppl: 94.7253, loss: 4.5792
	step 542:lm_loss: 4.5517, ppl: 94.7890, loss: 4.5799
	step 543:lm_loss: 4.5533, ppl: 94.9474, loss: 4.5814
	step 544:lm_loss: 4.5507, ppl: 94.7018, loss: 4.5785
	step 545:lm_loss: 4.5508, ppl: 94.7054, loss: 4.5785
	step 546:lm_loss: 4.5506, ppl: 94.6935, loss: 4.5783
	step 547:lm_loss: 4.5526, ppl: 94.8833, loss: 4.5802
	step 548:lm_loss: 4.5526, ppl: 94.8830, loss: 4.5802
	step 549:lm_loss: 4.5527, ppl: 94.8898, loss: 4.5802
	step 550:lm_loss: 4.5518, ppl: 94.8074, loss: 4.5795
	step 551:lm_loss: 4.5511, ppl: 94.7407, loss: 4.5788
	step 552:lm_loss: 4.5501, ppl: 94.6414, loss: 4.5782
	step 553:lm_loss: 4.5530, ppl: 94.9163, loss: 4.5816
	step 554:lm_loss: 4.5543, ppl: 95.0381, loss: 4.5830
	step 555:lm_loss: 4.5538, ppl: 94.9886, loss: 4.5826
	step 556:lm_loss: 4.5547, ppl: 95.0796, loss: 4.5835
	step 557:lm_loss: 4.5529, ppl: 94.9047, loss: 4.5815
	step 558:lm_loss: 4.5512, ppl: 94.7483, loss: 4.5787
	step 559:lm_loss: 4.5502, ppl: 94.6508, loss: 4.5775
	step 560:lm_loss: 4.5515, ppl: 94.7705, loss: 4.5787
	step 561:lm_loss: 4.5504, ppl: 94.6709, loss: 4.5777
	step 562:lm_loss: 4.5507, ppl: 94.6940, loss: 4.5780
	step 563:lm_loss: 4.5497, ppl: 94.6047, loss: 4.5768
	step 564:lm_loss: 4.5483, ppl: 94.4758, loss: 4.5754
	step 565:lm_loss: 4.5494, ppl: 94.5774, loss: 4.5766
	step 566:lm_loss: 4.5489, ppl: 94.5292, loss: 4.5760
	step 567:lm_loss: 4.5487, ppl: 94.5090, loss: 4.5757
	step 568:lm_loss: 4.5474, ppl: 94.3887, loss: 4.5742
	step 569:lm_loss: 4.5492, ppl: 94.5606, loss: 4.5775
	step 570:lm_loss: 4.5498, ppl: 94.6135, loss: 4.5780
	step 571:lm_loss: 4.5514, ppl: 94.7671, loss: 4.5797
	step 572:lm_loss: 4.5503, ppl: 94.6601, loss: 4.5790
	step 573:lm_loss: 4.5509, ppl: 94.7191, loss: 4.5793
	step 574:lm_loss: 4.5511, ppl: 94.7339, loss: 4.5795
	step 575:lm_loss: 4.5518, ppl: 94.8014, loss: 4.5803
	step 576:lm_loss: 4.5526, ppl: 94.8829, loss: 4.5809
	step 577:lm_loss: 4.5511, ppl: 94.7411, loss: 4.5788
	step 578:lm_loss: 4.5520, ppl: 94.8225, loss: 4.5798
	step 579:lm_loss: 4.5535, ppl: 94.9610, loss: 4.5816
	step 580:lm_loss: 4.5530, ppl: 94.9194, loss: 4.5808
	step 581:lm_loss: 4.5528, ppl: 94.8979, loss: 4.5805
	step 582:lm_loss: 4.5508, ppl: 94.7048, loss: 4.5786
	step 583:lm_loss: 4.5503, ppl: 94.6624, loss: 4.5780
	step 584:lm_loss: 4.5496, ppl: 94.5967, loss: 4.5767
	step 585:lm_loss: 4.5493, ppl: 94.5626, loss: 4.5764
	step 586:lm_loss: 4.5492, ppl: 94.5589, loss: 4.5763
	step 587:lm_loss: 4.5485, ppl: 94.4937, loss: 4.5756
	step 588:lm_loss: 4.5486, ppl: 94.4998, loss: 4.5756
	step 589:lm_loss: 4.5479, ppl: 94.4337, loss: 4.5750
	step 590:lm_loss: 4.5463, ppl: 94.2872, loss: 4.5737
	step 591:lm_loss: 4.5443, ppl: 94.0943, loss: 4.5718
	step 592:lm_loss: 4.5446, ppl: 94.1205, loss: 4.5720
	step 593:lm_loss: 4.5454, ppl: 94.1973, loss: 4.5729
	step 594:lm_loss: 4.5462, ppl: 94.2717, loss: 4.5738
	step 595:lm_loss: 4.5452, ppl: 94.1763, loss: 4.5725
	step 596:lm_loss: 4.5472, ppl: 94.3698, loss: 4.5740
	step 597:lm_loss: 4.5477, ppl: 94.4149, loss: 4.5743
	step 598:lm_loss: 4.5472, ppl: 94.3687, loss: 4.5738
	step 599:lm_loss: 4.5478, ppl: 94.4283, loss: 4.5746
	step 600:lm_loss: 4.5483, ppl: 94.4764, loss: 4.5751
	step 601:lm_loss: 4.5478, ppl: 94.4255, loss: 4.5747
	step 602:lm_loss: 4.5471, ppl: 94.3564, loss: 4.5740
	step 603:lm_loss: 4.5477, ppl: 94.4119, loss: 4.5745
	step 604:lm_loss: 4.5447, ppl: 94.1358, loss: 4.5732
	step 605:lm_loss: 4.5446, ppl: 94.1265, loss: 4.5730
	step 606:lm_loss: 4.5444, ppl: 94.1053, loss: 4.5727
	step 607:lm_loss: 4.5449, ppl: 94.1517, loss: 4.5733
	step 608:lm_loss: 4.5453, ppl: 94.1883, loss: 4.5735
	step 609:lm_loss: 4.5461, ppl: 94.2641, loss: 4.5746
	step 610:lm_loss: 4.5460, ppl: 94.2560, loss: 4.5745
	step 611:lm_loss: 4.5468, ppl: 94.3307, loss: 4.5756
	step 612:lm_loss: 4.5466, ppl: 94.3081, loss: 4.5752
	step 613:lm_loss: 4.5469, ppl: 94.3425, loss: 4.5756
	step 614:lm_loss: 4.5479, ppl: 94.4313, loss: 4.5769
	step 615:lm_loss: 4.5467, ppl: 94.3240, loss: 4.5753
	step 616:lm_loss: 4.5469, ppl: 94.3429, loss: 4.5755
	step 617:lm_loss: 4.5479, ppl: 94.4307, loss: 4.5765
	step 618:lm_loss: 4.5482, ppl: 94.4648, loss: 4.5768
	step 619:lm_loss: 4.5470, ppl: 94.3468, loss: 4.5755
	step 620:lm_loss: 4.5481, ppl: 94.4483, loss: 4.5772
	step 621:lm_loss: 4.5493, ppl: 94.5696, loss: 4.5779
	step 622:lm_loss: 4.5501, ppl: 94.6450, loss: 4.5786
	step 623:lm_loss: 4.5500, ppl: 94.6335, loss: 4.5784
	step 624:lm_loss: 4.5492, ppl: 94.5523, loss: 4.5774
	step 625:lm_loss: 4.5479, ppl: 94.4316, loss: 4.5757
	step 626:lm_loss: 4.5482, ppl: 94.4654, loss: 4.5759
	step 627:lm_loss: 4.5470, ppl: 94.3536, loss: 4.5744
	step 628:lm_loss: 4.5473, ppl: 94.3750, loss: 4.5746
	step 629:lm_loss: 4.5491, ppl: 94.5496, loss: 4.5772
	step 630:lm_loss: 4.5471, ppl: 94.3567, loss: 4.5760
	step 631:lm_loss: 4.5470, ppl: 94.3468, loss: 4.5759
	step 632:lm_loss: 4.5459, ppl: 94.2444, loss: 4.5745
	step 633:lm_loss: 4.5455, ppl: 94.2091, loss: 4.5739
	step 634:lm_loss: 4.5457, ppl: 94.2238, loss: 4.5740
	step 635:lm_loss: 4.5465, ppl: 94.3001, loss: 4.5747
	step 636:lm_loss: 4.5469, ppl: 94.3384, loss: 4.5752
	step 637:lm_loss: 4.5480, ppl: 94.4452, loss: 4.5767
	step 638:lm_loss: 4.5468, ppl: 94.3272, loss: 4.5753
	step 639:lm_loss: 4.5464, ppl: 94.2946, loss: 4.5747
	step 640:lm_loss: 4.5467, ppl: 94.3200, loss: 4.5749
	step 641:lm_loss: 4.5464, ppl: 94.2923, loss: 4.5746
	step 642:lm_loss: 4.5454, ppl: 94.2026, loss: 4.5732
	step 643:lm_loss: 4.5454, ppl: 94.1969, loss: 4.5731
	step 644:lm_loss: 4.5439, ppl: 94.0544, loss: 4.5714
	step 645:lm_loss: 4.5431, ppl: 93.9778, loss: 4.5704
	step 646:lm_loss: 4.5440, ppl: 94.0672, loss: 4.5714
	step 647:lm_loss: 4.5422, ppl: 93.8983, loss: 4.5698
	step 648:lm_loss: 4.5421, ppl: 93.8870, loss: 4.5697
	step 649:lm_loss: 4.5408, ppl: 93.7645, loss: 4.5682
	step 650:lm_loss: 4.5384, ppl: 93.5440, loss: 4.5664
	step 651:lm_loss: 4.5388, ppl: 93.5747, loss: 4.5668
	step 652:lm_loss: 4.5411, ppl: 93.7950, loss: 4.5691
	step 653:lm_loss: 4.5407, ppl: 93.7569, loss: 4.5685
	step 654:lm_loss: 4.5402, ppl: 93.7118, loss: 4.5678
	step 655:lm_loss: 4.5404, ppl: 93.7261, loss: 4.5679
	step 656:lm_loss: 4.5398, ppl: 93.6695, loss: 4.5675
	step 657:lm_loss: 4.5416, ppl: 93.8368, loss: 4.5687
	step 658:lm_loss: 4.5410, ppl: 93.7869, loss: 4.5681
	step 659:lm_loss: 4.5407, ppl: 93.7610, loss: 4.5676
	step 660:lm_loss: 4.5411, ppl: 93.7974, loss: 4.5679
	step 661:lm_loss: 4.5416, ppl: 93.8454, loss: 4.5682
	step 662:lm_loss: 4.5399, ppl: 93.6845, loss: 4.5668
	step 663:lm_loss: 4.5398, ppl: 93.6698, loss: 4.5666
	step 664:lm_loss: 4.5399, ppl: 93.6799, loss: 4.5667
	step 665:lm_loss: 4.5395, ppl: 93.6432, loss: 4.5662
	step 666:lm_loss: 4.5395, ppl: 93.6474, loss: 4.5662
	step 667:lm_loss: 4.5380, ppl: 93.5029, loss: 4.5644
	step 668:lm_loss: 4.5384, ppl: 93.5396, loss: 4.5646
	step 669:lm_loss: 4.5381, ppl: 93.5162, loss: 4.5643
	step 670:lm_loss: 4.5384, ppl: 93.5410, loss: 4.5645
	step 671:lm_loss: 4.5377, ppl: 93.4765, loss: 4.5640
	step 672:lm_loss: 4.5367, ppl: 93.3829, loss: 4.5634
	step 673:lm_loss: 4.5380, ppl: 93.5041, loss: 4.5650
	step 674:lm_loss: 4.5379, ppl: 93.4970, loss: 4.5649
	step 675:lm_loss: 4.5351, ppl: 93.2319, loss: 4.5624
	step 676:lm_loss: 4.5363, ppl: 93.3456, loss: 4.5635
	step 677:lm_loss: 4.5363, ppl: 93.3444, loss: 4.5635
	step 678:lm_loss: 4.5316, ppl: 92.9091, loss: 4.5612
	step 679:lm_loss: 4.5324, ppl: 92.9814, loss: 4.5619
	step 680:lm_loss: 4.5340, ppl: 93.1338, loss: 4.5638
	step 681:lm_loss: 4.5314, ppl: 92.8842, loss: 4.5611
	step 682:lm_loss: 4.5310, ppl: 92.8504, loss: 4.5607
	step 683:lm_loss: 4.5309, ppl: 92.8389, loss: 4.5606
	step 684:lm_loss: 4.5304, ppl: 92.7943, loss: 4.5601
	step 685:lm_loss: 4.5321, ppl: 92.9574, loss: 4.5617
	step 686:lm_loss: 4.5318, ppl: 92.9285, loss: 4.5613
	step 687:lm_loss: 4.5303, ppl: 92.7863, loss: 4.5604
	step 688:lm_loss: 4.5301, ppl: 92.7704, loss: 4.5602
	step 689:lm_loss: 4.5308, ppl: 92.8330, loss: 4.5607
	step 690:lm_loss: 4.5304, ppl: 92.7926, loss: 4.5601
	step 691:lm_loss: 4.5312, ppl: 92.8699, loss: 4.5607
	step 692:lm_loss: 4.5319, ppl: 92.9353, loss: 4.5613
	step 693:lm_loss: 4.5327, ppl: 93.0087, loss: 4.5621
	step 694:lm_loss: 4.5342, ppl: 93.1536, loss: 4.5637
	step 695:lm_loss: 4.5348, ppl: 93.2039, loss: 4.5644
	step 696:lm_loss: 4.5349, ppl: 93.2132, loss: 4.5644
	step 697:lm_loss: 4.5347, ppl: 93.1953, loss: 4.5641
	step 698:lm_loss: 4.5346, ppl: 93.1881, loss: 4.5640
	step 699:lm_loss: 4.5346, ppl: 93.1897, loss: 4.5640
	step 700:lm_loss: 4.5336, ppl: 93.0969, loss: 4.5625
	step 701:lm_loss: 4.5324, ppl: 92.9820, loss: 4.5613
	step 702:lm_loss: 4.5316, ppl: 92.9117, loss: 4.5605
	step 703:lm_loss: 4.5315, ppl: 92.8937, loss: 4.5603
	step 704:lm_loss: 4.5302, ppl: 92.7785, loss: 4.5595
	step 705:lm_loss: 4.5296, ppl: 92.7256, loss: 4.5589
	step 706:lm_loss: 4.5283, ppl: 92.6001, loss: 4.5577
	step 707:lm_loss: 4.5273, ppl: 92.5056, loss: 4.5563
	step 708:lm_loss: 4.5271, ppl: 92.4911, loss: 4.5561
	step 709:lm_loss: 4.5243, ppl: 92.2308, loss: 4.5536
	step 710:lm_loss: 4.5259, ppl: 92.3767, loss: 4.5557
	step 711:lm_loss: 4.5248, ppl: 92.2793, loss: 4.5543
	step 712:lm_loss: 4.5240, ppl: 92.2061, loss: 4.5535
	step 713:lm_loss: 4.5225, ppl: 92.0619, loss: 4.5519
	step 714:lm_loss: 4.5231, ppl: 92.1242, loss: 4.5523
	step 715:lm_loss: 4.5244, ppl: 92.2394, loss: 4.5536
	step 716:lm_loss: 4.5245, ppl: 92.2535, loss: 4.5537
	step 717:lm_loss: 4.5240, ppl: 92.2019, loss: 4.5529
	step 718:lm_loss: 4.5229, ppl: 92.0993, loss: 4.5519
	step 719:lm_loss: 4.5232, ppl: 92.1257, loss: 4.5521
	step 720:lm_loss: 4.5221, ppl: 92.0321, loss: 4.5510
	step 721:lm_loss: 4.5213, ppl: 91.9593, loss: 4.5499
	step 722:lm_loss: 4.5213, ppl: 91.9510, loss: 4.5497
	step 723:lm_loss: 4.5218, ppl: 92.0030, loss: 4.5505
	step 724:lm_loss: 4.5216, ppl: 91.9815, loss: 4.5501
	step 725:lm_loss: 4.5213, ppl: 91.9589, loss: 4.5498
	step 726:lm_loss: 4.5210, ppl: 91.9252, loss: 4.5493
	step 727:lm_loss: 4.5205, ppl: 91.8855, loss: 4.5487
	step 728:lm_loss: 4.5212, ppl: 91.9504, loss: 4.5495
	step 729:lm_loss: 4.5210, ppl: 91.9243, loss: 4.5490
	step 730:lm_loss: 4.5210, ppl: 91.9303, loss: 4.5490
	step 731:lm_loss: 4.5222, ppl: 92.0402, loss: 4.5498
	step 732:lm_loss: 4.5220, ppl: 92.0184, loss: 4.5494
	step 733:lm_loss: 4.5231, ppl: 92.1177, loss: 4.5509
	step 734:lm_loss: 4.5223, ppl: 92.0490, loss: 4.5503
	step 735:lm_loss: 4.5236, ppl: 92.1639, loss: 4.5515
	step 736:lm_loss: 4.5226, ppl: 92.0759, loss: 4.5506
	step 737:lm_loss: 4.5236, ppl: 92.1685, loss: 4.5518
	step 738:lm_loss: 4.5244, ppl: 92.2449, loss: 4.5525
	step 739:lm_loss: 4.5251, ppl: 92.3078, loss: 4.5531
	step 740:lm_loss: 4.5253, ppl: 92.3207, loss: 4.5533
	step 741:lm_loss: 4.5256, ppl: 92.3524, loss: 4.5535
	step 742:lm_loss: 4.5256, ppl: 92.3532, loss: 4.5534
	step 743:lm_loss: 4.5255, ppl: 92.3396, loss: 4.5532
	step 744:lm_loss: 4.5249, ppl: 92.2911, loss: 4.5528
	step 745:lm_loss: 4.5227, ppl: 92.0838, loss: 4.5511
	step 746:lm_loss: 4.5221, ppl: 92.0294, loss: 4.5505
	step 747:lm_loss: 4.5222, ppl: 92.0415, loss: 4.5507
	step 748:lm_loss: 4.5219, ppl: 92.0085, loss: 4.5503
	step 749:lm_loss: 4.5219, ppl: 92.0097, loss: 4.5503
	step 750:lm_loss: 4.5209, ppl: 91.9192, loss: 4.5492
	step 751:lm_loss: 4.5207, ppl: 91.9013, loss: 4.5491
	step 752:lm_loss: 4.5212, ppl: 91.9489, loss: 4.5495
	step 753:lm_loss: 4.5207, ppl: 91.8976, loss: 4.5490
	step 754:lm_loss: 4.5176, ppl: 91.6155, loss: 4.5478
	step 755:lm_loss: 4.5178, ppl: 91.6369, loss: 4.5480
	step 756:lm_loss: 4.5174, ppl: 91.5959, loss: 4.5474
	step 757:lm_loss: 4.5174, ppl: 91.6008, loss: 4.5474
	step 758:lm_loss: 4.5176, ppl: 91.6178, loss: 4.5475
	step 759:lm_loss: 4.5188, ppl: 91.7218, loss: 4.5488
	step 760:lm_loss: 4.5204, ppl: 91.8704, loss: 4.5507
	step 761:lm_loss: 4.5203, ppl: 91.8657, loss: 4.5506
	step 762:lm_loss: 4.5201, ppl: 91.8481, loss: 4.5504
	step 763:lm_loss: 4.5211, ppl: 91.9364, loss: 4.5512
	step 764:lm_loss: 4.5211, ppl: 91.9353, loss: 4.5512
	step 765:lm_loss: 4.5207, ppl: 91.8991, loss: 4.5507
	step 766:lm_loss: 4.5190, ppl: 91.7467, loss: 4.5489
	step 767:lm_loss: 4.5196, ppl: 91.7997, loss: 4.5494
	step 768:lm_loss: 4.5202, ppl: 91.8519, loss: 4.5498
	step 769:lm_loss: 4.5214, ppl: 91.9626, loss: 4.5507
	step 770:lm_loss: 4.5216, ppl: 91.9852, loss: 4.5509
	step 771:lm_loss: 4.5216, ppl: 91.9860, loss: 4.5509
	step 772:lm_loss: 4.5211, ppl: 91.9379, loss: 4.5504
	step 773:lm_loss: 4.5193, ppl: 91.7699, loss: 4.5494
	step 774:lm_loss: 4.5190, ppl: 91.7441, loss: 4.5491
	step 775:lm_loss: 4.5183, ppl: 91.6801, loss: 4.5483
	step 776:lm_loss: 4.5193, ppl: 91.7715, loss: 4.5495
	step 777:lm_loss: 4.5201, ppl: 91.8434, loss: 4.5508
	step 778:lm_loss: 4.5201, ppl: 91.8422, loss: 4.5507
	step 779:lm_loss: 4.5201, ppl: 91.8408, loss: 4.5507
	step 780:lm_loss: 4.5200, ppl: 91.8338, loss: 4.5506
	step 781:lm_loss: 4.5204, ppl: 91.8724, loss: 4.5510
	step 782:lm_loss: 4.5213, ppl: 91.9567, loss: 4.5520
	step 783:lm_loss: 4.5213, ppl: 91.9538, loss: 4.5519
	step 784:lm_loss: 4.5212, ppl: 91.9450, loss: 4.5518
	step 785:lm_loss: 4.5223, ppl: 92.0443, loss: 4.5524
	step 786:lm_loss: 4.5216, ppl: 91.9838, loss: 4.5517
	step 787:lm_loss: 4.5213, ppl: 91.9570, loss: 4.5514
	step 788:lm_loss: 4.5212, ppl: 91.9486, loss: 4.5513
	step 789:lm_loss: 4.5207, ppl: 91.8990, loss: 4.5505
	step 790:lm_loss: 4.5215, ppl: 91.9741, loss: 4.5511
	step 791:lm_loss: 4.5218, ppl: 91.9982, loss: 4.5516
	step 792:lm_loss: 4.5197, ppl: 91.8089, loss: 4.5495
	step 793:lm_loss: 4.5202, ppl: 91.8579, loss: 4.5501
	step 794:lm_loss: 4.5206, ppl: 91.8916, loss: 4.5505
	step 795:lm_loss: 4.5197, ppl: 91.8061, loss: 4.5494
	step 796:lm_loss: 4.5211, ppl: 91.9353, loss: 4.5504
	step 797:lm_loss: 4.5219, ppl: 92.0075, loss: 4.5509
	step 798:lm_loss: 4.5224, ppl: 92.0556, loss: 4.5513
	step 799:lm_loss: 4.5230, ppl: 92.1149, loss: 4.5518
	step 800:lm_loss: 4.5219, ppl: 92.0119, loss: 4.5502
	step 801:lm_loss: 4.5217, ppl: 91.9897, loss: 4.5499
	step 802:lm_loss: 4.5226, ppl: 92.0789, loss: 4.5512
	step 803:lm_loss: 4.5234, ppl: 92.1444, loss: 4.5517
	step 804:lm_loss: 4.5229, ppl: 92.1015, loss: 4.5512
	step 805:lm_loss: 4.5218, ppl: 91.9995, loss: 4.5499
	step 806:lm_loss: 4.5214, ppl: 91.9682, loss: 4.5494
	step 807:lm_loss: 4.5212, ppl: 91.9427, loss: 4.5491
	step 808:lm_loss: 4.5185, ppl: 91.6938, loss: 4.5477
	step 809:lm_loss: 4.5192, ppl: 91.7634, loss: 4.5484
	step 810:lm_loss: 4.5199, ppl: 91.8288, loss: 4.5491
	step 811:lm_loss: 4.5201, ppl: 91.8443, loss: 4.5492
	step 812:lm_loss: 4.5196, ppl: 91.8006, loss: 4.5486
	step 813:lm_loss: 4.5202, ppl: 91.8504, loss: 4.5492
	step 814:lm_loss: 4.5212, ppl: 91.9455, loss: 4.5502
	step 815:lm_loss: 4.5199, ppl: 91.8234, loss: 4.5487
	step 816:lm_loss: 4.5209, ppl: 91.9209, loss: 4.5496
	step 817:lm_loss: 4.5205, ppl: 91.8819, loss: 4.5491
	step 818:lm_loss: 4.5216, ppl: 91.9836, loss: 4.5507
	step 819:lm_loss: 4.5207, ppl: 91.8983, loss: 4.5500
	step 820:lm_loss: 4.5199, ppl: 91.8267, loss: 4.5492
	step 821:lm_loss: 4.5203, ppl: 91.8602, loss: 4.5495
	step 822:lm_loss: 4.5198, ppl: 91.8143, loss: 4.5488
	step 823:lm_loss: 4.5210, ppl: 91.9252, loss: 4.5499
	step 824:lm_loss: 4.5210, ppl: 91.9265, loss: 4.5498
	step 825:lm_loss: 4.5218, ppl: 92.0049, loss: 4.5505
	step 826:lm_loss: 4.5198, ppl: 91.8138, loss: 4.5488
	step 827:lm_loss: 4.5196, ppl: 91.8014, loss: 4.5486
	step 828:lm_loss: 4.5195, ppl: 91.7926, loss: 4.5484
	step 829:lm_loss: 4.5197, ppl: 91.8086, loss: 4.5486
	step 830:lm_loss: 4.5193, ppl: 91.7679, loss: 4.5481
	step 831:lm_loss: 4.5207, ppl: 91.8990, loss: 4.5492
	step 832:lm_loss: 4.5207, ppl: 91.9024, loss: 4.5492
	step 833:lm_loss: 4.5203, ppl: 91.8612, loss: 4.5485
	step 834:lm_loss: 4.5201, ppl: 91.8413, loss: 4.5481
	step 835:lm_loss: 4.5217, ppl: 91.9915, loss: 4.5497
	step 836:lm_loss: 4.5218, ppl: 92.0044, loss: 4.5498
	step 837:lm_loss: 4.5223, ppl: 92.0433, loss: 4.5504
	step 838:lm_loss: 4.5213, ppl: 91.9549, loss: 4.5493
	step 839:lm_loss: 4.5213, ppl: 91.9531, loss: 4.5493
	step 840:lm_loss: 4.5216, ppl: 91.9867, loss: 4.5495
	step 841:lm_loss: 4.5219, ppl: 92.0065, loss: 4.5497
	step 842:lm_loss: 4.5209, ppl: 91.9225, loss: 4.5487
	step 843:lm_loss: 4.5204, ppl: 91.8680, loss: 4.5480
	step 844:lm_loss: 4.5196, ppl: 91.7954, loss: 4.5472
	step 845:lm_loss: 4.5193, ppl: 91.7747, loss: 4.5469
	step 846:lm_loss: 4.5200, ppl: 91.8327, loss: 4.5476
	step 847:lm_loss: 4.5196, ppl: 91.7979, loss: 4.5472
	step 848:lm_loss: 4.5199, ppl: 91.8292, loss: 4.5476
	step 849:lm_loss: 4.5204, ppl: 91.8689, loss: 4.5479
	step 850:lm_loss: 4.5204, ppl: 91.8716, loss: 4.5479
	step 851:lm_loss: 4.5198, ppl: 91.8177, loss: 4.5474
	step 852:lm_loss: 4.5193, ppl: 91.7742, loss: 4.5470
	step 853:lm_loss: 4.5189, ppl: 91.7361, loss: 4.5467
	step 854:lm_loss: 4.5187, ppl: 91.7147, loss: 4.5465
	step 855:lm_loss: 4.5180, ppl: 91.6553, loss: 4.5460
	step 856:lm_loss: 4.5199, ppl: 91.8257, loss: 4.5480
	step 857:lm_loss: 4.5216, ppl: 91.9792, loss: 4.5493
	step 858:lm_loss: 4.5218, ppl: 92.0012, loss: 4.5495
	step 859:lm_loss: 4.5226, ppl: 92.0762, loss: 4.5505
	step 860:lm_loss: 4.5225, ppl: 92.0654, loss: 4.5503
	step 861:lm_loss: 4.5223, ppl: 92.0506, loss: 4.5502
	step 862:lm_loss: 4.5231, ppl: 92.1164, loss: 4.5510
	step 863:lm_loss: 4.5221, ppl: 92.0263, loss: 4.5499
	step 864:lm_loss: 4.5213, ppl: 91.9509, loss: 4.5491
	step 865:lm_loss: 4.5211, ppl: 91.9406, loss: 4.5489
	step 866:lm_loss: 4.5224, ppl: 92.0521, loss: 4.5508
	step 867:lm_loss: 4.5225, ppl: 92.0665, loss: 4.5508
	step 868:lm_loss: 4.5220, ppl: 92.0164, loss: 4.5502
	step 869:lm_loss: 4.5226, ppl: 92.0781, loss: 4.5507
	step 870:lm_loss: 4.5210, ppl: 91.9277, loss: 4.5491
	step 871:lm_loss: 4.5211, ppl: 91.9341, loss: 4.5491
	step 872:lm_loss: 4.5206, ppl: 91.8933, loss: 4.5485
	step 873:lm_loss: 4.5216, ppl: 91.9797, loss: 4.5492
	step 874:lm_loss: 4.5231, ppl: 92.1179, loss: 4.5504
	step 875:lm_loss: 4.5239, ppl: 92.1928, loss: 4.5514
	step 876:lm_loss: 4.5236, ppl: 92.1661, loss: 4.5509
	step 877:lm_loss: 4.5235, ppl: 92.1549, loss: 4.5508
	step 878:lm_loss: 4.5226, ppl: 92.0724, loss: 4.5499
	step 879:lm_loss: 4.5230, ppl: 92.1125, loss: 4.5503
	step 880:lm_loss: 4.5234, ppl: 92.1516, loss: 4.5508
	step 881:lm_loss: 4.5239, ppl: 92.1907, loss: 4.5510
	step 882:lm_loss: 4.5237, ppl: 92.1802, loss: 4.5508
	step 883:lm_loss: 4.5241, ppl: 92.2127, loss: 4.5511
	step 884:lm_loss: 4.5261, ppl: 92.3997, loss: 4.5532
	step 885:lm_loss: 4.5262, ppl: 92.4102, loss: 4.5533
	step 886:lm_loss: 4.5236, ppl: 92.1676, loss: 4.5518
	step 887:lm_loss: 4.5233, ppl: 92.1433, loss: 4.5516
	step 888:lm_loss: 4.5230, ppl: 92.1094, loss: 4.5512
	step 889:lm_loss: 4.5221, ppl: 92.0248, loss: 4.5497
	step 890:lm_loss: 4.5215, ppl: 91.9713, loss: 4.5493
	step 891:lm_loss: 4.5216, ppl: 91.9863, loss: 4.5494
	step 892:lm_loss: 4.5219, ppl: 92.0115, loss: 4.5496
	step 893:lm_loss: 4.5221, ppl: 92.0260, loss: 4.5497
	step 894:lm_loss: 4.5219, ppl: 92.0099, loss: 4.5494
	step 895:lm_loss: 4.5217, ppl: 91.9908, loss: 4.5491
	step 896:lm_loss: 4.5210, ppl: 91.9295, loss: 4.5486
	step 897:lm_loss: 4.5215, ppl: 91.9762, loss: 4.5491
	step 898:lm_loss: 4.5199, ppl: 91.8241, loss: 4.5473
	step 899:lm_loss: 4.5196, ppl: 91.7972, loss: 4.5470
	step 900:lm_loss: 4.5183, ppl: 91.6807, loss: 4.5459
	step 901:lm_loss: 4.5188, ppl: 91.7254, loss: 4.5462
	step 902:lm_loss: 4.5190, ppl: 91.7437, loss: 4.5464
	step 903:lm_loss: 4.5191, ppl: 91.7495, loss: 4.5464
	step 904:lm_loss: 4.5192, ppl: 91.7612, loss: 4.5465
	step 905:lm_loss: 4.5187, ppl: 91.7159, loss: 4.5462
	step 906:lm_loss: 4.5191, ppl: 91.7512, loss: 4.5467
	step 907:lm_loss: 4.5195, ppl: 91.7941, loss: 4.5471
	step 908:lm_loss: 4.5198, ppl: 91.8143, loss: 4.5473
	step 909:lm_loss: 4.5202, ppl: 91.8557, loss: 4.5480
	step 910:lm_loss: 4.5202, ppl: 91.8513, loss: 4.5479
	step 911:lm_loss: 4.5196, ppl: 91.7990, loss: 4.5474
	step 912:lm_loss: 4.5198, ppl: 91.8166, loss: 4.5475
	step 913:lm_loss: 4.5205, ppl: 91.8854, loss: 4.5484
	step 914:lm_loss: 4.5199, ppl: 91.8302, loss: 4.5481
	step 915:lm_loss: 4.5200, ppl: 91.8384, loss: 4.5482
	step 916:lm_loss: 4.5205, ppl: 91.8835, loss: 4.5486
	step 917:lm_loss: 4.5212, ppl: 91.9440, loss: 4.5494
	step 918:lm_loss: 4.5205, ppl: 91.8828, loss: 4.5485
	step 919:lm_loss: 4.5209, ppl: 91.9221, loss: 4.5488
	step 920:lm_loss: 4.5206, ppl: 91.8911, loss: 4.5483
	step 921:lm_loss: 4.5214, ppl: 91.9673, loss: 4.5493
	step 922:lm_loss: 4.5216, ppl: 91.9800, loss: 4.5494
	step 923:lm_loss: 4.5232, ppl: 92.1292, loss: 4.5516
	step 924:lm_loss: 4.5233, ppl: 92.1406, loss: 4.5517
	step 925:lm_loss: 4.5214, ppl: 91.9671, loss: 4.5504
	step 926:lm_loss: 4.5216, ppl: 91.9833, loss: 4.5505
	step 927:lm_loss: 4.5220, ppl: 92.0208, loss: 4.5508
	step 928:lm_loss: 4.5230, ppl: 92.1129, loss: 4.5520
	step 929:lm_loss: 4.5238, ppl: 92.1875, loss: 4.5529
	step 930:lm_loss: 4.5232, ppl: 92.1345, loss: 4.5523
	step 931:lm_loss: 4.5239, ppl: 92.1913, loss: 4.5531
	step 932:lm_loss: 4.5241, ppl: 92.2095, loss: 4.5532
	step 933:lm_loss: 4.5238, ppl: 92.1823, loss: 4.5528
	step 934:lm_loss: 4.5233, ppl: 92.1388, loss: 4.5524
	step 935:lm_loss: 4.5224, ppl: 92.0565, loss: 4.5513
	step 936:lm_loss: 4.5228, ppl: 92.0969, loss: 4.5516
	step 937:lm_loss: 4.5230, ppl: 92.1134, loss: 4.5518
	step 938:lm_loss: 4.5233, ppl: 92.1415, loss: 4.5521
	step 939:lm_loss: 4.5233, ppl: 92.1369, loss: 4.5520
	step 940:lm_loss: 4.5228, ppl: 92.0950, loss: 4.5513
	step 941:lm_loss: 4.5235, ppl: 92.1535, loss: 4.5522
	step 942:lm_loss: 4.5242, ppl: 92.2233, loss: 4.5529
	step 943:lm_loss: 4.5239, ppl: 92.1961, loss: 4.5527
	step 944:lm_loss: 4.5222, ppl: 92.0414, loss: 4.5506
	step 945:lm_loss: 4.5228, ppl: 92.0914, loss: 4.5514
	step 946:lm_loss: 4.5224, ppl: 92.0523, loss: 4.5508
	step 947:lm_loss: 4.5232, ppl: 92.1298, loss: 4.5516
	step 948:lm_loss: 4.5233, ppl: 92.1358, loss: 4.5516
	step 949:lm_loss: 4.5224, ppl: 92.0593, loss: 4.5507
	step 950:lm_loss: 4.5220, ppl: 92.0180, loss: 4.5501
	step 951:lm_loss: 4.5218, ppl: 92.0002, loss: 4.5500
	step 952:lm_loss: 4.5217, ppl: 91.9917, loss: 4.5499
	step 953:lm_loss: 4.5218, ppl: 91.9988, loss: 4.5499
	step 954:lm_loss: 4.5217, ppl: 91.9917, loss: 4.5498
	step 955:lm_loss: 4.5224, ppl: 92.0561, loss: 4.5507
	step 956:lm_loss: 4.5225, ppl: 92.0695, loss: 4.5508
	step 957:lm_loss: 4.5189, ppl: 91.7369, loss: 4.5489
	step 958:lm_loss: 4.5181, ppl: 91.6604, loss: 4.5477
	step 959:lm_loss: 4.5175, ppl: 91.6108, loss: 4.5472
	step 960:lm_loss: 4.5170, ppl: 91.5593, loss: 4.5467
	step 961:lm_loss: 4.5172, ppl: 91.5811, loss: 4.5470
	step 962:lm_loss: 4.5169, ppl: 91.5503, loss: 4.5466
	step 963:lm_loss: 4.5171, ppl: 91.5714, loss: 4.5468
	step 964:lm_loss: 4.5169, ppl: 91.5537, loss: 4.5465
	step 965:lm_loss: 4.5172, ppl: 91.5757, loss: 4.5467
	step 966:lm_loss: 4.5168, ppl: 91.5411, loss: 4.5463
	step 967:lm_loss: 4.5163, ppl: 91.4969, loss: 4.5455
	step 968:lm_loss: 4.5173, ppl: 91.5897, loss: 4.5463
	step 969:lm_loss: 4.5165, ppl: 91.5103, loss: 4.5458
	step 970:lm_loss: 4.5166, ppl: 91.5228, loss: 4.5459
	step 971:lm_loss: 4.5164, ppl: 91.5041, loss: 4.5456
	step 972:lm_loss: 4.5169, ppl: 91.5517, loss: 4.5464
	step 973:lm_loss: 4.5175, ppl: 91.6051, loss: 4.5469
	step 974:lm_loss: 4.5176, ppl: 91.6199, loss: 4.5470
	step 975:lm_loss: 4.5185, ppl: 91.6982, loss: 4.5478
	step 976:lm_loss: 4.5188, ppl: 91.7293, loss: 4.5481
	step 977:lm_loss: 4.5188, ppl: 91.7273, loss: 4.5480
	step 978:lm_loss: 4.5193, ppl: 91.7717, loss: 4.5486
	step 979:lm_loss: 4.5208, ppl: 91.9079, loss: 4.5495
	step 980:lm_loss: 4.5213, ppl: 91.9584, loss: 4.5503
	step 981:lm_loss: 4.5215, ppl: 91.9776, loss: 4.5506
	step 982:lm_loss: 4.5222, ppl: 92.0371, loss: 4.5512
	step 983:lm_loss: 4.5228, ppl: 92.0949, loss: 4.5519
	step 984:lm_loss: 4.5235, ppl: 92.1593, loss: 4.5527
	step 985:lm_loss: 4.5239, ppl: 92.1908, loss: 4.5531
	step 986:lm_loss: 4.5239, ppl: 92.1908, loss: 4.5531
	step 987:lm_loss: 4.5236, ppl: 92.1701, loss: 4.5528
	step 988:lm_loss: 4.5239, ppl: 92.1970, loss: 4.5531
	step 989:lm_loss: 4.5249, ppl: 92.2866, loss: 4.5547
	step 990:lm_loss: 4.5242, ppl: 92.2187, loss: 4.5538
	step 991:lm_loss: 4.5236, ppl: 92.1711, loss: 4.5531
	step 992:lm_loss: 4.5240, ppl: 92.2030, loss: 4.5535
	step 993:lm_loss: 4.5234, ppl: 92.1452, loss: 4.5531
	step 994:lm_loss: 4.5244, ppl: 92.2396, loss: 4.5542
	step 995:lm_loss: 4.5256, ppl: 92.3485, loss: 4.5553
	step 996:lm_loss: 4.5266, ppl: 92.4453, loss: 4.5566
	step 997:lm_loss: 4.5268, ppl: 92.4607, loss: 4.5567
	step 998:lm_loss: 4.5259, ppl: 92.3811, loss: 4.5556
	step 999:lm_loss: 4.5258, ppl: 92.3740, loss: 4.5554
	step 1000:lm_loss: 4.5269, ppl: 92.4683, loss: 4.5567
	step 1001:lm_loss: 4.5275, ppl: 92.5230, loss: 4.5573
	step 1002:lm_loss: 4.5274, ppl: 92.5203, loss: 4.5572
	step 1003:lm_loss: 4.5265, ppl: 92.4386, loss: 4.5567
	step 1004:lm_loss: 4.5262, ppl: 92.4038, loss: 4.5563
	step 1005:lm_loss: 4.5266, ppl: 92.4463, loss: 4.5569
	step 1006:lm_loss: 4.5259, ppl: 92.3757, loss: 4.5561
	step 1007:lm_loss: 4.5255, ppl: 92.3412, loss: 4.5556
	step 1008:lm_loss: 4.5235, ppl: 92.1615, loss: 4.5531
	step 1009:lm_loss: 4.5240, ppl: 92.1995, loss: 4.5534
	step 1010:lm_loss: 4.5252, ppl: 92.3164, loss: 4.5540
	step 1011:lm_loss: 4.5244, ppl: 92.2373, loss: 4.5530
	step 1012:lm_loss: 4.5232, ppl: 92.1327, loss: 4.5521
	step 1013:lm_loss: 4.5233, ppl: 92.1351, loss: 4.5521
	step 1014:lm_loss: 4.5229, ppl: 92.1057, loss: 4.5517
	step 1015:lm_loss: 4.5248, ppl: 92.2785, loss: 4.5534
	step 1016:lm_loss: 4.5247, ppl: 92.2717, loss: 4.5533
	step 1017:lm_loss: 4.5266, ppl: 92.4475, loss: 4.5551
	step 1018:lm_loss: 4.5238, ppl: 92.1830, loss: 4.5530
	step 1019:lm_loss: 4.5256, ppl: 92.3482, loss: 4.5549
	step 1020:lm_loss: 4.5263, ppl: 92.4148, loss: 4.5555
	step 1021:lm_loss: 4.5259, ppl: 92.3791, loss: 4.5551
	step 1022:lm_loss: 4.5253, ppl: 92.3276, loss: 4.5546
	step 1023:lm_loss: 4.5252, ppl: 92.3146, loss: 4.5543
	step 1024:lm_loss: 4.5254, ppl: 92.3338, loss: 4.5545
	step 1025:lm_loss: 4.5253, ppl: 92.3265, loss: 4.5543
	step 1026:lm_loss: 4.5254, ppl: 92.3331, loss: 4.5544
	step 1027:lm_loss: 4.5240, ppl: 92.2036, loss: 4.5528
	step 1028:lm_loss: 4.5239, ppl: 92.1941, loss: 4.5527
	step 1029:lm_loss: 4.5243, ppl: 92.2334, loss: 4.5532
	step 1030:lm_loss: 4.5248, ppl: 92.2752, loss: 4.5536
	step 1031:lm_loss: 4.5254, ppl: 92.3349, loss: 4.5540
	step 1032:lm_loss: 4.5258, ppl: 92.3707, loss: 4.5543
	step 1033:lm_loss: 4.5244, ppl: 92.2374, loss: 4.5528
	step 1034:lm_loss: 4.5238, ppl: 92.1822, loss: 4.5519
	step 1035:lm_loss: 4.5232, ppl: 92.1330, loss: 4.5511
	step 1036:lm_loss: 4.5229, ppl: 92.0986, loss: 4.5507
	step 1037:lm_loss: 4.5224, ppl: 92.0551, loss: 4.5502
	step 1038:lm_loss: 4.5216, ppl: 91.9868, loss: 4.5496
	step 1039:lm_loss: 4.5225, ppl: 92.0694, loss: 4.5509
	step 1040:lm_loss: 4.5228, ppl: 92.0916, loss: 4.5511
	step 1041:lm_loss: 4.5226, ppl: 92.0776, loss: 4.5509
	step 1042:lm_loss: 4.5222, ppl: 92.0412, loss: 4.5506
	step 1043:lm_loss: 4.5231, ppl: 92.1220, loss: 4.5512
	step 1044:lm_loss: 4.5241, ppl: 92.2130, loss: 4.5518
	step 1045:lm_loss: 4.5248, ppl: 92.2740, loss: 4.5524
	step 1046:lm_loss: 4.5249, ppl: 92.2900, loss: 4.5526
	step 1047:lm_loss: 4.5250, ppl: 92.2964, loss: 4.5526
	step 1048:lm_loss: 4.5249, ppl: 92.2843, loss: 4.5525
	step 1049:lm_loss: 4.5252, ppl: 92.3127, loss: 4.5529
	step 1050:lm_loss: 4.5256, ppl: 92.3471, loss: 4.5532
	step 1051:lm_loss: 4.5258, ppl: 92.3652, loss: 4.5533
	step 1052:lm_loss: 4.5252, ppl: 92.3138, loss: 4.5530
	step 1053:lm_loss: 4.5263, ppl: 92.4205, loss: 4.5548
	step 1054:lm_loss: 4.5264, ppl: 92.4226, loss: 4.5548
	step 1055:lm_loss: 4.5268, ppl: 92.4581, loss: 4.5553
	step 1056:lm_loss: 4.5268, ppl: 92.4643, loss: 4.5554
	step 1057:lm_loss: 4.5280, ppl: 92.5692, loss: 4.5565
	step 1058:lm_loss: 4.5281, ppl: 92.5824, loss: 4.5566
	step 1059:lm_loss: 4.5278, ppl: 92.5565, loss: 4.5562
	step 1060:lm_loss: 4.5273, ppl: 92.5058, loss: 4.5556
	step 1061:lm_loss: 4.5268, ppl: 92.4622, loss: 4.5551
	step 1062:lm_loss: 4.5258, ppl: 92.3672, loss: 4.5537
	step 1063:lm_loss: 4.5262, ppl: 92.4037, loss: 4.5542
	step 1064:lm_loss: 4.5267, ppl: 92.4490, loss: 4.5547
	step 1065:lm_loss: 4.5274, ppl: 92.5220, loss: 4.5560
	step 1066:lm_loss: 4.5277, ppl: 92.5488, loss: 4.5564
	step 1067:lm_loss: 4.5296, ppl: 92.7203, loss: 4.5595
	step 1068:lm_loss: 4.5301, ppl: 92.7654, loss: 4.5601
	step 1069:lm_loss: 4.5302, ppl: 92.7802, loss: 4.5602
	step 1070:lm_loss: 4.5298, ppl: 92.7408, loss: 4.5598
	step 1071:lm_loss: 4.5296, ppl: 92.7241, loss: 4.5596
	step 1072:lm_loss: 4.5303, ppl: 92.7865, loss: 4.5604
	step 1073:lm_loss: 4.5295, ppl: 92.7095, loss: 4.5599
	step 1074:lm_loss: 4.5291, ppl: 92.6783, loss: 4.5594
	step 1075:lm_loss: 4.5283, ppl: 92.6046, loss: 4.5588
	step 1076:lm_loss: 4.5290, ppl: 92.6642, loss: 4.5595
	step 1077:lm_loss: 4.5299, ppl: 92.7502, loss: 4.5607
	step 1078:lm_loss: 4.5307, ppl: 92.8254, loss: 4.5611
	step 1079:lm_loss: 4.5301, ppl: 92.7679, loss: 4.5606
	step 1080:lm_loss: 4.5302, ppl: 92.7732, loss: 4.5606
	step 1081:lm_loss: 4.5305, ppl: 92.8072, loss: 4.5610
	step 1082:lm_loss: 4.5297, ppl: 92.7267, loss: 4.5604
	step 1083:lm_loss: 4.5302, ppl: 92.7774, loss: 4.5608
	step 1084:lm_loss: 4.5304, ppl: 92.7937, loss: 4.5609
	step 1085:lm_loss: 4.5303, ppl: 92.7876, loss: 4.5608
	step 1086:lm_loss: 4.5310, ppl: 92.8499, loss: 4.5612
	step 1087:lm_loss: 4.5316, ppl: 92.9046, loss: 4.5617
	step 1088:lm_loss: 4.5321, ppl: 92.9525, loss: 4.5621
	step 1089:lm_loss: 4.5323, ppl: 92.9729, loss: 4.5623
	step 1090:lm_loss: 4.5317, ppl: 92.9149, loss: 4.5614
	step 1091:lm_loss: 4.5314, ppl: 92.8894, loss: 4.5610
	step 1092:lm_loss: 4.5312, ppl: 92.8687, loss: 4.5609
	step 1093:lm_loss: 4.5314, ppl: 92.8884, loss: 4.5611
	step 1094:lm_loss: 4.5314, ppl: 92.8894, loss: 4.5610
	step 1095:lm_loss: 4.5321, ppl: 92.9506, loss: 4.5616
	step 1096:lm_loss: 4.5311, ppl: 92.8612, loss: 4.5607
	step 1097:lm_loss: 4.5311, ppl: 92.8614, loss: 4.5607
	step 1098:lm_loss: 4.5302, ppl: 92.7765, loss: 4.5597
	step 1099:lm_loss: 4.5310, ppl: 92.8536, loss: 4.5607
	step 1100:lm_loss: 4.5306, ppl: 92.8124, loss: 4.5603
	step 1101:lm_loss: 4.5313, ppl: 92.8783, loss: 4.5611
	step 1102:lm_loss: 4.5322, ppl: 92.9661, loss: 4.5621
	step 1103:lm_loss: 4.5328, ppl: 93.0152, loss: 4.5628
	step 1104:lm_loss: 4.5338, ppl: 93.1096, loss: 4.5639
	step 1105:lm_loss: 4.5338, ppl: 93.1133, loss: 4.5640
	step 1106:lm_loss: 4.5330, ppl: 93.0372, loss: 4.5631
	step 1107:lm_loss: 4.5334, ppl: 93.0772, loss: 4.5637
	step 1108:lm_loss: 4.5333, ppl: 93.0622, loss: 4.5635
	step 1109:lm_loss: 4.5334, ppl: 93.0785, loss: 4.5637
	step 1110:lm_loss: 4.5332, ppl: 93.0578, loss: 4.5635
	step 1111:lm_loss: 4.5334, ppl: 93.0703, loss: 4.5636
	step 1112:lm_loss: 4.5338, ppl: 93.1149, loss: 4.5640
	step 1113:lm_loss: 4.5340, ppl: 93.1326, loss: 4.5642
	step 1114:lm_loss: 4.5327, ppl: 93.0073, loss: 4.5628
	step 1115:lm_loss: 4.5324, ppl: 92.9817, loss: 4.5624
	step 1116:lm_loss: 4.5324, ppl: 92.9810, loss: 4.5624
	step 1117:lm_loss: 4.5327, ppl: 93.0122, loss: 4.5628
	step 1118:lm_loss: 4.5342, ppl: 93.1480, loss: 4.5648
	step 1119:lm_loss: 4.5351, ppl: 93.2350, loss: 4.5661
	step 1120:lm_loss: 4.5359, ppl: 93.3028, loss: 4.5670
	step 1121:lm_loss: 4.5361, ppl: 93.3240, loss: 4.5672
	step 1122:lm_loss: 4.5368, ppl: 93.3947, loss: 4.5678
	step 1123:lm_loss: 4.5375, ppl: 93.4591, loss: 4.5684
	step 1124:lm_loss: 4.5340, ppl: 93.1337, loss: 4.5661
	step 1125:lm_loss: 4.5328, ppl: 93.0188, loss: 4.5653
	step 1126:lm_loss: 4.5329, ppl: 93.0320, loss: 4.5654
	step 1127:lm_loss: 4.5321, ppl: 92.9573, loss: 4.5643
	step 1128:lm_loss: 4.5321, ppl: 92.9558, loss: 4.5642
	step 1129:lm_loss: 4.5318, ppl: 92.9224, loss: 4.5637
	step 1130:lm_loss: 4.5321, ppl: 92.9509, loss: 4.5640
	step 1131:lm_loss: 4.5307, ppl: 92.8221, loss: 4.5631
	step 1132:lm_loss: 4.5299, ppl: 92.7501, loss: 4.5619
	step 1133:lm_loss: 4.5303, ppl: 92.7894, loss: 4.5624
	step 1134:lm_loss: 4.5301, ppl: 92.7716, loss: 4.5621
	step 1135:lm_loss: 4.5295, ppl: 92.7151, loss: 4.5613
	step 1136:lm_loss: 4.5297, ppl: 92.7309, loss: 4.5614
	step 1137:lm_loss: 4.5295, ppl: 92.7119, loss: 4.5612
	step 1138:lm_loss: 4.5307, ppl: 92.8217, loss: 4.5625
	step 1139:lm_loss: 4.5309, ppl: 92.8445, loss: 4.5627
	step 1140:lm_loss: 4.5298, ppl: 92.7444, loss: 4.5619
	step 1141:lm_loss: 4.5299, ppl: 92.7466, loss: 4.5619
	step 1142:lm_loss: 4.5297, ppl: 92.7313, loss: 4.5616
	step 1143:lm_loss: 4.5304, ppl: 92.7965, loss: 4.5624
	step 1144:lm_loss: 4.5307, ppl: 92.8229, loss: 4.5626
	step 1145:lm_loss: 4.5301, ppl: 92.7703, loss: 4.5621
	step 1146:lm_loss: 4.5291, ppl: 92.6712, loss: 4.5609
	step 1147:lm_loss: 4.5301, ppl: 92.7716, loss: 4.5620
	step 1148:lm_loss: 4.5297, ppl: 92.7307, loss: 4.5615
	step 1149:lm_loss: 4.5298, ppl: 92.7370, loss: 4.5616
	step 1150:lm_loss: 4.5295, ppl: 92.7154, loss: 4.5613
	step 1151:lm_loss: 4.5307, ppl: 92.8255, loss: 4.5624
	step 1152:lm_loss: 4.5313, ppl: 92.8776, loss: 4.5629
	step 1153:lm_loss: 4.5307, ppl: 92.8220, loss: 4.5619
	step 1154:lm_loss: 4.5307, ppl: 92.8261, loss: 4.5619
	step 1155:lm_loss: 4.5303, ppl: 92.7829, loss: 4.5616
	step 1156:lm_loss: 4.5313, ppl: 92.8778, loss: 4.5624
	step 1157:lm_loss: 4.5305, ppl: 92.8090, loss: 4.5615
	step 1158:lm_loss: 4.5303, ppl: 92.7903, loss: 4.5612
	step 1159:lm_loss: 4.5307, ppl: 92.8251, loss: 4.5617
	step 1160:lm_loss: 4.5306, ppl: 92.8130, loss: 4.5616
	step 1161:lm_loss: 4.5293, ppl: 92.6930, loss: 4.5604
	step 1162:lm_loss: 4.5298, ppl: 92.7398, loss: 4.5610
	step 1163:lm_loss: 4.5298, ppl: 92.7372, loss: 4.5610
	step 1164:lm_loss: 4.5297, ppl: 92.7318, loss: 4.5609
	step 1165:lm_loss: 4.5302, ppl: 92.7732, loss: 4.5613
	step 1166:lm_loss: 4.5307, ppl: 92.8193, loss: 4.5618
	step 1167:lm_loss: 4.5310, ppl: 92.8527, loss: 4.5621
	step 1168:lm_loss: 4.5319, ppl: 92.9329, loss: 4.5629
	step 1169:lm_loss: 4.5313, ppl: 92.8828, loss: 4.5622
	step 1170:lm_loss: 4.5303, ppl: 92.7889, loss: 4.5615
	step 1171:lm_loss: 4.5301, ppl: 92.7641, loss: 4.5613
	step 1172:lm_loss: 4.5294, ppl: 92.6987, loss: 4.5606
	step 1173:lm_loss: 4.5291, ppl: 92.6767, loss: 4.5603
	step 1174:lm_loss: 4.5293, ppl: 92.6895, loss: 4.5604
	step 1175:lm_loss: 4.5294, ppl: 92.7029, loss: 4.5605
	step 1176:lm_loss: 4.5290, ppl: 92.6640, loss: 4.5601
	step 1177:lm_loss: 4.5286, ppl: 92.6249, loss: 4.5598
	step 1178:lm_loss: 4.5287, ppl: 92.6425, loss: 4.5600
	step 1179:lm_loss: 4.5285, ppl: 92.6213, loss: 4.5597
	step 1180:lm_loss: 4.5277, ppl: 92.5415, loss: 4.5584
	step 1181:lm_loss: 4.5289, ppl: 92.6556, loss: 4.5593
	step 1182:lm_loss: 4.5289, ppl: 92.6552, loss: 4.5592
	step 1183:lm_loss: 4.5290, ppl: 92.6637, loss: 4.5593
	step 1184:lm_loss: 4.5296, ppl: 92.7187, loss: 4.5597
	step 1185:lm_loss: 4.5293, ppl: 92.6945, loss: 4.5593
	step 1186:lm_loss: 4.5292, ppl: 92.6865, loss: 4.5592
	step 1187:lm_loss: 4.5281, ppl: 92.5828, loss: 4.5579
	step 1188:lm_loss: 4.5280, ppl: 92.5768, loss: 4.5578
	step 1189:lm_loss: 4.5281, ppl: 92.5856, loss: 4.5579
	step 1190:lm_loss: 4.5281, ppl: 92.5868, loss: 4.5579
	step 1191:lm_loss: 4.5280, ppl: 92.5778, loss: 4.5578
	step 1192:lm_loss: 4.5279, ppl: 92.5618, loss: 4.5577
	step 1193:lm_loss: 4.5276, ppl: 92.5407, loss: 4.5573
	step 1194:lm_loss: 4.5273, ppl: 92.5112, loss: 4.5571
	step 1195:lm_loss: 4.5267, ppl: 92.4573, loss: 4.5562
	step 1196:lm_loss: 4.5263, ppl: 92.4192, loss: 4.5559
	step 1197:lm_loss: 4.5265, ppl: 92.4356, loss: 4.5561
	step 1198:lm_loss: 4.5273, ppl: 92.5121, loss: 4.5566
	step 1199:lm_loss: 4.5276, ppl: 92.5329, loss: 4.5568
	step 1200:lm_loss: 4.5271, ppl: 92.4906, loss: 4.5564
	step 1201:lm_loss: 4.5278, ppl: 92.5529, loss: 4.5571
	step 1202:lm_loss: 4.5284, ppl: 92.6086, loss: 4.5576
	step 1203:lm_loss: 4.5283, ppl: 92.6004, loss: 4.5575
	step 1204:lm_loss: 4.5290, ppl: 92.6688, loss: 4.5579
	step 1205:lm_loss: 4.5294, ppl: 92.7026, loss: 4.5583
	step 1206:lm_loss: 4.5295, ppl: 92.7154, loss: 4.5584
	step 1207:lm_loss: 4.5296, ppl: 92.7206, loss: 4.5585
	step 1208:lm_loss: 4.5300, ppl: 92.7605, loss: 4.5591
	step 1209:lm_loss: 4.5304, ppl: 92.7947, loss: 4.5595
	step 1210:lm_loss: 4.5302, ppl: 92.7808, loss: 4.5593
	step 1211:lm_loss: 4.5302, ppl: 92.7761, loss: 4.5592
	step 1212:lm_loss: 4.5298, ppl: 92.7375, loss: 4.5589
	step 1213:lm_loss: 4.5297, ppl: 92.7344, loss: 4.5589
	step 1214:lm_loss: 4.5305, ppl: 92.8049, loss: 4.5598
	step 1215:lm_loss: 4.5310, ppl: 92.8493, loss: 4.5602
	step 1216:lm_loss: 4.5313, ppl: 92.8823, loss: 4.5605
	step 1217:lm_loss: 4.5318, ppl: 92.9235, loss: 4.5608
	step 1218:lm_loss: 4.5317, ppl: 92.9118, loss: 4.5606
	step 1219:lm_loss: 4.5312, ppl: 92.8658, loss: 4.5603
	step 1220:lm_loss: 4.5312, ppl: 92.8692, loss: 4.5603
	step 1221:lm_loss: 4.5305, ppl: 92.8070, loss: 4.5593
	step 1222:lm_loss: 4.5293, ppl: 92.6903, loss: 4.5582
	step 1223:lm_loss: 4.5300, ppl: 92.7599, loss: 4.5587
	step 1224:lm_loss: 4.5297, ppl: 92.7272, loss: 4.5582
	step 1225:lm_loss: 4.5287, ppl: 92.6384, loss: 4.5568
	step 1226:lm_loss: 4.5285, ppl: 92.6186, loss: 4.5566
	step 1227:lm_loss: 4.5286, ppl: 92.6331, loss: 4.5566
	step 1228:lm_loss: 4.5291, ppl: 92.6735, loss: 4.5570
	step 1229:lm_loss: 4.5290, ppl: 92.6628, loss: 4.5568
	step 1230:lm_loss: 4.5287, ppl: 92.6421, loss: 4.5566
	step 1231:lm_loss: 4.5293, ppl: 92.6957, loss: 4.5571
	step 1232:lm_loss: 4.5297, ppl: 92.7311, loss: 4.5575
	step 1233:lm_loss: 4.5310, ppl: 92.8509, loss: 4.5589
	step 1234:lm_loss: 4.5317, ppl: 92.9122, loss: 4.5595
	step 1235:lm_loss: 4.5311, ppl: 92.8596, loss: 4.5589
	step 1236:lm_loss: 4.5310, ppl: 92.8551, loss: 4.5588
	step 1237:lm_loss: 4.5309, ppl: 92.8413, loss: 4.5587
	step 1238:lm_loss: 4.5310, ppl: 92.8481, loss: 4.5588
	step 1239:lm_loss: 4.5317, ppl: 92.9130, loss: 4.5596
	step 1240:lm_loss: 4.5324, ppl: 92.9826, loss: 4.5601
	step 1241:lm_loss: 4.5325, ppl: 92.9942, loss: 4.5602
	step 1242:lm_loss: 4.5329, ppl: 93.0312, loss: 4.5607
	step 1243:lm_loss: 4.5332, ppl: 93.0546, loss: 4.5609
	step 1244:lm_loss: 4.5330, ppl: 93.0394, loss: 4.5607
	step 1245:lm_loss: 4.5331, ppl: 93.0425, loss: 4.5607
	step 1246:lm_loss: 4.5323, ppl: 92.9742, loss: 4.5598
	step 1247:lm_loss: 4.5323, ppl: 92.9725, loss: 4.5597
	step 1248:lm_loss: 4.5329, ppl: 93.0266, loss: 4.5605
	step 1249:lm_loss: 4.5329, ppl: 93.0233, loss: 4.5605
	step 1250:lm_loss: 4.5330, ppl: 93.0372, loss: 4.5606
	step 1251:lm_loss: 4.5327, ppl: 93.0122, loss: 4.5602
	step 1252:lm_loss: 4.5332, ppl: 93.0567, loss: 4.5607
	step 1253:lm_loss: 4.5335, ppl: 93.0810, loss: 4.5610
	step 1254:lm_loss: 4.5336, ppl: 93.0916, loss: 4.5610
	step 1255:lm_loss: 4.5328, ppl: 93.0220, loss: 4.5603
	step 1256:lm_loss: 4.5335, ppl: 93.0837, loss: 4.5609
	step 1257:lm_loss: 4.5334, ppl: 93.0784, loss: 4.5608
	step 1258:lm_loss: 4.5328, ppl: 93.0150, loss: 4.5599
	step 1259:lm_loss: 4.5334, ppl: 93.0773, loss: 4.5602
	step 1260:lm_loss: 4.5339, ppl: 93.1205, loss: 4.5606
	step 1261:lm_loss: 4.5333, ppl: 93.0640, loss: 4.5600
	step 1262:lm_loss: 4.5325, ppl: 92.9935, loss: 4.5591
	step 1263:lm_loss: 4.5331, ppl: 93.0480, loss: 4.5596
	step 1264:lm_loss: 4.5330, ppl: 93.0415, loss: 4.5595
	step 1265:lm_loss: 4.5338, ppl: 93.1101, loss: 4.5607
	step 1266:lm_loss: 4.5344, ppl: 93.1659, loss: 4.5613
	step 1267:lm_loss: 4.5343, ppl: 93.1579, loss: 4.5612
	step 1268:lm_loss: 4.5349, ppl: 93.2111, loss: 4.5616
	step 1269:lm_loss: 4.5342, ppl: 93.1523, loss: 4.5613
	step 1270:lm_loss: 4.5321, ppl: 92.9565, loss: 4.5602
	step 1271:lm_loss: 4.5316, ppl: 92.9099, loss: 4.5597
	step 1272:lm_loss: 4.5317, ppl: 92.9151, loss: 4.5598
	step 1273:lm_loss: 4.5325, ppl: 92.9916, loss: 4.5608
	step 1274:lm_loss: 4.5327, ppl: 93.0051, loss: 4.5609
	step 1275:lm_loss: 4.5322, ppl: 92.9588, loss: 4.5604
	step 1276:lm_loss: 4.5315, ppl: 92.8960, loss: 4.5599
	step 1277:lm_loss: 4.5314, ppl: 92.8898, loss: 4.5598
	step 1278:lm_loss: 4.5331, ppl: 93.0425, loss: 4.5615
	step 1279:lm_loss: 4.5334, ppl: 93.0720, loss: 4.5617
	step 1280:lm_loss: 4.5332, ppl: 93.0544, loss: 4.5614
	step 1281:lm_loss: 4.5335, ppl: 93.0821, loss: 4.5619
	step 1282:lm_loss: 4.5334, ppl: 93.0718, loss: 4.5617
	step 1283:lm_loss: 4.5333, ppl: 93.0618, loss: 4.5616
	step 1284:lm_loss: 4.5337, ppl: 93.1027, loss: 4.5620
	step 1285:lm_loss: 4.5343, ppl: 93.1608, loss: 4.5626
	step 1286:lm_loss: 4.5348, ppl: 93.2079, loss: 4.5630
	step 1287:lm_loss: 4.5345, ppl: 93.1745, loss: 4.5627
	step 1288:lm_loss: 4.5352, ppl: 93.2416, loss: 4.5634
	step 1289:lm_loss: 4.5356, ppl: 93.2777, loss: 4.5638
	step 1290:lm_loss: 4.5354, ppl: 93.2603, loss: 4.5636
	step 1291:lm_loss: 4.5354, ppl: 93.2571, loss: 4.5635
	step 1292:lm_loss: 4.5353, ppl: 93.2495, loss: 4.5634
	step 1293:lm_loss: 4.5348, ppl: 93.2081, loss: 4.5629
	step 1294:lm_loss: 4.5360, ppl: 93.3124, loss: 4.5643
	step 1295:lm_loss: 4.5367, ppl: 93.3841, loss: 4.5651
	step 1296:lm_loss: 4.5370, ppl: 93.4126, loss: 4.5653
	step 1297:lm_loss: 4.5369, ppl: 93.3972, loss: 4.5652
	step 1298:lm_loss: 4.5351, ppl: 93.2310, loss: 4.5640
	step 1299:lm_loss: 4.5352, ppl: 93.2397, loss: 4.5641
	step 1300:lm_loss: 4.5348, ppl: 93.2003, loss: 4.5636
	step 1301:lm_loss: 4.5353, ppl: 93.2555, loss: 4.5642
	step 1302:lm_loss: 4.5360, ppl: 93.3131, loss: 4.5648
	step 1303:lm_loss: 4.5362, ppl: 93.3372, loss: 4.5651
	step 1304:lm_loss: 4.5352, ppl: 93.2392, loss: 4.5638
	step 1305:lm_loss: 4.5356, ppl: 93.2765, loss: 4.5641
	step 1306:lm_loss: 4.5352, ppl: 93.2409, loss: 4.5638
	step 1307:lm_loss: 4.5353, ppl: 93.2556, loss: 4.5639
	step 1308:lm_loss: 4.5351, ppl: 93.2283, loss: 4.5635
	step 1309:lm_loss: 4.5356, ppl: 93.2791, loss: 4.5640
	step 1310:lm_loss: 4.5354, ppl: 93.2615, loss: 4.5637
	step 1311:lm_loss: 4.5341, ppl: 93.1362, loss: 4.5624
	step 1312:lm_loss: 4.5338, ppl: 93.1090, loss: 4.5620
	step 1313:lm_loss: 4.5343, ppl: 93.1537, loss: 4.5623
	step 1314:lm_loss: 4.5346, ppl: 93.1908, loss: 4.5626
	step 1315:lm_loss: 4.5343, ppl: 93.1562, loss: 4.5621
	step 1316:lm_loss: 4.5336, ppl: 93.0887, loss: 4.5616
	step 1317:lm_loss: 4.5339, ppl: 93.1198, loss: 4.5618
	step 1318:lm_loss: 4.5338, ppl: 93.1088, loss: 4.5617
	step 1319:lm_loss: 4.5338, ppl: 93.1122, loss: 4.5617
	step 1320:lm_loss: 4.5332, ppl: 93.0592, loss: 4.5611
	step 1321:lm_loss: 4.5336, ppl: 93.0904, loss: 4.5615
	step 1322:lm_loss: 4.5335, ppl: 93.0827, loss: 4.5614
	step 1323:lm_loss: 4.5332, ppl: 93.0554, loss: 4.5609
	step 1324:lm_loss: 4.5330, ppl: 93.0374, loss: 4.5607
	step 1325:lm_loss: 4.5323, ppl: 92.9714, loss: 4.5601
	step 1326:lm_loss: 4.5329, ppl: 93.0312, loss: 4.5606
	step 1327:lm_loss: 4.5332, ppl: 93.0597, loss: 4.5609
	step 1328:lm_loss: 4.5337, ppl: 93.1011, loss: 4.5616
	step 1329:lm_loss: 4.5337, ppl: 93.0987, loss: 4.5615
	step 1330:lm_loss: 4.5338, ppl: 93.1131, loss: 4.5616
	step 1331:lm_loss: 4.5339, ppl: 93.1169, loss: 4.5617
	step 1332:lm_loss: 4.5341, ppl: 93.1387, loss: 4.5619
	step 1333:lm_loss: 4.5338, ppl: 93.1102, loss: 4.5615
	step 1334:lm_loss: 4.5340, ppl: 93.1327, loss: 4.5619
	step 1335:lm_loss: 4.5340, ppl: 93.1260, loss: 4.5618
	step 1336:lm_loss: 4.5342, ppl: 93.1519, loss: 4.5620
	step 1337:lm_loss: 4.5343, ppl: 93.1540, loss: 4.5620
	step 1338:lm_loss: 4.5359, ppl: 93.3040, loss: 4.5631
	step 1339:lm_loss: 4.5352, ppl: 93.2410, loss: 4.5626
	step 1340:lm_loss: 4.5349, ppl: 93.2150, loss: 4.5622
	step 1341:lm_loss: 4.5350, ppl: 93.2273, loss: 4.5623
	step 1342:lm_loss: 4.5342, ppl: 93.1511, loss: 4.5614
	step 1343:lm_loss: 4.5343, ppl: 93.1600, loss: 4.5615
	step 1344:lm_loss: 4.5344, ppl: 93.1720, loss: 4.5616
	step 1345:lm_loss: 4.5347, ppl: 93.1949, loss: 4.5618
	step 1346:lm_loss: 4.5345, ppl: 93.1767, loss: 4.5616
	step 1347:lm_loss: 4.5338, ppl: 93.1159, loss: 4.5608
	step 1348:lm_loss: 4.5333, ppl: 93.0618, loss: 4.5603
	step 1349:lm_loss: 4.5337, ppl: 93.1005, loss: 4.5606
	step 1350:lm_loss: 4.5336, ppl: 93.0893, loss: 4.5604
	step 1351:lm_loss: 4.5338, ppl: 93.1095, loss: 4.5607
	step 1352:lm_loss: 4.5344, ppl: 93.1639, loss: 4.5612
	step 1353:lm_loss: 4.5346, ppl: 93.1885, loss: 4.5614
	step 1354:lm_loss: 4.5346, ppl: 93.1841, loss: 4.5614
	step 1355:lm_loss: 4.5348, ppl: 93.2083, loss: 4.5617
	step 1356:lm_loss: 4.5351, ppl: 93.2283, loss: 4.5620
	step 1357:lm_loss: 4.5358, ppl: 93.3006, loss: 4.5630
	step 1358:lm_loss: 4.5358, ppl: 93.3012, loss: 4.5630
	step 1359:lm_loss: 4.5359, ppl: 93.3105, loss: 4.5631
	step 1360:lm_loss: 4.5360, ppl: 93.3199, loss: 4.5632
	step 1361:lm_loss: 4.5350, ppl: 93.2190, loss: 4.5622
	step 1362:lm_loss: 4.5341, ppl: 93.1420, loss: 4.5611
	step 1363:lm_loss: 4.5340, ppl: 93.1289, loss: 4.5609
	step 1364:lm_loss: 4.5335, ppl: 93.0858, loss: 4.5604
	step 1365:lm_loss: 4.5313, ppl: 92.8777, loss: 4.5583
	step 1366:lm_loss: 4.5319, ppl: 92.9366, loss: 4.5588
	step 1367:lm_loss: 4.5332, ppl: 93.0512, loss: 4.5601
	step 1368:lm_loss: 4.5334, ppl: 93.0702, loss: 4.5603
	step 1369:lm_loss: 4.5331, ppl: 93.0439, loss: 4.5599
	step 1370:lm_loss: 4.5328, ppl: 93.0165, loss: 4.5595
	step 1371:lm_loss: 4.5331, ppl: 93.0457, loss: 4.5598
	step 1372:lm_loss: 4.5325, ppl: 92.9889, loss: 4.5589
	step 1373:lm_loss: 4.5320, ppl: 92.9446, loss: 4.5585
	step 1374:lm_loss: 4.5317, ppl: 92.9166, loss: 4.5581
	step 1375:lm_loss: 4.5299, ppl: 92.7476, loss: 4.5571
	step 1376:lm_loss: 4.5301, ppl: 92.7694, loss: 4.5573
	step 1377:lm_loss: 4.5304, ppl: 92.7978, loss: 4.5577
	step 1378:lm_loss: 4.5310, ppl: 92.8501, loss: 4.5581
	step 1379:lm_loss: 4.5320, ppl: 92.9397, loss: 4.5591
	step 1380:lm_loss: 4.5315, ppl: 92.9022, loss: 4.5588
	step 1381:lm_loss: 4.5321, ppl: 92.9498, loss: 4.5594
	step 1382:lm_loss: 4.5306, ppl: 92.8100, loss: 4.5584
	step 1383:lm_loss: 4.5302, ppl: 92.7792, loss: 4.5581
	step 1384:lm_loss: 4.5299, ppl: 92.7480, loss: 4.5579
	step 1385:lm_loss: 4.5294, ppl: 92.6989, loss: 4.5572
	step 1386:lm_loss: 4.5295, ppl: 92.7086, loss: 4.5573
	step 1387:lm_loss: 4.5297, ppl: 92.7293, loss: 4.5575
	step 1388:lm_loss: 4.5300, ppl: 92.7570, loss: 4.5578
	step 1389:lm_loss: 4.5297, ppl: 92.7321, loss: 4.5575
	step 1390:lm_loss: 4.5297, ppl: 92.7286, loss: 4.5574
	step 1391:lm_loss: 4.5295, ppl: 92.7104, loss: 4.5571
	step 1392:lm_loss: 4.5301, ppl: 92.7660, loss: 4.5576
	step 1393:lm_loss: 4.5305, ppl: 92.8053, loss: 4.5579
	step 1394:lm_loss: 4.5292, ppl: 92.6812, loss: 4.5572
	step 1395:lm_loss: 4.5292, ppl: 92.6881, loss: 4.5572
	step 1396:lm_loss: 4.5295, ppl: 92.7132, loss: 4.5574
	step 1397:lm_loss: 4.5301, ppl: 92.7651, loss: 4.5578
	step 1398:lm_loss: 4.5299, ppl: 92.7516, loss: 4.5577
	step 1399:lm_loss: 4.5307, ppl: 92.8220, loss: 4.5583
	step 1400:lm_loss: 4.5306, ppl: 92.8115, loss: 4.5582
	step 1401:lm_loss: 4.5310, ppl: 92.8516, loss: 4.5585
	step 1402:lm_loss: 4.5317, ppl: 92.9161, loss: 4.5594
	step 1403:lm_loss: 4.5316, ppl: 92.9102, loss: 4.5593
	step 1404:lm_loss: 4.5312, ppl: 92.8742, loss: 4.5588
	step 1405:lm_loss: 4.5311, ppl: 92.8572, loss: 4.5586
	step 1406:lm_loss: 4.5311, ppl: 92.8652, loss: 4.5586
	step 1407:lm_loss: 4.5308, ppl: 92.8336, loss: 4.5583
	step 1408:lm_loss: 4.5310, ppl: 92.8490, loss: 4.5584
	step 1409:lm_loss: 4.5302, ppl: 92.7760, loss: 4.5577
	step 1410:lm_loss: 4.5301, ppl: 92.7692, loss: 4.5576
	step 1411:lm_loss: 4.5307, ppl: 92.8198, loss: 4.5580
	step 1412:lm_loss: 4.5313, ppl: 92.8783, loss: 4.5585
	step 1413:lm_loss: 4.5299, ppl: 92.7521, loss: 4.5573
	step 1414:lm_loss: 4.5301, ppl: 92.7710, loss: 4.5575
	step 1415:lm_loss: 4.5307, ppl: 92.8281, loss: 4.5578
	step 1416:lm_loss: 4.5315, ppl: 92.8948, loss: 4.5586
	step 1417:lm_loss: 4.5319, ppl: 92.9325, loss: 4.5591
	step 1418:lm_loss: 4.5320, ppl: 92.9423, loss: 4.5592
	step 1419:lm_loss: 4.5322, ppl: 92.9609, loss: 4.5594
	step 1420:lm_loss: 4.5330, ppl: 93.0342, loss: 4.5601
	step 1421:lm_loss: 4.5331, ppl: 93.0492, loss: 4.5602
	step 1422:lm_loss: 4.5334, ppl: 93.0699, loss: 4.5604
	step 1423:lm_loss: 4.5342, ppl: 93.1497, loss: 4.5615
	step 1424:lm_loss: 4.5340, ppl: 93.1347, loss: 4.5613
	step 1425:lm_loss: 4.5340, ppl: 93.1283, loss: 4.5612
	step 1426:lm_loss: 4.5336, ppl: 93.0884, loss: 4.5606
	step 1427:lm_loss: 4.5336, ppl: 93.0954, loss: 4.5607
	step 1428:lm_loss: 4.5339, ppl: 93.1177, loss: 4.5609
	step 1429:lm_loss: 4.5343, ppl: 93.1625, loss: 4.5613
	step 1430:lm_loss: 4.5349, ppl: 93.2138, loss: 4.5617
	step 1431:lm_loss: 4.5352, ppl: 93.2413, loss: 4.5620
	step 1432:lm_loss: 4.5354, ppl: 93.2607, loss: 4.5622
	step 1433:lm_loss: 4.5348, ppl: 93.2018, loss: 4.5616
	step 1434:lm_loss: 4.5349, ppl: 93.2150, loss: 4.5617
	step 1435:lm_loss: 4.5351, ppl: 93.2344, loss: 4.5618
	step 1436:lm_loss: 4.5356, ppl: 93.2836, loss: 4.5624
	step 1437:lm_loss: 4.5342, ppl: 93.1528, loss: 4.5617
	step 1438:lm_loss: 4.5341, ppl: 93.1370, loss: 4.5614
	step 1439:lm_loss: 4.5342, ppl: 93.1497, loss: 4.5615
	step 1440:lm_loss: 4.5349, ppl: 93.2103, loss: 4.5623
	step 1441:lm_loss: 4.5337, ppl: 93.1040, loss: 4.5612
	step 1442:lm_loss: 4.5341, ppl: 93.1361, loss: 4.5615
	step 1443:lm_loss: 4.5344, ppl: 93.1717, loss: 4.5618
	step 1444:lm_loss: 4.5348, ppl: 93.2089, loss: 4.5622
	step 1445:lm_loss: 4.5345, ppl: 93.1740, loss: 4.5618
	step 1446:lm_loss: 4.5348, ppl: 93.2082, loss: 4.5621
	step 1447:lm_loss: 4.5346, ppl: 93.1888, loss: 4.5620
	step 1448:lm_loss: 4.5344, ppl: 93.1642, loss: 4.5616
	step 1449:lm_loss: 4.5342, ppl: 93.1467, loss: 4.5615
	step 1450:lm_loss: 4.5355, ppl: 93.2738, loss: 4.5627
	step 1451:lm_loss: 4.5355, ppl: 93.2724, loss: 4.5626
	step 1452:lm_loss: 4.5350, ppl: 93.2252, loss: 4.5620
	step 1453:lm_loss: 4.5354, ppl: 93.2614, loss: 4.5624
	step 1454:lm_loss: 4.5355, ppl: 93.2679, loss: 4.5624
	step 1455:lm_loss: 4.5361, ppl: 93.3277, loss: 4.5628
	step 1456:lm_loss: 4.5361, ppl: 93.3252, loss: 4.5628
	step 1457:lm_loss: 4.5366, ppl: 93.3687, loss: 4.5633
	step 1458:lm_loss: 4.5369, ppl: 93.3967, loss: 4.5635
	step 1459:lm_loss: 4.5375, ppl: 93.4612, loss: 4.5640
	step 1460:lm_loss: 4.5371, ppl: 93.4194, loss: 4.5636
	step 1461:lm_loss: 4.5370, ppl: 93.4142, loss: 4.5635
	step 1462:lm_loss: 4.5367, ppl: 93.3849, loss: 4.5632
	step 1463:lm_loss: 4.5376, ppl: 93.4656, loss: 4.5638
	step 1464:lm_loss: 4.5377, ppl: 93.4801, loss: 4.5639
	step 1465:lm_loss: 4.5379, ppl: 93.4939, loss: 4.5640
	step 1466:lm_loss: 4.5381, ppl: 93.5176, loss: 4.5642
	step 1467:lm_loss: 4.5382, ppl: 93.5233, loss: 4.5643
	step 1468:lm_loss: 4.5381, ppl: 93.5166, loss: 4.5642
	step 1469:lm_loss: 4.5384, ppl: 93.5394, loss: 4.5645
	step 1470:lm_loss: 4.5380, ppl: 93.5079, loss: 4.5641
	step 1471:lm_loss: 4.5386, ppl: 93.5642, loss: 4.5644
	step 1472:lm_loss: 4.5385, ppl: 93.5468, loss: 4.5642
	step 1473:lm_loss: 4.5383, ppl: 93.5344, loss: 4.5640
	step 1474:lm_loss: 4.5385, ppl: 93.5477, loss: 4.5641
	step 1475:lm_loss: 4.5384, ppl: 93.5436, loss: 4.5641
	step 1476:lm_loss: 4.5374, ppl: 93.4500, loss: 4.5630
	step 1477:lm_loss: 4.5378, ppl: 93.4832, loss: 4.5634
	step 1478:lm_loss: 4.5380, ppl: 93.5064, loss: 4.5637
	step 1479:lm_loss: 4.5373, ppl: 93.4416, loss: 4.5629
	step 1480:lm_loss: 4.5376, ppl: 93.4619, loss: 4.5632
	step 1481:lm_loss: 4.5377, ppl: 93.4720, loss: 4.5633
	step 1482:lm_loss: 4.5377, ppl: 93.4784, loss: 4.5634
	step 1483:lm_loss: 4.5384, ppl: 93.5453, loss: 4.5638
	step 1484:lm_loss: 4.5390, ppl: 93.5952, loss: 4.5644
	step 1485:lm_loss: 4.5399, ppl: 93.6782, loss: 4.5651
	step 1486:lm_loss: 4.5400, ppl: 93.6947, loss: 4.5652
	step 1487:lm_loss: 4.5404, ppl: 93.7278, loss: 4.5655
	step 1488:lm_loss: 4.5403, ppl: 93.7174, loss: 4.5654
	step 1489:lm_loss: 4.5398, ppl: 93.6760, loss: 4.5650
	step 1490:lm_loss: 4.5399, ppl: 93.6817, loss: 4.5651
	step 1491:lm_loss: 4.5400, ppl: 93.6908, loss: 4.5652
	step 1492:lm_loss: 4.5402, ppl: 93.7059, loss: 4.5653
	step 1493:lm_loss: 4.5401, ppl: 93.6992, loss: 4.5652
	step 1494:lm_loss: 4.5402, ppl: 93.7125, loss: 4.5654
	step 1495:lm_loss: 4.5404, ppl: 93.7259, loss: 4.5655
	step 1496:lm_loss: 4.5408, ppl: 93.7663, loss: 4.5660
	step 1497:lm_loss: 4.5413, ppl: 93.8080, loss: 4.5663
	step 1498:lm_loss: 4.5417, ppl: 93.8519, loss: 4.5669
	step 1499:lm_loss: 4.5413, ppl: 93.8118, loss: 4.5665
	step 1500:lm_loss: 4.5412, ppl: 93.8048, loss: 4.5664
	step 1501:lm_loss: 4.5414, ppl: 93.8266, loss: 4.5667
	step 1502:lm_loss: 4.5417, ppl: 93.8466, loss: 4.5670
	step 1503:lm_loss: 4.5416, ppl: 93.8429, loss: 4.5670
	step 1504:lm_loss: 4.5420, ppl: 93.8794, loss: 4.5674
	step 1505:lm_loss: 4.5416, ppl: 93.8421, loss: 4.5670
	step 1506:lm_loss: 4.5416, ppl: 93.8417, loss: 4.5670
	step 1507:lm_loss: 4.5419, ppl: 93.8693, loss: 4.5673
	step 1508:lm_loss: 4.5413, ppl: 93.8129, loss: 4.5667
	step 1509:lm_loss: 4.5411, ppl: 93.7957, loss: 4.5666
	step 1510:lm_loss: 4.5409, ppl: 93.7785, loss: 4.5664
	step 1511:lm_loss: 4.5404, ppl: 93.7305, loss: 4.5660
	step 1512:lm_loss: 4.5409, ppl: 93.7706, loss: 4.5665
	step 1513:lm_loss: 4.5404, ppl: 93.7270, loss: 4.5660
	step 1514:lm_loss: 4.5405, ppl: 93.7368, loss: 4.5661
	step 1515:lm_loss: 4.5399, ppl: 93.6770, loss: 4.5657
	step 1516:lm_loss: 4.5397, ppl: 93.6653, loss: 4.5656
	step 1517:lm_loss: 4.5395, ppl: 93.6422, loss: 4.5653
	step 1518:lm_loss: 4.5397, ppl: 93.6599, loss: 4.5655
	step 1519:lm_loss: 4.5397, ppl: 93.6586, loss: 4.5655
	step 1520:lm_loss: 4.5396, ppl: 93.6521, loss: 4.5654
	step 1521:lm_loss: 4.5382, ppl: 93.5205, loss: 4.5644
	step 1522:lm_loss: 4.5382, ppl: 93.5216, loss: 4.5644
	step 1523:lm_loss: 4.5384, ppl: 93.5370, loss: 4.5646
	step 1524:lm_loss: 4.5380, ppl: 93.4999, loss: 4.5639
	step 1525:lm_loss: 4.5381, ppl: 93.5155, loss: 4.5641
	step 1526:lm_loss: 4.5386, ppl: 93.5584, loss: 4.5647
	step 1527:lm_loss: 4.5387, ppl: 93.5727, loss: 4.5648
	step 1528:lm_loss: 4.5383, ppl: 93.5336, loss: 4.5643
	step 1529:lm_loss: 4.5378, ppl: 93.4859, loss: 4.5639
	step 1530:lm_loss: 4.5372, ppl: 93.4323, loss: 4.5636
	step 1531:lm_loss: 4.5374, ppl: 93.4457, loss: 4.5637
	step 1532:lm_loss: 4.5376, ppl: 93.4640, loss: 4.5639
	step 1533:lm_loss: 4.5377, ppl: 93.4725, loss: 4.5640
	step 1534:lm_loss: 4.5386, ppl: 93.5578, loss: 4.5651
	step 1535:lm_loss: 4.5384, ppl: 93.5372, loss: 4.5649
	step 1536:lm_loss: 4.5385, ppl: 93.5526, loss: 4.5650
	step 1537:lm_loss: 4.5385, ppl: 93.5496, loss: 4.5650
	step 1538:lm_loss: 4.5389, ppl: 93.5885, loss: 4.5653
	step 1539:lm_loss: 4.5389, ppl: 93.5861, loss: 4.5652
	step 1540:lm_loss: 4.5375, ppl: 93.4526, loss: 4.5642
	step 1541:lm_loss: 4.5379, ppl: 93.4932, loss: 4.5646
	step 1542:lm_loss: 4.5384, ppl: 93.5376, loss: 4.5649
	step 1543:lm_loss: 4.5385, ppl: 93.5475, loss: 4.5650
	step 1544:lm_loss: 4.5382, ppl: 93.5213, loss: 4.5647
	step 1545:lm_loss: 4.5379, ppl: 93.4905, loss: 4.5644
	step 1546:lm_loss: 4.5383, ppl: 93.5355, loss: 4.5646
	step 1547:lm_loss: 4.5382, ppl: 93.5232, loss: 4.5645
	step 1548:lm_loss: 4.5378, ppl: 93.4855, loss: 4.5639
	step 1549:lm_loss: 4.5383, ppl: 93.5302, loss: 4.5643
	step 1550:lm_loss: 4.5382, ppl: 93.5228, loss: 4.5643
	step 1551:lm_loss: 4.5387, ppl: 93.5683, loss: 4.5647
	step 1552:lm_loss: 4.5385, ppl: 93.5527, loss: 4.5645
	step 1553:lm_loss: 4.5384, ppl: 93.5398, loss: 4.5643
	step 1554:lm_loss: 4.5386, ppl: 93.5596, loss: 4.5644
	step 1555:lm_loss: 4.5389, ppl: 93.5856, loss: 4.5646
	step 1556:lm_loss: 4.5391, ppl: 93.6049, loss: 4.5648
	step 1557:lm_loss: 4.5383, ppl: 93.5324, loss: 4.5641
	step 1558:lm_loss: 4.5387, ppl: 93.5680, loss: 4.5645
	step 1559:lm_loss: 4.5388, ppl: 93.5751, loss: 4.5646
	step 1560:lm_loss: 4.5389, ppl: 93.5902, loss: 4.5646
	step 1561:lm_loss: 4.5396, ppl: 93.6568, loss: 4.5653
	step 1562:lm_loss: 4.5393, ppl: 93.6209, loss: 4.5649
	step 1563:lm_loss: 4.5393, ppl: 93.6247, loss: 4.5649
	step 1564:lm_loss: 4.5395, ppl: 93.6478, loss: 4.5650
	step 1565:lm_loss: 4.5397, ppl: 93.6653, loss: 4.5653
	step 1566:lm_loss: 4.5399, ppl: 93.6791, loss: 4.5655
	step 1567:lm_loss: 4.5398, ppl: 93.6758, loss: 4.5654
	step 1568:lm_loss: 4.5402, ppl: 93.7134, loss: 4.5659
	step 1569:lm_loss: 4.5406, ppl: 93.7428, loss: 4.5662
	step 1570:lm_loss: 4.5410, ppl: 93.7831, loss: 4.5667
	step 1571:lm_loss: 4.5415, ppl: 93.8319, loss: 4.5674
	step 1572:lm_loss: 4.5414, ppl: 93.8262, loss: 4.5673
	step 1573:lm_loss: 4.5398, ppl: 93.6762, loss: 4.5661
	step 1574:lm_loss: 4.5396, ppl: 93.6559, loss: 4.5658
	step 1575:lm_loss: 4.5393, ppl: 93.6251, loss: 4.5653
	step 1576:lm_loss: 4.5388, ppl: 93.5818, loss: 4.5648
	step 1577:lm_loss: 4.5383, ppl: 93.5316, loss: 4.5643
	step 1578:lm_loss: 4.5384, ppl: 93.5379, loss: 4.5644
	step 1579:lm_loss: 4.5388, ppl: 93.5829, loss: 4.5649
	step 1580:lm_loss: 4.5388, ppl: 93.5746, loss: 4.5648
	step 1581:lm_loss: 4.5381, ppl: 93.5092, loss: 4.5639
	step 1582:lm_loss: 4.5374, ppl: 93.4467, loss: 4.5633
	step 1583:lm_loss: 4.5372, ppl: 93.4249, loss: 4.5630
	step 1584:lm_loss: 4.5372, ppl: 93.4314, loss: 4.5631
	step 1585:lm_loss: 4.5364, ppl: 93.3546, loss: 4.5623
	step 1586:lm_loss: 4.5364, ppl: 93.3577, loss: 4.5624
	step 1587:lm_loss: 4.5369, ppl: 93.3963, loss: 4.5629
	step 1588:lm_loss: 4.5377, ppl: 93.4756, loss: 4.5634
	step 1589:lm_loss: 4.5367, ppl: 93.3856, loss: 4.5628
	step 1590:lm_loss: 4.5369, ppl: 93.3995, loss: 4.5630
	step 1591:lm_loss: 4.5375, ppl: 93.4527, loss: 4.5636
	step 1592:lm_loss: 4.5377, ppl: 93.4746, loss: 4.5638
	step 1593:lm_loss: 4.5378, ppl: 93.4880, loss: 4.5639
	step 1594:lm_loss: 4.5382, ppl: 93.5186, loss: 4.5642
	step 1595:lm_loss: 4.5386, ppl: 93.5633, loss: 4.5649
	step 1596:lm_loss: 4.5381, ppl: 93.5149, loss: 4.5642
	step 1597:lm_loss: 4.5377, ppl: 93.4757, loss: 4.5638
	step 1598:lm_loss: 4.5375, ppl: 93.4588, loss: 4.5637
	step 1599:lm_loss: 4.5378, ppl: 93.4834, loss: 4.5639
	step 1600:lm_loss: 4.5382, ppl: 93.5231, loss: 4.5646
	step 1601:lm_loss: 4.5389, ppl: 93.5833, loss: 4.5654
	step 1602:lm_loss: 4.5384, ppl: 93.5403, loss: 4.5648
	step 1603:lm_loss: 4.5384, ppl: 93.5398, loss: 4.5648
	step 1604:lm_loss: 4.5368, ppl: 93.3951, loss: 4.5635
	step 1605:lm_loss: 4.5363, ppl: 93.3486, loss: 4.5630
	step 1606:lm_loss: 4.5361, ppl: 93.3264, loss: 4.5628
	step 1607:lm_loss: 4.5359, ppl: 93.3105, loss: 4.5626
	step 1608:lm_loss: 4.5364, ppl: 93.3509, loss: 4.5631
	step 1609:lm_loss: 4.5362, ppl: 93.3394, loss: 4.5630
	step 1610:lm_loss: 4.5364, ppl: 93.3583, loss: 4.5632
	step 1611:lm_loss: 4.5373, ppl: 93.4414, loss: 4.5639
	step 1612:lm_loss: 4.5373, ppl: 93.4396, loss: 4.5639
	step 1613:lm_loss: 4.5374, ppl: 93.4497, loss: 4.5640
	step 1614:lm_loss: 4.5374, ppl: 93.4512, loss: 4.5640
	step 1615:lm_loss: 4.5371, ppl: 93.4196, loss: 4.5638
	step 1616:lm_loss: 4.5371, ppl: 93.4158, loss: 4.5638
	step 1617:lm_loss: 4.5373, ppl: 93.4354, loss: 4.5641
	step 1618:lm_loss: 4.5377, ppl: 93.4789, loss: 4.5645
	step 1619:lm_loss: 4.5381, ppl: 93.5092, loss: 4.5649
	step 1620:lm_loss: 4.5373, ppl: 93.4360, loss: 4.5645
	step 1621:lm_loss: 4.5370, ppl: 93.4145, loss: 4.5643
	step 1622:lm_loss: 4.5368, ppl: 93.3949, loss: 4.5642
	step 1623:lm_loss: 4.5373, ppl: 93.4426, loss: 4.5646
	step 1624:lm_loss: 4.5375, ppl: 93.4597, loss: 4.5648
	step 1625:lm_loss: 4.5376, ppl: 93.4683, loss: 4.5648
	step 1626:lm_loss: 4.5378, ppl: 93.4896, loss: 4.5650
	step 1627:lm_loss: 4.5384, ppl: 93.5439, loss: 4.5656
	step 1628:lm_loss: 4.5383, ppl: 93.5340, loss: 4.5655
	step 1629:lm_loss: 4.5383, ppl: 93.5326, loss: 4.5655
	step 1630:lm_loss: 4.5378, ppl: 93.4858, loss: 4.5649
	step 1631:lm_loss: 4.5380, ppl: 93.4991, loss: 4.5650
	step 1632:lm_loss: 4.5381, ppl: 93.5172, loss: 4.5652
	step 1633:lm_loss: 4.5385, ppl: 93.5520, loss: 4.5656
	step 1634:lm_loss: 4.5385, ppl: 93.5482, loss: 4.5655
	step 1635:lm_loss: 4.5382, ppl: 93.5247, loss: 4.5652
	step 1636:lm_loss: 4.5383, ppl: 93.5320, loss: 4.5653
	step 1637:lm_loss: 4.5379, ppl: 93.4909, loss: 4.5648
	step 1638:lm_loss: 4.5376, ppl: 93.4674, loss: 4.5645
	step 1639:lm_loss: 4.5377, ppl: 93.4759, loss: 4.5647
	step 1640:lm_loss: 4.5378, ppl: 93.4874, loss: 4.5648
	step 1641:lm_loss: 4.5376, ppl: 93.4667, loss: 4.5645
	step 1642:lm_loss: 4.5379, ppl: 93.4972, loss: 4.5647
	step 1643:lm_loss: 4.5383, ppl: 93.5339, loss: 4.5651
	step 1644:lm_loss: 4.5387, ppl: 93.5708, loss: 4.5655
	step 1645:lm_loss: 4.5387, ppl: 93.5704, loss: 4.5654
	step 1646:lm_loss: 4.5392, ppl: 93.6136, loss: 4.5659
	step 1647:lm_loss: 4.5398, ppl: 93.6699, loss: 4.5665
	step 1648:lm_loss: 4.5399, ppl: 93.6807, loss: 4.5666
	step 1649:lm_loss: 4.5401, ppl: 93.7038, loss: 4.5669
	step 1650:lm_loss: 4.5399, ppl: 93.6785, loss: 4.5665
	step 1651:lm_loss: 4.5390, ppl: 93.6000, loss: 4.5657
	step 1652:lm_loss: 4.5383, ppl: 93.5359, loss: 4.5650
	step 1653:lm_loss: 4.5380, ppl: 93.5068, loss: 4.5646
	step 1654:lm_loss: 4.5373, ppl: 93.4378, loss: 4.5638
	step 1655:lm_loss: 4.5371, ppl: 93.4213, loss: 4.5636
	step 1656:lm_loss: 4.5371, ppl: 93.4149, loss: 4.5635
	step 1657:lm_loss: 4.5376, ppl: 93.4620, loss: 4.5642
	step 1658:lm_loss: 4.5377, ppl: 93.4755, loss: 4.5643
	step 1659:lm_loss: 4.5377, ppl: 93.4754, loss: 4.5643
	step 1660:lm_loss: 4.5379, ppl: 93.4930, loss: 4.5646
	step 1661:lm_loss: 4.5381, ppl: 93.5145, loss: 4.5648
	step 1662:lm_loss: 4.5376, ppl: 93.4708, loss: 4.5642
	step 1663:lm_loss: 4.5375, ppl: 93.4605, loss: 4.5641
	step 1664:lm_loss: 4.5373, ppl: 93.4419, loss: 4.5638
	step 1665:lm_loss: 4.5366, ppl: 93.3741, loss: 4.5630
	step 1666:lm_loss: 4.5373, ppl: 93.4381, loss: 4.5643
	step 1667:lm_loss: 4.5372, ppl: 93.4255, loss: 4.5642
	step 1668:lm_loss: 4.5377, ppl: 93.4785, loss: 4.5647
	step 1669:lm_loss: 4.5376, ppl: 93.4659, loss: 4.5646
	step 1670:lm_loss: 4.5382, ppl: 93.5222, loss: 4.5651
	step 1671:lm_loss: 4.5387, ppl: 93.5716, loss: 4.5656
	step 1672:lm_loss: 4.5390, ppl: 93.5982, loss: 4.5659
	step 1673:lm_loss: 4.5391, ppl: 93.6051, loss: 4.5660
	step 1674:lm_loss: 4.5396, ppl: 93.6537, loss: 4.5667
	step 1675:lm_loss: 4.5397, ppl: 93.6657, loss: 4.5668
	step 1676:lm_loss: 4.5401, ppl: 93.7038, loss: 4.5673
	step 1677:lm_loss: 4.5408, ppl: 93.7626, loss: 4.5678
	step 1678:lm_loss: 4.5404, ppl: 93.7266, loss: 4.5674
	step 1679:lm_loss: 4.5402, ppl: 93.7092, loss: 4.5672
	step 1680:lm_loss: 4.5404, ppl: 93.7260, loss: 4.5674
	step 1681:lm_loss: 4.5397, ppl: 93.6629, loss: 4.5668
	step 1682:lm_loss: 4.5400, ppl: 93.6863, loss: 4.5670
	step 1683:lm_loss: 4.5400, ppl: 93.6942, loss: 4.5670
	step 1684:lm_loss: 4.5390, ppl: 93.5957, loss: 4.5659
	step 1685:lm_loss: 4.5389, ppl: 93.5876, loss: 4.5658
	step 1686:lm_loss: 4.5390, ppl: 93.5965, loss: 4.5659
	step 1687:lm_loss: 4.5389, ppl: 93.5897, loss: 4.5658
	step 1688:lm_loss: 4.5391, ppl: 93.6110, loss: 4.5661
	step 1689:lm_loss: 4.5393, ppl: 93.6220, loss: 4.5662
	step 1690:lm_loss: 4.5391, ppl: 93.6100, loss: 4.5660
	step 1691:lm_loss: 4.5397, ppl: 93.6653, loss: 4.5665
	step 1692:lm_loss: 4.5400, ppl: 93.6892, loss: 4.5666
	step 1693:lm_loss: 4.5401, ppl: 93.7034, loss: 4.5667
	step 1694:lm_loss: 4.5393, ppl: 93.6212, loss: 4.5662
	step 1695:lm_loss: 4.5391, ppl: 93.6078, loss: 4.5661
	step 1696:lm_loss: 4.5390, ppl: 93.6000, loss: 4.5660
	step 1697:lm_loss: 4.5392, ppl: 93.6199, loss: 4.5661
	step 1698:lm_loss: 4.5391, ppl: 93.6112, loss: 4.5660
	step 1699:lm_loss: 4.5389, ppl: 93.5868, loss: 4.5659
	step 1700:lm_loss: 4.5394, ppl: 93.6372, loss: 4.5665
	step 1701:lm_loss: 4.5391, ppl: 93.6099, loss: 4.5662
	step 1702:lm_loss: 4.5394, ppl: 93.6359, loss: 4.5665
	step 1703:lm_loss: 4.5402, ppl: 93.7082, loss: 4.5674
	step 1704:lm_loss: 4.5400, ppl: 93.6877, loss: 4.5671
	step 1705:lm_loss: 4.5397, ppl: 93.6633, loss: 4.5668
	step 1706:lm_loss: 4.5399, ppl: 93.6817, loss: 4.5671
	step 1707:lm_loss: 4.5400, ppl: 93.6910, loss: 4.5672
	step 1708:lm_loss: 4.5403, ppl: 93.7157, loss: 4.5674
	step 1709:lm_loss: 4.5404, ppl: 93.7324, loss: 4.5675
	step 1710:lm_loss: 4.5407, ppl: 93.7594, loss: 4.5677
	step 1711:lm_loss: 4.5406, ppl: 93.7468, loss: 4.5675
	step 1712:lm_loss: 4.5407, ppl: 93.7598, loss: 4.5676
	step 1713:lm_loss: 4.5412, ppl: 93.8050, loss: 4.5683
	step 1714:lm_loss: 4.5400, ppl: 93.6941, loss: 4.5675
	step 1715:lm_loss: 4.5405, ppl: 93.7365, loss: 4.5681
	step 1716:lm_loss: 4.5403, ppl: 93.7193, loss: 4.5678
	step 1717:lm_loss: 4.5402, ppl: 93.7111, loss: 4.5677
	step 1718:lm_loss: 4.5404, ppl: 93.7258, loss: 4.5678
	step 1719:lm_loss: 4.5403, ppl: 93.7222, loss: 4.5677
	step 1720:lm_loss: 4.5403, ppl: 93.7171, loss: 4.5677
	step 1721:lm_loss: 4.5407, ppl: 93.7585, loss: 4.5680
	step 1722:lm_loss: 4.5407, ppl: 93.7577, loss: 4.5680
	step 1723:lm_loss: 4.5407, ppl: 93.7563, loss: 4.5679
	step 1724:lm_loss: 4.5406, ppl: 93.7444, loss: 4.5678
	step 1725:lm_loss: 4.5407, ppl: 93.7538, loss: 4.5679
	step 1726:lm_loss: 4.5414, ppl: 93.8231, loss: 4.5686
	step 1727:lm_loss: 4.5419, ppl: 93.8653, loss: 4.5690
	step 1728:lm_loss: 4.5419, ppl: 93.8734, loss: 4.5691
	step 1729:lm_loss: 4.5416, ppl: 93.8448, loss: 4.5688
	step 1730:lm_loss: 4.5421, ppl: 93.8852, loss: 4.5691
	step 1731:lm_loss: 4.5422, ppl: 93.8942, loss: 4.5691
	step 1732:lm_loss: 4.5422, ppl: 93.8933, loss: 4.5691
	step 1733:lm_loss: 4.5426, ppl: 93.9331, loss: 4.5695
	step 1734:lm_loss: 4.5421, ppl: 93.8838, loss: 4.5690
	step 1735:lm_loss: 4.5425, ppl: 93.9263, loss: 4.5693
	step 1736:lm_loss: 4.5423, ppl: 93.9108, loss: 4.5690
	step 1737:lm_loss: 4.5424, ppl: 93.9131, loss: 4.5691
	step 1738:lm_loss: 4.5423, ppl: 93.9087, loss: 4.5690
	step 1739:lm_loss: 4.5424, ppl: 93.9186, loss: 4.5691
	step 1740:lm_loss: 4.5429, ppl: 93.9612, loss: 4.5695
	step 1741:lm_loss: 4.5426, ppl: 93.9391, loss: 4.5693
	step 1742:lm_loss: 4.5431, ppl: 93.9795, loss: 4.5698
	step 1743:lm_loss: 4.5421, ppl: 93.8869, loss: 4.5689
	step 1744:lm_loss: 4.5427, ppl: 93.9435, loss: 4.5694
	step 1745:lm_loss: 4.5425, ppl: 93.9252, loss: 4.5691
	step 1746:lm_loss: 4.5422, ppl: 93.8987, loss: 4.5689
	step 1747:lm_loss: 4.5428, ppl: 93.9582, loss: 4.5696
	step 1748:lm_loss: 4.5424, ppl: 93.9166, loss: 4.5693
	step 1749:lm_loss: 4.5425, ppl: 93.9288, loss: 4.5694
	step 1750:lm_loss: 4.5427, ppl: 93.9408, loss: 4.5695
	step 1751:lm_loss: 4.5427, ppl: 93.9428, loss: 4.5695
	step 1752:lm_loss: 4.5424, ppl: 93.9120, loss: 4.5692
	step 1753:lm_loss: 4.5429, ppl: 93.9673, loss: 4.5696
	step 1754:lm_loss: 4.5437, ppl: 94.0385, loss: 4.5704
	step 1755:lm_loss: 4.5435, ppl: 94.0228, loss: 4.5702
	step 1756:lm_loss: 4.5437, ppl: 94.0384, loss: 4.5704
	step 1757:lm_loss: 4.5438, ppl: 94.0518, loss: 4.5706
	step 1758:lm_loss: 4.5438, ppl: 94.0447, loss: 4.5705
	step 1759:lm_loss: 4.5435, ppl: 94.0151, loss: 4.5701
	step 1760:lm_loss: 4.5433, ppl: 93.9959, loss: 4.5699
	step 1761:lm_loss: 4.5434, ppl: 94.0143, loss: 4.5701
	step 1762:lm_loss: 4.5433, ppl: 93.9990, loss: 4.5700
	step 1763:lm_loss: 4.5429, ppl: 93.9649, loss: 4.5695
	step 1764:lm_loss: 4.5430, ppl: 93.9743, loss: 4.5696
	step 1765:lm_loss: 4.5431, ppl: 93.9837, loss: 4.5697
	step 1766:lm_loss: 4.5433, ppl: 93.9967, loss: 4.5699
	step 1767:lm_loss: 4.5431, ppl: 93.9850, loss: 4.5696
	step 1768:lm_loss: 4.5435, ppl: 94.0195, loss: 4.5700
	step 1769:lm_loss: 4.5434, ppl: 94.0094, loss: 4.5699
	step 1770:lm_loss: 4.5431, ppl: 93.9773, loss: 4.5695
	step 1771:lm_loss: 4.5430, ppl: 93.9740, loss: 4.5695
	step 1772:lm_loss: 4.5434, ppl: 94.0064, loss: 4.5699
	step 1773:lm_loss: 4.5437, ppl: 94.0378, loss: 4.5703
	step 1774:lm_loss: 4.5447, ppl: 94.1287, loss: 4.5708
	step 1775:lm_loss: 4.5451, ppl: 94.1725, loss: 4.5712
	step 1776:lm_loss: 4.5455, ppl: 94.2086, loss: 4.5716
	step 1777:lm_loss: 4.5454, ppl: 94.2021, loss: 4.5715
	step 1778:lm_loss: 4.5457, ppl: 94.2224, loss: 4.5717
	step 1779:lm_loss: 4.5459, ppl: 94.2476, loss: 4.5720
	step 1780:lm_loss: 4.5456, ppl: 94.2188, loss: 4.5717
	step 1781:lm_loss: 4.5456, ppl: 94.2190, loss: 4.5717
	step 1782:lm_loss: 4.5451, ppl: 94.1694, loss: 4.5713
	step 1783:lm_loss: 4.5446, ppl: 94.1218, loss: 4.5707
	step 1784:lm_loss: 4.5444, ppl: 94.1072, loss: 4.5706
	step 1785:lm_loss: 4.5445, ppl: 94.1101, loss: 4.5706
	step 1786:lm_loss: 4.5447, ppl: 94.1333, loss: 4.5708
	step 1787:lm_loss: 4.5446, ppl: 94.1226, loss: 4.5706
	step 1788:lm_loss: 4.5445, ppl: 94.1153, loss: 4.5705
	step 1789:lm_loss: 4.5450, ppl: 94.1587, loss: 4.5713
	step 1790:lm_loss: 4.5453, ppl: 94.1861, loss: 4.5716
	step 1791:lm_loss: 4.5453, ppl: 94.1842, loss: 4.5716
	step 1792:lm_loss: 4.5454, ppl: 94.1996, loss: 4.5717
	step 1793:lm_loss: 4.5458, ppl: 94.2343, loss: 4.5721
	step 1794:lm_loss: 4.5460, ppl: 94.2512, loss: 4.5722
	step 1795:lm_loss: 4.5460, ppl: 94.2585, loss: 4.5723
	step 1796:lm_loss: 4.5459, ppl: 94.2498, loss: 4.5722
	step 1797:lm_loss: 4.5461, ppl: 94.2639, loss: 4.5724
	step 1798:lm_loss: 4.5456, ppl: 94.2209, loss: 4.5719
	step 1799:lm_loss: 4.5461, ppl: 94.2613, loss: 4.5722
	step 1800:lm_loss: 4.5462, ppl: 94.2719, loss: 4.5723
	step 1801:lm_loss: 4.5472, ppl: 94.3643, loss: 4.5736
	step 1802:lm_loss: 4.5474, ppl: 94.3831, loss: 4.5739
	step 1803:lm_loss: 4.5472, ppl: 94.3635, loss: 4.5736
	step 1804:lm_loss: 4.5474, ppl: 94.3826, loss: 4.5738
	step 1805:lm_loss: 4.5475, ppl: 94.3958, loss: 4.5739
	step 1806:lm_loss: 4.5479, ppl: 94.4342, loss: 4.5744
	step 1807:lm_loss: 4.5483, ppl: 94.4725, loss: 4.5746
	step 1808:lm_loss: 4.5487, ppl: 94.5060, loss: 4.5750
	step 1809:lm_loss: 4.5489, ppl: 94.5321, loss: 4.5752
	step 1810:lm_loss: 4.5487, ppl: 94.5141, loss: 4.5750
	step 1811:lm_loss: 4.5491, ppl: 94.5518, loss: 4.5756
	step 1812:lm_loss: 4.5488, ppl: 94.5184, loss: 4.5752
	step 1813:lm_loss: 4.5492, ppl: 94.5596, loss: 4.5757
	step 1814:lm_loss: 4.5492, ppl: 94.5536, loss: 4.5756
	step 1815:lm_loss: 4.5488, ppl: 94.5182, loss: 4.5751
	step 1816:lm_loss: 4.5495, ppl: 94.5820, loss: 4.5757
	step 1817:lm_loss: 4.5494, ppl: 94.5749, loss: 4.5757
	step 1818:lm_loss: 4.5493, ppl: 94.5640, loss: 4.5755
	step 1819:lm_loss: 4.5496, ppl: 94.5958, loss: 4.5758
	step 1820:lm_loss: 4.5502, ppl: 94.6537, loss: 4.5763
	step 1821:lm_loss: 4.5508, ppl: 94.7098, loss: 4.5768
	step 1822:lm_loss: 4.5506, ppl: 94.6876, loss: 4.5765
	step 1823:lm_loss: 4.5506, ppl: 94.6897, loss: 4.5765
	step 1824:lm_loss: 4.5510, ppl: 94.7314, loss: 4.5770
	step 1825:lm_loss: 4.5508, ppl: 94.7074, loss: 4.5767
	step 1826:lm_loss: 4.5511, ppl: 94.7319, loss: 4.5769
	step 1827:lm_loss: 4.5516, ppl: 94.7845, loss: 4.5773
	step 1828:lm_loss: 4.5517, ppl: 94.7890, loss: 4.5773
	step 1829:lm_loss: 4.5515, ppl: 94.7747, loss: 4.5771
	step 1830:lm_loss: 4.5512, ppl: 94.7466, loss: 4.5769
	step 1831:lm_loss: 4.5511, ppl: 94.7403, loss: 4.5768
	step 1832:lm_loss: 4.5511, ppl: 94.7335, loss: 4.5766
	step 1833:lm_loss: 4.5514, ppl: 94.7697, loss: 4.5769
	step 1834:lm_loss: 4.5512, ppl: 94.7458, loss: 4.5766
	step 1835:lm_loss: 4.5512, ppl: 94.7443, loss: 4.5765
	step 1836:lm_loss: 4.5515, ppl: 94.7723, loss: 4.5769
	step 1837:lm_loss: 4.5520, ppl: 94.8200, loss: 4.5776
	step 1838:lm_loss: 4.5523, ppl: 94.8498, loss: 4.5779
	step 1839:lm_loss: 4.5518, ppl: 94.8024, loss: 4.5774
	step 1840:lm_loss: 4.5512, ppl: 94.7497, loss: 4.5767
	step 1841:lm_loss: 4.5514, ppl: 94.7646, loss: 4.5768
	step 1842:lm_loss: 4.5513, ppl: 94.7511, loss: 4.5766
	step 1843:lm_loss: 4.5511, ppl: 94.7396, loss: 4.5765
	step 1844:lm_loss: 4.5502, ppl: 94.6540, loss: 4.5758
	step 1845:lm_loss: 4.5502, ppl: 94.6473, loss: 4.5757
	step 1846:lm_loss: 4.5499, ppl: 94.6227, loss: 4.5755
	step 1847:lm_loss: 4.5500, ppl: 94.6283, loss: 4.5755
	step 1848:lm_loss: 4.5502, ppl: 94.6532, loss: 4.5757
	step 1849:lm_loss: 4.5502, ppl: 94.6476, loss: 4.5757
	step 1850:lm_loss: 4.5497, ppl: 94.6017, loss: 4.5753
	step 1851:lm_loss: 4.5503, ppl: 94.6578, loss: 4.5758
	step 1852:lm_loss: 4.5508, ppl: 94.7082, loss: 4.5766
	step 1853:lm_loss: 4.5506, ppl: 94.6885, loss: 4.5764
	step 1854:lm_loss: 4.5501, ppl: 94.6403, loss: 4.5759
	step 1855:lm_loss: 4.5501, ppl: 94.6413, loss: 4.5759
	step 1856:lm_loss: 4.5500, ppl: 94.6292, loss: 4.5758
	step 1857:lm_loss: 4.5507, ppl: 94.7009, loss: 4.5764
	step 1858:lm_loss: 4.5511, ppl: 94.7378, loss: 4.5767
	step 1859:lm_loss: 4.5509, ppl: 94.7133, loss: 4.5765
	step 1860:lm_loss: 4.5503, ppl: 94.6652, loss: 4.5761
	step 1861:lm_loss: 4.5507, ppl: 94.6992, loss: 4.5765
	step 1862:lm_loss: 4.5505, ppl: 94.6758, loss: 4.5762
	step 1863:lm_loss: 4.5502, ppl: 94.6545, loss: 4.5758
	step 1864:lm_loss: 4.5505, ppl: 94.6768, loss: 4.5761
	step 1865:lm_loss: 4.5505, ppl: 94.6794, loss: 4.5762
	step 1866:lm_loss: 4.5504, ppl: 94.6733, loss: 4.5761
	step 1867:lm_loss: 4.5504, ppl: 94.6746, loss: 4.5761
	step 1868:lm_loss: 4.5498, ppl: 94.6132, loss: 4.5755
	step 1869:lm_loss: 4.5494, ppl: 94.5744, loss: 4.5749
	step 1870:lm_loss: 4.5494, ppl: 94.5761, loss: 4.5749
	step 1871:lm_loss: 4.5496, ppl: 94.5902, loss: 4.5751
	step 1872:lm_loss: 4.5491, ppl: 94.5497, loss: 4.5748
	step 1873:lm_loss: 4.5491, ppl: 94.5453, loss: 4.5747
	step 1874:lm_loss: 4.5493, ppl: 94.5636, loss: 4.5749
	step 1875:lm_loss: 4.5497, ppl: 94.6004, loss: 4.5754
	step 1876:lm_loss: 4.5493, ppl: 94.5673, loss: 4.5750
	step 1877:lm_loss: 4.5488, ppl: 94.5211, loss: 4.5745
	step 1878:lm_loss: 4.5487, ppl: 94.5141, loss: 4.5744
	step 1879:lm_loss: 4.5487, ppl: 94.5137, loss: 4.5744
	step 1880:lm_loss: 4.5493, ppl: 94.5686, loss: 4.5750
	step 1881:lm_loss: 4.5493, ppl: 94.5663, loss: 4.5750
	step 1882:lm_loss: 4.5489, ppl: 94.5317, loss: 4.5746
	step 1883:lm_loss: 4.5491, ppl: 94.5439, loss: 4.5748
	step 1884:lm_loss: 4.5489, ppl: 94.5290, loss: 4.5746
	step 1885:lm_loss: 4.5487, ppl: 94.5067, loss: 4.5744
	step 1886:lm_loss: 4.5486, ppl: 94.4965, loss: 4.5743
	step 1887:lm_loss: 4.5487, ppl: 94.5106, loss: 4.5744
	step 1888:lm_loss: 4.5485, ppl: 94.4887, loss: 4.5741
	step 1889:lm_loss: 4.5478, ppl: 94.4263, loss: 4.5732
	step 1890:lm_loss: 4.5481, ppl: 94.4495, loss: 4.5736
	step 1891:lm_loss: 4.5480, ppl: 94.4430, loss: 4.5735
	step 1892:lm_loss: 4.5475, ppl: 94.3941, loss: 4.5731
	step 1893:lm_loss: 4.5474, ppl: 94.3846, loss: 4.5730
	step 1894:lm_loss: 4.5470, ppl: 94.3475, loss: 4.5727
	step 1895:lm_loss: 4.5460, ppl: 94.2566, loss: 4.5724
	step 1896:lm_loss: 4.5461, ppl: 94.2609, loss: 4.5724
	step 1897:lm_loss: 4.5459, ppl: 94.2420, loss: 4.5722
	step 1898:lm_loss: 4.5457, ppl: 94.2235, loss: 4.5719
	step 1899:lm_loss: 4.5459, ppl: 94.2461, loss: 4.5722
	step 1900:lm_loss: 4.5459, ppl: 94.2470, loss: 4.5722
	step 1901:lm_loss: 4.5464, ppl: 94.2896, loss: 4.5726
	step 1902:lm_loss: 4.5462, ppl: 94.2709, loss: 4.5725
	step 1903:lm_loss: 4.5463, ppl: 94.2852, loss: 4.5727
	step 1904:lm_loss: 4.5462, ppl: 94.2753, loss: 4.5725
	step 1905:lm_loss: 4.5459, ppl: 94.2471, loss: 4.5722
	step 1906:lm_loss: 4.5456, ppl: 94.2189, loss: 4.5719
	step 1907:lm_loss: 4.5450, ppl: 94.1607, loss: 4.5714
	step 1908:lm_loss: 4.5453, ppl: 94.1917, loss: 4.5717
	step 1909:lm_loss: 4.5453, ppl: 94.1892, loss: 4.5716
	step 1910:lm_loss: 4.5453, ppl: 94.1855, loss: 4.5716
	step 1911:lm_loss: 4.5459, ppl: 94.2459, loss: 4.5723
	step 1912:lm_loss: 4.5458, ppl: 94.2327, loss: 4.5722
	step 1913:lm_loss: 4.5460, ppl: 94.2520, loss: 4.5723
	step 1914:lm_loss: 4.5465, ppl: 94.2991, loss: 4.5731
	step 1915:lm_loss: 4.5464, ppl: 94.2914, loss: 4.5730
	step 1916:lm_loss: 4.5460, ppl: 94.2516, loss: 4.5723
	step 1917:lm_loss: 4.5460, ppl: 94.2548, loss: 4.5724
	step 1918:lm_loss: 4.5465, ppl: 94.3050, loss: 4.5728
	step 1919:lm_loss: 4.5470, ppl: 94.3446, loss: 4.5732
	step 1920:lm_loss: 4.5474, ppl: 94.3892, loss: 4.5735
	step 1921:lm_loss: 4.5472, ppl: 94.3696, loss: 4.5733
	step 1922:lm_loss: 4.5476, ppl: 94.4011, loss: 4.5738
	step 1923:lm_loss: 4.5478, ppl: 94.4271, loss: 4.5740
	step 1924:lm_loss: 4.5478, ppl: 94.4250, loss: 4.5740
	step 1925:lm_loss: 4.5482, ppl: 94.4607, loss: 4.5744
	step 1926:lm_loss: 4.5476, ppl: 94.4099, loss: 4.5739
	step 1927:lm_loss: 4.5475, ppl: 94.3997, loss: 4.5738
	step 1928:lm_loss: 4.5473, ppl: 94.3795, loss: 4.5734
	step 1929:lm_loss: 4.5472, ppl: 94.3656, loss: 4.5732
	step 1930:lm_loss: 4.5473, ppl: 94.3769, loss: 4.5733
	step 1931:lm_loss: 4.5472, ppl: 94.3709, loss: 4.5733
	step 1932:lm_loss: 4.5470, ppl: 94.3487, loss: 4.5729
	step 1933:lm_loss: 4.5461, ppl: 94.2607, loss: 4.5722
	step 1934:lm_loss: 4.5462, ppl: 94.2764, loss: 4.5723
	step 1935:lm_loss: 4.5462, ppl: 94.2763, loss: 4.5723
	step 1936:lm_loss: 4.5464, ppl: 94.2962, loss: 4.5725
	step 1937:lm_loss: 4.5463, ppl: 94.2859, loss: 4.5724
	step 1938:lm_loss: 4.5469, ppl: 94.3397, loss: 4.5733
	step 1939:lm_loss: 4.5468, ppl: 94.3294, loss: 4.5731
	step 1940:lm_loss: 4.5469, ppl: 94.3432, loss: 4.5732
	step 1941:lm_loss: 4.5472, ppl: 94.3686, loss: 4.5734
	step 1942:lm_loss: 4.5476, ppl: 94.4013, loss: 4.5738
	step 1943:lm_loss: 4.5479, ppl: 94.4365, loss: 4.5740
	step 1944:lm_loss: 4.5472, ppl: 94.3637, loss: 4.5735
	step 1945:lm_loss: 4.5465, ppl: 94.2975, loss: 4.5730
	step 1946:lm_loss: 4.5446, ppl: 94.1264, loss: 4.5719
	step 1947:lm_loss: 4.5443, ppl: 94.0981, loss: 4.5716
	step 1948:lm_loss: 4.5443, ppl: 94.0903, loss: 4.5715
	step 1949:lm_loss: 4.5443, ppl: 94.0963, loss: 4.5716
	step 1950:lm_loss: 4.5448, ppl: 94.1422, loss: 4.5722
	step 1951:lm_loss: 4.5450, ppl: 94.1569, loss: 4.5723
	step 1952:lm_loss: 4.5449, ppl: 94.1478, loss: 4.5722
	step 1953:lm_loss: 4.5449, ppl: 94.1476, loss: 4.5722
	step 1954:lm_loss: 4.5451, ppl: 94.1724, loss: 4.5724
	step 1955:lm_loss: 4.5447, ppl: 94.1311, loss: 4.5719
	step 1956:lm_loss: 4.5446, ppl: 94.1193, loss: 4.5717
	step 1957:lm_loss: 4.5451, ppl: 94.1681, loss: 4.5724
	step 1958:lm_loss: 4.5453, ppl: 94.1903, loss: 4.5725
	step 1959:lm_loss: 4.5454, ppl: 94.1962, loss: 4.5726
	step 1960:lm_loss: 4.5442, ppl: 94.0858, loss: 4.5715
	step 1961:lm_loss: 4.5436, ppl: 94.0330, loss: 4.5712
	step 1962:lm_loss: 4.5435, ppl: 94.0205, loss: 4.5711
	step 1963:lm_loss: 4.5433, ppl: 94.0004, loss: 4.5710
	step 1964:lm_loss: 4.5432, ppl: 93.9956, loss: 4.5709
	step 1965:lm_loss: 4.5431, ppl: 93.9860, loss: 4.5708
	step 1966:lm_loss: 4.5434, ppl: 94.0106, loss: 4.5711
	step 1967:lm_loss: 4.5434, ppl: 94.0138, loss: 4.5711
	step 1968:lm_loss: 4.5438, ppl: 94.0452, loss: 4.5715
	step 1969:lm_loss: 4.5435, ppl: 94.0198, loss: 4.5712
	step 1970:lm_loss: 4.5427, ppl: 93.9478, loss: 4.5707
	step 1971:lm_loss: 4.5430, ppl: 93.9734, loss: 4.5710
	step 1972:lm_loss: 4.5431, ppl: 93.9777, loss: 4.5711
	step 1973:lm_loss: 4.5403, ppl: 93.7220, loss: 4.5705
	step 1974:lm_loss: 4.5399, ppl: 93.6778, loss: 4.5699
	step 1975:lm_loss: 4.5397, ppl: 93.6605, loss: 4.5697
	step 1976:lm_loss: 4.5396, ppl: 93.6493, loss: 4.5695
	step 1977:lm_loss: 4.5400, ppl: 93.6936, loss: 4.5700
	step 1978:lm_loss: 4.5389, ppl: 93.5880, loss: 4.5694
	step 1979:lm_loss: 4.5389, ppl: 93.5869, loss: 4.5694
	step 1980:lm_loss: 4.5391, ppl: 93.6087, loss: 4.5696
	step 1981:lm_loss: 4.5389, ppl: 93.5864, loss: 4.5693
	step 1982:lm_loss: 4.5388, ppl: 93.5804, loss: 4.5692
	step 1983:lm_loss: 4.5389, ppl: 93.5904, loss: 4.5693
	step 1984:lm_loss: 4.5388, ppl: 93.5742, loss: 4.5691
	step 1985:lm_loss: 4.5384, ppl: 93.5417, loss: 4.5689
	step 1986:lm_loss: 4.5381, ppl: 93.5136, loss: 4.5685
	step 1987:lm_loss: 4.5385, ppl: 93.5477, loss: 4.5690
	step 1988:lm_loss: 4.5388, ppl: 93.5744, loss: 4.5692
	step 1989:lm_loss: 4.5382, ppl: 93.5224, loss: 4.5688
	step 1990:lm_loss: 4.5380, ppl: 93.5063, loss: 4.5686
	step 1991:lm_loss: 4.5382, ppl: 93.5208, loss: 4.5687
	step 1992:lm_loss: 4.5382, ppl: 93.5231, loss: 4.5687
	step 1993:lm_loss: 4.5376, ppl: 93.4683, loss: 4.5682
	step 1994:lm_loss: 4.5374, ppl: 93.4500, loss: 4.5679
	step 1995:lm_loss: 4.5375, ppl: 93.4544, loss: 4.5679
	step 1996:lm_loss: 4.5375, ppl: 93.4564, loss: 4.5679
	step 1997:lm_loss: 4.5373, ppl: 93.4386, loss: 4.5676
	step 1998:lm_loss: 4.5372, ppl: 93.4328, loss: 4.5675
	step 1999:lm_loss: 4.5371, ppl: 93.4226, loss: 4.5674
	step 2000:lm_loss: 4.5371, ppl: 93.4201, loss: 4.5674
	step 2001:lm_loss: 4.5369, ppl: 93.3991, loss: 4.5672
	step 2002:lm_loss: 4.5370, ppl: 93.4093, loss: 4.5673
	step 2003:lm_loss: 4.5366, ppl: 93.3747, loss: 4.5670
	step 2004:lm_loss: 4.5370, ppl: 93.4069, loss: 4.5675
	step 2005:lm_loss: 4.5374, ppl: 93.4447, loss: 4.5677
	step 2006:lm_loss: 4.5371, ppl: 93.4231, loss: 4.5674
	step 2007:lm_loss: 4.5373, ppl: 93.4360, loss: 4.5676
	step 2008:lm_loss: 4.5373, ppl: 93.4343, loss: 4.5676
	step 2009:lm_loss: 4.5375, ppl: 93.4608, loss: 4.5678
	step 2010:lm_loss: 4.5376, ppl: 93.4657, loss: 4.5678
	step 2011:lm_loss: 4.5380, ppl: 93.5006, loss: 4.5680
	step 2012:lm_loss: 4.5380, ppl: 93.5009, loss: 4.5680
	step 2013:lm_loss: 4.5382, ppl: 93.5198, loss: 4.5682
	step 2014:lm_loss: 4.5383, ppl: 93.5297, loss: 4.5683
	step 2015:lm_loss: 4.5391, ppl: 93.6106, loss: 4.5688
	step 2016:lm_loss: 4.5380, ppl: 93.5079, loss: 4.5680
	step 2017:lm_loss: 4.5379, ppl: 93.4974, loss: 4.5679
	step 2018:lm_loss: 4.5382, ppl: 93.5203, loss: 4.5682
	step 2019:lm_loss: 4.5380, ppl: 93.5077, loss: 4.5680
	step 2020:lm_loss: 4.5379, ppl: 93.4953, loss: 4.5679
	step 2021:lm_loss: 4.5367, ppl: 93.3834, loss: 4.5663
	step 2022:lm_loss: 4.5374, ppl: 93.4490, loss: 4.5670
	step 2023:lm_loss: 4.5376, ppl: 93.4652, loss: 4.5672
	step 2024:lm_loss: 4.5377, ppl: 93.4754, loss: 4.5673
	step 2025:lm_loss: 4.5379, ppl: 93.4916, loss: 4.5675
	step 2026:lm_loss: 4.5369, ppl: 93.4014, loss: 4.5669
	step 2027:lm_loss: 4.5373, ppl: 93.4389, loss: 4.5672
	step 2028:lm_loss: 4.5371, ppl: 93.4228, loss: 4.5670
	step 2029:lm_loss: 4.5374, ppl: 93.4488, loss: 4.5674
	step 2030:lm_loss: 4.5373, ppl: 93.4400, loss: 4.5673
	step 2031:lm_loss: 4.5371, ppl: 93.4208, loss: 4.5671
	step 2032:lm_loss: 4.5371, ppl: 93.4233, loss: 4.5672
	step 2033:lm_loss: 4.5375, ppl: 93.4601, loss: 4.5675
	step 2034:lm_loss: 4.5373, ppl: 93.4406, loss: 4.5673
	step 2035:lm_loss: 4.5366, ppl: 93.3701, loss: 4.5668
	step 2036:lm_loss: 4.5366, ppl: 93.3767, loss: 4.5669
	step 2037:lm_loss: 4.5362, ppl: 93.3310, loss: 4.5664
	step 2038:lm_loss: 4.5364, ppl: 93.3540, loss: 4.5667
	step 2039:lm_loss: 4.5365, ppl: 93.3655, loss: 4.5669
	step 2040:lm_loss: 4.5366, ppl: 93.3709, loss: 4.5669
	step 2041:lm_loss: 4.5365, ppl: 93.3621, loss: 4.5668
	step 2042:lm_loss: 4.5361, ppl: 93.3293, loss: 4.5665
	step 2043:lm_loss: 4.5362, ppl: 93.3342, loss: 4.5665
	step 2044:lm_loss: 4.5357, ppl: 93.2882, loss: 4.5660
	step 2045:lm_loss: 4.5349, ppl: 93.2097, loss: 4.5653
	step 2046:lm_loss: 4.5345, ppl: 93.1744, loss: 4.5650
	step 2047:lm_loss: 4.5345, ppl: 93.1782, loss: 4.5650
	step 2048:lm_loss: 4.5350, ppl: 93.2215, loss: 4.5654
	step 2049:lm_loss: 4.5343, ppl: 93.1586, loss: 4.5650
	step 2050:lm_loss: 4.5349, ppl: 93.2128, loss: 4.5655
	step 2051:lm_loss: 4.5352, ppl: 93.2447, loss: 4.5659
	step 2052:lm_loss: 4.5353, ppl: 93.2530, loss: 4.5660
	step 2053:lm_loss: 4.5349, ppl: 93.2123, loss: 4.5657
	step 2054:lm_loss: 4.5344, ppl: 93.1672, loss: 4.5650
	step 2055:lm_loss: 4.5339, ppl: 93.1188, loss: 4.5645
	step 2056:lm_loss: 4.5340, ppl: 93.1287, loss: 4.5645
	step 2057:lm_loss: 4.5333, ppl: 93.0631, loss: 4.5637
	step 2058:lm_loss: 4.5331, ppl: 93.0454, loss: 4.5635
	step 2059:lm_loss: 4.5331, ppl: 93.0507, loss: 4.5635
	step 2060:lm_loss: 4.5327, ppl: 93.0084, loss: 4.5630
	step 2061:lm_loss: 4.5331, ppl: 93.0426, loss: 4.5633
	step 2062:lm_loss: 4.5328, ppl: 93.0223, loss: 4.5630
	step 2063:lm_loss: 4.5326, ppl: 92.9966, loss: 4.5626
	step 2064:lm_loss: 4.5328, ppl: 93.0182, loss: 4.5629
	step 2065:lm_loss: 4.5327, ppl: 93.0111, loss: 4.5628
	step 2066:lm_loss: 4.5327, ppl: 93.0105, loss: 4.5628
	step 2067:lm_loss: 4.5322, ppl: 92.9651, loss: 4.5622
	step 2068:lm_loss: 4.5316, ppl: 92.9074, loss: 4.5614
	step 2069:lm_loss: 4.5316, ppl: 92.9039, loss: 4.5614
	step 2070:lm_loss: 4.5315, ppl: 92.8974, loss: 4.5613
	step 2071:lm_loss: 4.5307, ppl: 92.8193, loss: 4.5606
	step 2072:lm_loss: 4.5305, ppl: 92.8011, loss: 4.5603
	step 2073:lm_loss: 4.5308, ppl: 92.8288, loss: 4.5606
	step 2074:lm_loss: 4.5308, ppl: 92.8307, loss: 4.5607
	step 2075:lm_loss: 4.5308, ppl: 92.8305, loss: 4.5606
	step 2076:lm_loss: 4.5309, ppl: 92.8408, loss: 4.5608
	step 2077:lm_loss: 4.5311, ppl: 92.8565, loss: 4.5609
	step 2078:lm_loss: 4.5312, ppl: 92.8655, loss: 4.5610
	step 2079:lm_loss: 4.5310, ppl: 92.8476, loss: 4.5609
	step 2080:lm_loss: 4.5309, ppl: 92.8425, loss: 4.5608
	step 2081:lm_loss: 4.5309, ppl: 92.8421, loss: 4.5608
	step 2082:lm_loss: 4.5307, ppl: 92.8202, loss: 4.5605
	step 2083:lm_loss: 4.5304, ppl: 92.7975, loss: 4.5602
	step 2084:lm_loss: 4.5304, ppl: 92.7973, loss: 4.5602
	step 2085:lm_loss: 4.5307, ppl: 92.8204, loss: 4.5605
	step 2086:lm_loss: 4.5310, ppl: 92.8544, loss: 4.5608
	step 2087:lm_loss: 4.5310, ppl: 92.8543, loss: 4.5607
	step 2088:lm_loss: 4.5311, ppl: 92.8625, loss: 4.5609
	step 2089:lm_loss: 4.5312, ppl: 92.8676, loss: 4.5609
	step 2090:lm_loss: 4.5308, ppl: 92.8353, loss: 4.5607
	step 2091:lm_loss: 4.5312, ppl: 92.8737, loss: 4.5610
	step 2092:lm_loss: 4.5309, ppl: 92.8449, loss: 4.5607
	step 2093:lm_loss: 4.5308, ppl: 92.8292, loss: 4.5605
	step 2094:lm_loss: 4.5313, ppl: 92.8785, loss: 4.5610
	step 2095:lm_loss: 4.5307, ppl: 92.8255, loss: 4.5603
	step 2096:lm_loss: 4.5309, ppl: 92.8396, loss: 4.5605
	step 2097:lm_loss: 4.5312, ppl: 92.8677, loss: 4.5608
	step 2098:lm_loss: 4.5314, ppl: 92.8895, loss: 4.5609
	step 2099:lm_loss: 4.5316, ppl: 92.9035, loss: 4.5611
	step 2100:lm_loss: 4.5306, ppl: 92.8136, loss: 4.5602
	step 2101:lm_loss: 4.5305, ppl: 92.8052, loss: 4.5601
	step 2102:lm_loss: 4.5303, ppl: 92.7825, loss: 4.5598
	step 2103:lm_loss: 4.5304, ppl: 92.7984, loss: 4.5600
	step 2104:lm_loss: 4.5305, ppl: 92.8032, loss: 4.5600
	step 2105:lm_loss: 4.5307, ppl: 92.8242, loss: 4.5602
	step 2106:lm_loss: 4.5303, ppl: 92.7848, loss: 4.5598
	step 2107:lm_loss: 4.5295, ppl: 92.7078, loss: 4.5589
	step 2108:lm_loss: 4.5294, ppl: 92.7066, loss: 4.5589
	step 2109:lm_loss: 4.5290, ppl: 92.6688, loss: 4.5585
	step 2110:lm_loss: 4.5292, ppl: 92.6853, loss: 4.5587
	step 2111:lm_loss: 4.5293, ppl: 92.6971, loss: 4.5589
	step 2112:lm_loss: 4.5293, ppl: 92.6902, loss: 4.5588
	step 2113:lm_loss: 4.5286, ppl: 92.6283, loss: 4.5581
	step 2114:lm_loss: 4.5290, ppl: 92.6662, loss: 4.5583
	step 2115:lm_loss: 4.5288, ppl: 92.6516, loss: 4.5581
	step 2116:lm_loss: 4.5291, ppl: 92.6710, loss: 4.5583
	step 2117:lm_loss: 4.5290, ppl: 92.6621, loss: 4.5582
	step 2118:lm_loss: 4.5292, ppl: 92.6876, loss: 4.5585
	step 2119:lm_loss: 4.5293, ppl: 92.6958, loss: 4.5586
	step 2120:lm_loss: 4.5291, ppl: 92.6745, loss: 4.5584
	step 2121:lm_loss: 4.5287, ppl: 92.6367, loss: 4.5582
	step 2122:lm_loss: 4.5287, ppl: 92.6341, loss: 4.5581
	step 2123:lm_loss: 4.5280, ppl: 92.5695, loss: 4.5575
	step 2124:lm_loss: 4.5277, ppl: 92.5458, loss: 4.5572
	step 2125:lm_loss: 4.5279, ppl: 92.5652, loss: 4.5574
	step 2126:lm_loss: 4.5274, ppl: 92.5178, loss: 4.5568
	step 2127:lm_loss: 4.5268, ppl: 92.4626, loss: 4.5565
	step 2128:lm_loss: 4.5266, ppl: 92.4434, loss: 4.5563
	step 2129:lm_loss: 4.5263, ppl: 92.4197, loss: 4.5560
	step 2130:lm_loss: 4.5262, ppl: 92.4043, loss: 4.5558
	step 2131:lm_loss: 4.5260, ppl: 92.3860, loss: 4.5556
	step 2132:lm_loss: 4.5260, ppl: 92.3928, loss: 4.5557
	step 2133:lm_loss: 4.5262, ppl: 92.4088, loss: 4.5559
	step 2134:lm_loss: 4.5262, ppl: 92.4056, loss: 4.5559
	step 2135:lm_loss: 4.5262, ppl: 92.4036, loss: 4.5558
	step 2136:lm_loss: 4.5263, ppl: 92.4136, loss: 4.5559
	step 2137:lm_loss: 4.5260, ppl: 92.3879, loss: 4.5556
	step 2138:lm_loss: 4.5260, ppl: 92.3865, loss: 4.5555
	step 2139:lm_loss: 4.5261, ppl: 92.3979, loss: 4.5556
	step 2140:lm_loss: 4.5259, ppl: 92.3824, loss: 4.5555
	step 2141:lm_loss: 4.5261, ppl: 92.3996, loss: 4.5556
	step 2142:lm_loss: 4.5259, ppl: 92.3771, loss: 4.5552
	step 2143:lm_loss: 4.5261, ppl: 92.3997, loss: 4.5555
	step 2144:lm_loss: 4.5260, ppl: 92.3896, loss: 4.5554
	step 2145:lm_loss: 4.5259, ppl: 92.3827, loss: 4.5554
	step 2146:lm_loss: 4.5251, ppl: 92.3048, loss: 4.5545
	step 2147:lm_loss: 4.5258, ppl: 92.3694, loss: 4.5551
	step 2148:lm_loss: 4.5259, ppl: 92.3785, loss: 4.5552
	step 2149:lm_loss: 4.5262, ppl: 92.4107, loss: 4.5554
	step 2150:lm_loss: 4.5259, ppl: 92.3835, loss: 4.5551
	step 2151:lm_loss: 4.5261, ppl: 92.3986, loss: 4.5552
	step 2152:lm_loss: 4.5256, ppl: 92.3527, loss: 4.5548
	step 2153:lm_loss: 4.5259, ppl: 92.3793, loss: 4.5551
	step 2154:lm_loss: 4.5256, ppl: 92.3478, loss: 4.5547
	step 2155:lm_loss: 4.5253, ppl: 92.3253, loss: 4.5544
	step 2156:lm_loss: 4.5254, ppl: 92.3300, loss: 4.5545
	step 2157:lm_loss: 4.5248, ppl: 92.2797, loss: 4.5540
	step 2158:lm_loss: 4.5249, ppl: 92.2849, loss: 4.5541
	step 2159:lm_loss: 4.5247, ppl: 92.2700, loss: 4.5539
	step 2160:lm_loss: 4.5255, ppl: 92.3383, loss: 4.5546
	step 2161:lm_loss: 4.5254, ppl: 92.3339, loss: 4.5546
	step 2162:lm_loss: 4.5250, ppl: 92.2955, loss: 4.5542
	step 2163:lm_loss: 4.5254, ppl: 92.3296, loss: 4.5545
	step 2164:lm_loss: 4.5256, ppl: 92.3475, loss: 4.5548
	step 2165:lm_loss: 4.5255, ppl: 92.3435, loss: 4.5547
	step 2166:lm_loss: 4.5250, ppl: 92.2916, loss: 4.5543
	step 2167:lm_loss: 4.5245, ppl: 92.2458, loss: 4.5537
	step 2168:lm_loss: 4.5244, ppl: 92.2374, loss: 4.5536
	step 2169:lm_loss: 4.5247, ppl: 92.2658, loss: 4.5540
	step 2170:lm_loss: 4.5251, ppl: 92.3078, loss: 4.5544
	step 2171:lm_loss: 4.5258, ppl: 92.3696, loss: 4.5549
	step 2172:lm_loss: 4.5260, ppl: 92.3864, loss: 4.5551
	step 2173:lm_loss: 4.5260, ppl: 92.3900, loss: 4.5551
	step 2174:lm_loss: 4.5261, ppl: 92.4002, loss: 4.5552
	step 2175:lm_loss: 4.5259, ppl: 92.3753, loss: 4.5549
	step 2176:lm_loss: 4.5256, ppl: 92.3514, loss: 4.5546
	step 2177:lm_loss: 4.5256, ppl: 92.3483, loss: 4.5546
	step 2178:lm_loss: 4.5256, ppl: 92.3496, loss: 4.5546
	step 2179:lm_loss: 4.5255, ppl: 92.3412, loss: 4.5545
	step 2180:lm_loss: 4.5257, ppl: 92.3570, loss: 4.5546
	step 2181:lm_loss: 4.5258, ppl: 92.3672, loss: 4.5547
	step 2182:lm_loss: 4.5263, ppl: 92.4199, loss: 4.5551
	step 2183:lm_loss: 4.5265, ppl: 92.4311, loss: 4.5553
	step 2184:lm_loss: 4.5264, ppl: 92.4298, loss: 4.5552
	step 2185:lm_loss: 4.5265, ppl: 92.4310, loss: 4.5552
	step 2186:lm_loss: 4.5253, ppl: 92.3237, loss: 4.5545
	step 2187:lm_loss: 4.5259, ppl: 92.3822, loss: 4.5550
	step 2188:lm_loss: 4.5253, ppl: 92.3248, loss: 4.5545
	step 2189:lm_loss: 4.5255, ppl: 92.3400, loss: 4.5547
	step 2190:lm_loss: 4.5258, ppl: 92.3676, loss: 4.5551
	step 2191:lm_loss: 4.5262, ppl: 92.4031, loss: 4.5557
	step 2192:lm_loss: 4.5259, ppl: 92.3764, loss: 4.5554
	step 2193:lm_loss: 4.5264, ppl: 92.4211, loss: 4.5557
	step 2194:lm_loss: 4.5259, ppl: 92.3828, loss: 4.5552
	step 2195:lm_loss: 4.5258, ppl: 92.3662, loss: 4.5550
	step 2196:lm_loss: 4.5260, ppl: 92.3894, loss: 4.5553
	step 2197:lm_loss: 4.5261, ppl: 92.3933, loss: 4.5553
	step 2198:lm_loss: 4.5256, ppl: 92.3546, loss: 4.5549
	step 2199:lm_loss: 4.5259, ppl: 92.3804, loss: 4.5552
	step 2200:lm_loss: 4.5260, ppl: 92.3924, loss: 4.5554
	step 2201:lm_loss: 4.5258, ppl: 92.3691, loss: 4.5551
	step 2202:lm_loss: 4.5260, ppl: 92.3851, loss: 4.5552
	step 2203:lm_loss: 4.5262, ppl: 92.4038, loss: 4.5554
	step 2204:lm_loss: 4.5262, ppl: 92.4082, loss: 4.5555
	step 2205:lm_loss: 4.5269, ppl: 92.4713, loss: 4.5560
	step 2206:lm_loss: 4.5269, ppl: 92.4714, loss: 4.5560
	step 2207:lm_loss: 4.5267, ppl: 92.4561, loss: 4.5558
	step 2208:lm_loss: 4.5267, ppl: 92.4488, loss: 4.5557
	step 2209:lm_loss: 4.5266, ppl: 92.4479, loss: 4.5556
	step 2210:lm_loss: 4.5268, ppl: 92.4590, loss: 4.5558
	step 2211:lm_loss: 4.5271, ppl: 92.4856, loss: 4.5561
	step 2212:lm_loss: 4.5273, ppl: 92.5039, loss: 4.5563
	step 2213:lm_loss: 4.5271, ppl: 92.4867, loss: 4.5560
	step 2214:lm_loss: 4.5276, ppl: 92.5398, loss: 4.5565
	step 2215:lm_loss: 4.5270, ppl: 92.4822, loss: 4.5558
	step 2216:lm_loss: 4.5275, ppl: 92.5298, loss: 4.5561
	step 2217:lm_loss: 4.5277, ppl: 92.5451, loss: 4.5562
	step 2218:lm_loss: 4.5277, ppl: 92.5468, loss: 4.5562
	step 2219:lm_loss: 4.5279, ppl: 92.5596, loss: 4.5564
	step 2220:lm_loss: 4.5277, ppl: 92.5434, loss: 4.5562
	step 2221:lm_loss: 4.5275, ppl: 92.5307, loss: 4.5560
	step 2222:lm_loss: 4.5275, ppl: 92.5245, loss: 4.5559
	step 2223:lm_loss: 4.5271, ppl: 92.4929, loss: 4.5557
	step 2224:lm_loss: 4.5269, ppl: 92.4670, loss: 4.5554
	step 2225:lm_loss: 4.5272, ppl: 92.5016, loss: 4.5557
	step 2226:lm_loss: 4.5274, ppl: 92.5143, loss: 4.5558
	step 2227:lm_loss: 4.5278, ppl: 92.5564, loss: 4.5564
	step 2228:lm_loss: 4.5281, ppl: 92.5816, loss: 4.5566
	step 2229:lm_loss: 4.5281, ppl: 92.5790, loss: 4.5566
	step 2230:lm_loss: 4.5282, ppl: 92.5952, loss: 4.5567
	step 2231:lm_loss: 4.5281, ppl: 92.5781, loss: 4.5565
	step 2232:lm_loss: 4.5278, ppl: 92.5557, loss: 4.5562
	step 2233:lm_loss: 4.5267, ppl: 92.4493, loss: 4.5556
	step 2234:lm_loss: 4.5265, ppl: 92.4354, loss: 4.5554
	step 2235:lm_loss: 4.5264, ppl: 92.4212, loss: 4.5552
	step 2236:lm_loss: 4.5268, ppl: 92.4652, loss: 4.5556
	step 2237:lm_loss: 4.5270, ppl: 92.4795, loss: 4.5558
	step 2238:lm_loss: 4.5266, ppl: 92.4458, loss: 4.5554
	step 2239:lm_loss: 4.5270, ppl: 92.4781, loss: 4.5557
	step 2240:lm_loss: 4.5269, ppl: 92.4707, loss: 4.5556
	step 2241:lm_loss: 4.5272, ppl: 92.4961, loss: 4.5560
	step 2242:lm_loss: 4.5278, ppl: 92.5564, loss: 4.5569
	step 2243:lm_loss: 4.5280, ppl: 92.5769, loss: 4.5570
	step 2244:lm_loss: 4.5282, ppl: 92.5872, loss: 4.5571
	step 2245:lm_loss: 4.5278, ppl: 92.5580, loss: 4.5566
	step 2246:lm_loss: 4.5274, ppl: 92.5151, loss: 4.5565
	step 2247:lm_loss: 4.5271, ppl: 92.4890, loss: 4.5562
	step 2248:lm_loss: 4.5274, ppl: 92.5205, loss: 4.5564
	step 2249:lm_loss: 4.5274, ppl: 92.5213, loss: 4.5564
	step 2250:lm_loss: 4.5277, ppl: 92.5453, loss: 4.5567
	step 2251:lm_loss: 4.5274, ppl: 92.5182, loss: 4.5565
	step 2252:lm_loss: 4.5266, ppl: 92.4426, loss: 4.5561
	step 2253:lm_loss: 4.5265, ppl: 92.4363, loss: 4.5560
	step 2254:lm_loss: 4.5264, ppl: 92.4256, loss: 4.5559
	step 2255:lm_loss: 4.5263, ppl: 92.4171, loss: 4.5557
	step 2256:lm_loss: 4.5261, ppl: 92.4000, loss: 4.5555
	step 2257:lm_loss: 4.5261, ppl: 92.3963, loss: 4.5554
	step 2258:lm_loss: 4.5260, ppl: 92.3881, loss: 4.5553
	step 2259:lm_loss: 4.5264, ppl: 92.4266, loss: 4.5557
	step 2260:lm_loss: 4.5264, ppl: 92.4294, loss: 4.5557
	step 2261:lm_loss: 4.5274, ppl: 92.5142, loss: 4.5564
	step 2262:lm_loss: 4.5273, ppl: 92.5094, loss: 4.5563
	step 2263:lm_loss: 4.5276, ppl: 92.5367, loss: 4.5567
	step 2264:lm_loss: 4.5283, ppl: 92.5996, loss: 4.5572
	step 2265:lm_loss: 4.5287, ppl: 92.6343, loss: 4.5577
	step 2266:lm_loss: 4.5288, ppl: 92.6495, loss: 4.5578
	step 2267:lm_loss: 4.5290, ppl: 92.6682, loss: 4.5580
	step 2268:lm_loss: 4.5288, ppl: 92.6490, loss: 4.5577
	step 2269:lm_loss: 4.5286, ppl: 92.6324, loss: 4.5575
	step 2270:lm_loss: 4.5290, ppl: 92.6681, loss: 4.5578
	step 2271:lm_loss: 4.5295, ppl: 92.7106, loss: 4.5584
	step 2272:lm_loss: 4.5292, ppl: 92.6876, loss: 4.5583
	step 2273:lm_loss: 4.5293, ppl: 92.6908, loss: 4.5583
	step 2274:lm_loss: 4.5289, ppl: 92.6568, loss: 4.5580
	step 2275:lm_loss: 4.5292, ppl: 92.6833, loss: 4.5583
	step 2276:lm_loss: 4.5290, ppl: 92.6630, loss: 4.5581
	step 2277:lm_loss: 4.5293, ppl: 92.6923, loss: 4.5583
	step 2278:lm_loss: 4.5295, ppl: 92.7153, loss: 4.5585
	step 2279:lm_loss: 4.5297, ppl: 92.7278, loss: 4.5586
	step 2280:lm_loss: 4.5293, ppl: 92.6955, loss: 4.5581
	step 2281:lm_loss: 4.5292, ppl: 92.6837, loss: 4.5579
	step 2282:lm_loss: 4.5290, ppl: 92.6695, loss: 4.5578
	step 2283:lm_loss: 4.5290, ppl: 92.6619, loss: 4.5578
	step 2284:lm_loss: 4.5290, ppl: 92.6661, loss: 4.5578
	step 2285:lm_loss: 4.5291, ppl: 92.6789, loss: 4.5579
	step 2286:lm_loss: 4.5294, ppl: 92.7073, loss: 4.5581
	step 2287:lm_loss: 4.5297, ppl: 92.7267, loss: 4.5583
	step 2288:lm_loss: 4.5294, ppl: 92.7037, loss: 4.5580
	step 2289:lm_loss: 4.5290, ppl: 92.6624, loss: 4.5576
	step 2290:lm_loss: 4.5291, ppl: 92.6709, loss: 4.5576
	step 2291:lm_loss: 4.5296, ppl: 92.7211, loss: 4.5581
	step 2292:lm_loss: 4.5297, ppl: 92.7273, loss: 4.5581
	step 2293:lm_loss: 4.5297, ppl: 92.7339, loss: 4.5582
	step 2294:lm_loss: 4.5295, ppl: 92.7112, loss: 4.5579
	step 2295:lm_loss: 4.5300, ppl: 92.7558, loss: 4.5582
	step 2296:lm_loss: 4.5300, ppl: 92.7570, loss: 4.5582
	step 2297:lm_loss: 4.5305, ppl: 92.8080, loss: 4.5589
	step 2298:lm_loss: 4.5308, ppl: 92.8373, loss: 4.5593
	step 2299:lm_loss: 4.5309, ppl: 92.8447, loss: 4.5594
	step 2300:lm_loss: 4.5311, ppl: 92.8579, loss: 4.5595
	step 2301:lm_loss: 4.5311, ppl: 92.8575, loss: 4.5595
	step 2302:lm_loss: 4.5309, ppl: 92.8440, loss: 4.5594
	step 2303:lm_loss: 4.5306, ppl: 92.8162, loss: 4.5591
	step 2304:lm_loss: 4.5312, ppl: 92.8709, loss: 4.5596
	step 2305:lm_loss: 4.5312, ppl: 92.8685, loss: 4.5595
	step 2306:lm_loss: 4.5318, ppl: 92.9226, loss: 4.5600
	step 2307:lm_loss: 4.5323, ppl: 92.9750, loss: 4.5605
	step 2308:lm_loss: 4.5310, ppl: 92.8468, loss: 4.5599
	step 2309:lm_loss: 4.5299, ppl: 92.7469, loss: 4.5592
	step 2310:lm_loss: 4.5295, ppl: 92.7129, loss: 4.5588
	step 2311:lm_loss: 4.5291, ppl: 92.6782, loss: 4.5584
	step 2312:lm_loss: 4.5290, ppl: 92.6697, loss: 4.5583
	step 2313:lm_loss: 4.5290, ppl: 92.6616, loss: 4.5581
	step 2314:lm_loss: 4.5288, ppl: 92.6469, loss: 4.5580
	step 2315:lm_loss: 4.5286, ppl: 92.6300, loss: 4.5578
	step 2316:lm_loss: 4.5289, ppl: 92.6552, loss: 4.5582
	step 2317:lm_loss: 4.5284, ppl: 92.6058, loss: 4.5576
	step 2318:lm_loss: 4.5285, ppl: 92.6198, loss: 4.5578
	step 2319:lm_loss: 4.5278, ppl: 92.5528, loss: 4.5572
	step 2320:lm_loss: 4.5279, ppl: 92.5645, loss: 4.5573
	step 2321:lm_loss: 4.5281, ppl: 92.5816, loss: 4.5575
	step 2322:lm_loss: 4.5281, ppl: 92.5852, loss: 4.5575
	step 2323:lm_loss: 4.5281, ppl: 92.5804, loss: 4.5575
	step 2324:lm_loss: 4.5282, ppl: 92.5922, loss: 4.5576
	step 2325:lm_loss: 4.5283, ppl: 92.5969, loss: 4.5576
	step 2326:lm_loss: 4.5281, ppl: 92.5797, loss: 4.5574
	step 2327:lm_loss: 4.5281, ppl: 92.5782, loss: 4.5574
	step 2328:lm_loss: 4.5279, ppl: 92.5633, loss: 4.5572
	step 2329:lm_loss: 4.5279, ppl: 92.5622, loss: 4.5572
	step 2330:lm_loss: 4.5279, ppl: 92.5601, loss: 4.5571
	step 2331:lm_loss: 4.5278, ppl: 92.5553, loss: 4.5571
	step 2332:lm_loss: 4.5277, ppl: 92.5496, loss: 4.5570
	step 2333:lm_loss: 4.5277, ppl: 92.5491, loss: 4.5569
	step 2334:lm_loss: 4.5276, ppl: 92.5361, loss: 4.5568
	step 2335:lm_loss: 4.5277, ppl: 92.5426, loss: 4.5568
	step 2336:lm_loss: 4.5279, ppl: 92.5671, loss: 4.5573
	step 2337:lm_loss: 4.5278, ppl: 92.5567, loss: 4.5571
	step 2338:lm_loss: 4.5276, ppl: 92.5395, loss: 4.5570
	step 2339:lm_loss: 4.5277, ppl: 92.5481, loss: 4.5571
	step 2340:lm_loss: 4.5278, ppl: 92.5576, loss: 4.5571
	step 2341:lm_loss: 4.5283, ppl: 92.6015, loss: 4.5576
	step 2342:lm_loss: 4.5284, ppl: 92.6105, loss: 4.5576
	step 2343:lm_loss: 4.5282, ppl: 92.5880, loss: 4.5573
	step 2344:lm_loss: 4.5281, ppl: 92.5836, loss: 4.5572
	step 2345:lm_loss: 4.5281, ppl: 92.5798, loss: 4.5571
	step 2346:lm_loss: 4.5280, ppl: 92.5691, loss: 4.5570
	step 2347:lm_loss: 4.5278, ppl: 92.5537, loss: 4.5568
	step 2348:lm_loss: 4.5273, ppl: 92.5048, loss: 4.5563
	step 2349:lm_loss: 4.5270, ppl: 92.4768, loss: 4.5561
	step 2350:lm_loss: 4.5268, ppl: 92.4591, loss: 4.5561
	step 2351:lm_loss: 4.5265, ppl: 92.4351, loss: 4.5557
	step 2352:lm_loss: 4.5269, ppl: 92.4706, loss: 4.5563
	step 2353:lm_loss: 4.5270, ppl: 92.4833, loss: 4.5564
	step 2354:lm_loss: 4.5268, ppl: 92.4664, loss: 4.5563
	step 2355:lm_loss: 4.5267, ppl: 92.4533, loss: 4.5561
	step 2356:lm_loss: 4.5269, ppl: 92.4712, loss: 4.5564
	step 2357:lm_loss: 4.5271, ppl: 92.4888, loss: 4.5566
	step 2358:lm_loss: 4.5267, ppl: 92.4514, loss: 4.5559
	step 2359:lm_loss: 4.5268, ppl: 92.4658, loss: 4.5562
	step 2360:lm_loss: 4.5270, ppl: 92.4769, loss: 4.5563
	step 2361:lm_loss: 4.5269, ppl: 92.4739, loss: 4.5562
	step 2362:lm_loss: 4.5272, ppl: 92.4983, loss: 4.5565
	step 2363:lm_loss: 4.5272, ppl: 92.4982, loss: 4.5565
	step 2364:lm_loss: 4.5276, ppl: 92.5345, loss: 4.5568
	step 2365:lm_loss: 4.5273, ppl: 92.5117, loss: 4.5565
	step 2366:lm_loss: 4.5273, ppl: 92.5127, loss: 4.5565
	step 2367:lm_loss: 4.5265, ppl: 92.4385, loss: 4.5560
	step 2368:lm_loss: 4.5264, ppl: 92.4245, loss: 4.5558
	step 2369:lm_loss: 4.5268, ppl: 92.4641, loss: 4.5561
	step 2370:lm_loss: 4.5268, ppl: 92.4633, loss: 4.5561
	step 2371:lm_loss: 4.5269, ppl: 92.4686, loss: 4.5562
	step 2372:lm_loss: 4.5268, ppl: 92.4590, loss: 4.5561
	step 2373:lm_loss: 4.5265, ppl: 92.4325, loss: 4.5558
	step 2374:lm_loss: 4.5263, ppl: 92.4191, loss: 4.5557
	step 2375:lm_loss: 4.5260, ppl: 92.3838, loss: 4.5550
	step 2376:lm_loss: 4.5257, ppl: 92.3566, loss: 4.5545
	step 2377:lm_loss: 4.5255, ppl: 92.3393, loss: 4.5544
	step 2378:lm_loss: 4.5256, ppl: 92.3480, loss: 4.5545
	step 2379:lm_loss: 4.5256, ppl: 92.3519, loss: 4.5545
	step 2380:lm_loss: 4.5260, ppl: 92.3852, loss: 4.5548
	step 2381:lm_loss: 4.5260, ppl: 92.3869, loss: 4.5548
	step 2382:lm_loss: 4.5263, ppl: 92.4177, loss: 4.5551
	step 2383:lm_loss: 4.5259, ppl: 92.3765, loss: 4.5548
	step 2384:lm_loss: 4.5259, ppl: 92.3762, loss: 4.5548
	step 2385:lm_loss: 4.5260, ppl: 92.3912, loss: 4.5550
	step 2386:lm_loss: 4.5262, ppl: 92.4076, loss: 4.5552
	step 2387:lm_loss: 4.5263, ppl: 92.4135, loss: 4.5552
	step 2388:lm_loss: 4.5268, ppl: 92.4598, loss: 4.5556
	step 2389:lm_loss: 4.5265, ppl: 92.4369, loss: 4.5553
	step 2390:lm_loss: 4.5260, ppl: 92.3843, loss: 4.5548
	step 2391:lm_loss: 4.5261, ppl: 92.3967, loss: 4.5549
	step 2392:lm_loss: 4.5257, ppl: 92.3607, loss: 4.5545
	step 2393:lm_loss: 4.5255, ppl: 92.3466, loss: 4.5542
	step 2394:lm_loss: 4.5248, ppl: 92.2749, loss: 4.5538
	step 2395:lm_loss: 4.5237, ppl: 92.1735, loss: 4.5531
	step 2396:lm_loss: 4.5240, ppl: 92.2046, loss: 4.5535
	step 2397:lm_loss: 4.5236, ppl: 92.1691, loss: 4.5530
	step 2398:lm_loss: 4.5234, ppl: 92.1523, loss: 4.5528
	step 2399:lm_loss: 4.5233, ppl: 92.1368, loss: 4.5526
	step 2400:lm_loss: 4.5232, ppl: 92.1316, loss: 4.5526
	step 2401:lm_loss: 4.5233, ppl: 92.1387, loss: 4.5526
	step 2402:lm_loss: 4.5231, ppl: 92.1204, loss: 4.5525
	step 2403:lm_loss: 4.5234, ppl: 92.1439, loss: 4.5527
	step 2404:lm_loss: 4.5232, ppl: 92.1323, loss: 4.5526
	step 2405:lm_loss: 4.5229, ppl: 92.1020, loss: 4.5522
	step 2406:lm_loss: 4.5229, ppl: 92.1035, loss: 4.5523
	step 2407:lm_loss: 4.5228, ppl: 92.0939, loss: 4.5521
	step 2408:lm_loss: 4.5233, ppl: 92.1394, loss: 4.5528
	step 2409:lm_loss: 4.5226, ppl: 92.0739, loss: 4.5522
	step 2410:lm_loss: 4.5230, ppl: 92.1127, loss: 4.5528
	step 2411:lm_loss: 4.5232, ppl: 92.1278, loss: 4.5529
	step 2412:lm_loss: 4.5233, ppl: 92.1382, loss: 4.5530
	step 2413:lm_loss: 4.5233, ppl: 92.1425, loss: 4.5530
	step 2414:lm_loss: 4.5230, ppl: 92.1114, loss: 4.5528
	step 2415:lm_loss: 4.5231, ppl: 92.1221, loss: 4.5530
	step 2416:lm_loss: 4.5235, ppl: 92.1574, loss: 4.5533
	step 2417:lm_loss: 4.5236, ppl: 92.1654, loss: 4.5533
	step 2418:lm_loss: 4.5240, ppl: 92.1997, loss: 4.5537
	step 2419:lm_loss: 4.5240, ppl: 92.2047, loss: 4.5538
	step 2420:lm_loss: 4.5238, ppl: 92.1896, loss: 4.5536
	step 2421:lm_loss: 4.5232, ppl: 92.1323, loss: 4.5531
	step 2422:lm_loss: 4.5232, ppl: 92.1334, loss: 4.5531
	step 2423:lm_loss: 4.5232, ppl: 92.1262, loss: 4.5530
	step 2424:lm_loss: 4.5228, ppl: 92.0907, loss: 4.5526
	step 2425:lm_loss: 4.5226, ppl: 92.0755, loss: 4.5524
	step 2426:lm_loss: 4.5231, ppl: 92.1246, loss: 4.5529
	step 2427:lm_loss: 4.5231, ppl: 92.1219, loss: 4.5529
	step 2428:lm_loss: 4.5229, ppl: 92.1009, loss: 4.5525
	step 2429:lm_loss: 4.5230, ppl: 92.1112, loss: 4.5527
	step 2430:lm_loss: 4.5222, ppl: 92.0374, loss: 4.5523
	step 2431:lm_loss: 4.5222, ppl: 92.0353, loss: 4.5522
	step 2432:lm_loss: 4.5220, ppl: 92.0173, loss: 4.5520
	step 2433:lm_loss: 4.5225, ppl: 92.0629, loss: 4.5523
	step 2434:lm_loss: 4.5223, ppl: 92.0472, loss: 4.5521
	step 2435:lm_loss: 4.5221, ppl: 92.0325, loss: 4.5520
	step 2436:lm_loss: 4.5224, ppl: 92.0574, loss: 4.5522
	step 2437:lm_loss: 4.5222, ppl: 92.0383, loss: 4.5520
	step 2438:lm_loss: 4.5220, ppl: 92.0194, loss: 4.5517
	step 2439:lm_loss: 4.5221, ppl: 92.0327, loss: 4.5518
	step 2440:lm_loss: 4.5221, ppl: 92.0297, loss: 4.5518
	step 2441:lm_loss: 4.5222, ppl: 92.0375, loss: 4.5518
	step 2442:lm_loss: 4.5220, ppl: 92.0224, loss: 4.5517
	step 2443:lm_loss: 4.5222, ppl: 92.0389, loss: 4.5519
	step 2444:lm_loss: 4.5220, ppl: 92.0160, loss: 4.5517
	step 2445:lm_loss: 4.5222, ppl: 92.0391, loss: 4.5521
	step 2446:lm_loss: 4.5224, ppl: 92.0570, loss: 4.5523
	step 2447:lm_loss: 4.5225, ppl: 92.0674, loss: 4.5524
	step 2448:lm_loss: 4.5224, ppl: 92.0598, loss: 4.5523
	step 2449:lm_loss: 4.5223, ppl: 92.0517, loss: 4.5522
	step 2450:lm_loss: 4.5225, ppl: 92.0669, loss: 4.5524
	step 2451:lm_loss: 4.5228, ppl: 92.0887, loss: 4.5526
	step 2452:lm_loss: 4.5230, ppl: 92.1113, loss: 4.5529
	step 2453:lm_loss: 4.5231, ppl: 92.1246, loss: 4.5530
	step 2454:lm_loss: 4.5226, ppl: 92.0749, loss: 4.5523
	step 2455:lm_loss: 4.5220, ppl: 92.0229, loss: 4.5519
	step 2456:lm_loss: 4.5223, ppl: 92.0427, loss: 4.5521
	step 2457:lm_loss: 4.5222, ppl: 92.0384, loss: 4.5520
	step 2458:lm_loss: 4.5225, ppl: 92.0639, loss: 4.5523
	step 2459:lm_loss: 4.5231, ppl: 92.1181, loss: 4.5530
	step 2460:lm_loss: 4.5235, ppl: 92.1542, loss: 4.5533
	step 2461:lm_loss: 4.5234, ppl: 92.1515, loss: 4.5533
	step 2462:lm_loss: 4.5237, ppl: 92.1777, loss: 4.5534
	step 2463:lm_loss: 4.5229, ppl: 92.1053, loss: 4.5527
	step 2464:lm_loss: 4.5229, ppl: 92.1031, loss: 4.5527
	step 2465:lm_loss: 4.5235, ppl: 92.1595, loss: 4.5533
	step 2466:lm_loss: 4.5235, ppl: 92.1605, loss: 4.5533
	step 2467:lm_loss: 4.5233, ppl: 92.1359, loss: 4.5531
	step 2468:lm_loss: 4.5231, ppl: 92.1239, loss: 4.5529
	step 2469:lm_loss: 4.5229, ppl: 92.0997, loss: 4.5526
	step 2470:lm_loss: 4.5228, ppl: 92.0905, loss: 4.5525
	step 2471:lm_loss: 4.5224, ppl: 92.0527, loss: 4.5519
	step 2472:lm_loss: 4.5224, ppl: 92.0555, loss: 4.5519
	step 2473:lm_loss: 4.5224, ppl: 92.0577, loss: 4.5519
	step 2474:lm_loss: 4.5225, ppl: 92.0633, loss: 4.5520
	step 2475:lm_loss: 4.5223, ppl: 92.0473, loss: 4.5517
	step 2476:lm_loss: 4.5225, ppl: 92.0642, loss: 4.5519
	step 2477:lm_loss: 4.5224, ppl: 92.0607, loss: 4.5518
	step 2478:lm_loss: 4.5215, ppl: 91.9774, loss: 4.5513
	step 2479:lm_loss: 4.5207, ppl: 91.8998, loss: 4.5505
	step 2480:lm_loss: 4.5207, ppl: 91.8974, loss: 4.5505
	step 2481:lm_loss: 4.5207, ppl: 91.9028, loss: 4.5506
	step 2482:lm_loss: 4.5207, ppl: 91.9005, loss: 4.5505
	step 2483:lm_loss: 4.5210, ppl: 91.9243, loss: 4.5509
	step 2484:lm_loss: 4.5214, ppl: 91.9640, loss: 4.5513
	step 2485:lm_loss: 4.5215, ppl: 91.9697, loss: 4.5514
	step 2486:lm_loss: 4.5216, ppl: 91.9861, loss: 4.5516
	step 2487:lm_loss: 4.5215, ppl: 91.9719, loss: 4.5514
	step 2488:lm_loss: 4.5220, ppl: 92.0149, loss: 4.5518
	step 2489:lm_loss: 4.5222, ppl: 92.0413, loss: 4.5522
	step 2490:lm_loss: 4.5215, ppl: 91.9746, loss: 4.5517
	step 2491:lm_loss: 4.5218, ppl: 92.0015, loss: 4.5518
	step 2492:lm_loss: 4.5219, ppl: 92.0058, loss: 4.5519
	step 2493:lm_loss: 4.5224, ppl: 92.0565, loss: 4.5523
	step 2494:lm_loss: 4.5203, ppl: 91.8610, loss: 4.5518
	step 2495:lm_loss: 4.5202, ppl: 91.8567, loss: 4.5518
	step 2496:lm_loss: 4.5206, ppl: 91.8926, loss: 4.5522
	step 2497:lm_loss: 4.5204, ppl: 91.8759, loss: 4.5519
	step 2498:lm_loss: 4.5206, ppl: 91.8916, loss: 4.5520
	step 2499:lm_loss: 4.5205, ppl: 91.8786, loss: 4.5518
	step 2500:lm_loss: 4.5199, ppl: 91.8296, loss: 4.5513
	step 2501:lm_loss: 4.5199, ppl: 91.8283, loss: 4.5513
	step 2502:lm_loss: 4.5197, ppl: 91.8104, loss: 4.5510
	step 2503:lm_loss: 4.5197, ppl: 91.8042, loss: 4.5510
	step 2504:lm_loss: 4.5198, ppl: 91.8170, loss: 4.5512
	step 2505:lm_loss: 4.5207, ppl: 91.9006, loss: 4.5518
	step 2506:lm_loss: 4.5207, ppl: 91.8997, loss: 4.5518
	step 2507:lm_loss: 4.5205, ppl: 91.8812, loss: 4.5515
	step 2508:lm_loss: 4.5199, ppl: 91.8221, loss: 4.5510
	step 2509:lm_loss: 4.5198, ppl: 91.8149, loss: 4.5509
	step 2510:lm_loss: 4.5198, ppl: 91.8199, loss: 4.5509
	step 2511:lm_loss: 4.5193, ppl: 91.7719, loss: 4.5506
	step 2512:lm_loss: 4.5195, ppl: 91.7879, loss: 4.5507
	step 2513:lm_loss: 4.5195, ppl: 91.7902, loss: 4.5507
	step 2514:lm_loss: 4.5198, ppl: 91.8213, loss: 4.5511
	step 2515:lm_loss: 4.5199, ppl: 91.8231, loss: 4.5512
	step 2516:lm_loss: 4.5190, ppl: 91.7429, loss: 4.5508
	step 2517:lm_loss: 4.5190, ppl: 91.7473, loss: 4.5508
	step 2518:lm_loss: 4.5194, ppl: 91.7810, loss: 4.5512
	step 2519:lm_loss: 4.5195, ppl: 91.7876, loss: 4.5512
	step 2520:lm_loss: 4.5195, ppl: 91.7933, loss: 4.5513
	step 2521:lm_loss: 4.5192, ppl: 91.7629, loss: 4.5508
	step 2522:lm_loss: 4.5195, ppl: 91.7897, loss: 4.5511
	step 2523:lm_loss: 4.5201, ppl: 91.8427, loss: 4.5514
	step 2524:lm_loss: 4.5195, ppl: 91.7900, loss: 4.5510
	step 2525:lm_loss: 4.5195, ppl: 91.7878, loss: 4.5509
	step 2526:lm_loss: 4.5193, ppl: 91.7702, loss: 4.5507
	step 2527:lm_loss: 4.5193, ppl: 91.7753, loss: 4.5508
	step 2528:lm_loss: 4.5197, ppl: 91.8116, loss: 4.5512
	step 2529:lm_loss: 4.5199, ppl: 91.8248, loss: 4.5513
	step 2530:lm_loss: 4.5197, ppl: 91.8126, loss: 4.5512
	step 2531:lm_loss: 4.5199, ppl: 91.8289, loss: 4.5513
	step 2532:lm_loss: 4.5202, ppl: 91.8584, loss: 4.5518
	step 2533:lm_loss: 4.5202, ppl: 91.8554, loss: 4.5518
	step 2534:lm_loss: 4.5200, ppl: 91.8378, loss: 4.5516
	step 2535:lm_loss: 4.5201, ppl: 91.8404, loss: 4.5516
	step 2536:lm_loss: 4.5200, ppl: 91.8344, loss: 4.5515
	step 2537:lm_loss: 4.5204, ppl: 91.8692, loss: 4.5520
	step 2538:lm_loss: 4.5206, ppl: 91.8894, loss: 4.5523
	step 2539:lm_loss: 4.5202, ppl: 91.8530, loss: 4.5518
	step 2540:lm_loss: 4.5198, ppl: 91.8151, loss: 4.5516
	step 2541:lm_loss: 4.5200, ppl: 91.8363, loss: 4.5518
	step 2542:lm_loss: 4.5200, ppl: 91.8393, loss: 4.5518
	step 2543:lm_loss: 4.5196, ppl: 91.7954, loss: 4.5512
	step 2544:lm_loss: 4.5195, ppl: 91.7929, loss: 4.5512
	step 2545:lm_loss: 4.5190, ppl: 91.7479, loss: 4.5506
	step 2546:lm_loss: 4.5191, ppl: 91.7553, loss: 4.5507
	step 2547:lm_loss: 4.5191, ppl: 91.7540, loss: 4.5507
	step 2548:lm_loss: 4.5194, ppl: 91.7841, loss: 4.5508
	step 2549:lm_loss: 4.5196, ppl: 91.7967, loss: 4.5509
	step 2550:lm_loss: 4.5201, ppl: 91.8472, loss: 4.5514
	step 2551:lm_loss: 4.5204, ppl: 91.8725, loss: 4.5516
	step 2552:lm_loss: 4.5202, ppl: 91.8565, loss: 4.5514
	step 2553:lm_loss: 4.5205, ppl: 91.8775, loss: 4.5516
	step 2554:lm_loss: 4.5202, ppl: 91.8542, loss: 4.5513
	step 2555:lm_loss: 4.5199, ppl: 91.8269, loss: 4.5510
	step 2556:lm_loss: 4.5199, ppl: 91.8246, loss: 4.5510
	step 2557:lm_loss: 4.5199, ppl: 91.8260, loss: 4.5510
	step 2558:lm_loss: 4.5198, ppl: 91.8177, loss: 4.5509
	step 2559:lm_loss: 4.5200, ppl: 91.8386, loss: 4.5512
	step 2560:lm_loss: 4.5198, ppl: 91.8203, loss: 4.5511
	step 2561:lm_loss: 4.5195, ppl: 91.7919, loss: 4.5508
	step 2562:lm_loss: 4.5200, ppl: 91.8329, loss: 4.5511
	step 2563:lm_loss: 4.5195, ppl: 91.7920, loss: 4.5509
	step 2564:lm_loss: 4.5198, ppl: 91.8154, loss: 4.5511
	step 2565:lm_loss: 4.5201, ppl: 91.8427, loss: 4.5513
	step 2566:lm_loss: 4.5201, ppl: 91.8490, loss: 4.5514
	step 2567:lm_loss: 4.5204, ppl: 91.8685, loss: 4.5515
	step 2568:lm_loss: 4.5201, ppl: 91.8429, loss: 4.5513
	step 2569:lm_loss: 4.5200, ppl: 91.8394, loss: 4.5512
	step 2570:lm_loss: 4.5200, ppl: 91.8397, loss: 4.5512
	step 2571:lm_loss: 4.5204, ppl: 91.8711, loss: 4.5515
	step 2572:lm_loss: 4.5203, ppl: 91.8633, loss: 4.5514
	step 2573:lm_loss: 4.5200, ppl: 91.8359, loss: 4.5511
	step 2574:lm_loss: 4.5197, ppl: 91.8112, loss: 4.5507
	step 2575:lm_loss: 4.5197, ppl: 91.8041, loss: 4.5506
	step 2576:lm_loss: 4.5197, ppl: 91.8050, loss: 4.5506
	step 2577:lm_loss: 4.5199, ppl: 91.8305, loss: 4.5508
	step 2578:lm_loss: 4.5194, ppl: 91.7827, loss: 4.5502
	step 2579:lm_loss: 4.5193, ppl: 91.7742, loss: 4.5501
	step 2580:lm_loss: 4.5194, ppl: 91.7795, loss: 4.5501
	step 2581:lm_loss: 4.5194, ppl: 91.7768, loss: 4.5501
	step 2582:lm_loss: 4.5187, ppl: 91.7144, loss: 4.5497
	step 2583:lm_loss: 4.5185, ppl: 91.6965, loss: 4.5494
	step 2584:lm_loss: 4.5183, ppl: 91.6816, loss: 4.5492
	step 2585:lm_loss: 4.5186, ppl: 91.7064, loss: 4.5494
	step 2586:lm_loss: 4.5189, ppl: 91.7391, loss: 4.5499
	step 2587:lm_loss: 4.5188, ppl: 91.7257, loss: 4.5497
	step 2588:lm_loss: 4.5183, ppl: 91.6807, loss: 4.5493
	step 2589:lm_loss: 4.5180, ppl: 91.6538, loss: 4.5490
	step 2590:lm_loss: 4.5185, ppl: 91.7011, loss: 4.5494
	step 2591:lm_loss: 4.5189, ppl: 91.7375, loss: 4.5500
	step 2592:lm_loss: 4.5190, ppl: 91.7441, loss: 4.5500
	step 2593:lm_loss: 4.5190, ppl: 91.7426, loss: 4.5500
	step 2594:lm_loss: 4.5187, ppl: 91.7189, loss: 4.5497
	step 2595:lm_loss: 4.5188, ppl: 91.7218, loss: 4.5497
	step 2596:lm_loss: 4.5184, ppl: 91.6886, loss: 4.5493
	step 2597:lm_loss: 4.5185, ppl: 91.7006, loss: 4.5495
	step 2598:lm_loss: 4.5186, ppl: 91.7110, loss: 4.5496
	step 2599:lm_loss: 4.5190, ppl: 91.7477, loss: 4.5500
	step 2600:lm_loss: 4.5194, ppl: 91.7779, loss: 4.5503
	step 2601:lm_loss: 4.5196, ppl: 91.7980, loss: 4.5506
	step 2602:lm_loss: 4.5192, ppl: 91.7665, loss: 4.5501
	step 2603:lm_loss: 4.5197, ppl: 91.8077, loss: 4.5507
	step 2604:lm_loss: 4.5198, ppl: 91.8139, loss: 4.5507
	step 2605:lm_loss: 4.5201, ppl: 91.8487, loss: 4.5510
	step 2606:lm_loss: 4.5206, ppl: 91.8918, loss: 4.5515
	step 2607:lm_loss: 4.5202, ppl: 91.8507, loss: 4.5512
	step 2608:lm_loss: 4.5198, ppl: 91.8165, loss: 4.5508
	step 2609:lm_loss: 4.5198, ppl: 91.8156, loss: 4.5507
	step 2610:lm_loss: 4.5201, ppl: 91.8426, loss: 4.5510
	step 2611:lm_loss: 4.5206, ppl: 91.8901, loss: 4.5517
	step 2612:lm_loss: 4.5201, ppl: 91.8429, loss: 4.5511
	step 2613:lm_loss: 4.5191, ppl: 91.7521, loss: 4.5505
	step 2614:lm_loss: 4.5194, ppl: 91.7787, loss: 4.5508
	step 2615:lm_loss: 4.5195, ppl: 91.7919, loss: 4.5509
	step 2616:lm_loss: 4.5199, ppl: 91.8247, loss: 4.5513
	step 2617:lm_loss: 4.5196, ppl: 91.7970, loss: 4.5510
	step 2618:lm_loss: 4.5196, ppl: 91.8031, loss: 4.5511
	step 2619:lm_loss: 4.5197, ppl: 91.8046, loss: 4.5511
	step 2620:lm_loss: 4.5197, ppl: 91.8092, loss: 4.5511
	step 2621:lm_loss: 4.5197, ppl: 91.8081, loss: 4.5511
	step 2622:lm_loss: 4.5194, ppl: 91.7851, loss: 4.5509
	step 2623:lm_loss: 4.5194, ppl: 91.7777, loss: 4.5507
	step 2624:lm_loss: 4.5192, ppl: 91.7647, loss: 4.5506
	step 2625:lm_loss: 4.5195, ppl: 91.7919, loss: 4.5509
	step 2626:lm_loss: 4.5193, ppl: 91.7721, loss: 4.5508
	step 2627:lm_loss: 4.5193, ppl: 91.7750, loss: 4.5508
	step 2628:lm_loss: 4.5193, ppl: 91.7684, loss: 4.5507
	step 2629:lm_loss: 4.5190, ppl: 91.7468, loss: 4.5505
	step 2630:lm_loss: 4.5193, ppl: 91.7712, loss: 4.5507
	step 2631:lm_loss: 4.5191, ppl: 91.7553, loss: 4.5505
	step 2632:lm_loss: 4.5190, ppl: 91.7449, loss: 4.5504
	step 2633:lm_loss: 4.5193, ppl: 91.7729, loss: 4.5508
	step 2634:lm_loss: 4.5188, ppl: 91.7266, loss: 4.5505
	step 2635:lm_loss: 4.5188, ppl: 91.7289, loss: 4.5505
	step 2636:lm_loss: 4.5185, ppl: 91.6956, loss: 4.5503
	step 2637:lm_loss: 4.5183, ppl: 91.6835, loss: 4.5501
	step 2638:lm_loss: 4.5183, ppl: 91.6791, loss: 4.5500
	step 2639:lm_loss: 4.5183, ppl: 91.6784, loss: 4.5500
	step 2640:lm_loss: 4.5179, ppl: 91.6460, loss: 4.5496
	step 2641:lm_loss: 4.5184, ppl: 91.6892, loss: 4.5500
	step 2642:lm_loss: 4.5185, ppl: 91.6959, loss: 4.5501
	step 2643:lm_loss: 4.5186, ppl: 91.7080, loss: 4.5503
	step 2644:lm_loss: 4.5187, ppl: 91.7198, loss: 4.5503
	step 2645:lm_loss: 4.5187, ppl: 91.7127, loss: 4.5503
	step 2646:lm_loss: 4.5186, ppl: 91.7099, loss: 4.5502
	step 2647:lm_loss: 4.5189, ppl: 91.7336, loss: 4.5505
	step 2648:lm_loss: 4.5191, ppl: 91.7491, loss: 4.5507
	step 2649:lm_loss: 4.5192, ppl: 91.7609, loss: 4.5508
	step 2650:lm_loss: 4.5190, ppl: 91.7448, loss: 4.5506
	step 2651:lm_loss: 4.5192, ppl: 91.7644, loss: 4.5509
	step 2652:lm_loss: 4.5194, ppl: 91.7847, loss: 4.5512
	step 2653:lm_loss: 4.5198, ppl: 91.8137, loss: 4.5516
	step 2654:lm_loss: 4.5197, ppl: 91.8119, loss: 4.5515
	step 2655:lm_loss: 4.5197, ppl: 91.8061, loss: 4.5514
	step 2656:lm_loss: 4.5196, ppl: 91.8019, loss: 4.5514
	step 2657:lm_loss: 4.5198, ppl: 91.8162, loss: 4.5516
	step 2658:lm_loss: 4.5197, ppl: 91.8101, loss: 4.5514
	step 2659:lm_loss: 4.5198, ppl: 91.8136, loss: 4.5515
	step 2660:lm_loss: 4.5199, ppl: 91.8276, loss: 4.5516
	step 2661:lm_loss: 4.5198, ppl: 91.8134, loss: 4.5515
	step 2662:lm_loss: 4.5200, ppl: 91.8402, loss: 4.5517
	step 2663:lm_loss: 4.5203, ppl: 91.8666, loss: 4.5521
	step 2664:lm_loss: 4.5198, ppl: 91.8129, loss: 4.5515
	step 2665:lm_loss: 4.5199, ppl: 91.8221, loss: 4.5516
	step 2666:lm_loss: 4.5194, ppl: 91.7850, loss: 4.5511
	step 2667:lm_loss: 4.5196, ppl: 91.7978, loss: 4.5511
	step 2668:lm_loss: 4.5197, ppl: 91.8093, loss: 4.5512
	step 2669:lm_loss: 4.5196, ppl: 91.7950, loss: 4.5510
	step 2670:lm_loss: 4.5195, ppl: 91.7918, loss: 4.5509
	step 2671:lm_loss: 4.5198, ppl: 91.8161, loss: 4.5511
	step 2672:lm_loss: 4.5197, ppl: 91.8060, loss: 4.5510
	step 2673:lm_loss: 4.5201, ppl: 91.8415, loss: 4.5514
	step 2674:lm_loss: 4.5199, ppl: 91.8274, loss: 4.5513
	step 2675:lm_loss: 4.5201, ppl: 91.8464, loss: 4.5516
	step 2676:lm_loss: 4.5197, ppl: 91.8096, loss: 4.5513
	step 2677:lm_loss: 4.5200, ppl: 91.8348, loss: 4.5515
	step 2678:lm_loss: 4.5202, ppl: 91.8522, loss: 4.5517
	step 2679:lm_loss: 4.5202, ppl: 91.8568, loss: 4.5518
	step 2680:lm_loss: 4.5202, ppl: 91.8556, loss: 4.5517
	step 2681:lm_loss: 4.5202, ppl: 91.8549, loss: 4.5517
	step 2682:lm_loss: 4.5204, ppl: 91.8699, loss: 4.5518
	step 2683:lm_loss: 4.5205, ppl: 91.8856, loss: 4.5519
	step 2684:lm_loss: 4.5205, ppl: 91.8845, loss: 4.5519
	step 2685:lm_loss: 4.5205, ppl: 91.8772, loss: 4.5518
	step 2686:lm_loss: 4.5204, ppl: 91.8737, loss: 4.5518
	step 2687:lm_loss: 4.5204, ppl: 91.8756, loss: 4.5518
	step 2688:lm_loss: 4.5208, ppl: 91.9054, loss: 4.5521
	step 2689:lm_loss: 4.5207, ppl: 91.8992, loss: 4.5520
	step 2690:lm_loss: 4.5208, ppl: 91.9106, loss: 4.5522
	step 2691:lm_loss: 4.5207, ppl: 91.8963, loss: 4.5521
	step 2692:lm_loss: 4.5206, ppl: 91.8948, loss: 4.5521
	step 2693:lm_loss: 4.5210, ppl: 91.9251, loss: 4.5525
	step 2694:lm_loss: 4.5209, ppl: 91.9166, loss: 4.5523
	step 2695:lm_loss: 4.5207, ppl: 91.8975, loss: 4.5522
	step 2696:lm_loss: 4.5212, ppl: 91.9413, loss: 4.5526
	step 2697:lm_loss: 4.5215, ppl: 91.9768, loss: 4.5530
	step 2698:lm_loss: 4.5219, ppl: 92.0100, loss: 4.5532
	step 2699:lm_loss: 4.5217, ppl: 91.9908, loss: 4.5530
	step 2700:lm_loss: 4.5217, ppl: 91.9907, loss: 4.5529
	step 2701:lm_loss: 4.5219, ppl: 92.0111, loss: 4.5531
	step 2702:lm_loss: 4.5226, ppl: 92.0705, loss: 4.5536
	step 2703:lm_loss: 4.5223, ppl: 92.0475, loss: 4.5533
	step 2704:lm_loss: 4.5224, ppl: 92.0569, loss: 4.5534
	step 2705:lm_loss: 4.5223, ppl: 92.0480, loss: 4.5534
	step 2706:lm_loss: 4.5224, ppl: 92.0541, loss: 4.5534
	step 2707:lm_loss: 4.5224, ppl: 92.0521, loss: 4.5534
	step 2708:lm_loss: 4.5223, ppl: 92.0459, loss: 4.5533
	step 2709:lm_loss: 4.5223, ppl: 92.0439, loss: 4.5532
	step 2710:lm_loss: 4.5228, ppl: 92.0906, loss: 4.5536
	step 2711:lm_loss: 4.5227, ppl: 92.0847, loss: 4.5535
	step 2712:lm_loss: 4.5225, ppl: 92.0617, loss: 4.5532
	step 2713:lm_loss: 4.5223, ppl: 92.0464, loss: 4.5530
	step 2714:lm_loss: 4.5219, ppl: 92.0075, loss: 4.5525
	step 2715:lm_loss: 4.5218, ppl: 92.0027, loss: 4.5524
	step 2716:lm_loss: 4.5221, ppl: 92.0310, loss: 4.5526
	step 2717:lm_loss: 4.5219, ppl: 92.0127, loss: 4.5524
	step 2718:lm_loss: 4.5215, ppl: 91.9768, loss: 4.5522
	step 2719:lm_loss: 4.5216, ppl: 91.9793, loss: 4.5522
	step 2720:lm_loss: 4.5218, ppl: 91.9992, loss: 4.5525
	step 2721:lm_loss: 4.5218, ppl: 91.9968, loss: 4.5524
	step 2722:lm_loss: 4.5217, ppl: 91.9942, loss: 4.5524
	step 2723:lm_loss: 4.5216, ppl: 91.9784, loss: 4.5522
	step 2724:lm_loss: 4.5220, ppl: 92.0239, loss: 4.5526
	step 2725:lm_loss: 4.5223, ppl: 92.0478, loss: 4.5529
	step 2726:lm_loss: 4.5223, ppl: 92.0501, loss: 4.5530
	step 2727:lm_loss: 4.5222, ppl: 92.0384, loss: 4.5529
	step 2728:lm_loss: 4.5221, ppl: 92.0314, loss: 4.5528
	step 2729:lm_loss: 4.5222, ppl: 92.0417, loss: 4.5529
	step 2730:lm_loss: 4.5225, ppl: 92.0699, loss: 4.5532
	step 2731:lm_loss: 4.5227, ppl: 92.0878, loss: 4.5535
	step 2732:lm_loss: 4.5227, ppl: 92.0828, loss: 4.5534
	step 2733:lm_loss: 4.5226, ppl: 92.0738, loss: 4.5533
	step 2734:lm_loss: 4.5228, ppl: 92.0910, loss: 4.5534
	step 2735:lm_loss: 4.5229, ppl: 92.0985, loss: 4.5535
	step 2736:lm_loss: 4.5222, ppl: 92.0337, loss: 4.5529
	step 2737:lm_loss: 4.5223, ppl: 92.0428, loss: 4.5529
	step 2738:lm_loss: 4.5224, ppl: 92.0569, loss: 4.5531
	step 2739:lm_loss: 4.5223, ppl: 92.0452, loss: 4.5530
	step 2740:lm_loss: 4.5222, ppl: 92.0387, loss: 4.5530
	step 2741:lm_loss: 4.5224, ppl: 92.0553, loss: 4.5531
	step 2742:lm_loss: 4.5224, ppl: 92.0524, loss: 4.5530
	step 2743:lm_loss: 4.5226, ppl: 92.0775, loss: 4.5532
	step 2744:lm_loss: 4.5219, ppl: 92.0092, loss: 4.5528
	step 2745:lm_loss: 4.5222, ppl: 92.0338, loss: 4.5531
	step 2746:lm_loss: 4.5218, ppl: 92.0027, loss: 4.5527
	step 2747:lm_loss: 4.5219, ppl: 92.0138, loss: 4.5528
	step 2748:lm_loss: 4.5217, ppl: 91.9924, loss: 4.5524
	step 2749:lm_loss: 4.5217, ppl: 91.9949, loss: 4.5524
	step 2750:lm_loss: 4.5222, ppl: 92.0405, loss: 4.5527
	step 2751:lm_loss: 4.5223, ppl: 92.0446, loss: 4.5528
	step 2752:lm_loss: 4.5218, ppl: 92.0035, loss: 4.5524
	step 2753:lm_loss: 4.5219, ppl: 92.0127, loss: 4.5525
	step 2754:lm_loss: 4.5213, ppl: 91.9508, loss: 4.5519
	step 2755:lm_loss: 4.5210, ppl: 91.9320, loss: 4.5517
	step 2756:lm_loss: 4.5211, ppl: 91.9381, loss: 4.5518
	step 2757:lm_loss: 4.5212, ppl: 91.9495, loss: 4.5519
	step 2758:lm_loss: 4.5214, ppl: 91.9663, loss: 4.5520
	step 2759:lm_loss: 4.5212, ppl: 91.9414, loss: 4.5518
	step 2760:lm_loss: 4.5211, ppl: 91.9335, loss: 4.5517
	step 2761:lm_loss: 4.5211, ppl: 91.9411, loss: 4.5518
	step 2762:lm_loss: 4.5214, ppl: 91.9664, loss: 4.5520
	step 2763:lm_loss: 4.5213, ppl: 91.9566, loss: 4.5518
	step 2764:lm_loss: 4.5213, ppl: 91.9565, loss: 4.5518
	step 2765:lm_loss: 4.5216, ppl: 91.9795, loss: 4.5521
	step 2766:lm_loss: 4.5213, ppl: 91.9536, loss: 4.5516
	step 2767:lm_loss: 4.5212, ppl: 91.9492, loss: 4.5515
	step 2768:lm_loss: 4.5210, ppl: 91.9307, loss: 4.5513
	step 2769:lm_loss: 4.5208, ppl: 91.9121, loss: 4.5511
	step 2770:lm_loss: 4.5205, ppl: 91.8861, loss: 4.5508
	step 2771:lm_loss: 4.5204, ppl: 91.8716, loss: 4.5506
	step 2772:lm_loss: 4.5200, ppl: 91.8387, loss: 4.5503
	step 2773:lm_loss: 4.5198, ppl: 91.8157, loss: 4.5500
	step 2774:lm_loss: 4.5199, ppl: 91.8288, loss: 4.5502
	step 2775:lm_loss: 4.5198, ppl: 91.8173, loss: 4.5501
	step 2776:lm_loss: 4.5198, ppl: 91.8155, loss: 4.5500
	step 2777:lm_loss: 4.5198, ppl: 91.8203, loss: 4.5501
	step 2778:lm_loss: 4.5198, ppl: 91.8160, loss: 4.5500
	step 2779:lm_loss: 4.5202, ppl: 91.8535, loss: 4.5504
	step 2780:lm_loss: 4.5204, ppl: 91.8755, loss: 4.5505
	step 2781:lm_loss: 4.5208, ppl: 91.9075, loss: 4.5507
	step 2782:lm_loss: 4.5206, ppl: 91.8936, loss: 4.5506
	step 2783:lm_loss: 4.5206, ppl: 91.8882, loss: 4.5505
	step 2784:lm_loss: 4.5202, ppl: 91.8529, loss: 4.5502
	step 2785:lm_loss: 4.5201, ppl: 91.8488, loss: 4.5501
	step 2786:lm_loss: 4.5201, ppl: 91.8436, loss: 4.5501
	step 2787:lm_loss: 4.5208, ppl: 91.9063, loss: 4.5510
	step 2788:lm_loss: 4.5208, ppl: 91.9085, loss: 4.5510
	step 2789:lm_loss: 4.5205, ppl: 91.8822, loss: 4.5505
	step 2790:lm_loss: 4.5203, ppl: 91.8656, loss: 4.5503
	step 2791:lm_loss: 4.5203, ppl: 91.8622, loss: 4.5502
	step 2792:lm_loss: 4.5200, ppl: 91.8335, loss: 4.5500
	step 2793:lm_loss: 4.5201, ppl: 91.8439, loss: 4.5502
	step 2794:lm_loss: 4.5202, ppl: 91.8525, loss: 4.5503
	step 2795:lm_loss: 4.5201, ppl: 91.8433, loss: 4.5502
	step 2796:lm_loss: 4.5200, ppl: 91.8367, loss: 4.5501
	step 2797:lm_loss: 4.5203, ppl: 91.8618, loss: 4.5504
	step 2798:lm_loss: 4.5199, ppl: 91.8267, loss: 4.5500
	step 2799:lm_loss: 4.5203, ppl: 91.8630, loss: 4.5503
	step 2800:lm_loss: 4.5204, ppl: 91.8744, loss: 4.5504
	step 2801:lm_loss: 4.5205, ppl: 91.8856, loss: 4.5505
	step 2802:lm_loss: 4.5208, ppl: 91.9112, loss: 4.5507
	step 2803:lm_loss: 4.5210, ppl: 91.9296, loss: 4.5509
	step 2804:lm_loss: 4.5210, ppl: 91.9260, loss: 4.5508
	step 2805:lm_loss: 4.5211, ppl: 91.9350, loss: 4.5510
	step 2806:lm_loss: 4.5213, ppl: 91.9552, loss: 4.5511
	step 2807:lm_loss: 4.5211, ppl: 91.9365, loss: 4.5510
	step 2808:lm_loss: 4.5211, ppl: 91.9400, loss: 4.5510
	step 2809:lm_loss: 4.5211, ppl: 91.9365, loss: 4.5510
	step 2810:lm_loss: 4.5212, ppl: 91.9456, loss: 4.5511
	step 2811:lm_loss: 4.5212, ppl: 91.9413, loss: 4.5510
	step 2812:lm_loss: 4.5208, ppl: 91.9088, loss: 4.5507
	step 2813:lm_loss: 4.5206, ppl: 91.8883, loss: 4.5505
	step 2814:lm_loss: 4.5206, ppl: 91.8928, loss: 4.5505
	step 2815:lm_loss: 4.5209, ppl: 91.9201, loss: 4.5507
	step 2816:lm_loss: 4.5210, ppl: 91.9289, loss: 4.5508
	step 2817:lm_loss: 4.5209, ppl: 91.9192, loss: 4.5507
	step 2818:lm_loss: 4.5210, ppl: 91.9257, loss: 4.5507
	step 2819:lm_loss: 4.5209, ppl: 91.9140, loss: 4.5505
	step 2820:lm_loss: 4.5203, ppl: 91.8597, loss: 4.5499
	step 2821:lm_loss: 4.5203, ppl: 91.8628, loss: 4.5499
	step 2822:lm_loss: 4.5203, ppl: 91.8617, loss: 4.5499
	step 2823:lm_loss: 4.5203, ppl: 91.8660, loss: 4.5500
	step 2824:lm_loss: 4.5201, ppl: 91.8443, loss: 4.5497
	step 2825:lm_loss: 4.5198, ppl: 91.8183, loss: 4.5494
	step 2826:lm_loss: 4.5194, ppl: 91.7839, loss: 4.5489
	step 2827:lm_loss: 4.5195, ppl: 91.7862, loss: 4.5489
	step 2828:lm_loss: 4.5196, ppl: 91.7968, loss: 4.5491
	step 2829:lm_loss: 4.5190, ppl: 91.7400, loss: 4.5484
	step 2830:lm_loss: 4.5192, ppl: 91.7577, loss: 4.5486
	step 2831:lm_loss: 4.5193, ppl: 91.7739, loss: 4.5488
	step 2832:lm_loss: 4.5194, ppl: 91.7823, loss: 4.5489
	step 2833:lm_loss: 4.5194, ppl: 91.7804, loss: 4.5488
	step 2834:lm_loss: 4.5189, ppl: 91.7332, loss: 4.5484
	step 2835:lm_loss: 4.5189, ppl: 91.7320, loss: 4.5484
	step 2836:lm_loss: 4.5187, ppl: 91.7148, loss: 4.5483
	step 2837:lm_loss: 4.5187, ppl: 91.7200, loss: 4.5484
	step 2838:lm_loss: 4.5183, ppl: 91.6788, loss: 4.5481
	step 2839:lm_loss: 4.5187, ppl: 91.7207, loss: 4.5486
	step 2840:lm_loss: 4.5188, ppl: 91.7266, loss: 4.5486
	step 2841:lm_loss: 4.5189, ppl: 91.7327, loss: 4.5487
	step 2842:lm_loss: 4.5190, ppl: 91.7397, loss: 4.5488
	step 2843:lm_loss: 4.5180, ppl: 91.6541, loss: 4.5480
	step 2844:lm_loss: 4.5179, ppl: 91.6467, loss: 4.5479
	step 2845:lm_loss: 4.5177, ppl: 91.6239, loss: 4.5476
	step 2846:lm_loss: 4.5176, ppl: 91.6120, loss: 4.5474
	step 2847:lm_loss: 4.5171, ppl: 91.5684, loss: 4.5469
	step 2848:lm_loss: 4.5172, ppl: 91.5778, loss: 4.5469
	step 2849:lm_loss: 4.5174, ppl: 91.6001, loss: 4.5472
	step 2850:lm_loss: 4.5174, ppl: 91.5989, loss: 4.5472
	step 2851:lm_loss: 4.5173, ppl: 91.5876, loss: 4.5470
	step 2852:lm_loss: 4.5172, ppl: 91.5804, loss: 4.5469
	step 2853:lm_loss: 4.5170, ppl: 91.5648, loss: 4.5467
	step 2854:lm_loss: 4.5173, ppl: 91.5839, loss: 4.5469
	step 2855:lm_loss: 4.5170, ppl: 91.5635, loss: 4.5466
	step 2856:lm_loss: 4.5169, ppl: 91.5527, loss: 4.5464
	step 2857:lm_loss: 4.5164, ppl: 91.5095, loss: 4.5460
	step 2858:lm_loss: 4.5169, ppl: 91.5471, loss: 4.5466
	step 2859:lm_loss: 4.5168, ppl: 91.5380, loss: 4.5466
	step 2860:lm_loss: 4.5164, ppl: 91.5071, loss: 4.5461
	step 2861:lm_loss: 4.5168, ppl: 91.5450, loss: 4.5466
	step 2862:lm_loss: 4.5168, ppl: 91.5386, loss: 4.5466
	step 2863:lm_loss: 4.5166, ppl: 91.5248, loss: 4.5464
	step 2864:lm_loss: 4.5166, ppl: 91.5241, loss: 4.5464
	step 2865:lm_loss: 4.5167, ppl: 91.5298, loss: 4.5464
	step 2866:lm_loss: 4.5167, ppl: 91.5290, loss: 4.5464
	step 2867:lm_loss: 4.5169, ppl: 91.5515, loss: 4.5465
	step 2868:lm_loss: 4.5171, ppl: 91.5694, loss: 4.5469
	step 2869:lm_loss: 4.5170, ppl: 91.5640, loss: 4.5468
	step 2870:lm_loss: 4.5169, ppl: 91.5532, loss: 4.5466
	step 2871:lm_loss: 4.5167, ppl: 91.5338, loss: 4.5464
	step 2872:lm_loss: 4.5165, ppl: 91.5156, loss: 4.5463
	step 2873:lm_loss: 4.5163, ppl: 91.4994, loss: 4.5460
	step 2874:lm_loss: 4.5159, ppl: 91.4594, loss: 4.5457
	step 2875:lm_loss: 4.5153, ppl: 91.4042, loss: 4.5450
	step 2876:lm_loss: 4.5152, ppl: 91.3957, loss: 4.5449
	step 2877:lm_loss: 4.5152, ppl: 91.3975, loss: 4.5449
	step 2878:lm_loss: 4.5153, ppl: 91.4042, loss: 4.5450
	step 2879:lm_loss: 4.5155, ppl: 91.4190, loss: 4.5451
	step 2880:lm_loss: 4.5156, ppl: 91.4361, loss: 4.5454
	step 2881:lm_loss: 4.5155, ppl: 91.4234, loss: 4.5453
	step 2882:lm_loss: 4.5154, ppl: 91.4149, loss: 4.5451
	step 2883:lm_loss: 4.5156, ppl: 91.4283, loss: 4.5454
	step 2884:lm_loss: 4.5155, ppl: 91.4261, loss: 4.5453
	step 2885:lm_loss: 4.5155, ppl: 91.4234, loss: 4.5453
	step 2886:lm_loss: 4.5156, ppl: 91.4304, loss: 4.5453
	step 2887:lm_loss: 4.5156, ppl: 91.4342, loss: 4.5454
	step 2888:lm_loss: 4.5156, ppl: 91.4344, loss: 4.5453
	step 2889:lm_loss: 4.5162, ppl: 91.4883, loss: 4.5457
	step 2890:lm_loss: 4.5162, ppl: 91.4889, loss: 4.5457
	step 2891:lm_loss: 4.5161, ppl: 91.4782, loss: 4.5455
	step 2892:lm_loss: 4.5163, ppl: 91.4955, loss: 4.5456
	step 2893:lm_loss: 4.5162, ppl: 91.4886, loss: 4.5455
	step 2894:lm_loss: 4.5157, ppl: 91.4434, loss: 4.5451
	step 2895:lm_loss: 4.5156, ppl: 91.4329, loss: 4.5450
	step 2896:lm_loss: 4.5157, ppl: 91.4395, loss: 4.5451
	step 2897:lm_loss: 4.5156, ppl: 91.4312, loss: 4.5450
	step 2898:lm_loss: 4.5155, ppl: 91.4268, loss: 4.5449
	step 2899:lm_loss: 4.5157, ppl: 91.4381, loss: 4.5450
	step 2900:lm_loss: 4.5159, ppl: 91.4597, loss: 4.5453
	step 2901:lm_loss: 4.5161, ppl: 91.4749, loss: 4.5456
	step 2902:lm_loss: 4.5161, ppl: 91.4762, loss: 4.5456
	step 2903:lm_loss: 4.5163, ppl: 91.5008, loss: 4.5458
	step 2904:lm_loss: 4.5165, ppl: 91.5160, loss: 4.5459
	step 2905:lm_loss: 4.5156, ppl: 91.4303, loss: 4.5451
	step 2906:lm_loss: 4.5150, ppl: 91.3808, loss: 4.5446
	step 2907:lm_loss: 4.5149, ppl: 91.3698, loss: 4.5445
	step 2908:lm_loss: 4.5153, ppl: 91.4077, loss: 4.5448
	step 2909:lm_loss: 4.5154, ppl: 91.4121, loss: 4.5449
	step 2910:lm_loss: 4.5156, ppl: 91.4363, loss: 4.5452
	step 2911:lm_loss: 4.5156, ppl: 91.4293, loss: 4.5451
	step 2912:lm_loss: 4.5152, ppl: 91.3948, loss: 4.5446
	step 2913:lm_loss: 4.5147, ppl: 91.3538, loss: 4.5443
	step 2914:lm_loss: 4.5150, ppl: 91.3769, loss: 4.5445
	step 2915:lm_loss: 4.5149, ppl: 91.3717, loss: 4.5444
	step 2916:lm_loss: 4.5151, ppl: 91.3891, loss: 4.5446
	step 2917:lm_loss: 4.5154, ppl: 91.4164, loss: 4.5448
	step 2918:lm_loss: 4.5152, ppl: 91.3969, loss: 4.5447
	step 2919:lm_loss: 4.5151, ppl: 91.3844, loss: 4.5445
	step 2920:lm_loss: 4.5151, ppl: 91.3848, loss: 4.5445
	step 2921:lm_loss: 4.5148, ppl: 91.3603, loss: 4.5440
	step 2922:lm_loss: 4.5144, ppl: 91.3229, loss: 4.5436
	step 2923:lm_loss: 4.5143, ppl: 91.3095, loss: 4.5435
	step 2924:lm_loss: 4.5139, ppl: 91.2794, loss: 4.5433
	step 2925:lm_loss: 4.5137, ppl: 91.2566, loss: 4.5429
	step 2926:lm_loss: 4.5138, ppl: 91.2656, loss: 4.5430
	step 2927:lm_loss: 4.5137, ppl: 91.2630, loss: 4.5430
	step 2928:lm_loss: 4.5138, ppl: 91.2714, loss: 4.5430
	step 2929:lm_loss: 4.5140, ppl: 91.2823, loss: 4.5432
	step 2930:lm_loss: 4.5143, ppl: 91.3145, loss: 4.5435
	step 2931:lm_loss: 4.5141, ppl: 91.2941, loss: 4.5432
	step 2932:lm_loss: 4.5144, ppl: 91.3270, loss: 4.5435
	step 2933:lm_loss: 4.5152, ppl: 91.3992, loss: 4.5443
	step 2934:lm_loss: 4.5154, ppl: 91.4100, loss: 4.5444
	step 2935:lm_loss: 4.5156, ppl: 91.4307, loss: 4.5447
	step 2936:lm_loss: 4.5157, ppl: 91.4415, loss: 4.5448
	step 2937:lm_loss: 4.5161, ppl: 91.4755, loss: 4.5451
	step 2938:lm_loss: 4.5160, ppl: 91.4729, loss: 4.5451
	step 2939:lm_loss: 4.5162, ppl: 91.4882, loss: 4.5452
	step 2940:lm_loss: 4.5164, ppl: 91.5063, loss: 4.5454
	step 2941:lm_loss: 4.5165, ppl: 91.5150, loss: 4.5455
	step 2942:lm_loss: 4.5164, ppl: 91.5032, loss: 4.5454
	step 2943:lm_loss: 4.5166, ppl: 91.5282, loss: 4.5457
	step 2944:lm_loss: 4.5167, ppl: 91.5373, loss: 4.5457
	step 2945:lm_loss: 4.5164, ppl: 91.5047, loss: 4.5454
	step 2946:lm_loss: 4.5166, ppl: 91.5196, loss: 4.5456
	step 2947:lm_loss: 4.5167, ppl: 91.5313, loss: 4.5458
	step 2948:lm_loss: 4.5165, ppl: 91.5173, loss: 4.5456
	step 2949:lm_loss: 4.5165, ppl: 91.5181, loss: 4.5456
	step 2950:lm_loss: 4.5164, ppl: 91.5055, loss: 4.5455
	step 2951:lm_loss: 4.5164, ppl: 91.5044, loss: 4.5455
	step 2952:lm_loss: 4.5163, ppl: 91.4950, loss: 4.5454
	step 2953:lm_loss: 4.5166, ppl: 91.5230, loss: 4.5457
	step 2954:lm_loss: 4.5163, ppl: 91.4955, loss: 4.5455
	step 2955:lm_loss: 4.5160, ppl: 91.4723, loss: 4.5454
	step 2956:lm_loss: 4.5159, ppl: 91.4626, loss: 4.5453
	step 2957:lm_loss: 4.5163, ppl: 91.4967, loss: 4.5456
	step 2958:lm_loss: 4.5164, ppl: 91.5018, loss: 4.5456
	step 2959:lm_loss: 4.5164, ppl: 91.5084, loss: 4.5457
	step 2960:lm_loss: 4.5166, ppl: 91.5221, loss: 4.5458
	step 2961:lm_loss: 4.5164, ppl: 91.5020, loss: 4.5457
	step 2962:lm_loss: 4.5166, ppl: 91.5227, loss: 4.5458
	step 2963:lm_loss: 4.5167, ppl: 91.5353, loss: 4.5460
	step 2964:lm_loss: 4.5171, ppl: 91.5685, loss: 4.5463
	step 2965:lm_loss: 4.5170, ppl: 91.5638, loss: 4.5462
	step 2966:lm_loss: 4.5172, ppl: 91.5826, loss: 4.5463
	step 2967:lm_loss: 4.5172, ppl: 91.5801, loss: 4.5463
	step 2968:lm_loss: 4.5173, ppl: 91.5920, loss: 4.5464
	step 2969:lm_loss: 4.5174, ppl: 91.5927, loss: 4.5464
	step 2970:lm_loss: 4.5174, ppl: 91.5943, loss: 4.5464
	step 2971:lm_loss: 4.5174, ppl: 91.5936, loss: 4.5464
	step 2972:lm_loss: 4.5174, ppl: 91.5988, loss: 4.5465
	step 2973:lm_loss: 4.5176, ppl: 91.6193, loss: 4.5467
	step 2974:lm_loss: 4.5177, ppl: 91.6250, loss: 4.5467
	step 2975:lm_loss: 4.5176, ppl: 91.6168, loss: 4.5466
	step 2976:lm_loss: 4.5176, ppl: 91.6113, loss: 4.5466
	step 2977:lm_loss: 4.5170, ppl: 91.5595, loss: 4.5462
	step 2978:lm_loss: 4.5173, ppl: 91.5869, loss: 4.5465
	step 2979:lm_loss: 4.5173, ppl: 91.5909, loss: 4.5466
	step 2980:lm_loss: 4.5175, ppl: 91.6083, loss: 4.5468
	step 2981:lm_loss: 4.5177, ppl: 91.6286, loss: 4.5471
	step 2982:lm_loss: 4.5172, ppl: 91.5801, loss: 4.5467
	step 2983:lm_loss: 4.5173, ppl: 91.5835, loss: 4.5468
	step 2984:lm_loss: 4.5169, ppl: 91.5546, loss: 4.5464
	step 2985:lm_loss: 4.5175, ppl: 91.6069, loss: 4.5471
	step 2986:lm_loss: 4.5177, ppl: 91.6241, loss: 4.5472
	step 2987:lm_loss: 4.5175, ppl: 91.6056, loss: 4.5470
	step 2988:lm_loss: 4.5178, ppl: 91.6320, loss: 4.5474
	step 2989:lm_loss: 4.5178, ppl: 91.6298, loss: 4.5474
	step 2990:lm_loss: 4.5178, ppl: 91.6353, loss: 4.5474
	step 2991:lm_loss: 4.5172, ppl: 91.5812, loss: 4.5471
	step 2992:lm_loss: 4.5176, ppl: 91.6175, loss: 4.5476
	step 2993:lm_loss: 4.5176, ppl: 91.6164, loss: 4.5476
	step 2994:lm_loss: 4.5176, ppl: 91.6197, loss: 4.5477
	step 2995:lm_loss: 4.5178, ppl: 91.6317, loss: 4.5478
	step 2996:lm_loss: 4.5177, ppl: 91.6276, loss: 4.5478
	step 2997:lm_loss: 4.5178, ppl: 91.6360, loss: 4.5479
	step 2998:lm_loss: 4.5178, ppl: 91.6381, loss: 4.5479
	step 2999:lm_loss: 4.5175, ppl: 91.6078, loss: 4.5477
	step 3000:lm_loss: 4.5170, ppl: 91.5644, loss: 4.5472
	step 3001:lm_loss: 4.5169, ppl: 91.5530, loss: 4.5470
	step 3002:lm_loss: 4.5167, ppl: 91.5335, loss: 4.5468
	step 3003:lm_loss: 4.5166, ppl: 91.5207, loss: 4.5466
	step 3004:lm_loss: 4.5167, ppl: 91.5325, loss: 4.5468
	step 3005:lm_loss: 4.5171, ppl: 91.5688, loss: 4.5470
	step 3006:lm_loss: 4.5169, ppl: 91.5536, loss: 4.5468
	step 3007:lm_loss: 4.5172, ppl: 91.5784, loss: 4.5470
	step 3008:lm_loss: 4.5174, ppl: 91.6008, loss: 4.5473
	step 3009:lm_loss: 4.5169, ppl: 91.5509, loss: 4.5467
	step 3010:lm_loss: 4.5167, ppl: 91.5359, loss: 4.5465
	step 3011:lm_loss: 4.5167, ppl: 91.5301, loss: 4.5464
	step 3012:lm_loss: 4.5164, ppl: 91.5081, loss: 4.5461
	step 3013:lm_loss: 4.5164, ppl: 91.5012, loss: 4.5460
	step 3014:lm_loss: 4.5164, ppl: 91.5074, loss: 4.5460
	step 3015:lm_loss: 4.5164, ppl: 91.5033, loss: 4.5460
	step 3016:lm_loss: 4.5163, ppl: 91.4977, loss: 4.5459
	step 3017:lm_loss: 4.5163, ppl: 91.4981, loss: 4.5459
	step 3018:lm_loss: 4.5162, ppl: 91.4902, loss: 4.5458
	step 3019:lm_loss: 4.5164, ppl: 91.5087, loss: 4.5459
	step 3020:lm_loss: 4.5162, ppl: 91.4837, loss: 4.5456
	step 3021:lm_loss: 4.5163, ppl: 91.4926, loss: 4.5457
	step 3022:lm_loss: 4.5161, ppl: 91.4816, loss: 4.5455
	step 3023:lm_loss: 4.5162, ppl: 91.4910, loss: 4.5457
	step 3024:lm_loss: 4.5161, ppl: 91.4795, loss: 4.5455
	step 3025:lm_loss: 4.5160, ppl: 91.4659, loss: 4.5454
	step 3026:lm_loss: 4.5156, ppl: 91.4336, loss: 4.5451
	step 3027:lm_loss: 4.5156, ppl: 91.4343, loss: 4.5451
	step 3028:lm_loss: 4.5155, ppl: 91.4221, loss: 4.5450
	step 3029:lm_loss: 4.5154, ppl: 91.4139, loss: 4.5449
	step 3030:lm_loss: 4.5155, ppl: 91.4251, loss: 4.5449
	step 3031:lm_loss: 4.5155, ppl: 91.4278, loss: 4.5450
	step 3032:lm_loss: 4.5157, ppl: 91.4402, loss: 4.5451
	step 3033:lm_loss: 4.5158, ppl: 91.4548, loss: 4.5453
	step 3034:lm_loss: 4.5157, ppl: 91.4456, loss: 4.5452
	step 3035:lm_loss: 4.5160, ppl: 91.4671, loss: 4.5454
	step 3036:lm_loss: 4.5161, ppl: 91.4793, loss: 4.5455
	step 3037:lm_loss: 4.5161, ppl: 91.4778, loss: 4.5455
	step 3038:lm_loss: 4.5159, ppl: 91.4594, loss: 4.5453
	step 3039:lm_loss: 4.5158, ppl: 91.4538, loss: 4.5452
	step 3040:lm_loss: 4.5157, ppl: 91.4437, loss: 4.5451
	step 3041:lm_loss: 4.5157, ppl: 91.4421, loss: 4.5451
	step 3042:lm_loss: 4.5160, ppl: 91.4691, loss: 4.5453
	step 3043:lm_loss: 4.5158, ppl: 91.4509, loss: 4.5451
	step 3044:lm_loss: 4.5155, ppl: 91.4226, loss: 4.5448
	step 3045:lm_loss: 4.5156, ppl: 91.4280, loss: 4.5449
	step 3046:lm_loss: 4.5158, ppl: 91.4474, loss: 4.5451
	step 3047:lm_loss: 4.5156, ppl: 91.4314, loss: 4.5448
	step 3048:lm_loss: 4.5157, ppl: 91.4414, loss: 4.5449
	step 3049:lm_loss: 4.5159, ppl: 91.4553, loss: 4.5451
	step 3050:lm_loss: 4.5158, ppl: 91.4526, loss: 4.5450
	step 3051:lm_loss: 4.5157, ppl: 91.4441, loss: 4.5450
	step 3052:lm_loss: 4.5158, ppl: 91.4529, loss: 4.5451
	step 3053:lm_loss: 4.5157, ppl: 91.4424, loss: 4.5449
	step 3054:lm_loss: 4.5160, ppl: 91.4645, loss: 4.5452
	step 3055:lm_loss: 4.5153, ppl: 91.4013, loss: 4.5445
	step 3056:lm_loss: 4.5151, ppl: 91.3842, loss: 4.5443
	step 3057:lm_loss: 4.5151, ppl: 91.3904, loss: 4.5444
	step 3058:lm_loss: 4.5150, ppl: 91.3752, loss: 4.5442
	step 3059:lm_loss: 4.5154, ppl: 91.4131, loss: 4.5445
	step 3060:lm_loss: 4.5149, ppl: 91.3674, loss: 4.5441
	step 3061:lm_loss: 4.5151, ppl: 91.3896, loss: 4.5443
	step 3062:lm_loss: 4.5154, ppl: 91.4125, loss: 4.5444
	step 3063:lm_loss: 4.5156, ppl: 91.4329, loss: 4.5447
	step 3064:lm_loss: 4.5156, ppl: 91.4295, loss: 4.5447
	step 3065:lm_loss: 4.5160, ppl: 91.4718, loss: 4.5452
	step 3066:lm_loss: 4.5165, ppl: 91.5155, loss: 4.5457
	step 3067:lm_loss: 4.5166, ppl: 91.5227, loss: 4.5458
	step 3068:lm_loss: 4.5162, ppl: 91.4912, loss: 4.5455
	step 3069:lm_loss: 4.5161, ppl: 91.4754, loss: 4.5454
	step 3070:lm_loss: 4.5163, ppl: 91.4971, loss: 4.5457
	step 3071:lm_loss: 4.5165, ppl: 91.5131, loss: 4.5458
	step 3072:lm_loss: 4.5164, ppl: 91.5095, loss: 4.5458
	step 3073:lm_loss: 4.5168, ppl: 91.5396, loss: 4.5461
	step 3074:lm_loss: 4.5169, ppl: 91.5480, loss: 4.5463
	step 3075:lm_loss: 4.5167, ppl: 91.5349, loss: 4.5461
	step 3076:lm_loss: 4.5166, ppl: 91.5272, loss: 4.5460
	step 3077:lm_loss: 4.5167, ppl: 91.5302, loss: 4.5460
	step 3078:lm_loss: 4.5169, ppl: 91.5546, loss: 4.5463
	step 3079:lm_loss: 4.5168, ppl: 91.5429, loss: 4.5461
	step 3080:lm_loss: 4.5170, ppl: 91.5568, loss: 4.5463
	step 3081:lm_loss: 4.5166, ppl: 91.5193, loss: 4.5459
	step 3082:lm_loss: 4.5164, ppl: 91.5073, loss: 4.5457
	step 3083:lm_loss: 4.5163, ppl: 91.4963, loss: 4.5456
	step 3084:lm_loss: 4.5165, ppl: 91.5146, loss: 4.5458
	step 3085:lm_loss: 4.5163, ppl: 91.4959, loss: 4.5455
	step 3086:lm_loss: 4.5163, ppl: 91.4986, loss: 4.5456
	step 3087:lm_loss: 4.5163, ppl: 91.4942, loss: 4.5455
	step 3088:lm_loss: 4.5163, ppl: 91.4920, loss: 4.5455
	step 3089:lm_loss: 4.5165, ppl: 91.5159, loss: 4.5457
	step 3090:lm_loss: 4.5165, ppl: 91.5179, loss: 4.5457
	step 3091:lm_loss: 4.5169, ppl: 91.5497, loss: 4.5461
	step 3092:lm_loss: 4.5170, ppl: 91.5576, loss: 4.5462
	step 3093:lm_loss: 4.5170, ppl: 91.5621, loss: 4.5463
	step 3094:lm_loss: 4.5169, ppl: 91.5531, loss: 4.5461
	step 3095:lm_loss: 4.5169, ppl: 91.5535, loss: 4.5461
	step 3096:lm_loss: 4.5165, ppl: 91.5136, loss: 4.5457
	step 3097:lm_loss: 4.5165, ppl: 91.5188, loss: 4.5458
	step 3098:lm_loss: 4.5167, ppl: 91.5288, loss: 4.5459
	step 3099:lm_loss: 4.5169, ppl: 91.5554, loss: 4.5461
	step 3100:lm_loss: 4.5155, ppl: 91.4268, loss: 4.5452
	step 3101:lm_loss: 4.5151, ppl: 91.3883, loss: 4.5449
	step 3102:lm_loss: 4.5152, ppl: 91.3969, loss: 4.5450
	step 3103:lm_loss: 4.5153, ppl: 91.4055, loss: 4.5451
	step 3104:lm_loss: 4.5151, ppl: 91.3906, loss: 4.5450
	step 3105:lm_loss: 4.5153, ppl: 91.4032, loss: 4.5452
	step 3106:lm_loss: 4.5155, ppl: 91.4275, loss: 4.5454
	step 3107:lm_loss: 4.5154, ppl: 91.4108, loss: 4.5452
	step 3108:lm_loss: 4.5153, ppl: 91.4040, loss: 4.5451
	step 3109:lm_loss: 4.5154, ppl: 91.4184, loss: 4.5452
	step 3110:lm_loss: 4.5159, ppl: 91.4592, loss: 4.5457
	step 3111:lm_loss: 4.5156, ppl: 91.4346, loss: 4.5453
	step 3112:lm_loss: 4.5158, ppl: 91.4551, loss: 4.5455
	step 3113:lm_loss: 4.5160, ppl: 91.4659, loss: 4.5456
	step 3114:lm_loss: 4.5161, ppl: 91.4784, loss: 4.5458
	step 3115:lm_loss: 4.5158, ppl: 91.4508, loss: 4.5455
	step 3116:lm_loss: 4.5159, ppl: 91.4579, loss: 4.5456
	step 3117:lm_loss: 4.5161, ppl: 91.4747, loss: 4.5457
	step 3118:lm_loss: 4.5161, ppl: 91.4753, loss: 4.5457
	step 3119:lm_loss: 4.5163, ppl: 91.4945, loss: 4.5458
	step 3120:lm_loss: 4.5162, ppl: 91.4911, loss: 4.5458
	step 3121:lm_loss: 4.5166, ppl: 91.5226, loss: 4.5462
	step 3122:lm_loss: 4.5165, ppl: 91.5137, loss: 4.5461
	step 3123:lm_loss: 4.5167, ppl: 91.5366, loss: 4.5462
	step 3124:lm_loss: 4.5167, ppl: 91.5305, loss: 4.5461
	step 3125:lm_loss: 4.5166, ppl: 91.5229, loss: 4.5460
	step 3126:lm_loss: 4.5166, ppl: 91.5203, loss: 4.5460
	step 3127:lm_loss: 4.5164, ppl: 91.5100, loss: 4.5458
	step 3128:lm_loss: 4.5162, ppl: 91.4854, loss: 4.5455
	step 3129:lm_loss: 4.5162, ppl: 91.4884, loss: 4.5455
	step 3130:lm_loss: 4.5162, ppl: 91.4850, loss: 4.5455
	step 3131:lm_loss: 4.5161, ppl: 91.4765, loss: 4.5453
	step 3132:lm_loss: 4.5162, ppl: 91.4854, loss: 4.5454
	step 3133:lm_loss: 4.5161, ppl: 91.4817, loss: 4.5454
	step 3134:lm_loss: 4.5163, ppl: 91.4959, loss: 4.5455
	step 3135:lm_loss: 4.5164, ppl: 91.5093, loss: 4.5456
	step 3136:lm_loss: 4.5164, ppl: 91.5068, loss: 4.5456
	step 3137:lm_loss: 4.5162, ppl: 91.4911, loss: 4.5454
	step 3138:lm_loss: 4.5164, ppl: 91.5085, loss: 4.5456
	step 3139:lm_loss: 4.5165, ppl: 91.5134, loss: 4.5457
	step 3140:lm_loss: 4.5161, ppl: 91.4755, loss: 4.5453
	step 3141:lm_loss: 4.5162, ppl: 91.4849, loss: 4.5453
	step 3142:lm_loss: 4.5163, ppl: 91.4978, loss: 4.5455
	step 3143:lm_loss: 4.5163, ppl: 91.4935, loss: 4.5454
	step 3144:lm_loss: 4.5161, ppl: 91.4779, loss: 4.5453
	step 3145:lm_loss: 4.5160, ppl: 91.4734, loss: 4.5452
	step 3146:lm_loss: 4.5158, ppl: 91.4533, loss: 4.5449
	step 3147:lm_loss: 4.5160, ppl: 91.4693, loss: 4.5451
	step 3148:lm_loss: 4.5158, ppl: 91.4463, loss: 4.5449
	step 3149:lm_loss: 4.5154, ppl: 91.4183, loss: 4.5445
	step 3150:lm_loss: 4.5157, ppl: 91.4388, loss: 4.5447
	step 3151:lm_loss: 4.5151, ppl: 91.3895, loss: 4.5443
	step 3152:lm_loss: 4.5152, ppl: 91.4003, loss: 4.5443
	step 3153:lm_loss: 4.5145, ppl: 91.3324, loss: 4.5437
	step 3154:lm_loss: 4.5141, ppl: 91.2954, loss: 4.5432
	step 3155:lm_loss: 4.5143, ppl: 91.3153, loss: 4.5434
	step 3156:lm_loss: 4.5145, ppl: 91.3318, loss: 4.5436
	step 3157:lm_loss: 4.5147, ppl: 91.3473, loss: 4.5437
	step 3158:lm_loss: 4.5150, ppl: 91.3776, loss: 4.5443
	step 3159:lm_loss: 4.5149, ppl: 91.3729, loss: 4.5442
	step 3160:lm_loss: 4.5152, ppl: 91.3935, loss: 4.5445
	step 3161:lm_loss: 4.5151, ppl: 91.3854, loss: 4.5445
	step 3162:lm_loss: 4.5150, ppl: 91.3789, loss: 4.5444
	step 3163:lm_loss: 4.5151, ppl: 91.3828, loss: 4.5444
	step 3164:lm_loss: 4.5149, ppl: 91.3649, loss: 4.5443
	step 3165:lm_loss: 4.5145, ppl: 91.3335, loss: 4.5440
	step 3166:lm_loss: 4.5145, ppl: 91.3348, loss: 4.5440
	step 3167:lm_loss: 4.5146, ppl: 91.3392, loss: 4.5441
	step 3168:lm_loss: 4.5146, ppl: 91.3397, loss: 4.5441
	step 3169:lm_loss: 4.5144, ppl: 91.3272, loss: 4.5439
	step 3170:lm_loss: 4.5146, ppl: 91.3445, loss: 4.5441
	step 3171:lm_loss: 4.5143, ppl: 91.3147, loss: 4.5438
	step 3172:lm_loss: 4.5142, ppl: 91.3071, loss: 4.5437
	step 3173:lm_loss: 4.5144, ppl: 91.3214, loss: 4.5439
	step 3174:lm_loss: 4.5146, ppl: 91.3384, loss: 4.5440
	step 3175:lm_loss: 4.5142, ppl: 91.3059, loss: 4.5438
	step 3176:lm_loss: 4.5143, ppl: 91.3179, loss: 4.5439
	step 3177:lm_loss: 4.5146, ppl: 91.3411, loss: 4.5441
	step 3178:lm_loss: 4.5145, ppl: 91.3306, loss: 4.5440
	step 3179:lm_loss: 4.5147, ppl: 91.3491, loss: 4.5442
	step 3180:lm_loss: 4.5143, ppl: 91.3167, loss: 4.5440
	step 3181:lm_loss: 4.5144, ppl: 91.3185, loss: 4.5440
	step 3182:lm_loss: 4.5145, ppl: 91.3309, loss: 4.5441
	step 3183:lm_loss: 4.5143, ppl: 91.3176, loss: 4.5439
	step 3184:lm_loss: 4.5141, ppl: 91.2965, loss: 4.5436
	step 3185:lm_loss: 4.5143, ppl: 91.3164, loss: 4.5438
	step 3186:lm_loss: 4.5145, ppl: 91.3311, loss: 4.5439
	step 3187:lm_loss: 4.5147, ppl: 91.3485, loss: 4.5441
	step 3188:lm_loss: 4.5141, ppl: 91.2960, loss: 4.5436
	step 3189:lm_loss: 4.5142, ppl: 91.3003, loss: 4.5437
	step 3190:lm_loss: 4.5145, ppl: 91.3319, loss: 4.5441
	step 3191:lm_loss: 4.5146, ppl: 91.3374, loss: 4.5442
	step 3192:lm_loss: 4.5149, ppl: 91.3665, loss: 4.5446
	step 3193:lm_loss: 4.5149, ppl: 91.3706, loss: 4.5446
	step 3194:lm_loss: 4.5148, ppl: 91.3626, loss: 4.5445
	step 3195:lm_loss: 4.5147, ppl: 91.3474, loss: 4.5443
	step 3196:lm_loss: 4.5149, ppl: 91.3640, loss: 4.5445
	step 3197:lm_loss: 4.5149, ppl: 91.3678, loss: 4.5446
	step 3198:lm_loss: 4.5149, ppl: 91.3684, loss: 4.5446
	step 3199:lm_loss: 4.5146, ppl: 91.3369, loss: 4.5443
	step 3200:lm_loss: 4.5146, ppl: 91.3453, loss: 4.5444
	step 3201:lm_loss: 4.5141, ppl: 91.2968, loss: 4.5439
	step 3202:lm_loss: 4.5142, ppl: 91.3085, loss: 4.5440
	step 3203:lm_loss: 4.5144, ppl: 91.3204, loss: 4.5441
	step 3204:lm_loss: 4.5146, ppl: 91.3455, loss: 4.5444
	step 3205:lm_loss: 4.5148, ppl: 91.3573, loss: 4.5445
	step 3206:lm_loss: 4.5149, ppl: 91.3716, loss: 4.5448
	step 3207:lm_loss: 4.5148, ppl: 91.3599, loss: 4.5446
	step 3208:lm_loss: 4.5149, ppl: 91.3696, loss: 4.5447
	step 3209:lm_loss: 4.5150, ppl: 91.3742, loss: 4.5448
	step 3210:lm_loss: 4.5151, ppl: 91.3856, loss: 4.5449
	step 3211:lm_loss: 4.5151, ppl: 91.3823, loss: 4.5448
	step 3212:lm_loss: 4.5147, ppl: 91.3502, loss: 4.5444
	step 3213:lm_loss: 4.5147, ppl: 91.3510, loss: 4.5444
	step 3214:lm_loss: 4.5147, ppl: 91.3525, loss: 4.5444
	step 3215:lm_loss: 4.5147, ppl: 91.3494, loss: 4.5444
	step 3216:lm_loss: 4.5143, ppl: 91.3108, loss: 4.5438
	step 3217:lm_loss: 4.5142, ppl: 91.3005, loss: 4.5436
	step 3218:lm_loss: 4.5142, ppl: 91.3090, loss: 4.5437
	step 3219:lm_loss: 4.5143, ppl: 91.3139, loss: 4.5438
	step 3220:lm_loss: 4.5139, ppl: 91.2755, loss: 4.5433
	step 3221:lm_loss: 4.5138, ppl: 91.2687, loss: 4.5432
	step 3222:lm_loss: 4.5138, ppl: 91.2672, loss: 4.5432
	step 3223:lm_loss: 4.5136, ppl: 91.2483, loss: 4.5429
	step 3224:lm_loss: 4.5136, ppl: 91.2534, loss: 4.5430
	step 3225:lm_loss: 4.5132, ppl: 91.2144, loss: 4.5427
	step 3226:lm_loss: 4.5128, ppl: 91.1799, loss: 4.5422
	step 3227:lm_loss: 4.5129, ppl: 91.1873, loss: 4.5423
	step 3228:lm_loss: 4.5130, ppl: 91.1917, loss: 4.5423
	step 3229:lm_loss: 4.5132, ppl: 91.2143, loss: 4.5424
	step 3230:lm_loss: 4.5132, ppl: 91.2162, loss: 4.5425
	step 3231:lm_loss: 4.5133, ppl: 91.2191, loss: 4.5425
	step 3232:lm_loss: 4.5131, ppl: 91.2019, loss: 4.5423
	step 3233:lm_loss: 4.5134, ppl: 91.2322, loss: 4.5428
	step 3234:lm_loss: 4.5136, ppl: 91.2454, loss: 4.5429
	step 3235:lm_loss: 4.5135, ppl: 91.2399, loss: 4.5429
	step 3236:lm_loss: 4.5136, ppl: 91.2539, loss: 4.5431
	step 3237:lm_loss: 4.5137, ppl: 91.2561, loss: 4.5431
	step 3238:lm_loss: 4.5139, ppl: 91.2758, loss: 4.5433
	step 3239:lm_loss: 4.5140, ppl: 91.2832, loss: 4.5434
	step 3240:lm_loss: 4.5143, ppl: 91.3113, loss: 4.5436
	step 3241:lm_loss: 4.5141, ppl: 91.2977, loss: 4.5435
	step 3242:lm_loss: 4.5144, ppl: 91.3194, loss: 4.5438
	step 3243:lm_loss: 4.5142, ppl: 91.3071, loss: 4.5436
	step 3244:lm_loss: 4.5141, ppl: 91.2941, loss: 4.5435
	step 3245:lm_loss: 4.5144, ppl: 91.3212, loss: 4.5439
	step 3246:lm_loss: 4.5147, ppl: 91.3510, loss: 4.5442
	step 3247:lm_loss: 4.5143, ppl: 91.3151, loss: 4.5439
	step 3248:lm_loss: 4.5146, ppl: 91.3371, loss: 4.5441
	step 3249:lm_loss: 4.5147, ppl: 91.3542, loss: 4.5443
	step 3250:lm_loss: 4.5149, ppl: 91.3652, loss: 4.5445
	step 3251:lm_loss: 4.5150, ppl: 91.3794, loss: 4.5447
	step 3252:lm_loss: 4.5149, ppl: 91.3706, loss: 4.5445
	step 3253:lm_loss: 4.5151, ppl: 91.3893, loss: 4.5447
	step 3254:lm_loss: 4.5147, ppl: 91.3534, loss: 4.5445
	step 3255:lm_loss: 4.5146, ppl: 91.3381, loss: 4.5442
	step 3256:lm_loss: 4.5141, ppl: 91.2916, loss: 4.5439
	step 3257:lm_loss: 4.5143, ppl: 91.3109, loss: 4.5442
	step 3258:lm_loss: 4.5145, ppl: 91.3358, loss: 4.5444
	step 3259:lm_loss: 4.5143, ppl: 91.3171, loss: 4.5443
	step 3260:lm_loss: 4.5143, ppl: 91.3146, loss: 4.5442
	step 3261:lm_loss: 4.5144, ppl: 91.3228, loss: 4.5443
	step 3262:lm_loss: 4.5146, ppl: 91.3393, loss: 4.5446
	step 3263:lm_loss: 4.5146, ppl: 91.3411, loss: 4.5446
	step 3264:lm_loss: 4.5148, ppl: 91.3611, loss: 4.5449
	step 3265:lm_loss: 4.5143, ppl: 91.3153, loss: 4.5445
	step 3266:lm_loss: 4.5142, ppl: 91.3032, loss: 4.5444
	step 3267:lm_loss: 4.5143, ppl: 91.3136, loss: 4.5445
	step 3268:lm_loss: 4.5144, ppl: 91.3234, loss: 4.5445
	step 3269:lm_loss: 4.5148, ppl: 91.3551, loss: 4.5447
	step 3270:lm_loss: 4.5149, ppl: 91.3696, loss: 4.5449
	step 3271:lm_loss: 4.5146, ppl: 91.3417, loss: 4.5446
	step 3272:lm_loss: 4.5147, ppl: 91.3471, loss: 4.5447
	step 3273:lm_loss: 4.5145, ppl: 91.3300, loss: 4.5444
	step 3274:lm_loss: 4.5140, ppl: 91.2900, loss: 4.5439
	step 3275:lm_loss: 4.5141, ppl: 91.2962, loss: 4.5440
	step 3276:lm_loss: 4.5141, ppl: 91.2984, loss: 4.5440
	step 3277:lm_loss: 4.5139, ppl: 91.2758, loss: 4.5438
	step 3278:lm_loss: 4.5137, ppl: 91.2599, loss: 4.5436
	step 3279:lm_loss: 4.5139, ppl: 91.2749, loss: 4.5438
	step 3280:lm_loss: 4.5139, ppl: 91.2814, loss: 4.5438
	step 3281:lm_loss: 4.5139, ppl: 91.2814, loss: 4.5438
	step 3282:lm_loss: 4.5139, ppl: 91.2762, loss: 4.5437
	step 3283:lm_loss: 4.5138, ppl: 91.2687, loss: 4.5437
	step 3284:lm_loss: 4.5139, ppl: 91.2762, loss: 4.5437
	step 3285:lm_loss: 4.5136, ppl: 91.2541, loss: 4.5434
	step 3286:lm_loss: 4.5136, ppl: 91.2502, loss: 4.5434
	step 3287:lm_loss: 4.5141, ppl: 91.2917, loss: 4.5439
	step 3288:lm_loss: 4.5140, ppl: 91.2902, loss: 4.5439
	step 3289:lm_loss: 4.5141, ppl: 91.2995, loss: 4.5440
	step 3290:lm_loss: 4.5142, ppl: 91.3069, loss: 4.5441
	step 3291:lm_loss: 4.5143, ppl: 91.3115, loss: 4.5442
	step 3292:lm_loss: 4.5143, ppl: 91.3147, loss: 4.5442
	step 3293:lm_loss: 4.5145, ppl: 91.3344, loss: 4.5445
	step 3294:lm_loss: 4.5143, ppl: 91.3144, loss: 4.5442
	step 3295:lm_loss: 4.5143, ppl: 91.3173, loss: 4.5442
	step 3296:lm_loss: 4.5146, ppl: 91.3400, loss: 4.5444
	step 3297:lm_loss: 4.5147, ppl: 91.3497, loss: 4.5445
	step 3298:lm_loss: 4.5149, ppl: 91.3703, loss: 4.5448
	step 3299:lm_loss: 4.5152, ppl: 91.3937, loss: 4.5449
	step 3300:lm_loss: 4.5153, ppl: 91.4050, loss: 4.5451
	step 3301:lm_loss: 4.5153, ppl: 91.4028, loss: 4.5451
	step 3302:lm_loss: 4.5152, ppl: 91.3916, loss: 4.5449
	step 3303:lm_loss: 4.5151, ppl: 91.3892, loss: 4.5448
	step 3304:lm_loss: 4.5149, ppl: 91.3667, loss: 4.5446
	step 3305:lm_loss: 4.5150, ppl: 91.3779, loss: 4.5447
	step 3306:lm_loss: 4.5146, ppl: 91.3454, loss: 4.5443
	step 3307:lm_loss: 4.5146, ppl: 91.3455, loss: 4.5443
	step 3308:lm_loss: 4.5146, ppl: 91.3380, loss: 4.5443
	step 3309:lm_loss: 4.5147, ppl: 91.3487, loss: 4.5444
	step 3310:lm_loss: 4.5149, ppl: 91.3718, loss: 4.5448
	step 3311:lm_loss: 4.5150, ppl: 91.3754, loss: 4.5448
	step 3312:lm_loss: 4.5153, ppl: 91.4069, loss: 4.5451
	step 3313:lm_loss: 4.5152, ppl: 91.3996, loss: 4.5451
	step 3314:lm_loss: 4.5153, ppl: 91.4026, loss: 4.5451
	step 3315:lm_loss: 4.5156, ppl: 91.4333, loss: 4.5455
	step 3316:lm_loss: 4.5155, ppl: 91.4251, loss: 4.5454
	step 3317:lm_loss: 4.5155, ppl: 91.4236, loss: 4.5453
	step 3318:lm_loss: 4.5155, ppl: 91.4200, loss: 4.5453
	step 3319:lm_loss: 4.5154, ppl: 91.4160, loss: 4.5452
	step 3320:lm_loss: 4.5154, ppl: 91.4130, loss: 4.5451
	step 3321:lm_loss: 4.5154, ppl: 91.4154, loss: 4.5451
	step 3322:lm_loss: 4.5154, ppl: 91.4151, loss: 4.5451
	step 3323:lm_loss: 4.5153, ppl: 91.4045, loss: 4.5450
	step 3324:lm_loss: 4.5153, ppl: 91.4018, loss: 4.5450
	step 3325:lm_loss: 4.5152, ppl: 91.3981, loss: 4.5449
	step 3326:lm_loss: 4.5153, ppl: 91.4015, loss: 4.5450
	step 3327:lm_loss: 4.5152, ppl: 91.3940, loss: 4.5449
	step 3328:lm_loss: 4.5155, ppl: 91.4213, loss: 4.5451
	step 3329:lm_loss: 4.5156, ppl: 91.4349, loss: 4.5453
	step 3330:lm_loss: 4.5156, ppl: 91.4347, loss: 4.5453
	step 3331:lm_loss: 4.5156, ppl: 91.4315, loss: 4.5452
	step 3332:lm_loss: 4.5156, ppl: 91.4299, loss: 4.5452
	step 3333:lm_loss: 4.5157, ppl: 91.4459, loss: 4.5453
	step 3334:lm_loss: 4.5159, ppl: 91.4632, loss: 4.5455
	step 3335:lm_loss: 4.5160, ppl: 91.4704, loss: 4.5456
	step 3336:lm_loss: 4.5159, ppl: 91.4634, loss: 4.5455
	step 3337:lm_loss: 4.5157, ppl: 91.4388, loss: 4.5451
	step 3338:lm_loss: 4.5157, ppl: 91.4428, loss: 4.5451
	step 3339:lm_loss: 4.5157, ppl: 91.4414, loss: 4.5451
	step 3340:lm_loss: 4.5160, ppl: 91.4728, loss: 4.5455
	step 3341:lm_loss: 4.5160, ppl: 91.4672, loss: 4.5455
	step 3342:lm_loss: 4.5161, ppl: 91.4739, loss: 4.5455
	step 3343:lm_loss: 4.5161, ppl: 91.4780, loss: 4.5456
	step 3344:lm_loss: 4.5163, ppl: 91.4926, loss: 4.5457
	step 3345:lm_loss: 4.5166, ppl: 91.5219, loss: 4.5460
	step 3346:lm_loss: 4.5168, ppl: 91.5401, loss: 4.5461
	step 3347:lm_loss: 4.5167, ppl: 91.5289, loss: 4.5460
	step 3348:lm_loss: 4.5167, ppl: 91.5330, loss: 4.5460
	step 3349:lm_loss: 4.5167, ppl: 91.5286, loss: 4.5460
	step 3350:lm_loss: 4.5165, ppl: 91.5169, loss: 4.5458
	step 3351:lm_loss: 4.5168, ppl: 91.5378, loss: 4.5461
	step 3352:lm_loss: 4.5170, ppl: 91.5569, loss: 4.5463
	step 3353:lm_loss: 4.5168, ppl: 91.5458, loss: 4.5462
	step 3354:lm_loss: 4.5169, ppl: 91.5491, loss: 4.5462
	step 3355:lm_loss: 4.5169, ppl: 91.5550, loss: 4.5462
	step 3356:lm_loss: 4.5168, ppl: 91.5460, loss: 4.5462
	step 3357:lm_loss: 4.5169, ppl: 91.5485, loss: 4.5462
	step 3358:lm_loss: 4.5168, ppl: 91.5452, loss: 4.5461
	step 3359:lm_loss: 4.5167, ppl: 91.5343, loss: 4.5460
	step 3360:lm_loss: 4.5170, ppl: 91.5617, loss: 4.5462
	step 3361:lm_loss: 4.5168, ppl: 91.5461, loss: 4.5460
	step 3362:lm_loss: 4.5170, ppl: 91.5599, loss: 4.5462
	step 3363:lm_loss: 4.5173, ppl: 91.5858, loss: 4.5465
	step 3364:lm_loss: 4.5172, ppl: 91.5833, loss: 4.5465
	step 3365:lm_loss: 4.5174, ppl: 91.5990, loss: 4.5467
	step 3366:lm_loss: 4.5176, ppl: 91.6181, loss: 4.5469
	step 3367:lm_loss: 4.5175, ppl: 91.6070, loss: 4.5468
	step 3368:lm_loss: 4.5174, ppl: 91.5960, loss: 4.5466
	step 3369:lm_loss: 4.5174, ppl: 91.5955, loss: 4.5466
	step 3370:lm_loss: 4.5174, ppl: 91.5959, loss: 4.5466
	step 3371:lm_loss: 4.5179, ppl: 91.6402, loss: 4.5470
	step 3372:lm_loss: 4.5184, ppl: 91.6872, loss: 4.5475
	step 3373:lm_loss: 4.5184, ppl: 91.6914, loss: 4.5476
	step 3374:lm_loss: 4.5184, ppl: 91.6893, loss: 4.5475
	step 3375:lm_loss: 4.5185, ppl: 91.6968, loss: 4.5476
	step 3376:lm_loss: 4.5186, ppl: 91.7035, loss: 4.5477
	step 3377:lm_loss: 4.5190, ppl: 91.7451, loss: 4.5480
	step 3378:lm_loss: 4.5193, ppl: 91.7730, loss: 4.5483
	step 3379:lm_loss: 4.5194, ppl: 91.7840, loss: 4.5485
	step 3380:lm_loss: 4.5196, ppl: 91.7970, loss: 4.5485
	step 3381:lm_loss: 4.5195, ppl: 91.7864, loss: 4.5484
	step 3382:lm_loss: 4.5191, ppl: 91.7530, loss: 4.5480
	step 3383:lm_loss: 4.5193, ppl: 91.7696, loss: 4.5482
	step 3384:lm_loss: 4.5195, ppl: 91.7869, loss: 4.5485
	step 3385:lm_loss: 4.5194, ppl: 91.7771, loss: 4.5484
	step 3386:lm_loss: 4.5192, ppl: 91.7579, loss: 4.5482
	step 3387:lm_loss: 4.5189, ppl: 91.7325, loss: 4.5480
	step 3388:lm_loss: 4.5186, ppl: 91.7072, loss: 4.5478
	step 3389:lm_loss: 4.5183, ppl: 91.6797, loss: 4.5476
	step 3390:lm_loss: 4.5184, ppl: 91.6845, loss: 4.5477
	step 3391:lm_loss: 4.5186, ppl: 91.7110, loss: 4.5479
	step 3392:lm_loss: 4.5187, ppl: 91.7133, loss: 4.5479
	step 3393:lm_loss: 4.5192, ppl: 91.7638, loss: 4.5483
	step 3394:lm_loss: 4.5193, ppl: 91.7701, loss: 4.5483
	step 3395:lm_loss: 4.5196, ppl: 91.7989, loss: 4.5488
	step 3396:lm_loss: 4.5199, ppl: 91.8258, loss: 4.5490
	step 3397:lm_loss: 4.5198, ppl: 91.8161, loss: 4.5489
	step 3398:lm_loss: 4.5197, ppl: 91.8069, loss: 4.5488
	step 3399:lm_loss: 4.5196, ppl: 91.7999, loss: 4.5487
	step 3400:lm_loss: 4.5196, ppl: 91.7962, loss: 4.5487
	step 3401:lm_loss: 4.5195, ppl: 91.7910, loss: 4.5486
	step 3402:lm_loss: 4.5194, ppl: 91.7849, loss: 4.5485
	step 3403:lm_loss: 4.5198, ppl: 91.8137, loss: 4.5490
	step 3404:lm_loss: 4.5198, ppl: 91.8217, loss: 4.5491
	step 3405:lm_loss: 4.5200, ppl: 91.8336, loss: 4.5492
	step 3406:lm_loss: 4.5198, ppl: 91.8196, loss: 4.5490
	step 3407:lm_loss: 4.5199, ppl: 91.8275, loss: 4.5491
	step 3408:lm_loss: 4.5198, ppl: 91.8217, loss: 4.5489
	step 3409:lm_loss: 4.5195, ppl: 91.7882, loss: 4.5484
	step 3410:lm_loss: 4.5195, ppl: 91.7856, loss: 4.5484
	step 3411:lm_loss: 4.5191, ppl: 91.7494, loss: 4.5480
	step 3412:lm_loss: 4.5189, ppl: 91.7343, loss: 4.5478
	step 3413:lm_loss: 4.5189, ppl: 91.7377, loss: 4.5479
	step 3414:lm_loss: 4.5190, ppl: 91.7477, loss: 4.5480
	step 3415:lm_loss: 4.5190, ppl: 91.7415, loss: 4.5479
	step 3416:lm_loss: 4.5188, ppl: 91.7262, loss: 4.5477
	step 3417:lm_loss: 4.5190, ppl: 91.7438, loss: 4.5479
	step 3418:lm_loss: 4.5188, ppl: 91.7233, loss: 4.5477
	step 3419:lm_loss: 4.5189, ppl: 91.7340, loss: 4.5479
	step 3420:lm_loss: 4.5190, ppl: 91.7454, loss: 4.5480
	step 3421:lm_loss: 4.5191, ppl: 91.7521, loss: 4.5480
	step 3422:lm_loss: 4.5192, ppl: 91.7633, loss: 4.5482
	step 3423:lm_loss: 4.5194, ppl: 91.7780, loss: 4.5484
	step 3424:lm_loss: 4.5194, ppl: 91.7772, loss: 4.5484
	step 3425:lm_loss: 4.5195, ppl: 91.7910, loss: 4.5485
	step 3426:lm_loss: 4.5199, ppl: 91.8231, loss: 4.5490
	step 3427:lm_loss: 4.5200, ppl: 91.8361, loss: 4.5491
	step 3428:lm_loss: 4.5202, ppl: 91.8550, loss: 4.5494
	step 3429:lm_loss: 4.5206, ppl: 91.8945, loss: 4.5497
	step 3430:lm_loss: 4.5208, ppl: 91.9047, loss: 4.5498
	step 3431:lm_loss: 4.5209, ppl: 91.9150, loss: 4.5499
	step 3432:lm_loss: 4.5209, ppl: 91.9184, loss: 4.5499
	step 3433:lm_loss: 4.5211, ppl: 91.9390, loss: 4.5502
	step 3434:lm_loss: 4.5209, ppl: 91.9163, loss: 4.5500
	step 3435:lm_loss: 4.5207, ppl: 91.8990, loss: 4.5498
	step 3436:lm_loss: 4.5207, ppl: 91.9029, loss: 4.5499
	step 3437:lm_loss: 4.5210, ppl: 91.9313, loss: 4.5503
	step 3438:lm_loss: 4.5212, ppl: 91.9477, loss: 4.5505
	step 3439:lm_loss: 4.5212, ppl: 91.9499, loss: 4.5505
	step 3440:lm_loss: 4.5209, ppl: 91.9220, loss: 4.5502
	step 3441:lm_loss: 4.5211, ppl: 91.9327, loss: 4.5503
	step 3442:lm_loss: 4.5211, ppl: 91.9388, loss: 4.5503
	step 3443:lm_loss: 4.5211, ppl: 91.9328, loss: 4.5502
	step 3444:lm_loss: 4.5212, ppl: 91.9432, loss: 4.5504
	step 3445:lm_loss: 4.5210, ppl: 91.9277, loss: 4.5502
	step 3446:lm_loss: 4.5209, ppl: 91.9196, loss: 4.5501
	step 3447:lm_loss: 4.5209, ppl: 91.9212, loss: 4.5501
	step 3448:lm_loss: 4.5207, ppl: 91.8984, loss: 4.5499
	step 3449:lm_loss: 4.5199, ppl: 91.8294, loss: 4.5496
	step 3450:lm_loss: 4.5201, ppl: 91.8435, loss: 4.5497
	step 3451:lm_loss: 4.5202, ppl: 91.8509, loss: 4.5498
	step 3452:lm_loss: 4.5202, ppl: 91.8556, loss: 4.5499
	step 3453:lm_loss: 4.5203, ppl: 91.8661, loss: 4.5501
	step 3454:lm_loss: 4.5206, ppl: 91.8921, loss: 4.5503
	step 3455:lm_loss: 4.5206, ppl: 91.8900, loss: 4.5502
	step 3456:lm_loss: 4.5200, ppl: 91.8385, loss: 4.5496
	step 3457:lm_loss: 4.5200, ppl: 91.8399, loss: 4.5496
	step 3458:lm_loss: 4.5201, ppl: 91.8408, loss: 4.5496
	step 3459:lm_loss: 4.5199, ppl: 91.8246, loss: 4.5494
	step 3460:lm_loss: 4.5198, ppl: 91.8174, loss: 4.5493
	step 3461:lm_loss: 4.5195, ppl: 91.7891, loss: 4.5491
	step 3462:lm_loss: 4.5195, ppl: 91.7873, loss: 4.5491
	step 3463:lm_loss: 4.5194, ppl: 91.7809, loss: 4.5490
	step 3464:lm_loss: 4.5196, ppl: 91.7994, loss: 4.5493
	step 3465:lm_loss: 4.5195, ppl: 91.7914, loss: 4.5492
	step 3466:lm_loss: 4.5196, ppl: 91.7985, loss: 4.5492
	step 3467:lm_loss: 4.5196, ppl: 91.8013, loss: 4.5493
	step 3468:lm_loss: 4.5201, ppl: 91.8449, loss: 4.5495
	step 3469:lm_loss: 4.5201, ppl: 91.8452, loss: 4.5495
	step 3470:lm_loss: 4.5202, ppl: 91.8573, loss: 4.5497
	step 3471:lm_loss: 4.5201, ppl: 91.8458, loss: 4.5496
	step 3472:lm_loss: 4.5201, ppl: 91.8449, loss: 4.5496
	step 3473:lm_loss: 4.5201, ppl: 91.8479, loss: 4.5496
	step 3474:lm_loss: 4.5201, ppl: 91.8467, loss: 4.5496
	step 3475:lm_loss: 4.5201, ppl: 91.8448, loss: 4.5496
	step 3476:lm_loss: 4.5204, ppl: 91.8700, loss: 4.5500
	step 3477:lm_loss: 4.5202, ppl: 91.8554, loss: 4.5498
	step 3478:lm_loss: 4.5199, ppl: 91.8296, loss: 4.5495
	step 3479:lm_loss: 4.5200, ppl: 91.8363, loss: 4.5495
	step 3480:lm_loss: 4.5197, ppl: 91.8093, loss: 4.5493
	step 3481:lm_loss: 4.5200, ppl: 91.8383, loss: 4.5498
	step 3482:lm_loss: 4.5199, ppl: 91.8266, loss: 4.5496
	step 3483:lm_loss: 4.5199, ppl: 91.8261, loss: 4.5496
	step 3484:lm_loss: 4.5199, ppl: 91.8224, loss: 4.5495
	step 3485:lm_loss: 4.5197, ppl: 91.8105, loss: 4.5494
	step 3486:lm_loss: 4.5202, ppl: 91.8501, loss: 4.5498
	step 3487:lm_loss: 4.5201, ppl: 91.8489, loss: 4.5497
	step 3488:lm_loss: 4.5202, ppl: 91.8502, loss: 4.5497
	step 3489:lm_loss: 4.5199, ppl: 91.8274, loss: 4.5496
	step 3490:lm_loss: 4.5200, ppl: 91.8390, loss: 4.5497
	step 3491:lm_loss: 4.5199, ppl: 91.8271, loss: 4.5496
	step 3492:lm_loss: 4.5202, ppl: 91.8506, loss: 4.5498
	step 3493:lm_loss: 4.5205, ppl: 91.8850, loss: 4.5500
	step 3494:lm_loss: 4.5207, ppl: 91.8965, loss: 4.5501
	step 3495:lm_loss: 4.5207, ppl: 91.8999, loss: 4.5502
	step 3496:lm_loss: 4.5209, ppl: 91.9205, loss: 4.5505
	step 3497:lm_loss: 4.5213, ppl: 91.9519, loss: 4.5507
	step 3498:lm_loss: 4.5211, ppl: 91.9377, loss: 4.5505
	step 3499:lm_loss: 4.5212, ppl: 91.9494, loss: 4.5507
	step 3500:lm_loss: 4.5214, ppl: 91.9643, loss: 4.5508
	step 3501:lm_loss: 4.5212, ppl: 91.9462, loss: 4.5506
	step 3502:lm_loss: 4.5214, ppl: 91.9615, loss: 4.5508
	step 3503:lm_loss: 4.5215, ppl: 91.9731, loss: 4.5509
	step 3504:lm_loss: 4.5217, ppl: 91.9927, loss: 4.5512
	step 3505:lm_loss: 4.5216, ppl: 91.9832, loss: 4.5511
	step 3506:lm_loss: 4.5214, ppl: 91.9684, loss: 4.5509
	step 3507:lm_loss: 4.5216, ppl: 91.9859, loss: 4.5512
	step 3508:lm_loss: 4.5213, ppl: 91.9560, loss: 4.5508
	step 3509:lm_loss: 4.5217, ppl: 91.9901, loss: 4.5511
	step 3510:lm_loss: 4.5216, ppl: 91.9824, loss: 4.5510
	step 3511:lm_loss: 4.5218, ppl: 91.9975, loss: 4.5512
	step 3512:lm_loss: 4.5217, ppl: 91.9954, loss: 4.5512
	step 3513:lm_loss: 4.5220, ppl: 92.0154, loss: 4.5514
	step 3514:lm_loss: 4.5216, ppl: 91.9857, loss: 4.5513
	step 3515:lm_loss: 4.5216, ppl: 91.9783, loss: 4.5512
	step 3516:lm_loss: 4.5218, ppl: 91.9990, loss: 4.5514
	step 3517:lm_loss: 4.5215, ppl: 91.9720, loss: 4.5512
	step 3518:lm_loss: 4.5215, ppl: 91.9746, loss: 4.5512
	step 3519:lm_loss: 4.5212, ppl: 91.9445, loss: 4.5507
	step 3520:lm_loss: 4.5207, ppl: 91.8957, loss: 4.5504
	step 3521:lm_loss: 4.5210, ppl: 91.9290, loss: 4.5507
	step 3522:lm_loss: 4.5211, ppl: 91.9344, loss: 4.5508
	step 3523:lm_loss: 4.5213, ppl: 91.9562, loss: 4.5510
	step 3524:lm_loss: 4.5209, ppl: 91.9215, loss: 4.5505
	step 3525:lm_loss: 4.5210, ppl: 91.9278, loss: 4.5506
	step 3526:lm_loss: 4.5208, ppl: 91.9112, loss: 4.5504
	step 3527:lm_loss: 4.5208, ppl: 91.9099, loss: 4.5504
	step 3528:lm_loss: 4.5210, ppl: 91.9268, loss: 4.5506
	step 3529:lm_loss: 4.5211, ppl: 91.9323, loss: 4.5507
	step 3530:lm_loss: 4.5214, ppl: 91.9602, loss: 4.5511
	step 3531:lm_loss: 4.5213, ppl: 91.9536, loss: 4.5510
	step 3532:lm_loss: 4.5211, ppl: 91.9412, loss: 4.5508
	step 3533:lm_loss: 4.5210, ppl: 91.9247, loss: 4.5506
	step 3534:lm_loss: 4.5209, ppl: 91.9202, loss: 4.5505
	step 3535:lm_loss: 4.5211, ppl: 91.9376, loss: 4.5507
	step 3536:lm_loss: 4.5213, ppl: 91.9560, loss: 4.5509
	step 3537:lm_loss: 4.5210, ppl: 91.9264, loss: 4.5507
	step 3538:lm_loss: 4.5211, ppl: 91.9381, loss: 4.5508
	step 3539:lm_loss: 4.5213, ppl: 91.9534, loss: 4.5510
	step 3540:lm_loss: 4.5215, ppl: 91.9763, loss: 4.5512
	step 3541:lm_loss: 4.5214, ppl: 91.9678, loss: 4.5511
	step 3542:lm_loss: 4.5218, ppl: 91.9980, loss: 4.5513
	step 3543:lm_loss: 4.5219, ppl: 92.0126, loss: 4.5515
	step 3544:lm_loss: 4.5214, ppl: 91.9607, loss: 4.5510
	step 3545:lm_loss: 4.5215, ppl: 91.9731, loss: 4.5511
	step 3546:lm_loss: 4.5214, ppl: 91.9652, loss: 4.5510
	step 3547:lm_loss: 4.5215, ppl: 91.9753, loss: 4.5511
	step 3548:lm_loss: 4.5216, ppl: 91.9845, loss: 4.5512
	step 3549:lm_loss: 4.5215, ppl: 91.9765, loss: 4.5511
	step 3550:lm_loss: 4.5215, ppl: 91.9724, loss: 4.5511
	step 3551:lm_loss: 4.5213, ppl: 91.9583, loss: 4.5509
	step 3552:lm_loss: 4.5212, ppl: 91.9469, loss: 4.5507
	step 3553:lm_loss: 4.5213, ppl: 91.9505, loss: 4.5508
	step 3554:lm_loss: 4.5213, ppl: 91.9559, loss: 4.5508
	step 3555:lm_loss: 4.5214, ppl: 91.9608, loss: 4.5509
	step 3556:lm_loss: 4.5216, ppl: 91.9797, loss: 4.5511
	step 3557:lm_loss: 4.5220, ppl: 92.0166, loss: 4.5517
	step 3558:lm_loss: 4.5220, ppl: 92.0201, loss: 4.5517
	step 3559:lm_loss: 4.5219, ppl: 92.0124, loss: 4.5516
	step 3560:lm_loss: 4.5219, ppl: 92.0127, loss: 4.5516
	step 3561:lm_loss: 4.5225, ppl: 92.0694, loss: 4.5522
	step 3562:lm_loss: 4.5220, ppl: 92.0191, loss: 4.5519
	step 3563:lm_loss: 4.5220, ppl: 92.0207, loss: 4.5519
	step 3564:lm_loss: 4.5220, ppl: 92.0234, loss: 4.5519
	step 3565:lm_loss: 4.5220, ppl: 92.0238, loss: 4.5519
	step 3566:lm_loss: 4.5221, ppl: 92.0286, loss: 4.5520
	step 3567:lm_loss: 4.5219, ppl: 92.0091, loss: 4.5516
	step 3568:lm_loss: 4.5221, ppl: 92.0266, loss: 4.5517
	step 3569:lm_loss: 4.5220, ppl: 92.0237, loss: 4.5517
	step 3570:lm_loss: 4.5223, ppl: 92.0440, loss: 4.5518
	step 3571:lm_loss: 4.5224, ppl: 92.0522, loss: 4.5519
	step 3572:lm_loss: 4.5225, ppl: 92.0652, loss: 4.5520
	step 3573:lm_loss: 4.5224, ppl: 92.0544, loss: 4.5519
	step 3574:lm_loss: 4.5224, ppl: 92.0567, loss: 4.5520
	step 3575:lm_loss: 4.5225, ppl: 92.0654, loss: 4.5521
	step 3576:lm_loss: 4.5229, ppl: 92.1013, loss: 4.5525
	step 3577:lm_loss: 4.5231, ppl: 92.1229, loss: 4.5527
	step 3578:lm_loss: 4.5233, ppl: 92.1361, loss: 4.5528
	step 3579:lm_loss: 4.5232, ppl: 92.1322, loss: 4.5528
	step 3580:lm_loss: 4.5231, ppl: 92.1218, loss: 4.5527
	step 3581:lm_loss: 4.5231, ppl: 92.1192, loss: 4.5526
	step 3582:lm_loss: 4.5230, ppl: 92.1098, loss: 4.5524
	step 3583:lm_loss: 4.5230, ppl: 92.1161, loss: 4.5525
	step 3584:lm_loss: 4.5230, ppl: 92.1102, loss: 4.5525
	step 3585:lm_loss: 4.5231, ppl: 92.1163, loss: 4.5525
	step 3586:lm_loss: 4.5233, ppl: 92.1403, loss: 4.5528
	step 3587:lm_loss: 4.5232, ppl: 92.1263, loss: 4.5527
	step 3588:lm_loss: 4.5231, ppl: 92.1237, loss: 4.5526
	step 3589:lm_loss: 4.5231, ppl: 92.1229, loss: 4.5526
	step 3590:lm_loss: 4.5230, ppl: 92.1107, loss: 4.5525
	step 3591:lm_loss: 4.5231, ppl: 92.1199, loss: 4.5525
	step 3592:lm_loss: 4.5232, ppl: 92.1345, loss: 4.5527
	step 3593:lm_loss: 4.5231, ppl: 92.1242, loss: 4.5526
	step 3594:lm_loss: 4.5229, ppl: 92.1046, loss: 4.5523
	step 3595:lm_loss: 4.5227, ppl: 92.0841, loss: 4.5520
	step 3596:lm_loss: 4.5228, ppl: 92.0931, loss: 4.5521
	step 3597:lm_loss: 4.5228, ppl: 92.0910, loss: 4.5521
	step 3598:lm_loss: 4.5229, ppl: 92.1041, loss: 4.5522
	step 3599:lm_loss: 4.5227, ppl: 92.0814, loss: 4.5519
	step 3600:lm_loss: 4.5222, ppl: 92.0420, loss: 4.5514
	step 3601:lm_loss: 4.5221, ppl: 92.0320, loss: 4.5513
	step 3602:lm_loss: 4.5224, ppl: 92.0561, loss: 4.5516
	step 3603:lm_loss: 4.5223, ppl: 92.0425, loss: 4.5514
	step 3604:lm_loss: 4.5225, ppl: 92.0667, loss: 4.5516
	step 3605:lm_loss: 4.5227, ppl: 92.0881, loss: 4.5519
	step 3606:lm_loss: 4.5227, ppl: 92.0850, loss: 4.5518
	step 3607:lm_loss: 4.5227, ppl: 92.0824, loss: 4.5518
	step 3608:lm_loss: 4.5229, ppl: 92.1023, loss: 4.5521
	step 3609:lm_loss: 4.5229, ppl: 92.1007, loss: 4.5521
	step 3610:lm_loss: 4.5233, ppl: 92.1346, loss: 4.5526
	step 3611:lm_loss: 4.5236, ppl: 92.1634, loss: 4.5528
	step 3612:lm_loss: 4.5233, ppl: 92.1391, loss: 4.5526
	step 3613:lm_loss: 4.5236, ppl: 92.1702, loss: 4.5528
	step 3614:lm_loss: 4.5237, ppl: 92.1787, loss: 4.5529
	step 3615:lm_loss: 4.5239, ppl: 92.1923, loss: 4.5530
	step 3616:lm_loss: 4.5238, ppl: 92.1823, loss: 4.5530
	step 3617:lm_loss: 4.5237, ppl: 92.1771, loss: 4.5529
	step 3618:lm_loss: 4.5240, ppl: 92.2067, loss: 4.5533
	step 3619:lm_loss: 4.5240, ppl: 92.2009, loss: 4.5532
	step 3620:lm_loss: 4.5239, ppl: 92.1923, loss: 4.5531
	step 3621:lm_loss: 4.5237, ppl: 92.1762, loss: 4.5530
	step 3622:lm_loss: 4.5239, ppl: 92.1990, loss: 4.5533
	step 3623:lm_loss: 4.5240, ppl: 92.2044, loss: 4.5534
	step 3624:lm_loss: 4.5241, ppl: 92.2083, loss: 4.5534
	step 3625:lm_loss: 4.5240, ppl: 92.2016, loss: 4.5533
	step 3626:lm_loss: 4.5245, ppl: 92.2496, loss: 4.5536
	step 3627:lm_loss: 4.5244, ppl: 92.2383, loss: 4.5535
	step 3628:lm_loss: 4.5245, ppl: 92.2482, loss: 4.5535
	step 3629:lm_loss: 4.5243, ppl: 92.2321, loss: 4.5534
	step 3630:lm_loss: 4.5239, ppl: 92.1945, loss: 4.5530
	step 3631:lm_loss: 4.5238, ppl: 92.1890, loss: 4.5529
	step 3632:lm_loss: 4.5239, ppl: 92.1981, loss: 4.5530
	step 3633:lm_loss: 4.5237, ppl: 92.1742, loss: 4.5528
	step 3634:lm_loss: 4.5233, ppl: 92.1432, loss: 4.5524
	step 3635:lm_loss: 4.5234, ppl: 92.1526, loss: 4.5525
	step 3636:lm_loss: 4.5235, ppl: 92.1546, loss: 4.5526
	step 3637:lm_loss: 4.5234, ppl: 92.1501, loss: 4.5525
	step 3638:lm_loss: 4.5233, ppl: 92.1424, loss: 4.5524
	step 3639:lm_loss: 4.5234, ppl: 92.1500, loss: 4.5524
	step 3640:lm_loss: 4.5234, ppl: 92.1448, loss: 4.5524
	step 3641:lm_loss: 4.5233, ppl: 92.1406, loss: 4.5523
	step 3642:lm_loss: 4.5233, ppl: 92.1362, loss: 4.5523
	step 3643:lm_loss: 4.5228, ppl: 92.0922, loss: 4.5521
	step 3644:lm_loss: 4.5228, ppl: 92.0898, loss: 4.5520
	step 3645:lm_loss: 4.5228, ppl: 92.0957, loss: 4.5521
	step 3646:lm_loss: 4.5228, ppl: 92.0972, loss: 4.5521
	step 3647:lm_loss: 4.5230, ppl: 92.1158, loss: 4.5523
	step 3648:lm_loss: 4.5233, ppl: 92.1398, loss: 4.5526
	step 3649:lm_loss: 4.5233, ppl: 92.1413, loss: 4.5526
	step 3650:lm_loss: 4.5233, ppl: 92.1404, loss: 4.5526
	step 3651:lm_loss: 4.5234, ppl: 92.1461, loss: 4.5526
	step 3652:lm_loss: 4.5235, ppl: 92.1618, loss: 4.5528
	step 3653:lm_loss: 4.5234, ppl: 92.1489, loss: 4.5527
	step 3654:lm_loss: 4.5234, ppl: 92.1478, loss: 4.5527
	step 3655:lm_loss: 4.5236, ppl: 92.1666, loss: 4.5529
	step 3656:lm_loss: 4.5233, ppl: 92.1376, loss: 4.5527
	step 3657:lm_loss: 4.5232, ppl: 92.1340, loss: 4.5526
	step 3658:lm_loss: 4.5234, ppl: 92.1453, loss: 4.5527
	step 3659:lm_loss: 4.5235, ppl: 92.1575, loss: 4.5528
	step 3660:lm_loss: 4.5235, ppl: 92.1597, loss: 4.5529
	step 3661:lm_loss: 4.5235, ppl: 92.1599, loss: 4.5529
	step 3662:lm_loss: 4.5234, ppl: 92.1477, loss: 4.5528
	step 3663:lm_loss: 4.5234, ppl: 92.1468, loss: 4.5527
	step 3664:lm_loss: 4.5238, ppl: 92.1819, loss: 4.5533
	step 3665:lm_loss: 4.5233, ppl: 92.1438, loss: 4.5530
	step 3666:lm_loss: 4.5235, ppl: 92.1601, loss: 4.5531
	step 3667:lm_loss: 4.5236, ppl: 92.1645, loss: 4.5532
	step 3668:lm_loss: 4.5231, ppl: 92.1226, loss: 4.5528
	step 3669:lm_loss: 4.5235, ppl: 92.1572, loss: 4.5531
	step 3670:lm_loss: 4.5236, ppl: 92.1702, loss: 4.5532
	step 3671:lm_loss: 4.5239, ppl: 92.1911, loss: 4.5534
	step 3672:lm_loss: 4.5238, ppl: 92.1858, loss: 4.5533
	step 3673:lm_loss: 4.5239, ppl: 92.1910, loss: 4.5534
	step 3674:lm_loss: 4.5239, ppl: 92.1901, loss: 4.5534
	step 3675:lm_loss: 4.5239, ppl: 92.1951, loss: 4.5534
	step 3676:lm_loss: 4.5240, ppl: 92.2021, loss: 4.5535
	step 3677:lm_loss: 4.5241, ppl: 92.2142, loss: 4.5536
	step 3678:lm_loss: 4.5240, ppl: 92.2053, loss: 4.5535
	step 3679:lm_loss: 4.5243, ppl: 92.2327, loss: 4.5538
	step 3680:lm_loss: 4.5237, ppl: 92.1720, loss: 4.5535
	step 3681:lm_loss: 4.5238, ppl: 92.1867, loss: 4.5536
	step 3682:lm_loss: 4.5239, ppl: 92.1933, loss: 4.5537
	step 3683:lm_loss: 4.5239, ppl: 92.1934, loss: 4.5537
	step 3684:lm_loss: 4.5240, ppl: 92.2056, loss: 4.5538
	step 3685:lm_loss: 4.5245, ppl: 92.2541, loss: 4.5541
	step 3686:lm_loss: 4.5248, ppl: 92.2764, loss: 4.5544
	step 3687:lm_loss: 4.5248, ppl: 92.2812, loss: 4.5544
	step 3688:lm_loss: 4.5250, ppl: 92.2950, loss: 4.5546
	step 3689:lm_loss: 4.5248, ppl: 92.2817, loss: 4.5544
	step 3690:lm_loss: 4.5250, ppl: 92.2936, loss: 4.5545
	step 3691:lm_loss: 4.5253, ppl: 92.3226, loss: 4.5548
	step 3692:lm_loss: 4.5253, ppl: 92.3246, loss: 4.5548
	step 3693:lm_loss: 4.5253, ppl: 92.3280, loss: 4.5548
	step 3694:lm_loss: 4.5256, ppl: 92.3494, loss: 4.5550
	step 3695:lm_loss: 4.5254, ppl: 92.3350, loss: 4.5548
	step 3696:lm_loss: 4.5255, ppl: 92.3400, loss: 4.5549
	step 3697:lm_loss: 4.5256, ppl: 92.3549, loss: 4.5550
	step 3698:lm_loss: 4.5257, ppl: 92.3641, loss: 4.5551
	step 3699:lm_loss: 4.5253, ppl: 92.3206, loss: 4.5546
	step 3700:lm_loss: 4.5252, ppl: 92.3126, loss: 4.5545
	step 3701:lm_loss: 4.5252, ppl: 92.3151, loss: 4.5546
	step 3702:lm_loss: 4.5251, ppl: 92.3084, loss: 4.5545
	step 3703:lm_loss: 4.5251, ppl: 92.3078, loss: 4.5545
	step 3704:lm_loss: 4.5250, ppl: 92.2982, loss: 4.5543
	step 3705:lm_loss: 4.5247, ppl: 92.2686, loss: 4.5541
	step 3706:lm_loss: 4.5248, ppl: 92.2801, loss: 4.5543
	step 3707:lm_loss: 4.5250, ppl: 92.2970, loss: 4.5545
	step 3708:lm_loss: 4.5252, ppl: 92.3112, loss: 4.5547
	step 3709:lm_loss: 4.5249, ppl: 92.2912, loss: 4.5543
	step 3710:lm_loss: 4.5249, ppl: 92.2887, loss: 4.5543
	step 3711:lm_loss: 4.5252, ppl: 92.3188, loss: 4.5545
	step 3712:lm_loss: 4.5251, ppl: 92.3096, loss: 4.5544
	step 3713:lm_loss: 4.5249, ppl: 92.2890, loss: 4.5542
	step 3714:lm_loss: 4.5246, ppl: 92.2564, loss: 4.5539
	step 3715:lm_loss: 4.5245, ppl: 92.2514, loss: 4.5538
	step 3716:lm_loss: 4.5244, ppl: 92.2441, loss: 4.5538
	step 3717:lm_loss: 4.5245, ppl: 92.2536, loss: 4.5538
	step 3718:lm_loss: 4.5243, ppl: 92.2350, loss: 4.5536
	step 3719:lm_loss: 4.5241, ppl: 92.2130, loss: 4.5534
	step 3720:lm_loss: 4.5237, ppl: 92.1747, loss: 4.5531
	step 3721:lm_loss: 4.5235, ppl: 92.1551, loss: 4.5529
	step 3722:lm_loss: 4.5236, ppl: 92.1664, loss: 4.5530
	step 3723:lm_loss: 4.5238, ppl: 92.1808, loss: 4.5532
	step 3724:lm_loss: 4.5240, ppl: 92.1998, loss: 4.5534
	step 3725:lm_loss: 4.5238, ppl: 92.1894, loss: 4.5533
	step 3726:lm_loss: 4.5233, ppl: 92.1407, loss: 4.5529
	step 3727:lm_loss: 4.5233, ppl: 92.1433, loss: 4.5530
	step 3728:lm_loss: 4.5238, ppl: 92.1855, loss: 4.5535
	step 3729:lm_loss: 4.5239, ppl: 92.1900, loss: 4.5536
	step 3730:lm_loss: 4.5236, ppl: 92.1691, loss: 4.5532
	step 3731:lm_loss: 4.5236, ppl: 92.1646, loss: 4.5531
	step 3732:lm_loss: 4.5236, ppl: 92.1625, loss: 4.5531
	step 3733:lm_loss: 4.5238, ppl: 92.1831, loss: 4.5533
	step 3734:lm_loss: 4.5239, ppl: 92.1901, loss: 4.5534
	step 3735:lm_loss: 4.5239, ppl: 92.1908, loss: 4.5534
	step 3736:lm_loss: 4.5237, ppl: 92.1799, loss: 4.5532
	step 3737:lm_loss: 4.5239, ppl: 92.1952, loss: 4.5534
	step 3738:lm_loss: 4.5239, ppl: 92.1910, loss: 4.5533
	step 3739:lm_loss: 4.5240, ppl: 92.2065, loss: 4.5536
	step 3740:lm_loss: 4.5237, ppl: 92.1730, loss: 4.5533
	step 3741:lm_loss: 4.5235, ppl: 92.1569, loss: 4.5532
	step 3742:lm_loss: 4.5236, ppl: 92.1703, loss: 4.5533
	step 3743:lm_loss: 4.5238, ppl: 92.1839, loss: 4.5535
	step 3744:lm_loss: 4.5239, ppl: 92.1942, loss: 4.5536
	step 3745:lm_loss: 4.5237, ppl: 92.1733, loss: 4.5533
	step 3746:lm_loss: 4.5236, ppl: 92.1666, loss: 4.5532
	step 3747:lm_loss: 4.5236, ppl: 92.1623, loss: 4.5531
	step 3748:lm_loss: 4.5235, ppl: 92.1614, loss: 4.5531
	step 3749:lm_loss: 4.5238, ppl: 92.1820, loss: 4.5533
	step 3750:lm_loss: 4.5238, ppl: 92.1817, loss: 4.5533
	step 3751:lm_loss: 4.5238, ppl: 92.1822, loss: 4.5533
	step 3752:lm_loss: 4.5237, ppl: 92.1774, loss: 4.5533
	step 3753:lm_loss: 4.5238, ppl: 92.1825, loss: 4.5533
	step 3754:lm_loss: 4.5240, ppl: 92.2011, loss: 4.5535
	step 3755:lm_loss: 4.5240, ppl: 92.2014, loss: 4.5535
	step 3756:lm_loss: 4.5237, ppl: 92.1793, loss: 4.5531
	step 3757:lm_loss: 4.5240, ppl: 92.2064, loss: 4.5533
	step 3758:lm_loss: 4.5241, ppl: 92.2152, loss: 4.5534
	step 3759:lm_loss: 4.5241, ppl: 92.2088, loss: 4.5533
	step 3760:lm_loss: 4.5239, ppl: 92.1945, loss: 4.5532
	step 3761:lm_loss: 4.5238, ppl: 92.1882, loss: 4.5532
	step 3762:lm_loss: 4.5238, ppl: 92.1814, loss: 4.5531
	step 3763:lm_loss: 4.5238, ppl: 92.1887, loss: 4.5532
	step 3764:lm_loss: 4.5236, ppl: 92.1625, loss: 4.5528
	step 3765:lm_loss: 4.5235, ppl: 92.1601, loss: 4.5528
	step 3766:lm_loss: 4.5238, ppl: 92.1828, loss: 4.5531
	step 3767:lm_loss: 4.5237, ppl: 92.1726, loss: 4.5531
	step 3768:lm_loss: 4.5236, ppl: 92.1689, loss: 4.5530
	step 3769:lm_loss: 4.5235, ppl: 92.1543, loss: 4.5529
	step 3770:lm_loss: 4.5232, ppl: 92.1297, loss: 4.5525
	step 3771:lm_loss: 4.5235, ppl: 92.1531, loss: 4.5528
	step 3772:lm_loss: 4.5234, ppl: 92.1465, loss: 4.5527
	step 3773:lm_loss: 4.5233, ppl: 92.1389, loss: 4.5526
	step 3774:lm_loss: 4.5234, ppl: 92.1524, loss: 4.5527
	step 3775:lm_loss: 4.5234, ppl: 92.1438, loss: 4.5526
	step 3776:lm_loss: 4.5234, ppl: 92.1513, loss: 4.5526
	step 3777:lm_loss: 4.5235, ppl: 92.1542, loss: 4.5527
	step 3778:lm_loss: 4.5234, ppl: 92.1467, loss: 4.5526
	step 3779:lm_loss: 4.5235, ppl: 92.1559, loss: 4.5527
	step 3780:lm_loss: 4.5235, ppl: 92.1575, loss: 4.5527
	step 3781:lm_loss: 4.5234, ppl: 92.1452, loss: 4.5525
	step 3782:lm_loss: 4.5236, ppl: 92.1653, loss: 4.5528
	step 3783:lm_loss: 4.5237, ppl: 92.1796, loss: 4.5530
	step 3784:lm_loss: 4.5239, ppl: 92.1976, loss: 4.5531
	step 3785:lm_loss: 4.5235, ppl: 92.1571, loss: 4.5528
	step 3786:lm_loss: 4.5235, ppl: 92.1605, loss: 4.5528
	step 3787:lm_loss: 4.5235, ppl: 92.1611, loss: 4.5528
	step 3788:lm_loss: 4.5236, ppl: 92.1679, loss: 4.5528
	step 3789:lm_loss: 4.5238, ppl: 92.1885, loss: 4.5530
	step 3790:lm_loss: 4.5241, ppl: 92.2153, loss: 4.5533
	step 3791:lm_loss: 4.5243, ppl: 92.2274, loss: 4.5534
	step 3792:lm_loss: 4.5245, ppl: 92.2476, loss: 4.5535
	step 3793:lm_loss: 4.5243, ppl: 92.2357, loss: 4.5534
	step 3794:lm_loss: 4.5245, ppl: 92.2473, loss: 4.5536
	step 3795:lm_loss: 4.5245, ppl: 92.2470, loss: 4.5536
	step 3796:lm_loss: 4.5242, ppl: 92.2191, loss: 4.5533
	step 3797:lm_loss: 4.5243, ppl: 92.2287, loss: 4.5534
	step 3798:lm_loss: 4.5244, ppl: 92.2386, loss: 4.5535
	step 3799:lm_loss: 4.5244, ppl: 92.2379, loss: 4.5535
	step 3800:lm_loss: 4.5243, ppl: 92.2312, loss: 4.5535
	step 3801:lm_loss: 4.5241, ppl: 92.2083, loss: 4.5533
	step 3802:lm_loss: 4.5240, ppl: 92.2046, loss: 4.5532
	step 3803:lm_loss: 4.5242, ppl: 92.2230, loss: 4.5534
	step 3804:lm_loss: 4.5241, ppl: 92.2100, loss: 4.5532
	step 3805:lm_loss: 4.5240, ppl: 92.2059, loss: 4.5532
	step 3806:lm_loss: 4.5240, ppl: 92.2038, loss: 4.5531
	step 3807:lm_loss: 4.5240, ppl: 92.2071, loss: 4.5532
	step 3808:lm_loss: 4.5237, ppl: 92.1795, loss: 4.5527
	step 3809:lm_loss: 4.5239, ppl: 92.1930, loss: 4.5529
	step 3810:lm_loss: 4.5239, ppl: 92.1911, loss: 4.5529
	step 3811:lm_loss: 4.5240, ppl: 92.2009, loss: 4.5530
	step 3812:lm_loss: 4.5238, ppl: 92.1826, loss: 4.5527
	step 3813:lm_loss: 4.5235, ppl: 92.1565, loss: 4.5525
	step 3814:lm_loss: 4.5236, ppl: 92.1665, loss: 4.5526
	step 3815:lm_loss: 4.5233, ppl: 92.1358, loss: 4.5523
	step 3816:lm_loss: 4.5233, ppl: 92.1385, loss: 4.5523
	step 3817:lm_loss: 4.5233, ppl: 92.1413, loss: 4.5523
	step 3818:lm_loss: 4.5232, ppl: 92.1291, loss: 4.5522
	step 3819:lm_loss: 4.5231, ppl: 92.1229, loss: 4.5521
	step 3820:lm_loss: 4.5230, ppl: 92.1146, loss: 4.5520
	step 3821:lm_loss: 4.5231, ppl: 92.1244, loss: 4.5521
	step 3822:lm_loss: 4.5234, ppl: 92.1498, loss: 4.5523
	step 3823:lm_loss: 4.5231, ppl: 92.1186, loss: 4.5521
	step 3824:lm_loss: 4.5230, ppl: 92.1139, loss: 4.5520
	step 3825:lm_loss: 4.5229, ppl: 92.0984, loss: 4.5519
	step 3826:lm_loss: 4.5229, ppl: 92.1008, loss: 4.5519
	step 3827:lm_loss: 4.5231, ppl: 92.1172, loss: 4.5521
	step 3828:lm_loss: 4.5232, ppl: 92.1289, loss: 4.5522
	step 3829:lm_loss: 4.5231, ppl: 92.1200, loss: 4.5521
	step 3830:lm_loss: 4.5230, ppl: 92.1109, loss: 4.5520
	step 3831:lm_loss: 4.5230, ppl: 92.1145, loss: 4.5521
	step 3832:lm_loss: 4.5231, ppl: 92.1253, loss: 4.5521
	step 3833:lm_loss: 4.5232, ppl: 92.1319, loss: 4.5522
	step 3834:lm_loss: 4.5233, ppl: 92.1348, loss: 4.5522
	step 3835:lm_loss: 4.5233, ppl: 92.1368, loss: 4.5522
	step 3836:lm_loss: 4.5231, ppl: 92.1211, loss: 4.5520
	step 3837:lm_loss: 4.5230, ppl: 92.1090, loss: 4.5519
	step 3838:lm_loss: 4.5226, ppl: 92.0780, loss: 4.5518
	step 3839:lm_loss: 4.5228, ppl: 92.0925, loss: 4.5519
	step 3840:lm_loss: 4.5230, ppl: 92.1103, loss: 4.5521
	step 3841:lm_loss: 4.5229, ppl: 92.1055, loss: 4.5520
	step 3842:lm_loss: 4.5229, ppl: 92.1006, loss: 4.5520
	step 3843:lm_loss: 4.5229, ppl: 92.0983, loss: 4.5519
	step 3844:lm_loss: 4.5232, ppl: 92.1261, loss: 4.5523
	step 3845:lm_loss: 4.5233, ppl: 92.1373, loss: 4.5523
	step 3846:lm_loss: 4.5231, ppl: 92.1227, loss: 4.5522
	step 3847:lm_loss: 4.5233, ppl: 92.1373, loss: 4.5524
	step 3848:lm_loss: 4.5234, ppl: 92.1491, loss: 4.5525
	step 3849:lm_loss: 4.5240, ppl: 92.2037, loss: 4.5532
	step 3850:lm_loss: 4.5240, ppl: 92.2050, loss: 4.5532
	step 3851:lm_loss: 4.5242, ppl: 92.2207, loss: 4.5533
	step 3852:lm_loss: 4.5242, ppl: 92.2225, loss: 4.5533
	step 3853:lm_loss: 4.5243, ppl: 92.2353, loss: 4.5535
	step 3854:lm_loss: 4.5243, ppl: 92.2299, loss: 4.5534
	step 3855:lm_loss: 4.5243, ppl: 92.2305, loss: 4.5534
	step 3856:lm_loss: 4.5243, ppl: 92.2336, loss: 4.5535
	step 3857:lm_loss: 4.5244, ppl: 92.2400, loss: 4.5535
	step 3858:lm_loss: 4.5238, ppl: 92.1875, loss: 4.5531
	step 3859:lm_loss: 4.5240, ppl: 92.2001, loss: 4.5532
	step 3860:lm_loss: 4.5240, ppl: 92.2055, loss: 4.5532
	step 3861:lm_loss: 4.5238, ppl: 92.1830, loss: 4.5529
	step 3862:lm_loss: 4.5239, ppl: 92.1960, loss: 4.5531
	step 3863:lm_loss: 4.5238, ppl: 92.1882, loss: 4.5530
	step 3864:lm_loss: 4.5239, ppl: 92.1974, loss: 4.5531
	step 3865:lm_loss: 4.5241, ppl: 92.2091, loss: 4.5532
	step 3866:lm_loss: 4.5238, ppl: 92.1898, loss: 4.5529
	step 3867:lm_loss: 4.5238, ppl: 92.1882, loss: 4.5529
	step 3868:lm_loss: 4.5239, ppl: 92.1948, loss: 4.5530
	step 3869:lm_loss: 4.5240, ppl: 92.2009, loss: 4.5530
	step 3870:lm_loss: 4.5240, ppl: 92.2073, loss: 4.5531
	step 3871:lm_loss: 4.5241, ppl: 92.2106, loss: 4.5531
	step 3872:lm_loss: 4.5241, ppl: 92.2154, loss: 4.5532
	step 3873:lm_loss: 4.5242, ppl: 92.2216, loss: 4.5533
	step 3874:lm_loss: 4.5245, ppl: 92.2468, loss: 4.5535
	step 3875:lm_loss: 4.5248, ppl: 92.2764, loss: 4.5539
	step 3876:lm_loss: 4.5247, ppl: 92.2689, loss: 4.5538
	step 3877:lm_loss: 4.5247, ppl: 92.2711, loss: 4.5538
	step 3878:lm_loss: 4.5248, ppl: 92.2805, loss: 4.5539
	step 3879:lm_loss: 4.5243, ppl: 92.2349, loss: 4.5536
	step 3880:lm_loss: 4.5246, ppl: 92.2606, loss: 4.5539
	step 3881:lm_loss: 4.5245, ppl: 92.2525, loss: 4.5538
	step 3882:lm_loss: 4.5247, ppl: 92.2714, loss: 4.5541
	step 3883:lm_loss: 4.5243, ppl: 92.2353, loss: 4.5539
	step 3884:lm_loss: 4.5244, ppl: 92.2360, loss: 4.5539
	step 3885:lm_loss: 4.5244, ppl: 92.2420, loss: 4.5540
	step 3886:lm_loss: 4.5243, ppl: 92.2276, loss: 4.5538
	step 3887:lm_loss: 4.5244, ppl: 92.2417, loss: 4.5539
	step 3888:lm_loss: 4.5243, ppl: 92.2321, loss: 4.5538
	step 3889:lm_loss: 4.5243, ppl: 92.2327, loss: 4.5538
	step 3890:lm_loss: 4.5243, ppl: 92.2353, loss: 4.5539
	step 3891:lm_loss: 4.5244, ppl: 92.2361, loss: 4.5539
	step 3892:lm_loss: 4.5247, ppl: 92.2651, loss: 4.5541
	step 3893:lm_loss: 4.5247, ppl: 92.2650, loss: 4.5541
	step 3894:lm_loss: 4.5246, ppl: 92.2603, loss: 4.5541
	step 3895:lm_loss: 4.5246, ppl: 92.2626, loss: 4.5541
	step 3896:lm_loss: 4.5247, ppl: 92.2703, loss: 4.5541
	step 3897:lm_loss: 4.5246, ppl: 92.2601, loss: 4.5540
	step 3898:lm_loss: 4.5248, ppl: 92.2762, loss: 4.5541
	step 3899:lm_loss: 4.5248, ppl: 92.2781, loss: 4.5541
	step 3900:lm_loss: 4.5250, ppl: 92.2953, loss: 4.5543
	step 3901:lm_loss: 4.5254, ppl: 92.3314, loss: 4.5545
	step 3902:lm_loss: 4.5255, ppl: 92.3418, loss: 4.5546
	step 3903:lm_loss: 4.5254, ppl: 92.3307, loss: 4.5545
	step 3904:lm_loss: 4.5256, ppl: 92.3550, loss: 4.5547
	step 3905:lm_loss: 4.5255, ppl: 92.3432, loss: 4.5545
	step 3906:lm_loss: 4.5255, ppl: 92.3438, loss: 4.5545
	step 3907:lm_loss: 4.5253, ppl: 92.3202, loss: 4.5542
	step 3908:lm_loss: 4.5255, ppl: 92.3417, loss: 4.5544
	step 3909:lm_loss: 4.5255, ppl: 92.3422, loss: 4.5544
	step 3910:lm_loss: 4.5258, ppl: 92.3721, loss: 4.5547
	step 3911:lm_loss: 4.5258, ppl: 92.3744, loss: 4.5547
	step 3912:lm_loss: 4.5260, ppl: 92.3851, loss: 4.5548
	step 3913:lm_loss: 4.5260, ppl: 92.3882, loss: 4.5549
	step 3914:lm_loss: 4.5260, ppl: 92.3884, loss: 4.5549
	step 3915:lm_loss: 4.5262, ppl: 92.4037, loss: 4.5550
	step 3916:lm_loss: 4.5262, ppl: 92.4080, loss: 4.5551
	step 3917:lm_loss: 4.5259, ppl: 92.3826, loss: 4.5548
	step 3918:lm_loss: 4.5260, ppl: 92.3871, loss: 4.5549
	step 3919:lm_loss: 4.5260, ppl: 92.3851, loss: 4.5548
	step 3920:lm_loss: 4.5261, ppl: 92.3930, loss: 4.5549
	step 3921:lm_loss: 4.5260, ppl: 92.3924, loss: 4.5549
	step 3922:lm_loss: 4.5262, ppl: 92.4111, loss: 4.5551
	step 3923:lm_loss: 4.5262, ppl: 92.4065, loss: 4.5550
	step 3924:lm_loss: 4.5261, ppl: 92.3958, loss: 4.5549
	step 3925:lm_loss: 4.5259, ppl: 92.3811, loss: 4.5547
	step 3926:lm_loss: 4.5259, ppl: 92.3764, loss: 4.5547
	step 3927:lm_loss: 4.5258, ppl: 92.3656, loss: 4.5546
	step 3928:lm_loss: 4.5261, ppl: 92.4008, loss: 4.5550
	step 3929:lm_loss: 4.5260, ppl: 92.3862, loss: 4.5549
	step 3930:lm_loss: 4.5260, ppl: 92.3889, loss: 4.5549
	step 3931:lm_loss: 4.5260, ppl: 92.3874, loss: 4.5549
	step 3932:lm_loss: 4.5258, ppl: 92.3731, loss: 4.5547
	step 3933:lm_loss: 4.5258, ppl: 92.3694, loss: 4.5546
	step 3934:lm_loss: 4.5258, ppl: 92.3727, loss: 4.5546
	step 3935:lm_loss: 4.5258, ppl: 92.3699, loss: 4.5546
	step 3936:lm_loss: 4.5255, ppl: 92.3462, loss: 4.5544
	step 3937:lm_loss: 4.5258, ppl: 92.3663, loss: 4.5547
	step 3938:lm_loss: 4.5258, ppl: 92.3678, loss: 4.5547
	step 3939:lm_loss: 4.5260, ppl: 92.3864, loss: 4.5548
	step 3940:lm_loss: 4.5259, ppl: 92.3822, loss: 4.5548
	step 3941:lm_loss: 4.5258, ppl: 92.3735, loss: 4.5547
	step 3942:lm_loss: 4.5256, ppl: 92.3538, loss: 4.5546
	step 3943:lm_loss: 4.5255, ppl: 92.3394, loss: 4.5544
	step 3944:lm_loss: 4.5255, ppl: 92.3461, loss: 4.5545
	step 3945:lm_loss: 4.5254, ppl: 92.3331, loss: 4.5543
	step 3946:lm_loss: 4.5254, ppl: 92.3328, loss: 4.5543
	step 3947:lm_loss: 4.5258, ppl: 92.3687, loss: 4.5548
	step 3948:lm_loss: 4.5257, ppl: 92.3637, loss: 4.5547
	step 3949:lm_loss: 4.5257, ppl: 92.3640, loss: 4.5547
	step 3950:lm_loss: 4.5258, ppl: 92.3729, loss: 4.5548
	step 3951:lm_loss: 4.5263, ppl: 92.4128, loss: 4.5554
	step 3952:lm_loss: 4.5263, ppl: 92.4118, loss: 4.5554
	step 3953:lm_loss: 4.5257, ppl: 92.3633, loss: 4.5550
	step 3954:lm_loss: 4.5258, ppl: 92.3704, loss: 4.5550
	step 3955:lm_loss: 4.5260, ppl: 92.3872, loss: 4.5552
	step 3956:lm_loss: 4.5255, ppl: 92.3455, loss: 4.5547
	step 3957:lm_loss: 4.5258, ppl: 92.3664, loss: 4.5549
	step 3958:lm_loss: 4.5258, ppl: 92.3662, loss: 4.5549
	step 3959:lm_loss: 4.5258, ppl: 92.3731, loss: 4.5549
	step 3960:lm_loss: 4.5258, ppl: 92.3712, loss: 4.5549
	step 3961:lm_loss: 4.5261, ppl: 92.4007, loss: 4.5552
	step 3962:lm_loss: 4.5262, ppl: 92.4084, loss: 4.5553
	step 3963:lm_loss: 4.5261, ppl: 92.3970, loss: 4.5551
	step 3964:lm_loss: 4.5259, ppl: 92.3836, loss: 4.5550
	step 3965:lm_loss: 4.5258, ppl: 92.3690, loss: 4.5548
	step 3966:lm_loss: 4.5256, ppl: 92.3539, loss: 4.5546
	step 3967:lm_loss: 4.5260, ppl: 92.3857, loss: 4.5550
	step 3968:lm_loss: 4.5264, ppl: 92.4284, loss: 4.5554
	step 3969:lm_loss: 4.5265, ppl: 92.4376, loss: 4.5555
	step 3970:lm_loss: 4.5266, ppl: 92.4396, loss: 4.5555
	step 3971:lm_loss: 4.5266, ppl: 92.4428, loss: 4.5555
	step 3972:lm_loss: 4.5266, ppl: 92.4420, loss: 4.5555
	step 3973:lm_loss: 4.5268, ppl: 92.4593, loss: 4.5557
	step 3974:lm_loss: 4.5269, ppl: 92.4683, loss: 4.5558
	step 3975:lm_loss: 4.5268, ppl: 92.4612, loss: 4.5557
	step 3976:lm_loss: 4.5273, ppl: 92.5095, loss: 4.5560
	step 3977:lm_loss: 4.5272, ppl: 92.4972, loss: 4.5559
	step 3978:lm_loss: 4.5271, ppl: 92.4906, loss: 4.5558
	step 3979:lm_loss: 4.5270, ppl: 92.4848, loss: 4.5557
	step 3980:lm_loss: 4.5272, ppl: 92.4962, loss: 4.5559
	step 3981:lm_loss: 4.5272, ppl: 92.4987, loss: 4.5559
	step 3982:lm_loss: 4.5272, ppl: 92.4994, loss: 4.5559
	step 3983:lm_loss: 4.5270, ppl: 92.4817, loss: 4.5557
	step 3984:lm_loss: 4.5270, ppl: 92.4814, loss: 4.5557
	step 3985:lm_loss: 4.5271, ppl: 92.4934, loss: 4.5558
	step 3986:lm_loss: 4.5270, ppl: 92.4813, loss: 4.5556
	step 3987:lm_loss: 4.5270, ppl: 92.4796, loss: 4.5556
	step 3988:lm_loss: 4.5270, ppl: 92.4847, loss: 4.5556
	step 3989:lm_loss: 4.5274, ppl: 92.5191, loss: 4.5560
	step 3990:lm_loss: 4.5270, ppl: 92.4773, loss: 4.5557
	step 3991:lm_loss: 4.5266, ppl: 92.4414, loss: 4.5553
	step 3992:lm_loss: 4.5264, ppl: 92.4239, loss: 4.5549
	step 3993:lm_loss: 4.5259, ppl: 92.3794, loss: 4.5543
	step 3994:lm_loss: 4.5259, ppl: 92.3770, loss: 4.5543
	step 3995:lm_loss: 4.5260, ppl: 92.3917, loss: 4.5544
	step 3996:lm_loss: 4.5257, ppl: 92.3631, loss: 4.5542
	step 3997:lm_loss: 4.5257, ppl: 92.3637, loss: 4.5542
	step 3998:lm_loss: 4.5257, ppl: 92.3643, loss: 4.5542
	step 3999:lm_loss: 4.5252, ppl: 92.3131, loss: 4.5540
	step 4000:lm_loss: 4.5251, ppl: 92.3042, loss: 4.5539
	step 4001:lm_loss: 4.5249, ppl: 92.2878, loss: 4.5536
	step 4002:lm_loss: 4.5246, ppl: 92.2608, loss: 4.5534
	step 4003:lm_loss: 4.5246, ppl: 92.2604, loss: 4.5534
	step 4004:lm_loss: 4.5248, ppl: 92.2750, loss: 4.5536
	step 4005:lm_loss: 4.5250, ppl: 92.2934, loss: 4.5538
	step 4006:lm_loss: 4.5249, ppl: 92.2891, loss: 4.5537
	step 4007:lm_loss: 4.5248, ppl: 92.2766, loss: 4.5536
	step 4008:lm_loss: 4.5246, ppl: 92.2602, loss: 4.5534
	step 4009:lm_loss: 4.5244, ppl: 92.2446, loss: 4.5531
	step 4010:lm_loss: 4.5241, ppl: 92.2100, loss: 4.5529
	step 4011:lm_loss: 4.5241, ppl: 92.2155, loss: 4.5530
	step 4012:lm_loss: 4.5244, ppl: 92.2438, loss: 4.5532
	step 4013:lm_loss: 4.5244, ppl: 92.2432, loss: 4.5531
	step 4014:lm_loss: 4.5244, ppl: 92.2444, loss: 4.5531
	step 4015:lm_loss: 4.5240, ppl: 92.2065, loss: 4.5529
	step 4016:lm_loss: 4.5240, ppl: 92.2055, loss: 4.5529
	step 4017:lm_loss: 4.5238, ppl: 92.1853, loss: 4.5526
	step 4018:lm_loss: 4.5237, ppl: 92.1803, loss: 4.5525
	step 4019:lm_loss: 4.5239, ppl: 92.1910, loss: 4.5526
	step 4020:lm_loss: 4.5239, ppl: 92.1922, loss: 4.5526
	step 4021:lm_loss: 4.5240, ppl: 92.2049, loss: 4.5528
	step 4022:lm_loss: 4.5244, ppl: 92.2452, loss: 4.5530
	step 4023:lm_loss: 4.5245, ppl: 92.2513, loss: 4.5531
	step 4024:lm_loss: 4.5243, ppl: 92.2295, loss: 4.5528
	step 4025:lm_loss: 4.5244, ppl: 92.2404, loss: 4.5529
	step 4026:lm_loss: 4.5245, ppl: 92.2454, loss: 4.5530
	step 4027:lm_loss: 4.5244, ppl: 92.2383, loss: 4.5529
	step 4028:lm_loss: 4.5243, ppl: 92.2317, loss: 4.5528
	step 4029:lm_loss: 4.5241, ppl: 92.2157, loss: 4.5526
	step 4030:lm_loss: 4.5244, ppl: 92.2413, loss: 4.5529
	step 4031:lm_loss: 4.5243, ppl: 92.2330, loss: 4.5528
	step 4032:lm_loss: 4.5244, ppl: 92.2442, loss: 4.5529
	step 4033:lm_loss: 4.5245, ppl: 92.2475, loss: 4.5529
	step 4034:lm_loss: 4.5243, ppl: 92.2333, loss: 4.5527
	step 4035:lm_loss: 4.5246, ppl: 92.2595, loss: 4.5530
	step 4036:lm_loss: 4.5246, ppl: 92.2603, loss: 4.5530
	step 4037:lm_loss: 4.5247, ppl: 92.2658, loss: 4.5530
	step 4038:lm_loss: 4.5250, ppl: 92.2955, loss: 4.5533
	step 4039:lm_loss: 4.5249, ppl: 92.2855, loss: 4.5532
	step 4040:lm_loss: 4.5247, ppl: 92.2700, loss: 4.5531
	step 4041:lm_loss: 4.5245, ppl: 92.2469, loss: 4.5529
	step 4042:lm_loss: 4.5242, ppl: 92.2231, loss: 4.5526
	step 4043:lm_loss: 4.5234, ppl: 92.1477, loss: 4.5523
	step 4044:lm_loss: 4.5233, ppl: 92.1378, loss: 4.5521
	step 4045:lm_loss: 4.5233, ppl: 92.1370, loss: 4.5521
	step 4046:lm_loss: 4.5231, ppl: 92.1239, loss: 4.5520
	step 4047:lm_loss: 4.5234, ppl: 92.1511, loss: 4.5523
	step 4048:lm_loss: 4.5234, ppl: 92.1460, loss: 4.5522
	step 4049:lm_loss: 4.5236, ppl: 92.1647, loss: 4.5523
	step 4050:lm_loss: 4.5234, ppl: 92.1511, loss: 4.5521
	step 4051:lm_loss: 4.5234, ppl: 92.1448, loss: 4.5520
	step 4052:lm_loss: 4.5236, ppl: 92.1683, loss: 4.5525
	step 4053:lm_loss: 4.5235, ppl: 92.1603, loss: 4.5524
	step 4054:lm_loss: 4.5234, ppl: 92.1528, loss: 4.5524
	step 4055:lm_loss: 4.5234, ppl: 92.1498, loss: 4.5523
	step 4056:lm_loss: 4.5233, ppl: 92.1399, loss: 4.5522
	step 4057:lm_loss: 4.5235, ppl: 92.1597, loss: 4.5523
	step 4058:lm_loss: 4.5234, ppl: 92.1490, loss: 4.5522
	step 4059:lm_loss: 4.5236, ppl: 92.1625, loss: 4.5524
	step 4060:lm_loss: 4.5235, ppl: 92.1575, loss: 4.5523
	step 4061:lm_loss: 4.5228, ppl: 92.0964, loss: 4.5518
	step 4062:lm_loss: 4.5230, ppl: 92.1130, loss: 4.5519
	step 4063:lm_loss: 4.5232, ppl: 92.1279, loss: 4.5521
	step 4064:lm_loss: 4.5234, ppl: 92.1521, loss: 4.5524
	step 4065:lm_loss: 4.5234, ppl: 92.1463, loss: 4.5523
	step 4066:lm_loss: 4.5232, ppl: 92.1339, loss: 4.5521
	step 4067:lm_loss: 4.5230, ppl: 92.1136, loss: 4.5518
	step 4068:lm_loss: 4.5231, ppl: 92.1172, loss: 4.5519
	step 4069:lm_loss: 4.5232, ppl: 92.1298, loss: 4.5520
	step 4070:lm_loss: 4.5228, ppl: 92.0939, loss: 4.5516
	step 4071:lm_loss: 4.5231, ppl: 92.1246, loss: 4.5521
	step 4072:lm_loss: 4.5234, ppl: 92.1453, loss: 4.5523
	step 4073:lm_loss: 4.5235, ppl: 92.1567, loss: 4.5524
	step 4074:lm_loss: 4.5234, ppl: 92.1488, loss: 4.5523
	step 4075:lm_loss: 4.5235, ppl: 92.1548, loss: 4.5523
	step 4076:lm_loss: 4.5235, ppl: 92.1567, loss: 4.5523
	step 4077:lm_loss: 4.5238, ppl: 92.1829, loss: 4.5525
	step 4078:lm_loss: 4.5237, ppl: 92.1717, loss: 4.5524
	step 4079:lm_loss: 4.5233, ppl: 92.1435, loss: 4.5522
	step 4080:lm_loss: 4.5232, ppl: 92.1273, loss: 4.5520
	step 4081:lm_loss: 4.5229, ppl: 92.1062, loss: 4.5519
	step 4082:lm_loss: 4.5228, ppl: 92.0966, loss: 4.5518
	step 4083:lm_loss: 4.5227, ppl: 92.0874, loss: 4.5517
	step 4084:lm_loss: 4.5230, ppl: 92.1090, loss: 4.5519
	step 4085:lm_loss: 4.5230, ppl: 92.1117, loss: 4.5519
	step 4086:lm_loss: 4.5230, ppl: 92.1145, loss: 4.5519
	step 4087:lm_loss: 4.5232, ppl: 92.1303, loss: 4.5522
	step 4088:lm_loss: 4.5233, ppl: 92.1402, loss: 4.5523
	step 4089:lm_loss: 4.5235, ppl: 92.1585, loss: 4.5524
	step 4090:lm_loss: 4.5235, ppl: 92.1539, loss: 4.5524
	step 4091:lm_loss: 4.5237, ppl: 92.1766, loss: 4.5526
	step 4092:lm_loss: 4.5237, ppl: 92.1775, loss: 4.5526
	step 4093:lm_loss: 4.5237, ppl: 92.1778, loss: 4.5526
	step 4094:lm_loss: 4.5240, ppl: 92.2002, loss: 4.5529
	step 4095:lm_loss: 4.5241, ppl: 92.2103, loss: 4.5530
	step 4096:lm_loss: 4.5240, ppl: 92.2059, loss: 4.5529
	step 4097:lm_loss: 4.5241, ppl: 92.2127, loss: 4.5530
	step 4098:lm_loss: 4.5241, ppl: 92.2150, loss: 4.5530
	step 4099:lm_loss: 4.5241, ppl: 92.2136, loss: 4.5530
	step 4100:lm_loss: 4.5245, ppl: 92.2526, loss: 4.5533
	step 4101:lm_loss: 4.5247, ppl: 92.2687, loss: 4.5535
	step 4102:lm_loss: 4.5247, ppl: 92.2720, loss: 4.5536
	step 4103:lm_loss: 4.5248, ppl: 92.2769, loss: 4.5536
	step 4104:lm_loss: 4.5247, ppl: 92.2684, loss: 4.5535
	step 4105:lm_loss: 4.5248, ppl: 92.2746, loss: 4.5536
	step 4106:lm_loss: 4.5248, ppl: 92.2739, loss: 4.5536
	step 4107:lm_loss: 4.5249, ppl: 92.2910, loss: 4.5537
	step 4108:lm_loss: 4.5247, ppl: 92.2660, loss: 4.5534
	step 4109:lm_loss: 4.5250, ppl: 92.3004, loss: 4.5537
	step 4110:lm_loss: 4.5245, ppl: 92.2540, loss: 4.5533
	step 4111:lm_loss: 4.5246, ppl: 92.2575, loss: 4.5533
	step 4112:lm_loss: 4.5245, ppl: 92.2478, loss: 4.5532
	step 4113:lm_loss: 4.5245, ppl: 92.2492, loss: 4.5532
	step 4114:lm_loss: 4.5241, ppl: 92.2172, loss: 4.5529
	step 4115:lm_loss: 4.5242, ppl: 92.2184, loss: 4.5529
	step 4116:lm_loss: 4.5243, ppl: 92.2353, loss: 4.5531
	step 4117:lm_loss: 4.5243, ppl: 92.2333, loss: 4.5531
	step 4118:lm_loss: 4.5242, ppl: 92.2187, loss: 4.5529
	step 4119:lm_loss: 4.5242, ppl: 92.2177, loss: 4.5528
	step 4120:lm_loss: 4.5241, ppl: 92.2151, loss: 4.5528
	step 4121:lm_loss: 4.5242, ppl: 92.2207, loss: 4.5529
	step 4122:lm_loss: 4.5241, ppl: 92.2121, loss: 4.5527
	step 4123:lm_loss: 4.5237, ppl: 92.1774, loss: 4.5523
	step 4124:lm_loss: 4.5235, ppl: 92.1598, loss: 4.5521
	step 4125:lm_loss: 4.5238, ppl: 92.1827, loss: 4.5525
	step 4126:lm_loss: 4.5238, ppl: 92.1879, loss: 4.5525
	step 4127:lm_loss: 4.5235, ppl: 92.1575, loss: 4.5521
	step 4128:lm_loss: 4.5233, ppl: 92.1354, loss: 4.5520
	step 4129:lm_loss: 4.5233, ppl: 92.1395, loss: 4.5520
	step 4130:lm_loss: 4.5233, ppl: 92.1384, loss: 4.5520
	step 4131:lm_loss: 4.5231, ppl: 92.1230, loss: 4.5518
	step 4132:lm_loss: 4.5231, ppl: 92.1177, loss: 4.5517
	step 4133:lm_loss: 4.5230, ppl: 92.1136, loss: 4.5516
	step 4134:lm_loss: 4.5231, ppl: 92.1212, loss: 4.5518
	step 4135:lm_loss: 4.5230, ppl: 92.1113, loss: 4.5517
	step 4136:lm_loss: 4.5224, ppl: 92.0554, loss: 4.5512
	step 4137:lm_loss: 4.5219, ppl: 92.0076, loss: 4.5507
	step 4138:lm_loss: 4.5219, ppl: 92.0096, loss: 4.5508
	step 4139:lm_loss: 4.5219, ppl: 92.0143, loss: 4.5508
	step 4140:lm_loss: 4.5221, ppl: 92.0295, loss: 4.5509
	step 4141:lm_loss: 4.5219, ppl: 92.0114, loss: 4.5508
	step 4142:lm_loss: 4.5219, ppl: 92.0097, loss: 4.5507
	step 4143:lm_loss: 4.5221, ppl: 92.0246, loss: 4.5509
	step 4144:lm_loss: 4.5223, ppl: 92.0462, loss: 4.5512
	step 4145:lm_loss: 4.5224, ppl: 92.0536, loss: 4.5512
	step 4146:lm_loss: 4.5224, ppl: 92.0555, loss: 4.5512
	step 4147:lm_loss: 4.5223, ppl: 92.0487, loss: 4.5511
	step 4148:lm_loss: 4.5226, ppl: 92.0704, loss: 4.5514
	step 4149:lm_loss: 4.5226, ppl: 92.0773, loss: 4.5515
	step 4150:lm_loss: 4.5227, ppl: 92.0823, loss: 4.5515
	step 4151:lm_loss: 4.5223, ppl: 92.0465, loss: 4.5510
	step 4152:lm_loss: 4.5224, ppl: 92.0589, loss: 4.5512
	step 4153:lm_loss: 4.5226, ppl: 92.0719, loss: 4.5513
	step 4154:lm_loss: 4.5227, ppl: 92.0845, loss: 4.5515
	step 4155:lm_loss: 4.5227, ppl: 92.0817, loss: 4.5514
	step 4156:lm_loss: 4.5222, ppl: 92.0398, loss: 4.5512
	step 4157:lm_loss: 4.5224, ppl: 92.0529, loss: 4.5513
	step 4158:lm_loss: 4.5225, ppl: 92.0691, loss: 4.5515
	step 4159:lm_loss: 4.5228, ppl: 92.0937, loss: 4.5517
	step 4160:lm_loss: 4.5230, ppl: 92.1119, loss: 4.5518
	step 4161:lm_loss: 4.5229, ppl: 92.1067, loss: 4.5518
	step 4162:lm_loss: 4.5232, ppl: 92.1285, loss: 4.5520
	step 4163:lm_loss: 4.5230, ppl: 92.1143, loss: 4.5518
	step 4164:lm_loss: 4.5232, ppl: 92.1282, loss: 4.5520
	step 4165:lm_loss: 4.5232, ppl: 92.1270, loss: 4.5520
	step 4166:lm_loss: 4.5227, ppl: 92.0872, loss: 4.5517
	step 4167:lm_loss: 4.5228, ppl: 92.0906, loss: 4.5517
	step 4168:lm_loss: 4.5229, ppl: 92.1044, loss: 4.5519
	step 4169:lm_loss: 4.5230, ppl: 92.1151, loss: 4.5520
	step 4170:lm_loss: 4.5228, ppl: 92.0906, loss: 4.5516
	step 4171:lm_loss: 4.5226, ppl: 92.0718, loss: 4.5515
	step 4172:lm_loss: 4.5226, ppl: 92.0758, loss: 4.5515
	step 4173:lm_loss: 4.5227, ppl: 92.0830, loss: 4.5516
	step 4174:lm_loss: 4.5228, ppl: 92.0922, loss: 4.5516
	step 4175:lm_loss: 4.5228, ppl: 92.0919, loss: 4.5516
	step 4176:lm_loss: 4.5227, ppl: 92.0876, loss: 4.5516
	step 4177:lm_loss: 4.5230, ppl: 92.1079, loss: 4.5517
	step 4178:lm_loss: 4.5228, ppl: 92.0959, loss: 4.5516
	step 4179:lm_loss: 4.5230, ppl: 92.1120, loss: 4.5518
	step 4180:lm_loss: 4.5231, ppl: 92.1202, loss: 4.5518
	step 4181:lm_loss: 4.5233, ppl: 92.1399, loss: 4.5521
	step 4182:lm_loss: 4.5232, ppl: 92.1281, loss: 4.5520
	step 4183:lm_loss: 4.5230, ppl: 92.1077, loss: 4.5517
	step 4184:lm_loss: 4.5230, ppl: 92.1148, loss: 4.5517
	step 4185:lm_loss: 4.5228, ppl: 92.0968, loss: 4.5514
	step 4186:lm_loss: 4.5229, ppl: 92.1031, loss: 4.5515
	step 4187:lm_loss: 4.5227, ppl: 92.0797, loss: 4.5510
	step 4188:lm_loss: 4.5226, ppl: 92.0735, loss: 4.5509
	step 4189:lm_loss: 4.5223, ppl: 92.0508, loss: 4.5507
	step 4190:lm_loss: 4.5222, ppl: 92.0371, loss: 4.5506
	step 4191:lm_loss: 4.5222, ppl: 92.0360, loss: 4.5505
	step 4192:lm_loss: 4.5222, ppl: 92.0379, loss: 4.5506
	step 4193:lm_loss: 4.5220, ppl: 92.0175, loss: 4.5503
	step 4194:lm_loss: 4.5220, ppl: 92.0182, loss: 4.5503
	step 4195:lm_loss: 4.5219, ppl: 92.0070, loss: 4.5503
	step 4196:lm_loss: 4.5219, ppl: 92.0113, loss: 4.5503
	step 4197:lm_loss: 4.5220, ppl: 92.0177, loss: 4.5504
	step 4198:lm_loss: 4.5220, ppl: 92.0233, loss: 4.5504
	step 4199:lm_loss: 4.5219, ppl: 92.0061, loss: 4.5503
	step 4200:lm_loss: 4.5217, ppl: 91.9929, loss: 4.5501
	step 4201:lm_loss: 4.5217, ppl: 91.9896, loss: 4.5501
	step 4202:lm_loss: 4.5216, ppl: 91.9823, loss: 4.5500
	step 4203:lm_loss: 4.5216, ppl: 91.9871, loss: 4.5500
	step 4204:lm_loss: 4.5217, ppl: 91.9886, loss: 4.5500
	step 4205:lm_loss: 4.5216, ppl: 91.9871, loss: 4.5500
	step 4206:lm_loss: 4.5219, ppl: 92.0065, loss: 4.5503
	step 4207:lm_loss: 4.5219, ppl: 92.0093, loss: 4.5503
	step 4208:lm_loss: 4.5219, ppl: 92.0139, loss: 4.5503
	step 4209:lm_loss: 4.5220, ppl: 92.0177, loss: 4.5503
	step 4210:lm_loss: 4.5222, ppl: 92.0389, loss: 4.5506
	step 4211:lm_loss: 4.5222, ppl: 92.0370, loss: 4.5505
	step 4212:lm_loss: 4.5221, ppl: 92.0328, loss: 4.5505
	step 4213:lm_loss: 4.5222, ppl: 92.0367, loss: 4.5505
	step 4214:lm_loss: 4.5224, ppl: 92.0578, loss: 4.5507
	step 4215:lm_loss: 4.5224, ppl: 92.0576, loss: 4.5507
	step 4216:lm_loss: 4.5222, ppl: 92.0336, loss: 4.5505
	step 4217:lm_loss: 4.5223, ppl: 92.0491, loss: 4.5508
	step 4218:lm_loss: 4.5227, ppl: 92.0794, loss: 4.5513
	step 4219:lm_loss: 4.5226, ppl: 92.0723, loss: 4.5512
	step 4220:lm_loss: 4.5227, ppl: 92.0805, loss: 4.5514
	step 4221:lm_loss: 4.5226, ppl: 92.0760, loss: 4.5513
	step 4222:lm_loss: 4.5224, ppl: 92.0559, loss: 4.5511
	step 4223:lm_loss: 4.5224, ppl: 92.0519, loss: 4.5510
	step 4224:lm_loss: 4.5220, ppl: 92.0229, loss: 4.5508
	step 4225:lm_loss: 4.5223, ppl: 92.0441, loss: 4.5510
	step 4226:lm_loss: 4.5222, ppl: 92.0386, loss: 4.5509
	step 4227:lm_loss: 4.5222, ppl: 92.0351, loss: 4.5509
	step 4228:lm_loss: 4.5222, ppl: 92.0381, loss: 4.5509
	step 4229:lm_loss: 4.5224, ppl: 92.0531, loss: 4.5511
	step 4230:lm_loss: 4.5224, ppl: 92.0569, loss: 4.5511
	step 4231:lm_loss: 4.5224, ppl: 92.0550, loss: 4.5511
	step 4232:lm_loss: 4.5224, ppl: 92.0581, loss: 4.5511
	step 4233:lm_loss: 4.5222, ppl: 92.0352, loss: 4.5508
	step 4234:lm_loss: 4.5223, ppl: 92.0447, loss: 4.5508
	step 4235:lm_loss: 4.5223, ppl: 92.0503, loss: 4.5509
	step 4236:lm_loss: 4.5221, ppl: 92.0311, loss: 4.5507
	step 4237:lm_loss: 4.5223, ppl: 92.0449, loss: 4.5508
	step 4238:lm_loss: 4.5222, ppl: 92.0355, loss: 4.5507
	step 4239:lm_loss: 4.5222, ppl: 92.0364, loss: 4.5507
	step 4240:lm_loss: 4.5220, ppl: 92.0237, loss: 4.5506
	step 4241:lm_loss: 4.5222, ppl: 92.0360, loss: 4.5507
	step 4242:lm_loss: 4.5223, ppl: 92.0429, loss: 4.5507
	step 4243:lm_loss: 4.5223, ppl: 92.0446, loss: 4.5508
	step 4244:lm_loss: 4.5225, ppl: 92.0670, loss: 4.5509
	step 4245:lm_loss: 4.5224, ppl: 92.0564, loss: 4.5508
	step 4246:lm_loss: 4.5223, ppl: 92.0495, loss: 4.5507
	step 4247:lm_loss: 4.5225, ppl: 92.0648, loss: 4.5508
	step 4248:lm_loss: 4.5225, ppl: 92.0633, loss: 4.5508
	step 4249:lm_loss: 4.5225, ppl: 92.0634, loss: 4.5508
	step 4250:lm_loss: 4.5225, ppl: 92.0657, loss: 4.5508
	step 4251:lm_loss: 4.5224, ppl: 92.0576, loss: 4.5507
	step 4252:lm_loss: 4.5225, ppl: 92.0619, loss: 4.5507
	step 4253:lm_loss: 4.5219, ppl: 92.0066, loss: 4.5502
	step 4254:lm_loss: 4.5220, ppl: 92.0162, loss: 4.5503
	step 4255:lm_loss: 4.5220, ppl: 92.0185, loss: 4.5504
	step 4256:lm_loss: 4.5220, ppl: 92.0164, loss: 4.5503
	step 4257:lm_loss: 4.5222, ppl: 92.0355, loss: 4.5505
	step 4258:lm_loss: 4.5220, ppl: 92.0232, loss: 4.5503
	step 4259:lm_loss: 4.5220, ppl: 92.0224, loss: 4.5503
	step 4260:lm_loss: 4.5215, ppl: 91.9779, loss: 4.5500
	step 4261:lm_loss: 4.5216, ppl: 91.9839, loss: 4.5501
	step 4262:lm_loss: 4.5216, ppl: 91.9821, loss: 4.5500
	step 4263:lm_loss: 4.5215, ppl: 91.9751, loss: 4.5500
	step 4264:lm_loss: 4.5219, ppl: 92.0063, loss: 4.5502
	step 4265:lm_loss: 4.5222, ppl: 92.0369, loss: 4.5506
	step 4266:lm_loss: 4.5222, ppl: 92.0387, loss: 4.5506
	step 4267:lm_loss: 4.5222, ppl: 92.0417, loss: 4.5507
	step 4268:lm_loss: 4.5221, ppl: 92.0328, loss: 4.5505
	step 4269:lm_loss: 4.5223, ppl: 92.0431, loss: 4.5507
	step 4270:lm_loss: 4.5223, ppl: 92.0445, loss: 4.5507
	step 4271:lm_loss: 4.5221, ppl: 92.0304, loss: 4.5506
	step 4272:lm_loss: 4.5224, ppl: 92.0542, loss: 4.5508
	step 4273:lm_loss: 4.5225, ppl: 92.0671, loss: 4.5510
	step 4274:lm_loss: 4.5224, ppl: 92.0571, loss: 4.5509
	step 4275:lm_loss: 4.5228, ppl: 92.0898, loss: 4.5513
	step 4276:lm_loss: 4.5223, ppl: 92.0497, loss: 4.5508
	step 4277:lm_loss: 4.5224, ppl: 92.0541, loss: 4.5509
	step 4278:lm_loss: 4.5225, ppl: 92.0653, loss: 4.5510
	step 4279:lm_loss: 4.5225, ppl: 92.0677, loss: 4.5510
	step 4280:lm_loss: 4.5228, ppl: 92.0909, loss: 4.5512
	step 4281:lm_loss: 4.5228, ppl: 92.0956, loss: 4.5512
	step 4282:lm_loss: 4.5230, ppl: 92.1138, loss: 4.5515
	step 4283:lm_loss: 4.5232, ppl: 92.1263, loss: 4.5515
	step 4284:lm_loss: 4.5232, ppl: 92.1305, loss: 4.5516
	step 4285:lm_loss: 4.5235, ppl: 92.1574, loss: 4.5520
	step 4286:lm_loss: 4.5236, ppl: 92.1642, loss: 4.5520
	step 4287:lm_loss: 4.5234, ppl: 92.1477, loss: 4.5519
	step 4288:lm_loss: 4.5234, ppl: 92.1483, loss: 4.5519
	step 4289:lm_loss: 4.5232, ppl: 92.1264, loss: 4.5517
	step 4290:lm_loss: 4.5233, ppl: 92.1412, loss: 4.5519
	step 4291:lm_loss: 4.5233, ppl: 92.1382, loss: 4.5519
	step 4292:lm_loss: 4.5232, ppl: 92.1317, loss: 4.5518
	step 4293:lm_loss: 4.5232, ppl: 92.1289, loss: 4.5518
	step 4294:lm_loss: 4.5235, ppl: 92.1559, loss: 4.5520
	step 4295:lm_loss: 4.5230, ppl: 92.1098, loss: 4.5517
	step 4296:lm_loss: 4.5230, ppl: 92.1113, loss: 4.5517
	step 4297:lm_loss: 4.5230, ppl: 92.1152, loss: 4.5518
	step 4298:lm_loss: 4.5232, ppl: 92.1290, loss: 4.5519
	step 4299:lm_loss: 4.5232, ppl: 92.1336, loss: 4.5520
	step 4300:lm_loss: 4.5232, ppl: 92.1330, loss: 4.5519
	step 4301:lm_loss: 4.5229, ppl: 92.1053, loss: 4.5517
	step 4302:lm_loss: 4.5227, ppl: 92.0833, loss: 4.5516
	step 4303:lm_loss: 4.5228, ppl: 92.0914, loss: 4.5516
	step 4304:lm_loss: 4.5229, ppl: 92.1059, loss: 4.5517
	step 4305:lm_loss: 4.5229, ppl: 92.1051, loss: 4.5517
	step 4306:lm_loss: 4.5229, ppl: 92.1023, loss: 4.5517
	step 4307:lm_loss: 4.5230, ppl: 92.1088, loss: 4.5517
	step 4308:lm_loss: 4.5229, ppl: 92.1031, loss: 4.5517
	step 4309:lm_loss: 4.5228, ppl: 92.0971, loss: 4.5516
	step 4310:lm_loss: 4.5230, ppl: 92.1138, loss: 4.5518
	step 4311:lm_loss: 4.5230, ppl: 92.1087, loss: 4.5517
	step 4312:lm_loss: 4.5232, ppl: 92.1260, loss: 4.5519
	step 4313:lm_loss: 4.5232, ppl: 92.1317, loss: 4.5519
	step 4314:lm_loss: 4.5233, ppl: 92.1354, loss: 4.5520
	step 4315:lm_loss: 4.5230, ppl: 92.1101, loss: 4.5518
	step 4316:lm_loss: 4.5229, ppl: 92.1031, loss: 4.5516
	step 4317:lm_loss: 4.5231, ppl: 92.1229, loss: 4.5518
	step 4318:lm_loss: 4.5231, ppl: 92.1241, loss: 4.5518
	step 4319:lm_loss: 4.5232, ppl: 92.1311, loss: 4.5519
	step 4320:lm_loss: 4.5234, ppl: 92.1448, loss: 4.5521
	step 4321:lm_loss: 4.5233, ppl: 92.1410, loss: 4.5520
	step 4322:lm_loss: 4.5232, ppl: 92.1270, loss: 4.5518
	step 4323:lm_loss: 4.5232, ppl: 92.1341, loss: 4.5519
	step 4324:lm_loss: 4.5233, ppl: 92.1408, loss: 4.5520
	step 4325:lm_loss: 4.5234, ppl: 92.1467, loss: 4.5520
	step 4326:lm_loss: 4.5235, ppl: 92.1542, loss: 4.5521
	step 4327:lm_loss: 4.5236, ppl: 92.1685, loss: 4.5522
	step 4328:lm_loss: 4.5234, ppl: 92.1478, loss: 4.5520
	step 4329:lm_loss: 4.5234, ppl: 92.1520, loss: 4.5520
	step 4330:lm_loss: 4.5231, ppl: 92.1227, loss: 4.5517
	step 4331:lm_loss: 4.5234, ppl: 92.1470, loss: 4.5518
	step 4332:lm_loss: 4.5236, ppl: 92.1691, loss: 4.5522
	step 4333:lm_loss: 4.5235, ppl: 92.1581, loss: 4.5521
	step 4334:lm_loss: 4.5234, ppl: 92.1438, loss: 4.5519
	step 4335:lm_loss: 4.5235, ppl: 92.1547, loss: 4.5521
	step 4336:lm_loss: 4.5232, ppl: 92.1292, loss: 4.5518
	step 4337:lm_loss: 4.5228, ppl: 92.0957, loss: 4.5515
	step 4338:lm_loss: 4.5229, ppl: 92.1005, loss: 4.5516
	step 4339:lm_loss: 4.5230, ppl: 92.1121, loss: 4.5518
	step 4340:lm_loss: 4.5232, ppl: 92.1273, loss: 4.5520
	step 4341:lm_loss: 4.5231, ppl: 92.1169, loss: 4.5518
	step 4342:lm_loss: 4.5233, ppl: 92.1427, loss: 4.5520
	step 4343:lm_loss: 4.5232, ppl: 92.1268, loss: 4.5519
	step 4344:lm_loss: 4.5227, ppl: 92.0804, loss: 4.5516
	step 4345:lm_loss: 4.5227, ppl: 92.0821, loss: 4.5516
	step 4346:lm_loss: 4.5227, ppl: 92.0825, loss: 4.5516
	step 4347:lm_loss: 4.5230, ppl: 92.1089, loss: 4.5518
	step 4348:lm_loss: 4.5232, ppl: 92.1279, loss: 4.5520
	step 4349:lm_loss: 4.5231, ppl: 92.1230, loss: 4.5520
	step 4350:lm_loss: 4.5233, ppl: 92.1394, loss: 4.5522
	step 4351:lm_loss: 4.5236, ppl: 92.1668, loss: 4.5524
	step 4352:lm_loss: 4.5235, ppl: 92.1562, loss: 4.5523
	step 4353:lm_loss: 4.5235, ppl: 92.1585, loss: 4.5523
	step 4354:lm_loss: 4.5236, ppl: 92.1671, loss: 4.5524
	step 4355:lm_loss: 4.5238, ppl: 92.1815, loss: 4.5526
	step 4356:lm_loss: 4.5238, ppl: 92.1838, loss: 4.5526
	step 4357:lm_loss: 4.5235, ppl: 92.1581, loss: 4.5524
	step 4358:lm_loss: 4.5236, ppl: 92.1631, loss: 4.5525
	step 4359:lm_loss: 4.5235, ppl: 92.1532, loss: 4.5524
	step 4360:lm_loss: 4.5233, ppl: 92.1395, loss: 4.5522
	step 4361:lm_loss: 4.5233, ppl: 92.1404, loss: 4.5522
	step 4362:lm_loss: 4.5234, ppl: 92.1465, loss: 4.5523
	step 4363:lm_loss: 4.5235, ppl: 92.1618, loss: 4.5524
	step 4364:lm_loss: 4.5235, ppl: 92.1574, loss: 4.5523
	step 4365:lm_loss: 4.5234, ppl: 92.1512, loss: 4.5522
	step 4366:lm_loss: 4.5234, ppl: 92.1479, loss: 4.5522
	step 4367:lm_loss: 4.5236, ppl: 92.1662, loss: 4.5524
	step 4368:lm_loss: 4.5232, ppl: 92.1334, loss: 4.5522
	step 4369:lm_loss: 4.5232, ppl: 92.1317, loss: 4.5522
	step 4370:lm_loss: 4.5230, ppl: 92.1080, loss: 4.5519
	step 4371:lm_loss: 4.5230, ppl: 92.1159, loss: 4.5520
	step 4372:lm_loss: 4.5230, ppl: 92.1145, loss: 4.5520
	step 4373:lm_loss: 4.5229, ppl: 92.1067, loss: 4.5519
	step 4374:lm_loss: 4.5229, ppl: 92.1037, loss: 4.5518
	step 4375:lm_loss: 4.5230, ppl: 92.1157, loss: 4.5519
	step 4376:lm_loss: 4.5232, ppl: 92.1321, loss: 4.5521
	step 4377:lm_loss: 4.5231, ppl: 92.1171, loss: 4.5520
	step 4378:lm_loss: 4.5233, ppl: 92.1355, loss: 4.5522
	step 4379:lm_loss: 4.5233, ppl: 92.1417, loss: 4.5523
	step 4380:lm_loss: 4.5232, ppl: 92.1331, loss: 4.5522
	step 4381:lm_loss: 4.5233, ppl: 92.1401, loss: 4.5523
	step 4382:lm_loss: 4.5234, ppl: 92.1483, loss: 4.5524
	step 4383:lm_loss: 4.5234, ppl: 92.1497, loss: 4.5524
	step 4384:lm_loss: 4.5233, ppl: 92.1376, loss: 4.5523
	step 4385:lm_loss: 4.5231, ppl: 92.1189, loss: 4.5521
	step 4386:lm_loss: 4.5234, ppl: 92.1454, loss: 4.5525
	step 4387:lm_loss: 4.5233, ppl: 92.1377, loss: 4.5524
	step 4388:lm_loss: 4.5237, ppl: 92.1716, loss: 4.5527
	step 4389:lm_loss: 4.5236, ppl: 92.1639, loss: 4.5526
	step 4390:lm_loss: 4.5239, ppl: 92.1941, loss: 4.5528
	step 4391:lm_loss: 4.5239, ppl: 92.1959, loss: 4.5529
	step 4392:lm_loss: 4.5240, ppl: 92.1991, loss: 4.5529
	step 4393:lm_loss: 4.5238, ppl: 92.1894, loss: 4.5527
	step 4394:lm_loss: 4.5236, ppl: 92.1635, loss: 4.5524
	step 4395:lm_loss: 4.5233, ppl: 92.1367, loss: 4.5523
	step 4396:lm_loss: 4.5234, ppl: 92.1526, loss: 4.5524
	step 4397:lm_loss: 4.5237, ppl: 92.1770, loss: 4.5526
	step 4398:lm_loss: 4.5238, ppl: 92.1874, loss: 4.5527
	step 4399:lm_loss: 4.5239, ppl: 92.1934, loss: 4.5528
	step 4400:lm_loss: 4.5240, ppl: 92.2076, loss: 4.5529
	step 4401:lm_loss: 4.5239, ppl: 92.1967, loss: 4.5527
	step 4402:lm_loss: 4.5239, ppl: 92.1937, loss: 4.5527
	step 4403:lm_loss: 4.5237, ppl: 92.1753, loss: 4.5525
	step 4404:lm_loss: 4.5238, ppl: 92.1892, loss: 4.5527
	step 4405:lm_loss: 4.5238, ppl: 92.1857, loss: 4.5527
	step 4406:lm_loss: 4.5240, ppl: 92.2062, loss: 4.5528
	step 4407:lm_loss: 4.5242, ppl: 92.2188, loss: 4.5529
	step 4408:lm_loss: 4.5244, ppl: 92.2374, loss: 4.5531
	step 4409:lm_loss: 4.5245, ppl: 92.2494, loss: 4.5533
	step 4410:lm_loss: 4.5241, ppl: 92.2085, loss: 4.5530
	step 4411:lm_loss: 4.5239, ppl: 92.1977, loss: 4.5528
	step 4412:lm_loss: 4.5239, ppl: 92.1967, loss: 4.5528
	step 4413:lm_loss: 4.5240, ppl: 92.2049, loss: 4.5529
	step 4414:lm_loss: 4.5241, ppl: 92.2099, loss: 4.5530
	step 4415:lm_loss: 4.5244, ppl: 92.2401, loss: 4.5532
	step 4416:lm_loss: 4.5244, ppl: 92.2372, loss: 4.5531
	step 4417:lm_loss: 4.5243, ppl: 92.2327, loss: 4.5531
	step 4418:lm_loss: 4.5244, ppl: 92.2389, loss: 4.5531
	step 4419:lm_loss: 4.5246, ppl: 92.2566, loss: 4.5532
	step 4420:lm_loss: 4.5247, ppl: 92.2671, loss: 4.5533
	step 4421:lm_loss: 4.5245, ppl: 92.2537, loss: 4.5532
	step 4422:lm_loss: 4.5247, ppl: 92.2697, loss: 4.5534
	step 4423:lm_loss: 4.5249, ppl: 92.2865, loss: 4.5537
	step 4424:lm_loss: 4.5247, ppl: 92.2654, loss: 4.5536
	step 4425:lm_loss: 4.5247, ppl: 92.2687, loss: 4.5536
	step 4426:lm_loss: 4.5245, ppl: 92.2497, loss: 4.5534
	step 4427:lm_loss: 4.5241, ppl: 92.2158, loss: 4.5531
	step 4428:lm_loss: 4.5241, ppl: 92.2172, loss: 4.5531
	step 4429:lm_loss: 4.5239, ppl: 92.1939, loss: 4.5528
	step 4430:lm_loss: 4.5238, ppl: 92.1810, loss: 4.5527
	step 4431:lm_loss: 4.5239, ppl: 92.1910, loss: 4.5528
	step 4432:lm_loss: 4.5235, ppl: 92.1538, loss: 4.5526
	step 4433:lm_loss: 4.5233, ppl: 92.1435, loss: 4.5525
	step 4434:lm_loss: 4.5234, ppl: 92.1440, loss: 4.5525
	step 4435:lm_loss: 4.5235, ppl: 92.1573, loss: 4.5526
	step 4436:lm_loss: 4.5235, ppl: 92.1541, loss: 4.5526
	step 4437:lm_loss: 4.5235, ppl: 92.1541, loss: 4.5526
	step 4438:lm_loss: 4.5236, ppl: 92.1633, loss: 4.5527
	step 4439:lm_loss: 4.5239, ppl: 92.1925, loss: 4.5530
	step 4440:lm_loss: 4.5238, ppl: 92.1891, loss: 4.5529
	step 4441:lm_loss: 4.5237, ppl: 92.1759, loss: 4.5527
	step 4442:lm_loss: 4.5237, ppl: 92.1806, loss: 4.5528
	step 4443:lm_loss: 4.5238, ppl: 92.1812, loss: 4.5528
	step 4444:lm_loss: 4.5238, ppl: 92.1863, loss: 4.5528
	step 4445:lm_loss: 4.5238, ppl: 92.1859, loss: 4.5528
	step 4446:lm_loss: 4.5239, ppl: 92.1982, loss: 4.5529
	step 4447:lm_loss: 4.5239, ppl: 92.1924, loss: 4.5528
	step 4448:lm_loss: 4.5240, ppl: 92.2025, loss: 4.5530
	step 4449:lm_loss: 4.5240, ppl: 92.2056, loss: 4.5530
	step 4450:lm_loss: 4.5242, ppl: 92.2232, loss: 4.5532
	step 4451:lm_loss: 4.5242, ppl: 92.2196, loss: 4.5532
	step 4452:lm_loss: 4.5240, ppl: 92.1993, loss: 4.5530
	step 4453:lm_loss: 4.5242, ppl: 92.2176, loss: 4.5532
	step 4454:lm_loss: 4.5241, ppl: 92.2138, loss: 4.5531
	step 4455:lm_loss: 4.5245, ppl: 92.2476, loss: 4.5534
	step 4456:lm_loss: 4.5241, ppl: 92.2160, loss: 4.5531
	step 4457:lm_loss: 4.5244, ppl: 92.2415, loss: 4.5534
	step 4458:lm_loss: 4.5245, ppl: 92.2538, loss: 4.5534
	step 4459:lm_loss: 4.5247, ppl: 92.2652, loss: 4.5535
	step 4460:lm_loss: 4.5249, ppl: 92.2824, loss: 4.5536
	step 4461:lm_loss: 4.5249, ppl: 92.2912, loss: 4.5537
	step 4462:lm_loss: 4.5250, ppl: 92.3000, loss: 4.5538
	step 4463:lm_loss: 4.5252, ppl: 92.3151, loss: 4.5539
	step 4464:lm_loss: 4.5251, ppl: 92.3077, loss: 4.5538
	step 4465:lm_loss: 4.5254, ppl: 92.3299, loss: 4.5541
	step 4466:lm_loss: 4.5254, ppl: 92.3336, loss: 4.5542
	step 4467:lm_loss: 4.5256, ppl: 92.3546, loss: 4.5544
	step 4468:lm_loss: 4.5258, ppl: 92.3701, loss: 4.5545
	step 4469:lm_loss: 4.5256, ppl: 92.3558, loss: 4.5544
	step 4470:lm_loss: 4.5257, ppl: 92.3643, loss: 4.5545
	step 4471:lm_loss: 4.5259, ppl: 92.3747, loss: 4.5545
	step 4472:lm_loss: 4.5258, ppl: 92.3677, loss: 4.5544
	step 4473:lm_loss: 4.5258, ppl: 92.3658, loss: 4.5544
	step 4474:lm_loss: 4.5256, ppl: 92.3516, loss: 4.5543
	step 4475:lm_loss: 4.5253, ppl: 92.3198, loss: 4.5540
	step 4476:lm_loss: 4.5251, ppl: 92.3084, loss: 4.5539
	step 4477:lm_loss: 4.5250, ppl: 92.2993, loss: 4.5538
	step 4478:lm_loss: 4.5251, ppl: 92.3055, loss: 4.5539
	step 4479:lm_loss: 4.5249, ppl: 92.2824, loss: 4.5537
	step 4480:lm_loss: 4.5250, ppl: 92.3003, loss: 4.5540
	step 4481:lm_loss: 4.5249, ppl: 92.2893, loss: 4.5538
	step 4482:lm_loss: 4.5250, ppl: 92.2924, loss: 4.5538
	step 4483:lm_loss: 4.5249, ppl: 92.2875, loss: 4.5538
	step 4484:lm_loss: 4.5248, ppl: 92.2740, loss: 4.5536
	step 4485:lm_loss: 4.5247, ppl: 92.2719, loss: 4.5536
	step 4486:lm_loss: 4.5248, ppl: 92.2774, loss: 4.5537
	step 4487:lm_loss: 4.5249, ppl: 92.2887, loss: 4.5538
	step 4488:lm_loss: 4.5250, ppl: 92.2987, loss: 4.5539
	step 4489:lm_loss: 4.5249, ppl: 92.2850, loss: 4.5538
	step 4490:lm_loss: 4.5244, ppl: 92.2409, loss: 4.5534
	step 4491:lm_loss: 4.5243, ppl: 92.2289, loss: 4.5532
	step 4492:lm_loss: 4.5241, ppl: 92.2167, loss: 4.5531
	step 4493:lm_loss: 4.5242, ppl: 92.2239, loss: 4.5532
	step 4494:lm_loss: 4.5244, ppl: 92.2385, loss: 4.5533
	step 4495:lm_loss: 4.5245, ppl: 92.2531, loss: 4.5535
	step 4496:lm_loss: 4.5248, ppl: 92.2782, loss: 4.5537
	step 4497:lm_loss: 4.5249, ppl: 92.2871, loss: 4.5537
	step 4498:lm_loss: 4.5248, ppl: 92.2751, loss: 4.5535
	step 4499:lm_loss: 4.5247, ppl: 92.2657, loss: 4.5534
	step 4500:lm_loss: 4.5246, ppl: 92.2567, loss: 4.5533
	step 4501:lm_loss: 4.5249, ppl: 92.2841, loss: 4.5536
	step 4502:lm_loss: 4.5250, ppl: 92.2983, loss: 4.5537
	step 4503:lm_loss: 4.5252, ppl: 92.3110, loss: 4.5538
	step 4504:lm_loss: 4.5251, ppl: 92.3042, loss: 4.5538
	step 4505:lm_loss: 4.5248, ppl: 92.2783, loss: 4.5534
	step 4506:lm_loss: 4.5248, ppl: 92.2804, loss: 4.5534
	step 4507:lm_loss: 4.5249, ppl: 92.2833, loss: 4.5534
	step 4508:lm_loss: 4.5250, ppl: 92.2941, loss: 4.5535
	step 4509:lm_loss: 4.5249, ppl: 92.2874, loss: 4.5534
	step 4510:lm_loss: 4.5251, ppl: 92.3041, loss: 4.5536
	step 4511:lm_loss: 4.5250, ppl: 92.2991, loss: 4.5535
	step 4512:lm_loss: 4.5252, ppl: 92.3129, loss: 4.5536
	step 4513:lm_loss: 4.5253, ppl: 92.3237, loss: 4.5538
	step 4514:lm_loss: 4.5254, ppl: 92.3311, loss: 4.5538
	step 4515:lm_loss: 4.5256, ppl: 92.3476, loss: 4.5540
	step 4516:lm_loss: 4.5252, ppl: 92.3173, loss: 4.5537
	step 4517:lm_loss: 4.5253, ppl: 92.3232, loss: 4.5538
	step 4518:lm_loss: 4.5255, ppl: 92.3457, loss: 4.5542
	step 4519:lm_loss: 4.5257, ppl: 92.3637, loss: 4.5544
	step 4520:lm_loss: 4.5258, ppl: 92.3698, loss: 4.5544
	step 4521:lm_loss: 4.5260, ppl: 92.3916, loss: 4.5546
	step 4522:lm_loss: 4.5255, ppl: 92.3387, loss: 4.5543
	step 4523:lm_loss: 4.5257, ppl: 92.3607, loss: 4.5546
	step 4524:lm_loss: 4.5255, ppl: 92.3422, loss: 4.5544
	step 4525:lm_loss: 4.5258, ppl: 92.3671, loss: 4.5547
	step 4526:lm_loss: 4.5258, ppl: 92.3661, loss: 4.5547
	step 4527:lm_loss: 4.5258, ppl: 92.3708, loss: 4.5547
	step 4528:lm_loss: 4.5258, ppl: 92.3683, loss: 4.5547
	step 4529:lm_loss: 4.5259, ppl: 92.3813, loss: 4.5548
	step 4530:lm_loss: 4.5259, ppl: 92.3821, loss: 4.5549
	step 4531:lm_loss: 4.5259, ppl: 92.3824, loss: 4.5548
	step 4532:lm_loss: 4.5260, ppl: 92.3879, loss: 4.5549
	step 4533:lm_loss: 4.5259, ppl: 92.3783, loss: 4.5548
	step 4534:lm_loss: 4.5259, ppl: 92.3806, loss: 4.5548
	step 4535:lm_loss: 4.5261, ppl: 92.3976, loss: 4.5549
	step 4536:lm_loss: 4.5264, ppl: 92.4254, loss: 4.5552
	step 4537:lm_loss: 4.5267, ppl: 92.4562, loss: 4.5555
	step 4538:lm_loss: 4.5263, ppl: 92.4183, loss: 4.5552
	step 4539:lm_loss: 4.5261, ppl: 92.3965, loss: 4.5550
	step 4540:lm_loss: 4.5261, ppl: 92.3981, loss: 4.5550
	step 4541:lm_loss: 4.5261, ppl: 92.4018, loss: 4.5550
	step 4542:lm_loss: 4.5262, ppl: 92.4038, loss: 4.5550
	step 4543:lm_loss: 4.5261, ppl: 92.3953, loss: 4.5549
	step 4544:lm_loss: 4.5259, ppl: 92.3765, loss: 4.5546
	step 4545:lm_loss: 4.5261, ppl: 92.3941, loss: 4.5547
	step 4546:lm_loss: 4.5260, ppl: 92.3910, loss: 4.5547
	step 4547:lm_loss: 4.5260, ppl: 92.3887, loss: 4.5546
	step 4548:lm_loss: 4.5259, ppl: 92.3780, loss: 4.5545
	step 4549:lm_loss: 4.5258, ppl: 92.3744, loss: 4.5544
	step 4550:lm_loss: 4.5262, ppl: 92.4088, loss: 4.5548
	step 4551:lm_loss: 4.5262, ppl: 92.4113, loss: 4.5549
	step 4552:lm_loss: 4.5260, ppl: 92.3919, loss: 4.5547
	step 4553:lm_loss: 4.5262, ppl: 92.4105, loss: 4.5549
	step 4554:lm_loss: 4.5264, ppl: 92.4218, loss: 4.5551
	step 4555:lm_loss: 4.5252, ppl: 92.3108, loss: 4.5548
	step 4556:lm_loss: 4.5249, ppl: 92.2890, loss: 4.5546
	step 4557:lm_loss: 4.5249, ppl: 92.2844, loss: 4.5545
	step 4558:lm_loss: 4.5248, ppl: 92.2771, loss: 4.5544
	step 4559:lm_loss: 4.5249, ppl: 92.2861, loss: 4.5546
	step 4560:lm_loss: 4.5249, ppl: 92.2867, loss: 4.5546
	step 4561:lm_loss: 4.5249, ppl: 92.2848, loss: 4.5545
	step 4562:lm_loss: 4.5251, ppl: 92.3078, loss: 4.5549
	step 4563:lm_loss: 4.5249, ppl: 92.2897, loss: 4.5547
	step 4564:lm_loss: 4.5250, ppl: 92.2961, loss: 4.5548
	step 4565:lm_loss: 4.5248, ppl: 92.2819, loss: 4.5546
	step 4566:lm_loss: 4.5249, ppl: 92.2840, loss: 4.5546
	step 4567:lm_loss: 4.5245, ppl: 92.2511, loss: 4.5543
	step 4568:lm_loss: 4.5246, ppl: 92.2609, loss: 4.5544
	step 4569:lm_loss: 4.5246, ppl: 92.2563, loss: 4.5543
	step 4570:lm_loss: 4.5246, ppl: 92.2592, loss: 4.5544
	step 4571:lm_loss: 4.5247, ppl: 92.2720, loss: 4.5545
	step 4572:lm_loss: 4.5247, ppl: 92.2706, loss: 4.5545
	step 4573:lm_loss: 4.5249, ppl: 92.2895, loss: 4.5547
	step 4574:lm_loss: 4.5252, ppl: 92.3118, loss: 4.5548
	step 4575:lm_loss: 4.5253, ppl: 92.3198, loss: 4.5549
	step 4576:lm_loss: 4.5254, ppl: 92.3313, loss: 4.5550
	step 4577:lm_loss: 4.5252, ppl: 92.3171, loss: 4.5548
	step 4578:lm_loss: 4.5252, ppl: 92.3171, loss: 4.5548
	step 4579:lm_loss: 4.5253, ppl: 92.3241, loss: 4.5549
	step 4580:lm_loss: 4.5253, ppl: 92.3254, loss: 4.5549
	step 4581:lm_loss: 4.5255, ppl: 92.3425, loss: 4.5551
	step 4582:lm_loss: 4.5256, ppl: 92.3547, loss: 4.5552
	step 4583:lm_loss: 4.5257, ppl: 92.3599, loss: 4.5552
	step 4584:lm_loss: 4.5258, ppl: 92.3673, loss: 4.5553
	step 4585:lm_loss: 4.5259, ppl: 92.3746, loss: 4.5554
	step 4586:lm_loss: 4.5259, ppl: 92.3765, loss: 4.5554
	step 4587:lm_loss: 4.5258, ppl: 92.3662, loss: 4.5553
	step 4588:lm_loss: 4.5259, ppl: 92.3794, loss: 4.5554
	step 4589:lm_loss: 4.5259, ppl: 92.3794, loss: 4.5554
	step 4590:lm_loss: 4.5263, ppl: 92.4180, loss: 4.5560
	step 4591:lm_loss: 4.5262, ppl: 92.4100, loss: 4.5559
	step 4592:lm_loss: 4.5262, ppl: 92.4072, loss: 4.5559
	step 4593:lm_loss: 4.5262, ppl: 92.4040, loss: 4.5558
	step 4594:lm_loss: 4.5261, ppl: 92.3944, loss: 4.5557
	step 4595:lm_loss: 4.5261, ppl: 92.3980, loss: 4.5558
	step 4596:lm_loss: 4.5262, ppl: 92.4050, loss: 4.5558
	step 4597:lm_loss: 4.5264, ppl: 92.4214, loss: 4.5561
	step 4598:lm_loss: 4.5263, ppl: 92.4187, loss: 4.5560
	step 4599:lm_loss: 4.5264, ppl: 92.4281, loss: 4.5561
	step 4600:lm_loss: 4.5260, ppl: 92.3915, loss: 4.5559
	step 4601:lm_loss: 4.5260, ppl: 92.3903, loss: 4.5558
	step 4602:lm_loss: 4.5259, ppl: 92.3755, loss: 4.5556
	step 4603:lm_loss: 4.5260, ppl: 92.3923, loss: 4.5558
	step 4604:lm_loss: 4.5260, ppl: 92.3861, loss: 4.5558
	step 4605:lm_loss: 4.5259, ppl: 92.3780, loss: 4.5557
	step 4606:lm_loss: 4.5263, ppl: 92.4160, loss: 4.5559
	step 4607:lm_loss: 4.5261, ppl: 92.4005, loss: 4.5558
	step 4608:lm_loss: 4.5261, ppl: 92.3969, loss: 4.5558
	step 4609:lm_loss: 4.5262, ppl: 92.4041, loss: 4.5558
	step 4610:lm_loss: 4.5259, ppl: 92.3792, loss: 4.5555
	step 4611:lm_loss: 4.5258, ppl: 92.3714, loss: 4.5554
	step 4612:lm_loss: 4.5259, ppl: 92.3755, loss: 4.5555
	step 4613:lm_loss: 4.5259, ppl: 92.3791, loss: 4.5555
	step 4614:lm_loss: 4.5258, ppl: 92.3736, loss: 4.5555
	step 4615:lm_loss: 4.5259, ppl: 92.3759, loss: 4.5555
	step 4616:lm_loss: 4.5255, ppl: 92.3386, loss: 4.5552
	step 4617:lm_loss: 4.5255, ppl: 92.3458, loss: 4.5553
	step 4618:lm_loss: 4.5256, ppl: 92.3493, loss: 4.5553
	step 4619:lm_loss: 4.5257, ppl: 92.3650, loss: 4.5554
	step 4620:lm_loss: 4.5259, ppl: 92.3805, loss: 4.5556
	step 4621:lm_loss: 4.5257, ppl: 92.3614, loss: 4.5555
	step 4622:lm_loss: 4.5257, ppl: 92.3618, loss: 4.5555
	step 4623:lm_loss: 4.5259, ppl: 92.3831, loss: 4.5557
	step 4624:lm_loss: 4.5259, ppl: 92.3774, loss: 4.5556
	step 4625:lm_loss: 4.5259, ppl: 92.3833, loss: 4.5557
	step 4626:lm_loss: 4.5260, ppl: 92.3929, loss: 4.5558
	step 4627:lm_loss: 4.5261, ppl: 92.3949, loss: 4.5558
	step 4628:lm_loss: 4.5261, ppl: 92.3966, loss: 4.5558
	step 4629:lm_loss: 4.5261, ppl: 92.3977, loss: 4.5558
	step 4630:lm_loss: 4.5262, ppl: 92.4021, loss: 4.5558
	step 4631:lm_loss: 4.5262, ppl: 92.4109, loss: 4.5559
	step 4632:lm_loss: 4.5264, ppl: 92.4209, loss: 4.5560
	step 4633:lm_loss: 4.5263, ppl: 92.4129, loss: 4.5560
	step 4634:lm_loss: 4.5263, ppl: 92.4196, loss: 4.5561
	step 4635:lm_loss: 4.5263, ppl: 92.4163, loss: 4.5560
	step 4636:lm_loss: 4.5261, ppl: 92.3954, loss: 4.5558
	step 4637:lm_loss: 4.5262, ppl: 92.4038, loss: 4.5559
	step 4638:lm_loss: 4.5261, ppl: 92.3965, loss: 4.5558
	step 4639:lm_loss: 4.5261, ppl: 92.3954, loss: 4.5558
	step 4640:lm_loss: 4.5264, ppl: 92.4266, loss: 4.5560
	step 4641:lm_loss: 4.5264, ppl: 92.4229, loss: 4.5560
	step 4642:lm_loss: 4.5264, ppl: 92.4215, loss: 4.5559
	step 4643:lm_loss: 4.5265, ppl: 92.4380, loss: 4.5561
	step 4644:lm_loss: 4.5266, ppl: 92.4391, loss: 4.5561
	step 4645:lm_loss: 4.5266, ppl: 92.4395, loss: 4.5561
	step 4646:lm_loss: 4.5266, ppl: 92.4409, loss: 4.5561
	step 4647:lm_loss: 4.5267, ppl: 92.4505, loss: 4.5562
	step 4648:lm_loss: 4.5266, ppl: 92.4436, loss: 4.5561
	step 4649:lm_loss: 4.5266, ppl: 92.4411, loss: 4.5561
	step 4650:lm_loss: 4.5264, ppl: 92.4292, loss: 4.5560
	step 4651:lm_loss: 4.5265, ppl: 92.4333, loss: 4.5560
	step 4652:lm_loss: 4.5264, ppl: 92.4219, loss: 4.5559
	step 4653:lm_loss: 4.5264, ppl: 92.4281, loss: 4.5560
	step 4654:lm_loss: 4.5263, ppl: 92.4139, loss: 4.5559
	step 4655:lm_loss: 4.5263, ppl: 92.4190, loss: 4.5559
	step 4656:lm_loss: 4.5263, ppl: 92.4201, loss: 4.5559
	step 4657:lm_loss: 4.5265, ppl: 92.4304, loss: 4.5561
	step 4658:lm_loss: 4.5264, ppl: 92.4219, loss: 4.5559
	step 4659:lm_loss: 4.5263, ppl: 92.4132, loss: 4.5558
	step 4660:lm_loss: 4.5264, ppl: 92.4269, loss: 4.5560
	step 4661:lm_loss: 4.5267, ppl: 92.4522, loss: 4.5562
	step 4662:lm_loss: 4.5267, ppl: 92.4490, loss: 4.5562
	step 4663:lm_loss: 4.5267, ppl: 92.4520, loss: 4.5562
	step 4664:lm_loss: 4.5269, ppl: 92.4743, loss: 4.5564
	step 4665:lm_loss: 4.5271, ppl: 92.4884, loss: 4.5565
	step 4666:lm_loss: 4.5275, ppl: 92.5264, loss: 4.5570
	step 4667:lm_loss: 4.5274, ppl: 92.5143, loss: 4.5568
	step 4668:lm_loss: 4.5275, ppl: 92.5290, loss: 4.5569
	step 4669:lm_loss: 4.5274, ppl: 92.5223, loss: 4.5568
	step 4670:lm_loss: 4.5276, ppl: 92.5394, loss: 4.5570
	step 4671:lm_loss: 4.5276, ppl: 92.5354, loss: 4.5570
	step 4672:lm_loss: 4.5275, ppl: 92.5264, loss: 4.5568
	step 4673:lm_loss: 4.5275, ppl: 92.5246, loss: 4.5568
	step 4674:lm_loss: 4.5275, ppl: 92.5300, loss: 4.5569
	step 4675:lm_loss: 4.5274, ppl: 92.5144, loss: 4.5567
	step 4676:lm_loss: 4.5278, ppl: 92.5552, loss: 4.5575
	step 4677:lm_loss: 4.5279, ppl: 92.5648, loss: 4.5576
	step 4678:lm_loss: 4.5280, ppl: 92.5741, loss: 4.5578
	step 4679:lm_loss: 4.5280, ppl: 92.5696, loss: 4.5577
	step 4680:lm_loss: 4.5280, ppl: 92.5718, loss: 4.5577
	step 4681:lm_loss: 4.5279, ppl: 92.5627, loss: 4.5576
	step 4682:lm_loss: 4.5280, ppl: 92.5756, loss: 4.5577
	step 4683:lm_loss: 4.5283, ppl: 92.5985, loss: 4.5579
	step 4684:lm_loss: 4.5282, ppl: 92.5907, loss: 4.5578
	step 4685:lm_loss: 4.5283, ppl: 92.6027, loss: 4.5579
	step 4686:lm_loss: 4.5280, ppl: 92.5772, loss: 4.5576
	step 4687:lm_loss: 4.5280, ppl: 92.5769, loss: 4.5576
	step 4688:lm_loss: 4.5280, ppl: 92.5764, loss: 4.5576
	step 4689:lm_loss: 4.5280, ppl: 92.5720, loss: 4.5576
	step 4690:lm_loss: 4.5279, ppl: 92.5622, loss: 4.5575
	step 4691:lm_loss: 4.5280, ppl: 92.5771, loss: 4.5577
	step 4692:lm_loss: 4.5282, ppl: 92.5916, loss: 4.5578
	step 4693:lm_loss: 4.5280, ppl: 92.5695, loss: 4.5577
	step 4694:lm_loss: 4.5283, ppl: 92.5999, loss: 4.5581
	step 4695:lm_loss: 4.5282, ppl: 92.5962, loss: 4.5580
	step 4696:lm_loss: 4.5282, ppl: 92.5880, loss: 4.5579
	step 4697:lm_loss: 4.5281, ppl: 92.5869, loss: 4.5579
	step 4698:lm_loss: 4.5281, ppl: 92.5800, loss: 4.5578
	step 4699:lm_loss: 4.5279, ppl: 92.5628, loss: 4.5577
	step 4700:lm_loss: 4.5277, ppl: 92.5476, loss: 4.5575
	step 4701:lm_loss: 4.5275, ppl: 92.5259, loss: 4.5574
	step 4702:lm_loss: 4.5274, ppl: 92.5159, loss: 4.5572
	step 4703:lm_loss: 4.5274, ppl: 92.5218, loss: 4.5573
	step 4704:lm_loss: 4.5275, ppl: 92.5285, loss: 4.5573
	step 4705:lm_loss: 4.5275, ppl: 92.5311, loss: 4.5574
	step 4706:lm_loss: 4.5275, ppl: 92.5281, loss: 4.5573
	step 4707:lm_loss: 4.5275, ppl: 92.5269, loss: 4.5573
	step 4708:lm_loss: 4.5276, ppl: 92.5324, loss: 4.5574
	step 4709:lm_loss: 4.5270, ppl: 92.4849, loss: 4.5571
	step 4710:lm_loss: 4.5268, ppl: 92.4660, loss: 4.5569
	step 4711:lm_loss: 4.5267, ppl: 92.4560, loss: 4.5567
	step 4712:lm_loss: 4.5267, ppl: 92.4554, loss: 4.5567
	step 4713:lm_loss: 4.5267, ppl: 92.4548, loss: 4.5567
	step 4714:lm_loss: 4.5266, ppl: 92.4415, loss: 4.5565
	step 4715:lm_loss: 4.5265, ppl: 92.4377, loss: 4.5565
	step 4716:lm_loss: 4.5268, ppl: 92.4649, loss: 4.5569
	step 4717:lm_loss: 4.5269, ppl: 92.4685, loss: 4.5569
	step 4718:lm_loss: 4.5267, ppl: 92.4567, loss: 4.5568
	step 4719:lm_loss: 4.5265, ppl: 92.4338, loss: 4.5566
	step 4720:lm_loss: 4.5267, ppl: 92.4554, loss: 4.5569
	step 4721:lm_loss: 4.5261, ppl: 92.3958, loss: 4.5565
	step 4722:lm_loss: 4.5260, ppl: 92.3858, loss: 4.5564
	step 4723:lm_loss: 4.5259, ppl: 92.3822, loss: 4.5563
	step 4724:lm_loss: 4.5260, ppl: 92.3842, loss: 4.5563
	step 4725:lm_loss: 4.5260, ppl: 92.3870, loss: 4.5564
	step 4726:lm_loss: 4.5258, ppl: 92.3685, loss: 4.5562
	step 4727:lm_loss: 4.5259, ppl: 92.3748, loss: 4.5563
	step 4728:lm_loss: 4.5260, ppl: 92.3879, loss: 4.5564
	step 4729:lm_loss: 4.5261, ppl: 92.3971, loss: 4.5565
	step 4730:lm_loss: 4.5261, ppl: 92.3991, loss: 4.5565
	step 4731:lm_loss: 4.5263, ppl: 92.4116, loss: 4.5566
	step 4732:lm_loss: 4.5264, ppl: 92.4266, loss: 4.5567
	step 4733:lm_loss: 4.5266, ppl: 92.4429, loss: 4.5570
	step 4734:lm_loss: 4.5267, ppl: 92.4551, loss: 4.5571
	step 4735:lm_loss: 4.5267, ppl: 92.4545, loss: 4.5571
	step 4736:lm_loss: 4.5267, ppl: 92.4535, loss: 4.5570
	step 4737:lm_loss: 4.5267, ppl: 92.4521, loss: 4.5570
	step 4738:lm_loss: 4.5265, ppl: 92.4354, loss: 4.5567
	step 4739:lm_loss: 4.5263, ppl: 92.4194, loss: 4.5566
	step 4740:lm_loss: 4.5265, ppl: 92.4339, loss: 4.5568
	step 4741:lm_loss: 4.5265, ppl: 92.4361, loss: 4.5568
	step 4742:lm_loss: 4.5268, ppl: 92.4635, loss: 4.5571
	step 4743:lm_loss: 4.5270, ppl: 92.4768, loss: 4.5573
	step 4744:lm_loss: 4.5269, ppl: 92.4758, loss: 4.5572
	step 4745:lm_loss: 4.5270, ppl: 92.4802, loss: 4.5573
	step 4746:lm_loss: 4.5270, ppl: 92.4849, loss: 4.5574
	step 4747:lm_loss: 4.5272, ppl: 92.4970, loss: 4.5575
	step 4748:lm_loss: 4.5272, ppl: 92.5025, loss: 4.5576
	step 4749:lm_loss: 4.5274, ppl: 92.5144, loss: 4.5577
	step 4750:lm_loss: 4.5273, ppl: 92.5048, loss: 4.5576
	step 4751:lm_loss: 4.5273, ppl: 92.5125, loss: 4.5577
	step 4752:lm_loss: 4.5272, ppl: 92.4954, loss: 4.5574
	step 4753:lm_loss: 4.5272, ppl: 92.5004, loss: 4.5575
	step 4754:lm_loss: 4.5274, ppl: 92.5185, loss: 4.5576
	step 4755:lm_loss: 4.5276, ppl: 92.5317, loss: 4.5578
	step 4756:lm_loss: 4.5275, ppl: 92.5262, loss: 4.5577
	step 4757:lm_loss: 4.5275, ppl: 92.5299, loss: 4.5577
	step 4758:lm_loss: 4.5275, ppl: 92.5271, loss: 4.5577
	step 4759:lm_loss: 4.5275, ppl: 92.5242, loss: 4.5577
	step 4760:lm_loss: 4.5276, ppl: 92.5385, loss: 4.5578
	step 4761:lm_loss: 4.5275, ppl: 92.5281, loss: 4.5576
	step 4762:lm_loss: 4.5280, ppl: 92.5701, loss: 4.5579
	step 4763:lm_loss: 4.5278, ppl: 92.5575, loss: 4.5578
	step 4764:lm_loss: 4.5280, ppl: 92.5739, loss: 4.5580
	step 4765:lm_loss: 4.5279, ppl: 92.5652, loss: 4.5579
	step 4766:lm_loss: 4.5279, ppl: 92.5669, loss: 4.5579
	step 4767:lm_loss: 4.5277, ppl: 92.5481, loss: 4.5577
	step 4768:lm_loss: 4.5274, ppl: 92.5153, loss: 4.5574
	step 4769:lm_loss: 4.5273, ppl: 92.5041, loss: 4.5573
	step 4770:lm_loss: 4.5272, ppl: 92.4997, loss: 4.5572
	step 4771:lm_loss: 4.5273, ppl: 92.5060, loss: 4.5573
	step 4772:lm_loss: 4.5272, ppl: 92.4956, loss: 4.5572
	step 4773:lm_loss: 4.5271, ppl: 92.4872, loss: 4.5571
	step 4774:lm_loss: 4.5272, ppl: 92.5000, loss: 4.5572
	step 4775:lm_loss: 4.5273, ppl: 92.5072, loss: 4.5572
	step 4776:lm_loss: 4.5274, ppl: 92.5172, loss: 4.5573
	step 4777:lm_loss: 4.5274, ppl: 92.5152, loss: 4.5573
	step 4778:lm_loss: 4.5272, ppl: 92.4972, loss: 4.5571
	step 4779:lm_loss: 4.5274, ppl: 92.5163, loss: 4.5573
	step 4780:lm_loss: 4.5274, ppl: 92.5175, loss: 4.5573
	step 4781:lm_loss: 4.5274, ppl: 92.5206, loss: 4.5573
	step 4782:lm_loss: 4.5275, ppl: 92.5291, loss: 4.5574
	step 4783:lm_loss: 4.5273, ppl: 92.5123, loss: 4.5572
	step 4784:lm_loss: 4.5272, ppl: 92.5023, loss: 4.5571
	step 4785:lm_loss: 4.5273, ppl: 92.5060, loss: 4.5571
	step 4786:lm_loss: 4.5274, ppl: 92.5197, loss: 4.5573
	step 4787:lm_loss: 4.5275, ppl: 92.5290, loss: 4.5574
	step 4788:lm_loss: 4.5277, ppl: 92.5499, loss: 4.5576
	step 4789:lm_loss: 4.5279, ppl: 92.5601, loss: 4.5577
	step 4790:lm_loss: 4.5278, ppl: 92.5552, loss: 4.5576
	step 4791:lm_loss: 4.5278, ppl: 92.5517, loss: 4.5576
	step 4792:lm_loss: 4.5278, ppl: 92.5503, loss: 4.5576
	step 4793:lm_loss: 4.5278, ppl: 92.5529, loss: 4.5576
	step 4794:lm_loss: 4.5278, ppl: 92.5564, loss: 4.5576
	step 4795:lm_loss: 4.5279, ppl: 92.5633, loss: 4.5577
	step 4796:lm_loss: 4.5279, ppl: 92.5612, loss: 4.5577
	step 4797:lm_loss: 4.5280, ppl: 92.5727, loss: 4.5578
	step 4798:lm_loss: 4.5280, ppl: 92.5687, loss: 4.5578
	step 4799:lm_loss: 4.5279, ppl: 92.5664, loss: 4.5577
	step 4800:lm_loss: 4.5278, ppl: 92.5589, loss: 4.5576
	step 4801:lm_loss: 4.5282, ppl: 92.5920, loss: 4.5580
	step 4802:lm_loss: 4.5281, ppl: 92.5852, loss: 4.5579
	step 4803:lm_loss: 4.5285, ppl: 92.6226, loss: 4.5582
	step 4804:lm_loss: 4.5285, ppl: 92.6166, loss: 4.5581
	step 4805:lm_loss: 4.5286, ppl: 92.6272, loss: 4.5582
	step 4806:lm_loss: 4.5286, ppl: 92.6325, loss: 4.5583
	step 4807:lm_loss: 4.5285, ppl: 92.6208, loss: 4.5582
	step 4808:lm_loss: 4.5285, ppl: 92.6164, loss: 4.5581
	step 4809:lm_loss: 4.5287, ppl: 92.6390, loss: 4.5582
	step 4810:lm_loss: 4.5286, ppl: 92.6318, loss: 4.5582
	step 4811:lm_loss: 4.5286, ppl: 92.6310, loss: 4.5582
	step 4812:lm_loss: 4.5285, ppl: 92.6231, loss: 4.5580
	step 4813:lm_loss: 4.5282, ppl: 92.5921, loss: 4.5577
	step 4814:lm_loss: 4.5282, ppl: 92.5947, loss: 4.5578
	step 4815:lm_loss: 4.5281, ppl: 92.5805, loss: 4.5576
	step 4816:lm_loss: 4.5282, ppl: 92.5929, loss: 4.5576
	step 4817:lm_loss: 4.5285, ppl: 92.6197, loss: 4.5580
	step 4818:lm_loss: 4.5285, ppl: 92.6211, loss: 4.5581
	step 4819:lm_loss: 4.5284, ppl: 92.6139, loss: 4.5580
	step 4820:lm_loss: 4.5285, ppl: 92.6206, loss: 4.5580
	step 4821:lm_loss: 4.5286, ppl: 92.6313, loss: 4.5581
	step 4822:lm_loss: 4.5286, ppl: 92.6328, loss: 4.5581
	step 4823:lm_loss: 4.5286, ppl: 92.6309, loss: 4.5581
	step 4824:lm_loss: 4.5289, ppl: 92.6606, loss: 4.5584
	step 4825:lm_loss: 4.5289, ppl: 92.6582, loss: 4.5584
	step 4826:lm_loss: 4.5291, ppl: 92.6758, loss: 4.5585
	step 4827:lm_loss: 4.5291, ppl: 92.6757, loss: 4.5585
	step 4828:lm_loss: 4.5288, ppl: 92.6454, loss: 4.5582
	step 4829:lm_loss: 4.5287, ppl: 92.6409, loss: 4.5581
	step 4830:lm_loss: 4.5287, ppl: 92.6377, loss: 4.5581
	step 4831:lm_loss: 4.5281, ppl: 92.5840, loss: 4.5577
	step 4832:lm_loss: 4.5280, ppl: 92.5711, loss: 4.5575
	step 4833:lm_loss: 4.5278, ppl: 92.5502, loss: 4.5573
	step 4834:lm_loss: 4.5278, ppl: 92.5509, loss: 4.5573
	step 4835:lm_loss: 4.5276, ppl: 92.5404, loss: 4.5572
	step 4836:lm_loss: 4.5272, ppl: 92.4958, loss: 4.5569
	step 4837:lm_loss: 4.5271, ppl: 92.4869, loss: 4.5568
	step 4838:lm_loss: 4.5272, ppl: 92.4967, loss: 4.5569
	step 4839:lm_loss: 4.5273, ppl: 92.5123, loss: 4.5570
	step 4840:lm_loss: 4.5276, ppl: 92.5339, loss: 4.5572
	step 4841:lm_loss: 4.5273, ppl: 92.5087, loss: 4.5571
	step 4842:lm_loss: 4.5275, ppl: 92.5284, loss: 4.5573
	step 4843:lm_loss: 4.5273, ppl: 92.5105, loss: 4.5571
	step 4844:lm_loss: 4.5274, ppl: 92.5176, loss: 4.5571
	step 4845:lm_loss: 4.5272, ppl: 92.4957, loss: 4.5569
	step 4846:lm_loss: 4.5272, ppl: 92.4960, loss: 4.5569
	step 4847:lm_loss: 4.5272, ppl: 92.4978, loss: 4.5569
	step 4848:lm_loss: 4.5273, ppl: 92.5104, loss: 4.5571
	step 4849:lm_loss: 4.5274, ppl: 92.5211, loss: 4.5572
	step 4850:lm_loss: 4.5272, ppl: 92.5019, loss: 4.5570
	step 4851:lm_loss: 4.5273, ppl: 92.5072, loss: 4.5570
	step 4852:lm_loss: 4.5269, ppl: 92.4686, loss: 4.5568
	step 4853:lm_loss: 4.5270, ppl: 92.4778, loss: 4.5568
	step 4854:lm_loss: 4.5270, ppl: 92.4811, loss: 4.5569
	step 4855:lm_loss: 4.5271, ppl: 92.4917, loss: 4.5570
	step 4856:lm_loss: 4.5273, ppl: 92.5063, loss: 4.5571
	step 4857:lm_loss: 4.5275, ppl: 92.5261, loss: 4.5573
	step 4858:lm_loss: 4.5275, ppl: 92.5285, loss: 4.5573
	step 4859:lm_loss: 4.5274, ppl: 92.5221, loss: 4.5572
	step 4860:lm_loss: 4.5275, ppl: 92.5238, loss: 4.5572
	step 4861:lm_loss: 4.5275, ppl: 92.5314, loss: 4.5573
	step 4862:lm_loss: 4.5272, ppl: 92.4977, loss: 4.5571
	step 4863:lm_loss: 4.5273, ppl: 92.5071, loss: 4.5572
	step 4864:lm_loss: 4.5273, ppl: 92.5041, loss: 4.5572
	step 4865:lm_loss: 4.5272, ppl: 92.5006, loss: 4.5571
	step 4866:lm_loss: 4.5274, ppl: 92.5149, loss: 4.5573
	step 4867:lm_loss: 4.5273, ppl: 92.5039, loss: 4.5572
	step 4868:lm_loss: 4.5267, ppl: 92.4539, loss: 4.5569
	step 4869:lm_loss: 4.5266, ppl: 92.4479, loss: 4.5568
	step 4870:lm_loss: 4.5268, ppl: 92.4635, loss: 4.5570
	step 4871:lm_loss: 4.5266, ppl: 92.4451, loss: 4.5568
	step 4872:lm_loss: 4.5265, ppl: 92.4389, loss: 4.5567
	step 4873:lm_loss: 4.5265, ppl: 92.4387, loss: 4.5567
	step 4874:lm_loss: 4.5267, ppl: 92.4521, loss: 4.5568
	step 4875:lm_loss: 4.5266, ppl: 92.4439, loss: 4.5567
	step 4876:lm_loss: 4.5266, ppl: 92.4465, loss: 4.5567
	step 4877:lm_loss: 4.5266, ppl: 92.4483, loss: 4.5567
	step 4878:lm_loss: 4.5268, ppl: 92.4586, loss: 4.5568
	step 4879:lm_loss: 4.5267, ppl: 92.4511, loss: 4.5567
	step 4880:lm_loss: 4.5264, ppl: 92.4240, loss: 4.5563
	step 4881:lm_loss: 4.5265, ppl: 92.4307, loss: 4.5564
	step 4882:lm_loss: 4.5264, ppl: 92.4294, loss: 4.5563
	step 4883:lm_loss: 4.5265, ppl: 92.4330, loss: 4.5564
	step 4884:lm_loss: 4.5264, ppl: 92.4262, loss: 4.5563
	step 4885:lm_loss: 4.5264, ppl: 92.4210, loss: 4.5562
	step 4886:lm_loss: 4.5265, ppl: 92.4326, loss: 4.5564
	step 4887:lm_loss: 4.5265, ppl: 92.4377, loss: 4.5565
	step 4888:lm_loss: 4.5265, ppl: 92.4307, loss: 4.5563
	step 4889:lm_loss: 4.5264, ppl: 92.4296, loss: 4.5563
	step 4890:lm_loss: 4.5264, ppl: 92.4269, loss: 4.5563
	step 4891:lm_loss: 4.5263, ppl: 92.4186, loss: 4.5562
	step 4892:lm_loss: 4.5261, ppl: 92.3983, loss: 4.5560
	step 4893:lm_loss: 4.5261, ppl: 92.3961, loss: 4.5560
	step 4894:lm_loss: 4.5261, ppl: 92.3950, loss: 4.5559
	step 4895:lm_loss: 4.5261, ppl: 92.3943, loss: 4.5559
	step 4896:lm_loss: 4.5263, ppl: 92.4177, loss: 4.5561
	step 4897:lm_loss: 4.5265, ppl: 92.4370, loss: 4.5564
	step 4898:lm_loss: 4.5267, ppl: 92.4503, loss: 4.5566
	step 4899:lm_loss: 4.5268, ppl: 92.4646, loss: 4.5567
	step 4900:lm_loss: 4.5268, ppl: 92.4658, loss: 4.5567
	step 4901:lm_loss: 4.5269, ppl: 92.4691, loss: 4.5567
	step 4902:lm_loss: 4.5270, ppl: 92.4800, loss: 4.5568
	step 4903:lm_loss: 4.5268, ppl: 92.4668, loss: 4.5567
	step 4904:lm_loss: 4.5265, ppl: 92.4389, loss: 4.5564
	step 4905:lm_loss: 4.5264, ppl: 92.4252, loss: 4.5562
	step 4906:lm_loss: 4.5266, ppl: 92.4396, loss: 4.5564
	step 4907:lm_loss: 4.5265, ppl: 92.4365, loss: 4.5563
	step 4908:lm_loss: 4.5264, ppl: 92.4237, loss: 4.5561
	step 4909:lm_loss: 4.5265, ppl: 92.4386, loss: 4.5563
	step 4910:lm_loss: 4.5266, ppl: 92.4427, loss: 4.5563
	step 4911:lm_loss: 4.5262, ppl: 92.4098, loss: 4.5560
	step 4912:lm_loss: 4.5259, ppl: 92.3830, loss: 4.5558
	step 4913:lm_loss: 4.5259, ppl: 92.3770, loss: 4.5557
	step 4914:lm_loss: 4.5258, ppl: 92.3739, loss: 4.5557
	step 4915:lm_loss: 4.5259, ppl: 92.3836, loss: 4.5558
	step 4916:lm_loss: 4.5259, ppl: 92.3773, loss: 4.5558
	step 4917:lm_loss: 4.5259, ppl: 92.3810, loss: 4.5558
	step 4918:lm_loss: 4.5259, ppl: 92.3778, loss: 4.5557
	step 4919:lm_loss: 4.5259, ppl: 92.3762, loss: 4.5557
	step 4920:lm_loss: 4.5260, ppl: 92.3860, loss: 4.5558
	step 4921:lm_loss: 4.5257, ppl: 92.3573, loss: 4.5556
	step 4922:lm_loss: 4.5256, ppl: 92.3489, loss: 4.5555
	step 4923:lm_loss: 4.5255, ppl: 92.3461, loss: 4.5554
	step 4924:lm_loss: 4.5256, ppl: 92.3553, loss: 4.5555
	step 4925:lm_loss: 4.5257, ppl: 92.3615, loss: 4.5556
	step 4926:lm_loss: 4.5257, ppl: 92.3587, loss: 4.5556
	step 4927:lm_loss: 4.5256, ppl: 92.3475, loss: 4.5554
	step 4928:lm_loss: 4.5255, ppl: 92.3454, loss: 4.5554
	step 4929:lm_loss: 4.5256, ppl: 92.3498, loss: 4.5554
	step 4930:lm_loss: 4.5254, ppl: 92.3319, loss: 4.5551
	step 4931:lm_loss: 4.5254, ppl: 92.3288, loss: 4.5551
	step 4932:lm_loss: 4.5254, ppl: 92.3329, loss: 4.5551
	step 4933:lm_loss: 4.5252, ppl: 92.3166, loss: 4.5550
	step 4934:lm_loss: 4.5252, ppl: 92.3114, loss: 4.5549
	step 4935:lm_loss: 4.5254, ppl: 92.3299, loss: 4.5550
	step 4936:lm_loss: 4.5252, ppl: 92.3184, loss: 4.5549
	step 4937:lm_loss: 4.5254, ppl: 92.3363, loss: 4.5551
	step 4938:lm_loss: 4.5254, ppl: 92.3315, loss: 4.5551
	step 4939:lm_loss: 4.5255, ppl: 92.3388, loss: 4.5551
	step 4940:lm_loss: 4.5254, ppl: 92.3294, loss: 4.5550
	step 4941:lm_loss: 4.5253, ppl: 92.3237, loss: 4.5550
	step 4942:lm_loss: 4.5252, ppl: 92.3152, loss: 4.5549
	step 4943:lm_loss: 4.5252, ppl: 92.3142, loss: 4.5548
	step 4944:lm_loss: 4.5253, ppl: 92.3194, loss: 4.5549
	step 4945:lm_loss: 4.5252, ppl: 92.3181, loss: 4.5548
	step 4946:lm_loss: 4.5251, ppl: 92.3042, loss: 4.5547
	step 4947:lm_loss: 4.5252, ppl: 92.3142, loss: 4.5549
	step 4948:lm_loss: 4.5254, ppl: 92.3356, loss: 4.5551
	step 4949:lm_loss: 4.5252, ppl: 92.3184, loss: 4.5548
	step 4950:lm_loss: 4.5253, ppl: 92.3199, loss: 4.5549
	step 4951:lm_loss: 4.5253, ppl: 92.3238, loss: 4.5549
	step 4952:lm_loss: 4.5254, ppl: 92.3348, loss: 4.5550
	step 4953:lm_loss: 4.5251, ppl: 92.3014, loss: 4.5547
	step 4954:lm_loss: 4.5251, ppl: 92.3048, loss: 4.5547
	step 4955:lm_loss: 4.5251, ppl: 92.3016, loss: 4.5547
	step 4956:lm_loss: 4.5251, ppl: 92.3007, loss: 4.5547
	step 4957:lm_loss: 4.5253, ppl: 92.3228, loss: 4.5549
	step 4958:lm_loss: 4.5253, ppl: 92.3260, loss: 4.5550
	step 4959:lm_loss: 4.5255, ppl: 92.3447, loss: 4.5552
	step 4960:lm_loss: 4.5256, ppl: 92.3550, loss: 4.5554
	step 4961:lm_loss: 4.5258, ppl: 92.3676, loss: 4.5556
	step 4962:lm_loss: 4.5259, ppl: 92.3762, loss: 4.5557
	step 4963:lm_loss: 4.5257, ppl: 92.3597, loss: 4.5555
	step 4964:lm_loss: 4.5256, ppl: 92.3514, loss: 4.5554
	step 4965:lm_loss: 4.5257, ppl: 92.3616, loss: 4.5556
	step 4966:lm_loss: 4.5256, ppl: 92.3520, loss: 4.5555
	step 4967:lm_loss: 4.5258, ppl: 92.3675, loss: 4.5558
	step 4968:lm_loss: 4.5258, ppl: 92.3653, loss: 4.5557
	step 4969:lm_loss: 4.5258, ppl: 92.3668, loss: 4.5557
	step 4970:lm_loss: 4.5258, ppl: 92.3678, loss: 4.5558
	step 4971:lm_loss: 4.5259, ppl: 92.3822, loss: 4.5559
	step 4972:lm_loss: 4.5258, ppl: 92.3692, loss: 4.5557
	step 4973:lm_loss: 4.5259, ppl: 92.3758, loss: 4.5558
	step 4974:lm_loss: 4.5258, ppl: 92.3721, loss: 4.5558
	step 4975:lm_loss: 4.5257, ppl: 92.3600, loss: 4.5556
	step 4976:lm_loss: 4.5259, ppl: 92.3762, loss: 4.5558
	step 4977:lm_loss: 4.5258, ppl: 92.3716, loss: 4.5557
	step 4978:lm_loss: 4.5257, ppl: 92.3605, loss: 4.5556
	step 4979:lm_loss: 4.5257, ppl: 92.3639, loss: 4.5556
	step 4980:lm_loss: 4.5257, ppl: 92.3636, loss: 4.5556
	step 4981:lm_loss: 4.5257, ppl: 92.3626, loss: 4.5556
	step 4982:lm_loss: 4.5255, ppl: 92.3415, loss: 4.5553
	step 4983:lm_loss: 4.5257, ppl: 92.3609, loss: 4.5555
	step 4984:lm_loss: 4.5256, ppl: 92.3521, loss: 4.5554
	step 4985:lm_loss: 4.5255, ppl: 92.3435, loss: 4.5553
	step 4986:lm_loss: 4.5255, ppl: 92.3413, loss: 4.5553
	step 4987:lm_loss: 4.5253, ppl: 92.3267, loss: 4.5552
	step 4988:lm_loss: 4.5252, ppl: 92.3155, loss: 4.5551
	step 4989:lm_loss: 4.5252, ppl: 92.3170, loss: 4.5551
	step 4990:lm_loss: 4.5252, ppl: 92.3187, loss: 4.5551
	step 4991:lm_loss: 4.5255, ppl: 92.3426, loss: 4.5555
	step 4992:lm_loss: 4.5255, ppl: 92.3378, loss: 4.5554
	step 4993:lm_loss: 4.5254, ppl: 92.3313, loss: 4.5554
	step 4994:lm_loss: 4.5253, ppl: 92.3202, loss: 4.5552
	step 4995:lm_loss: 4.5252, ppl: 92.3120, loss: 4.5551
	step 4996:lm_loss: 4.5254, ppl: 92.3297, loss: 4.5552
	step 4997:lm_loss: 4.5255, ppl: 92.3450, loss: 4.5554
	step 4998:lm_loss: 4.5255, ppl: 92.3436, loss: 4.5554
	step 4999:lm_loss: 4.5256, ppl: 92.3513, loss: 4.5555
	step 5000:lm_loss: 4.5255, ppl: 92.3385, loss: 4.5554
	step 5001:lm_loss: 4.5255, ppl: 92.3389, loss: 4.5553
	step 5002:lm_loss: 4.5253, ppl: 92.3201, loss: 4.5552
	step 5003:lm_loss: 4.5252, ppl: 92.3101, loss: 4.5550
	step 5004:lm_loss: 4.5250, ppl: 92.2961, loss: 4.5548
	step 5005:lm_loss: 4.5249, ppl: 92.2827, loss: 4.5547
	step 5006:lm_loss: 4.5248, ppl: 92.2760, loss: 4.5547
	step 5007:lm_loss: 4.5249, ppl: 92.2832, loss: 4.5547
	step 5008:lm_loss: 4.5249, ppl: 92.2845, loss: 4.5547
	step 5009:lm_loss: 4.5249, ppl: 92.2864, loss: 4.5548
	step 5010:lm_loss: 4.5249, ppl: 92.2870, loss: 4.5548
	step 5011:lm_loss: 4.5248, ppl: 92.2784, loss: 4.5546
	step 5012:lm_loss: 4.5249, ppl: 92.2833, loss: 4.5547
	step 5013:lm_loss: 4.5249, ppl: 92.2873, loss: 4.5548
	step 5014:lm_loss: 4.5247, ppl: 92.2713, loss: 4.5545
	step 5015:lm_loss: 4.5249, ppl: 92.2893, loss: 4.5547
	step 5016:lm_loss: 4.5249, ppl: 92.2872, loss: 4.5546
	step 5017:lm_loss: 4.5250, ppl: 92.2930, loss: 4.5547
	step 5018:lm_loss: 4.5247, ppl: 92.2689, loss: 4.5545
	step 5019:lm_loss: 4.5247, ppl: 92.2691, loss: 4.5544
	step 5020:lm_loss: 4.5247, ppl: 92.2695, loss: 4.5544
	step 5021:lm_loss: 4.5248, ppl: 92.2764, loss: 4.5545
	step 5022:lm_loss: 4.5248, ppl: 92.2809, loss: 4.5546
	step 5023:lm_loss: 4.5248, ppl: 92.2749, loss: 4.5545
	step 5024:lm_loss: 4.5250, ppl: 92.2991, loss: 4.5547
	step 5025:lm_loss: 4.5251, ppl: 92.3012, loss: 4.5548
	step 5026:lm_loss: 4.5249, ppl: 92.2877, loss: 4.5546
	step 5027:lm_loss: 4.5249, ppl: 92.2869, loss: 4.5546
	step 5028:lm_loss: 4.5250, ppl: 92.2944, loss: 4.5547
	step 5029:lm_loss: 4.5250, ppl: 92.2984, loss: 4.5548
	step 5030:lm_loss: 4.5250, ppl: 92.2959, loss: 4.5547
	step 5031:lm_loss: 4.5251, ppl: 92.3024, loss: 4.5548
	step 5032:lm_loss: 4.5253, ppl: 92.3246, loss: 4.5550
	step 5033:lm_loss: 4.5254, ppl: 92.3368, loss: 4.5552
	step 5034:lm_loss: 4.5253, ppl: 92.3240, loss: 4.5551
	step 5035:lm_loss: 4.5254, ppl: 92.3287, loss: 4.5551
	step 5036:lm_loss: 4.5253, ppl: 92.3203, loss: 4.5550
	step 5037:lm_loss: 4.5254, ppl: 92.3311, loss: 4.5551
	step 5038:lm_loss: 4.5255, ppl: 92.3427, loss: 4.5552
	step 5039:lm_loss: 4.5258, ppl: 92.3656, loss: 4.5554
	step 5040:lm_loss: 4.5254, ppl: 92.3346, loss: 4.5550
	step 5041:lm_loss: 4.5255, ppl: 92.3416, loss: 4.5551
	step 5042:lm_loss: 4.5256, ppl: 92.3530, loss: 4.5552
	step 5043:lm_loss: 4.5259, ppl: 92.3783, loss: 4.5554
	step 5044:lm_loss: 4.5258, ppl: 92.3687, loss: 4.5552
	step 5045:lm_loss: 4.5258, ppl: 92.3663, loss: 4.5552
	step 5046:lm_loss: 4.5259, ppl: 92.3833, loss: 4.5553
	step 5047:lm_loss: 4.5258, ppl: 92.3700, loss: 4.5552
	step 5048:lm_loss: 4.5258, ppl: 92.3693, loss: 4.5552
	step 5049:lm_loss: 4.5257, ppl: 92.3650, loss: 4.5551
	step 5050:lm_loss: 4.5259, ppl: 92.3769, loss: 4.5553
	step 5051:lm_loss: 4.5259, ppl: 92.3806, loss: 4.5553
	step 5052:lm_loss: 4.5260, ppl: 92.3903, loss: 4.5554
	step 5053:lm_loss: 4.5261, ppl: 92.4001, loss: 4.5556
	step 5054:lm_loss: 4.5264, ppl: 92.4213, loss: 4.5558
	step 5055:lm_loss: 4.5262, ppl: 92.4065, loss: 4.5557
	step 5056:lm_loss: 4.5262, ppl: 92.4076, loss: 4.5557
	step 5057:lm_loss: 4.5263, ppl: 92.4164, loss: 4.5558
	step 5058:lm_loss: 4.5263, ppl: 92.4164, loss: 4.5558
	step 5059:lm_loss: 4.5264, ppl: 92.4247, loss: 4.5559
	step 5060:lm_loss: 4.5265, ppl: 92.4387, loss: 4.5560
	step 5061:lm_loss: 4.5264, ppl: 92.4282, loss: 4.5558
	step 5062:lm_loss: 4.5263, ppl: 92.4201, loss: 4.5557
	step 5063:lm_loss: 4.5264, ppl: 92.4225, loss: 4.5558
	step 5064:lm_loss: 4.5264, ppl: 92.4256, loss: 4.5558
	step 5065:lm_loss: 4.5263, ppl: 92.4116, loss: 4.5556
	step 5066:lm_loss: 4.5263, ppl: 92.4168, loss: 4.5557
	step 5067:lm_loss: 4.5264, ppl: 92.4215, loss: 4.5558
	step 5068:lm_loss: 4.5264, ppl: 92.4274, loss: 4.5558
	step 5069:lm_loss: 4.5264, ppl: 92.4291, loss: 4.5558
	step 5070:lm_loss: 4.5265, ppl: 92.4345, loss: 4.5559
	step 5071:lm_loss: 4.5266, ppl: 92.4457, loss: 4.5560
	step 5072:lm_loss: 4.5265, ppl: 92.4357, loss: 4.5558
	step 5073:lm_loss: 4.5266, ppl: 92.4412, loss: 4.5558
	step 5074:lm_loss: 4.5266, ppl: 92.4437, loss: 4.5559
	step 5075:lm_loss: 4.5267, ppl: 92.4558, loss: 4.5559
	step 5076:lm_loss: 4.5268, ppl: 92.4587, loss: 4.5560
	step 5077:lm_loss: 4.5268, ppl: 92.4618, loss: 4.5560
	step 5078:lm_loss: 4.5268, ppl: 92.4605, loss: 4.5560
	step 5079:lm_loss: 4.5268, ppl: 92.4631, loss: 4.5560
	step 5080:lm_loss: 4.5268, ppl: 92.4645, loss: 4.5560
	step 5081:lm_loss: 4.5270, ppl: 92.4784, loss: 4.5562
	step 5082:lm_loss: 4.5270, ppl: 92.4770, loss: 4.5562
	step 5083:lm_loss: 4.5270, ppl: 92.4814, loss: 4.5562
	step 5084:lm_loss: 4.5270, ppl: 92.4769, loss: 4.5562
	step 5085:lm_loss: 4.5265, ppl: 92.4340, loss: 4.5559
	step 5086:lm_loss: 4.5266, ppl: 92.4451, loss: 4.5559
	step 5087:lm_loss: 4.5265, ppl: 92.4308, loss: 4.5558
	step 5088:lm_loss: 4.5265, ppl: 92.4303, loss: 4.5558
	step 5089:lm_loss: 4.5268, ppl: 92.4611, loss: 4.5561
	step 5090:lm_loss: 4.5265, ppl: 92.4383, loss: 4.5559
	step 5091:lm_loss: 4.5269, ppl: 92.4672, loss: 4.5563
	step 5092:lm_loss: 4.5268, ppl: 92.4668, loss: 4.5563
	step 5093:lm_loss: 4.5268, ppl: 92.4617, loss: 4.5562
	step 5094:lm_loss: 4.5268, ppl: 92.4661, loss: 4.5563
	step 5095:lm_loss: 4.5270, ppl: 92.4773, loss: 4.5564
	step 5096:lm_loss: 4.5270, ppl: 92.4777, loss: 4.5564
	step 5097:lm_loss: 4.5268, ppl: 92.4632, loss: 4.5562
	step 5098:lm_loss: 4.5269, ppl: 92.4698, loss: 4.5562
	step 5099:lm_loss: 4.5270, ppl: 92.4781, loss: 4.5564
	step 5100:lm_loss: 4.5271, ppl: 92.4880, loss: 4.5565
	step 5101:lm_loss: 4.5270, ppl: 92.4774, loss: 4.5564
	step 5102:lm_loss: 4.5269, ppl: 92.4750, loss: 4.5563
	step 5103:lm_loss: 4.5271, ppl: 92.4866, loss: 4.5564
	step 5104:lm_loss: 4.5271, ppl: 92.4887, loss: 4.5564
	step 5105:lm_loss: 4.5271, ppl: 92.4902, loss: 4.5565
	step 5106:lm_loss: 4.5270, ppl: 92.4838, loss: 4.5564
	step 5107:lm_loss: 4.5271, ppl: 92.4929, loss: 4.5565
	step 5108:lm_loss: 4.5273, ppl: 92.5098, loss: 4.5566
	step 5109:lm_loss: 4.5273, ppl: 92.5124, loss: 4.5567
	step 5110:lm_loss: 4.5276, ppl: 92.5323, loss: 4.5569
	step 5111:lm_loss: 4.5276, ppl: 92.5326, loss: 4.5569
	step 5112:lm_loss: 4.5273, ppl: 92.5121, loss: 4.5566
	step 5113:lm_loss: 4.5273, ppl: 92.5118, loss: 4.5566
	step 5114:lm_loss: 4.5274, ppl: 92.5218, loss: 4.5567
	step 5115:lm_loss: 4.5275, ppl: 92.5279, loss: 4.5567
	step 5116:lm_loss: 4.5274, ppl: 92.5151, loss: 4.5566
	step 5117:lm_loss: 4.5273, ppl: 92.5122, loss: 4.5566
	step 5118:lm_loss: 4.5273, ppl: 92.5117, loss: 4.5566
	step 5119:lm_loss: 4.5276, ppl: 92.5318, loss: 4.5567
	step 5120:lm_loss: 4.5275, ppl: 92.5301, loss: 4.5567
	step 5121:lm_loss: 4.5277, ppl: 92.5462, loss: 4.5569
	step 5122:lm_loss: 4.5278, ppl: 92.5538, loss: 4.5569
	step 5123:lm_loss: 4.5278, ppl: 92.5557, loss: 4.5569
	step 5124:lm_loss: 4.5279, ppl: 92.5597, loss: 4.5570
	step 5125:lm_loss: 4.5275, ppl: 92.5298, loss: 4.5567
	step 5126:lm_loss: 4.5274, ppl: 92.5207, loss: 4.5566
	step 5127:lm_loss: 4.5276, ppl: 92.5316, loss: 4.5566
	step 5128:lm_loss: 4.5276, ppl: 92.5323, loss: 4.5566
	step 5129:lm_loss: 4.5273, ppl: 92.5058, loss: 4.5562
	step 5130:lm_loss: 4.5275, ppl: 92.5252, loss: 4.5564
	step 5131:lm_loss: 4.5276, ppl: 92.5367, loss: 4.5565
	step 5132:lm_loss: 4.5274, ppl: 92.5135, loss: 4.5562
	step 5133:lm_loss: 4.5273, ppl: 92.5060, loss: 4.5561
	step 5134:lm_loss: 4.5274, ppl: 92.5138, loss: 4.5562
	step 5135:lm_loss: 4.5274, ppl: 92.5202, loss: 4.5563
	step 5136:lm_loss: 4.5274, ppl: 92.5140, loss: 4.5562
	step 5137:lm_loss: 4.5274, ppl: 92.5182, loss: 4.5562
	step 5138:lm_loss: 4.5274, ppl: 92.5177, loss: 4.5562
	step 5139:lm_loss: 4.5276, ppl: 92.5391, loss: 4.5565
	step 5140:lm_loss: 4.5277, ppl: 92.5417, loss: 4.5565
	step 5141:lm_loss: 4.5277, ppl: 92.5481, loss: 4.5566
	step 5142:lm_loss: 4.5276, ppl: 92.5393, loss: 4.5565
	step 5143:lm_loss: 4.5278, ppl: 92.5552, loss: 4.5567
	step 5144:lm_loss: 4.5279, ppl: 92.5638, loss: 4.5568
	step 5145:lm_loss: 4.5277, ppl: 92.5423, loss: 4.5565
	step 5146:lm_loss: 4.5275, ppl: 92.5274, loss: 4.5564
	step 5147:lm_loss: 4.5279, ppl: 92.5632, loss: 4.5566
	step 5148:lm_loss: 4.5279, ppl: 92.5644, loss: 4.5567
	step 5149:lm_loss: 4.5279, ppl: 92.5617, loss: 4.5566
	step 5150:lm_loss: 4.5279, ppl: 92.5606, loss: 4.5566
	step 5151:lm_loss: 4.5279, ppl: 92.5684, loss: 4.5567
	step 5152:lm_loss: 4.5278, ppl: 92.5565, loss: 4.5566
	step 5153:lm_loss: 4.5278, ppl: 92.5570, loss: 4.5566
	step 5154:lm_loss: 4.5281, ppl: 92.5798, loss: 4.5569
	step 5155:lm_loss: 4.5282, ppl: 92.5911, loss: 4.5571
	step 5156:lm_loss: 4.5281, ppl: 92.5846, loss: 4.5570
	step 5157:lm_loss: 4.5283, ppl: 92.6045, loss: 4.5573
	step 5158:lm_loss: 4.5285, ppl: 92.6176, loss: 4.5574
	step 5159:lm_loss: 4.5287, ppl: 92.6352, loss: 4.5576
	step 5160:lm_loss: 4.5287, ppl: 92.6425, loss: 4.5576
	step 5161:lm_loss: 4.5287, ppl: 92.6350, loss: 4.5575
	step 5162:lm_loss: 4.5288, ppl: 92.6451, loss: 4.5576
	step 5163:lm_loss: 4.5285, ppl: 92.6149, loss: 4.5574
	step 5164:lm_loss: 4.5283, ppl: 92.6023, loss: 4.5572
	step 5165:lm_loss: 4.5284, ppl: 92.6093, loss: 4.5573
	step 5166:lm_loss: 4.5282, ppl: 92.5889, loss: 4.5570
	step 5167:lm_loss: 4.5281, ppl: 92.5845, loss: 4.5570
	step 5168:lm_loss: 4.5281, ppl: 92.5782, loss: 4.5569
	step 5169:lm_loss: 4.5278, ppl: 92.5572, loss: 4.5567
	step 5170:lm_loss: 4.5279, ppl: 92.5671, loss: 4.5568
	step 5171:lm_loss: 4.5277, ppl: 92.5491, loss: 4.5566
	step 5172:lm_loss: 4.5277, ppl: 92.5474, loss: 4.5566
	step 5173:lm_loss: 4.5279, ppl: 92.5653, loss: 4.5567
	step 5174:lm_loss: 4.5276, ppl: 92.5370, loss: 4.5566
	step 5175:lm_loss: 4.5276, ppl: 92.5387, loss: 4.5566
	step 5176:lm_loss: 4.5277, ppl: 92.5474, loss: 4.5567
	step 5177:lm_loss: 4.5277, ppl: 92.5419, loss: 4.5566
	step 5178:lm_loss: 4.5271, ppl: 92.4858, loss: 4.5563
	step 5179:lm_loss: 4.5267, ppl: 92.4540, loss: 4.5561
	step 5180:lm_loss: 4.5264, ppl: 92.4289, loss: 4.5559
	step 5181:lm_loss: 4.5265, ppl: 92.4306, loss: 4.5559
	step 5182:lm_loss: 4.5265, ppl: 92.4321, loss: 4.5559
	step 5183:lm_loss: 4.5264, ppl: 92.4276, loss: 4.5558
	step 5184:lm_loss: 4.5264, ppl: 92.4266, loss: 4.5558
	step 5185:lm_loss: 4.5267, ppl: 92.4546, loss: 4.5562
	step 5186:lm_loss: 4.5269, ppl: 92.4688, loss: 4.5563
	step 5187:lm_loss: 4.5269, ppl: 92.4691, loss: 4.5563
	step 5188:lm_loss: 4.5269, ppl: 92.4751, loss: 4.5563
	step 5189:lm_loss: 4.5271, ppl: 92.4872, loss: 4.5565
	step 5190:lm_loss: 4.5269, ppl: 92.4746, loss: 4.5563
	step 5191:lm_loss: 4.5269, ppl: 92.4727, loss: 4.5563
	step 5192:lm_loss: 4.5268, ppl: 92.4637, loss: 4.5561
	step 5193:lm_loss: 4.5267, ppl: 92.4570, loss: 4.5560
	step 5194:lm_loss: 4.5267, ppl: 92.4536, loss: 4.5560
	step 5195:lm_loss: 4.5266, ppl: 92.4477, loss: 4.5559
	step 5196:lm_loss: 4.5271, ppl: 92.4873, loss: 4.5566
	step 5197:lm_loss: 4.5272, ppl: 92.4998, loss: 4.5567
	step 5198:lm_loss: 4.5271, ppl: 92.4856, loss: 4.5566
	step 5199:lm_loss: 4.5272, ppl: 92.4991, loss: 4.5567
	step 5200:lm_loss: 4.5273, ppl: 92.5088, loss: 4.5568
	step 5201:lm_loss: 4.5271, ppl: 92.4880, loss: 4.5566
	step 5202:lm_loss: 4.5271, ppl: 92.4863, loss: 4.5566
	step 5203:lm_loss: 4.5271, ppl: 92.4876, loss: 4.5566
	step 5204:lm_loss: 4.5270, ppl: 92.4824, loss: 4.5565
	step 5205:lm_loss: 4.5269, ppl: 92.4721, loss: 4.5563
	step 5206:lm_loss: 4.5267, ppl: 92.4551, loss: 4.5562
	step 5207:lm_loss: 4.5267, ppl: 92.4519, loss: 4.5561
	step 5208:lm_loss: 4.5269, ppl: 92.4673, loss: 4.5563
	step 5209:lm_loss: 4.5270, ppl: 92.4764, loss: 4.5564
	step 5210:lm_loss: 4.5270, ppl: 92.4803, loss: 4.5565
	step 5211:lm_loss: 4.5269, ppl: 92.4758, loss: 4.5564
	step 5212:lm_loss: 4.5270, ppl: 92.4843, loss: 4.5565
	step 5213:lm_loss: 4.5269, ppl: 92.4715, loss: 4.5563
	step 5214:lm_loss: 4.5267, ppl: 92.4559, loss: 4.5562
	step 5215:lm_loss: 4.5268, ppl: 92.4630, loss: 4.5562
	step 5216:lm_loss: 4.5270, ppl: 92.4827, loss: 4.5564
	step 5217:lm_loss: 4.5270, ppl: 92.4790, loss: 4.5563
	step 5218:lm_loss: 4.5272, ppl: 92.4967, loss: 4.5565
	step 5219:lm_loss: 4.5270, ppl: 92.4798, loss: 4.5563
	step 5220:lm_loss: 4.5270, ppl: 92.4775, loss: 4.5563
	step 5221:lm_loss: 4.5269, ppl: 92.4702, loss: 4.5563
	step 5222:lm_loss: 4.5270, ppl: 92.4804, loss: 4.5564
	step 5223:lm_loss: 4.5272, ppl: 92.4951, loss: 4.5565
	step 5224:lm_loss: 4.5274, ppl: 92.5206, loss: 4.5567
	step 5225:lm_loss: 4.5276, ppl: 92.5404, loss: 4.5569
	step 5226:lm_loss: 4.5278, ppl: 92.5580, loss: 4.5571
	step 5227:lm_loss: 4.5275, ppl: 92.5316, loss: 4.5568
	step 5228:lm_loss: 4.5277, ppl: 92.5490, loss: 4.5571
	step 5229:lm_loss: 4.5278, ppl: 92.5549, loss: 4.5571
	step 5230:lm_loss: 4.5279, ppl: 92.5663, loss: 4.5572
	step 5231:lm_loss: 4.5279, ppl: 92.5668, loss: 4.5572
	step 5232:lm_loss: 4.5278, ppl: 92.5567, loss: 4.5571
	step 5233:lm_loss: 4.5276, ppl: 92.5400, loss: 4.5570
	step 5234:lm_loss: 4.5277, ppl: 92.5456, loss: 4.5570
	step 5235:lm_loss: 4.5276, ppl: 92.5351, loss: 4.5569
	step 5236:lm_loss: 4.5277, ppl: 92.5446, loss: 4.5570
	step 5237:lm_loss: 4.5279, ppl: 92.5684, loss: 4.5572
	step 5238:lm_loss: 4.5279, ppl: 92.5673, loss: 4.5572
	step 5239:lm_loss: 4.5278, ppl: 92.5554, loss: 4.5570
	step 5240:lm_loss: 4.5280, ppl: 92.5705, loss: 4.5572
	step 5241:lm_loss: 4.5278, ppl: 92.5536, loss: 4.5570
	step 5242:lm_loss: 4.5276, ppl: 92.5385, loss: 4.5569
	step 5243:lm_loss: 4.5276, ppl: 92.5358, loss: 4.5568
	step 5244:lm_loss: 4.5274, ppl: 92.5158, loss: 4.5566
	step 5245:lm_loss: 4.5272, ppl: 92.5017, loss: 4.5564
	step 5246:lm_loss: 4.5271, ppl: 92.4900, loss: 4.5562
	step 5247:lm_loss: 4.5269, ppl: 92.4727, loss: 4.5560
	step 5248:lm_loss: 4.5273, ppl: 92.5049, loss: 4.5565
	step 5249:lm_loss: 4.5272, ppl: 92.5004, loss: 4.5564
	step 5250:lm_loss: 4.5273, ppl: 92.5057, loss: 4.5565
	step 5251:lm_loss: 4.5266, ppl: 92.4398, loss: 4.5560
	step 5252:lm_loss: 4.5265, ppl: 92.4373, loss: 4.5560
	step 5253:lm_loss: 4.5263, ppl: 92.4169, loss: 4.5556
	step 5254:lm_loss: 4.5263, ppl: 92.4144, loss: 4.5556
	step 5255:lm_loss: 4.5258, ppl: 92.3672, loss: 4.5553
	step 5256:lm_loss: 4.5259, ppl: 92.3765, loss: 4.5555
	step 5257:lm_loss: 4.5258, ppl: 92.3721, loss: 4.5554
	step 5258:lm_loss: 4.5260, ppl: 92.3906, loss: 4.5556
	step 5259:lm_loss: 4.5260, ppl: 92.3838, loss: 4.5555
	step 5260:lm_loss: 4.5259, ppl: 92.3824, loss: 4.5555
	step 5261:lm_loss: 4.5254, ppl: 92.3364, loss: 4.5552
	step 5262:lm_loss: 4.5253, ppl: 92.3281, loss: 4.5551
	step 5263:lm_loss: 4.5253, ppl: 92.3279, loss: 4.5551
	step 5264:lm_loss: 4.5249, ppl: 92.2859, loss: 4.5548
	step 5265:lm_loss: 4.5249, ppl: 92.2849, loss: 4.5548
	step 5266:lm_loss: 4.5249, ppl: 92.2889, loss: 4.5548
	step 5267:lm_loss: 4.5250, ppl: 92.2987, loss: 4.5549
	step 5268:lm_loss: 4.5250, ppl: 92.2932, loss: 4.5549
	step 5269:lm_loss: 4.5248, ppl: 92.2808, loss: 4.5547
	step 5270:lm_loss: 4.5247, ppl: 92.2664, loss: 4.5545
	step 5271:lm_loss: 4.5245, ppl: 92.2509, loss: 4.5544
	step 5272:lm_loss: 4.5247, ppl: 92.2726, loss: 4.5546
	step 5273:lm_loss: 4.5249, ppl: 92.2866, loss: 4.5547
	step 5274:lm_loss: 4.5250, ppl: 92.2949, loss: 4.5548
	step 5275:lm_loss: 4.5250, ppl: 92.3003, loss: 4.5548
	step 5276:lm_loss: 4.5251, ppl: 92.3006, loss: 4.5548
	step 5277:lm_loss: 4.5250, ppl: 92.2938, loss: 4.5548
	step 5278:lm_loss: 4.5248, ppl: 92.2768, loss: 4.5546
	step 5279:lm_loss: 4.5247, ppl: 92.2668, loss: 4.5544
	step 5280:lm_loss: 4.5246, ppl: 92.2635, loss: 4.5544
	step 5281:lm_loss: 4.5247, ppl: 92.2644, loss: 4.5544
	step 5282:lm_loss: 4.5247, ppl: 92.2671, loss: 4.5544
	step 5283:lm_loss: 4.5246, ppl: 92.2553, loss: 4.5543
	step 5284:lm_loss: 4.5246, ppl: 92.2547, loss: 4.5543
	step 5285:lm_loss: 4.5245, ppl: 92.2489, loss: 4.5542
	step 5286:lm_loss: 4.5246, ppl: 92.2629, loss: 4.5543
	step 5287:lm_loss: 4.5248, ppl: 92.2734, loss: 4.5545
	step 5288:lm_loss: 4.5248, ppl: 92.2770, loss: 4.5545
	step 5289:lm_loss: 4.5249, ppl: 92.2870, loss: 4.5546
	step 5290:lm_loss: 4.5249, ppl: 92.2905, loss: 4.5547
	step 5291:lm_loss: 4.5248, ppl: 92.2786, loss: 4.5546
	step 5292:lm_loss: 4.5249, ppl: 92.2844, loss: 4.5547
	step 5293:lm_loss: 4.5248, ppl: 92.2810, loss: 4.5546
	step 5294:lm_loss: 4.5247, ppl: 92.2652, loss: 4.5545
	step 5295:lm_loss: 4.5248, ppl: 92.2801, loss: 4.5546
	step 5296:lm_loss: 4.5250, ppl: 92.2958, loss: 4.5547
	step 5297:lm_loss: 4.5249, ppl: 92.2913, loss: 4.5547
	step 5298:lm_loss: 4.5251, ppl: 92.3007, loss: 4.5548
	step 5299:lm_loss: 4.5252, ppl: 92.3129, loss: 4.5549
	step 5300:lm_loss: 4.5252, ppl: 92.3162, loss: 4.5549
	step 5301:lm_loss: 4.5252, ppl: 92.3188, loss: 4.5550
	step 5302:lm_loss: 4.5254, ppl: 92.3319, loss: 4.5551
	step 5303:lm_loss: 4.5248, ppl: 92.2798, loss: 4.5549
	step 5304:lm_loss: 4.5249, ppl: 92.2848, loss: 4.5550
	step 5305:lm_loss: 4.5248, ppl: 92.2801, loss: 4.5549
	step 5306:lm_loss: 4.5247, ppl: 92.2650, loss: 4.5547
	step 5307:lm_loss: 4.5246, ppl: 92.2618, loss: 4.5547
	step 5308:lm_loss: 4.5246, ppl: 92.2593, loss: 4.5547
	step 5309:lm_loss: 4.5243, ppl: 92.2340, loss: 4.5544
	step 5310:lm_loss: 4.5243, ppl: 92.2345, loss: 4.5544
	step 5311:lm_loss: 4.5245, ppl: 92.2470, loss: 4.5546
	step 5312:lm_loss: 4.5243, ppl: 92.2359, loss: 4.5544
	step 5313:lm_loss: 4.5244, ppl: 92.2395, loss: 4.5545
	step 5314:lm_loss: 4.5242, ppl: 92.2223, loss: 4.5543
	step 5315:lm_loss: 4.5242, ppl: 92.2259, loss: 4.5543
	step 5316:lm_loss: 4.5242, ppl: 92.2256, loss: 4.5543
	step 5317:lm_loss: 4.5241, ppl: 92.2092, loss: 4.5542
	step 5318:lm_loss: 4.5239, ppl: 92.1982, loss: 4.5540
	step 5319:lm_loss: 4.5240, ppl: 92.2017, loss: 4.5540
	step 5320:lm_loss: 4.5237, ppl: 92.1741, loss: 4.5538
	step 5321:lm_loss: 4.5237, ppl: 92.1732, loss: 4.5538
	step 5322:lm_loss: 4.5236, ppl: 92.1678, loss: 4.5537
	step 5323:lm_loss: 4.5237, ppl: 92.1759, loss: 4.5538
	step 5324:lm_loss: 4.5235, ppl: 92.1554, loss: 4.5537
	step 5325:lm_loss: 4.5235, ppl: 92.1589, loss: 4.5538
	step 5326:lm_loss: 4.5236, ppl: 92.1687, loss: 4.5539
	step 5327:lm_loss: 4.5237, ppl: 92.1755, loss: 4.5540
	step 5328:lm_loss: 4.5236, ppl: 92.1630, loss: 4.5538
	step 5329:lm_loss: 4.5235, ppl: 92.1548, loss: 4.5537
	step 5330:lm_loss: 4.5235, ppl: 92.1620, loss: 4.5538
	step 5331:lm_loss: 4.5238, ppl: 92.1840, loss: 4.5540
	step 5332:lm_loss: 4.5240, ppl: 92.2078, loss: 4.5542
	step 5333:lm_loss: 4.5242, ppl: 92.2177, loss: 4.5543
	step 5334:lm_loss: 4.5242, ppl: 92.2201, loss: 4.5543
	step 5335:lm_loss: 4.5244, ppl: 92.2418, loss: 4.5545
	step 5336:lm_loss: 4.5242, ppl: 92.2259, loss: 4.5543
	step 5337:lm_loss: 4.5243, ppl: 92.2291, loss: 4.5544
	step 5338:lm_loss: 4.5241, ppl: 92.2144, loss: 4.5542
	step 5339:lm_loss: 4.5243, ppl: 92.2303, loss: 4.5544
	step 5340:lm_loss: 4.5242, ppl: 92.2225, loss: 4.5543
	step 5341:lm_loss: 4.5240, ppl: 92.2046, loss: 4.5541
	step 5342:lm_loss: 4.5240, ppl: 92.2074, loss: 4.5541
	step 5343:lm_loss: 4.5241, ppl: 92.2110, loss: 4.5542
	step 5344:lm_loss: 4.5240, ppl: 92.2082, loss: 4.5541
	step 5345:lm_loss: 4.5241, ppl: 92.2097, loss: 4.5541
	step 5346:lm_loss: 4.5240, ppl: 92.2066, loss: 4.5541
	step 5347:lm_loss: 4.5242, ppl: 92.2234, loss: 4.5543
	step 5348:lm_loss: 4.5243, ppl: 92.2350, loss: 4.5544
	step 5349:lm_loss: 4.5244, ppl: 92.2381, loss: 4.5544
	step 5350:lm_loss: 4.5246, ppl: 92.2605, loss: 4.5547
	step 5351:lm_loss: 4.5247, ppl: 92.2670, loss: 4.5548
	step 5352:lm_loss: 4.5246, ppl: 92.2558, loss: 4.5547
	step 5353:lm_loss: 4.5246, ppl: 92.2630, loss: 4.5547
	step 5354:lm_loss: 4.5247, ppl: 92.2681, loss: 4.5548
	step 5355:lm_loss: 4.5245, ppl: 92.2518, loss: 4.5546
	step 5356:lm_loss: 4.5245, ppl: 92.2537, loss: 4.5546
	step 5357:lm_loss: 4.5246, ppl: 92.2550, loss: 4.5546
	step 5358:lm_loss: 4.5247, ppl: 92.2661, loss: 4.5547
	step 5359:lm_loss: 4.5247, ppl: 92.2663, loss: 4.5547
	step 5360:lm_loss: 4.5247, ppl: 92.2710, loss: 4.5547
	step 5361:lm_loss: 4.5248, ppl: 92.2737, loss: 4.5548
	step 5362:lm_loss: 4.5248, ppl: 92.2795, loss: 4.5548
	step 5363:lm_loss: 4.5248, ppl: 92.2765, loss: 4.5548
	step 5364:lm_loss: 4.5249, ppl: 92.2851, loss: 4.5549
	step 5365:lm_loss: 4.5251, ppl: 92.3007, loss: 4.5550
	step 5366:lm_loss: 4.5250, ppl: 92.2914, loss: 4.5549
	step 5367:lm_loss: 4.5249, ppl: 92.2837, loss: 4.5548
	step 5368:lm_loss: 4.5250, ppl: 92.2935, loss: 4.5549
	step 5369:lm_loss: 4.5250, ppl: 92.3004, loss: 4.5550
	step 5370:lm_loss: 4.5250, ppl: 92.2989, loss: 4.5550
	step 5371:lm_loss: 4.5246, ppl: 92.2576, loss: 4.5547
	step 5372:lm_loss: 4.5248, ppl: 92.2748, loss: 4.5549
	step 5373:lm_loss: 4.5247, ppl: 92.2724, loss: 4.5548
	step 5374:lm_loss: 4.5245, ppl: 92.2494, loss: 4.5547
	step 5375:lm_loss: 4.5245, ppl: 92.2497, loss: 4.5547
	step 5376:lm_loss: 4.5244, ppl: 92.2424, loss: 4.5546
	step 5377:lm_loss: 4.5245, ppl: 92.2540, loss: 4.5547
	step 5378:lm_loss: 4.5244, ppl: 92.2404, loss: 4.5546
	step 5379:lm_loss: 4.5241, ppl: 92.2163, loss: 4.5544
	step 5380:lm_loss: 4.5243, ppl: 92.2276, loss: 4.5545
	step 5381:lm_loss: 4.5244, ppl: 92.2407, loss: 4.5546
	step 5382:lm_loss: 4.5242, ppl: 92.2259, loss: 4.5545
	step 5383:lm_loss: 4.5242, ppl: 92.2266, loss: 4.5545
	step 5384:lm_loss: 4.5244, ppl: 92.2422, loss: 4.5546
	step 5385:lm_loss: 4.5242, ppl: 92.2197, loss: 4.5544
	step 5386:lm_loss: 4.5242, ppl: 92.2240, loss: 4.5545
	step 5387:lm_loss: 4.5242, ppl: 92.2244, loss: 4.5545
	step 5388:lm_loss: 4.5241, ppl: 92.2168, loss: 4.5544
	step 5389:lm_loss: 4.5242, ppl: 92.2241, loss: 4.5545
	step 5390:lm_loss: 4.5242, ppl: 92.2215, loss: 4.5544
	step 5391:lm_loss: 4.5239, ppl: 92.1923, loss: 4.5541
	step 5392:lm_loss: 4.5240, ppl: 92.1997, loss: 4.5542
	step 5393:lm_loss: 4.5236, ppl: 92.1672, loss: 4.5538
	step 5394:lm_loss: 4.5237, ppl: 92.1750, loss: 4.5539
	step 5395:lm_loss: 4.5236, ppl: 92.1681, loss: 4.5538
	step 5396:lm_loss: 4.5236, ppl: 92.1634, loss: 4.5538
	step 5397:lm_loss: 4.5236, ppl: 92.1692, loss: 4.5538
	step 5398:lm_loss: 4.5237, ppl: 92.1799, loss: 4.5539
	step 5399:lm_loss: 4.5238, ppl: 92.1853, loss: 4.5540
	step 5400:lm_loss: 4.5239, ppl: 92.1941, loss: 4.5541
	step 5401:lm_loss: 4.5240, ppl: 92.2055, loss: 4.5542
	step 5402:lm_loss: 4.5240, ppl: 92.2014, loss: 4.5541
	step 5403:lm_loss: 4.5239, ppl: 92.1983, loss: 4.5541
	step 5404:lm_loss: 4.5240, ppl: 92.2067, loss: 4.5541
	step 5405:lm_loss: 4.5239, ppl: 92.1985, loss: 4.5541
	step 5406:lm_loss: 4.5241, ppl: 92.2097, loss: 4.5542
	step 5407:lm_loss: 4.5241, ppl: 92.2132, loss: 4.5542
	step 5408:lm_loss: 4.5242, ppl: 92.2252, loss: 4.5544
	step 5409:lm_loss: 4.5242, ppl: 92.2195, loss: 4.5543
	step 5410:lm_loss: 4.5243, ppl: 92.2286, loss: 4.5544
	step 5411:lm_loss: 4.5243, ppl: 92.2281, loss: 4.5544
	step 5412:lm_loss: 4.5244, ppl: 92.2407, loss: 4.5545
	step 5413:lm_loss: 4.5242, ppl: 92.2259, loss: 4.5544
	step 5414:lm_loss: 4.5245, ppl: 92.2528, loss: 4.5546
	step 5415:lm_loss: 4.5245, ppl: 92.2506, loss: 4.5546
	step 5416:lm_loss: 4.5245, ppl: 92.2481, loss: 4.5546
	step 5417:lm_loss: 4.5246, ppl: 92.2576, loss: 4.5547
	step 5418:lm_loss: 4.5246, ppl: 92.2558, loss: 4.5547
	step 5419:lm_loss: 4.5247, ppl: 92.2688, loss: 4.5548
	step 5420:lm_loss: 4.5246, ppl: 92.2585, loss: 4.5547
	step 5421:lm_loss: 4.5246, ppl: 92.2622, loss: 4.5547
	step 5422:lm_loss: 4.5246, ppl: 92.2554, loss: 4.5547
	step 5423:lm_loss: 4.5244, ppl: 92.2429, loss: 4.5545
	step 5424:lm_loss: 4.5244, ppl: 92.2443, loss: 4.5545
	step 5425:lm_loss: 4.5246, ppl: 92.2587, loss: 4.5548
	step 5426:lm_loss: 4.5247, ppl: 92.2698, loss: 4.5549
	step 5427:lm_loss: 4.5249, ppl: 92.2875, loss: 4.5551
	step 5428:lm_loss: 4.5249, ppl: 92.2885, loss: 4.5551
	step 5429:lm_loss: 4.5247, ppl: 92.2673, loss: 4.5549
	step 5430:lm_loss: 4.5245, ppl: 92.2541, loss: 4.5547
	step 5431:lm_loss: 4.5246, ppl: 92.2621, loss: 4.5548
	step 5432:lm_loss: 4.5244, ppl: 92.2385, loss: 4.5546
	step 5433:lm_loss: 4.5246, ppl: 92.2591, loss: 4.5548
	step 5434:lm_loss: 4.5248, ppl: 92.2752, loss: 4.5550
	step 5435:lm_loss: 4.5247, ppl: 92.2696, loss: 4.5549
	step 5436:lm_loss: 4.5248, ppl: 92.2812, loss: 4.5551
	step 5437:lm_loss: 4.5246, ppl: 92.2583, loss: 4.5548
	step 5438:lm_loss: 4.5247, ppl: 92.2664, loss: 4.5549
	step 5439:lm_loss: 4.5246, ppl: 92.2578, loss: 4.5548
	step 5440:lm_loss: 4.5246, ppl: 92.2626, loss: 4.5548
	step 5441:lm_loss: 4.5246, ppl: 92.2591, loss: 4.5548
	step 5442:lm_loss: 4.5245, ppl: 92.2514, loss: 4.5547
	step 5443:lm_loss: 4.5245, ppl: 92.2534, loss: 4.5547
	step 5444:lm_loss: 4.5243, ppl: 92.2279, loss: 4.5545
	step 5445:lm_loss: 4.5244, ppl: 92.2409, loss: 4.5547
	step 5446:lm_loss: 4.5244, ppl: 92.2387, loss: 4.5546
	step 5447:lm_loss: 4.5245, ppl: 92.2525, loss: 4.5548
	step 5448:lm_loss: 4.5244, ppl: 92.2441, loss: 4.5547
	step 5449:lm_loss: 4.5244, ppl: 92.2385, loss: 4.5546
	step 5450:lm_loss: 4.5244, ppl: 92.2423, loss: 4.5546
	step 5451:lm_loss: 4.5247, ppl: 92.2648, loss: 4.5548
	step 5452:lm_loss: 4.5245, ppl: 92.2469, loss: 4.5546
	step 5453:lm_loss: 4.5244, ppl: 92.2442, loss: 4.5546
	step 5454:lm_loss: 4.5246, ppl: 92.2613, loss: 4.5548
	step 5455:lm_loss: 4.5247, ppl: 92.2675, loss: 4.5549
	step 5456:lm_loss: 4.5249, ppl: 92.2823, loss: 4.5550
	step 5457:lm_loss: 4.5248, ppl: 92.2775, loss: 4.5550
	step 5458:lm_loss: 4.5247, ppl: 92.2662, loss: 4.5549
	step 5459:lm_loss: 4.5246, ppl: 92.2611, loss: 4.5548
	step 5460:lm_loss: 4.5246, ppl: 92.2574, loss: 4.5548
	step 5461:lm_loss: 4.5247, ppl: 92.2656, loss: 4.5549
	step 5462:lm_loss: 4.5247, ppl: 92.2703, loss: 4.5549
	step 5463:lm_loss: 4.5247, ppl: 92.2698, loss: 4.5549
	step 5464:lm_loss: 4.5248, ppl: 92.2796, loss: 4.5550
	step 5465:lm_loss: 4.5248, ppl: 92.2760, loss: 4.5550
	step 5466:lm_loss: 4.5248, ppl: 92.2740, loss: 4.5549
	step 5467:lm_loss: 4.5247, ppl: 92.2655, loss: 4.5548
	step 5468:lm_loss: 4.5247, ppl: 92.2666, loss: 4.5548
	step 5469:lm_loss: 4.5249, ppl: 92.2848, loss: 4.5550
	step 5470:lm_loss: 4.5249, ppl: 92.2863, loss: 4.5550
	step 5471:lm_loss: 4.5249, ppl: 92.2887, loss: 4.5550
	step 5472:lm_loss: 4.5249, ppl: 92.2905, loss: 4.5550
	step 5473:lm_loss: 4.5241, ppl: 92.2125, loss: 4.5547
	step 5474:lm_loss: 4.5243, ppl: 92.2323, loss: 4.5548
	step 5475:lm_loss: 4.5244, ppl: 92.2361, loss: 4.5549
	step 5476:lm_loss: 4.5245, ppl: 92.2515, loss: 4.5550
	step 5477:lm_loss: 4.5246, ppl: 92.2614, loss: 4.5551
	step 5478:lm_loss: 4.5247, ppl: 92.2652, loss: 4.5552
	step 5479:lm_loss: 4.5246, ppl: 92.2549, loss: 4.5551
	step 5480:lm_loss: 4.5245, ppl: 92.2470, loss: 4.5550
	step 5481:lm_loss: 4.5246, ppl: 92.2614, loss: 4.5551
	step 5482:lm_loss: 4.5246, ppl: 92.2612, loss: 4.5551
	step 5483:lm_loss: 4.5243, ppl: 92.2319, loss: 4.5549
	step 5484:lm_loss: 4.5242, ppl: 92.2249, loss: 4.5548
	step 5485:lm_loss: 4.5241, ppl: 92.2115, loss: 4.5546
	step 5486:lm_loss: 4.5242, ppl: 92.2190, loss: 4.5547
	step 5487:lm_loss: 4.5242, ppl: 92.2175, loss: 4.5546
	step 5488:lm_loss: 4.5244, ppl: 92.2378, loss: 4.5548
	step 5489:lm_loss: 4.5244, ppl: 92.2399, loss: 4.5548
	step 5490:lm_loss: 4.5243, ppl: 92.2273, loss: 4.5547
	step 5491:lm_loss: 4.5242, ppl: 92.2258, loss: 4.5547
	step 5492:lm_loss: 4.5241, ppl: 92.2155, loss: 4.5545
	step 5493:lm_loss: 4.5243, ppl: 92.2281, loss: 4.5547
	step 5494:lm_loss: 4.5243, ppl: 92.2346, loss: 4.5548
	step 5495:lm_loss: 4.5242, ppl: 92.2206, loss: 4.5545
	step 5496:lm_loss: 4.5241, ppl: 92.2149, loss: 4.5544
	step 5497:lm_loss: 4.5242, ppl: 92.2217, loss: 4.5545
	step 5498:lm_loss: 4.5242, ppl: 92.2228, loss: 4.5545
	step 5499:lm_loss: 4.5239, ppl: 92.1945, loss: 4.5544
	step 5500:lm_loss: 4.5240, ppl: 92.2004, loss: 4.5545
	step 5501:lm_loss: 4.5239, ppl: 92.1976, loss: 4.5544
	step 5502:lm_loss: 4.5241, ppl: 92.2110, loss: 4.5545
	step 5503:lm_loss: 4.5241, ppl: 92.2143, loss: 4.5545
	step 5504:lm_loss: 4.5241, ppl: 92.2097, loss: 4.5545
	step 5505:lm_loss: 4.5241, ppl: 92.2113, loss: 4.5545
	step 5506:lm_loss: 4.5240, ppl: 92.1992, loss: 4.5544
	step 5507:lm_loss: 4.5241, ppl: 92.2109, loss: 4.5545
	step 5508:lm_loss: 4.5241, ppl: 92.2140, loss: 4.5546
	step 5509:lm_loss: 4.5243, ppl: 92.2317, loss: 4.5548
	step 5510:lm_loss: 4.5244, ppl: 92.2416, loss: 4.5549
	step 5511:lm_loss: 4.5245, ppl: 92.2501, loss: 4.5550
	step 5512:lm_loss: 4.5243, ppl: 92.2350, loss: 4.5547
	step 5513:lm_loss: 4.5244, ppl: 92.2370, loss: 4.5547
	step 5514:lm_loss: 4.5243, ppl: 92.2359, loss: 4.5547
	step 5515:lm_loss: 4.5243, ppl: 92.2295, loss: 4.5547
	step 5516:lm_loss: 4.5242, ppl: 92.2263, loss: 4.5546
	step 5517:lm_loss: 4.5242, ppl: 92.2224, loss: 4.5546
	step 5518:lm_loss: 4.5244, ppl: 92.2398, loss: 4.5547
	step 5519:lm_loss: 4.5243, ppl: 92.2274, loss: 4.5545
	step 5520:lm_loss: 4.5240, ppl: 92.2067, loss: 4.5543
	step 5521:lm_loss: 4.5240, ppl: 92.2004, loss: 4.5542
	step 5522:lm_loss: 4.5240, ppl: 92.2023, loss: 4.5542
	step 5523:lm_loss: 4.5237, ppl: 92.1766, loss: 4.5539
	step 5524:lm_loss: 4.5236, ppl: 92.1656, loss: 4.5538
	step 5525:lm_loss: 4.5235, ppl: 92.1600, loss: 4.5537
	step 5526:lm_loss: 4.5238, ppl: 92.1848, loss: 4.5540
	step 5527:lm_loss: 4.5236, ppl: 92.1627, loss: 4.5537
	step 5528:lm_loss: 4.5234, ppl: 92.1443, loss: 4.5536
	step 5529:lm_loss: 4.5233, ppl: 92.1401, loss: 4.5535
	step 5530:lm_loss: 4.5235, ppl: 92.1562, loss: 4.5536
	step 5531:lm_loss: 4.5236, ppl: 92.1657, loss: 4.5537
	step 5532:lm_loss: 4.5237, ppl: 92.1751, loss: 4.5539
	step 5533:lm_loss: 4.5238, ppl: 92.1823, loss: 4.5539
	step 5534:lm_loss: 4.5237, ppl: 92.1740, loss: 4.5539
	step 5535:lm_loss: 4.5239, ppl: 92.1909, loss: 4.5540
	step 5536:lm_loss: 4.5239, ppl: 92.1930, loss: 4.5541
	step 5537:lm_loss: 4.5240, ppl: 92.2059, loss: 4.5542
	step 5538:lm_loss: 4.5242, ppl: 92.2175, loss: 4.5543
	step 5539:lm_loss: 4.5241, ppl: 92.2157, loss: 4.5543
	step 5540:lm_loss: 4.5241, ppl: 92.2122, loss: 4.5542
	step 5541:lm_loss: 4.5240, ppl: 92.2045, loss: 4.5541
	step 5542:lm_loss: 4.5240, ppl: 92.2058, loss: 4.5541
	step 5543:lm_loss: 4.5241, ppl: 92.2089, loss: 4.5542
	step 5544:lm_loss: 4.5241, ppl: 92.2146, loss: 4.5542
	step 5545:lm_loss: 4.5241, ppl: 92.2165, loss: 4.5542
	step 5546:lm_loss: 4.5239, ppl: 92.1957, loss: 4.5540
	step 5547:lm_loss: 4.5238, ppl: 92.1878, loss: 4.5538
	step 5548:lm_loss: 4.5238, ppl: 92.1874, loss: 4.5538
	step 5549:lm_loss: 4.5240, ppl: 92.1994, loss: 4.5539
	step 5550:lm_loss: 4.5238, ppl: 92.1844, loss: 4.5537
	step 5551:lm_loss: 4.5238, ppl: 92.1864, loss: 4.5537
	step 5552:lm_loss: 4.5238, ppl: 92.1871, loss: 4.5537
	step 5553:lm_loss: 4.5241, ppl: 92.2170, loss: 4.5541
	step 5554:lm_loss: 4.5242, ppl: 92.2219, loss: 4.5542
	step 5555:lm_loss: 4.5242, ppl: 92.2261, loss: 4.5542
	step 5556:lm_loss: 4.5240, ppl: 92.2050, loss: 4.5539
	step 5557:lm_loss: 4.5240, ppl: 92.2031, loss: 4.5539
	step 5558:lm_loss: 4.5237, ppl: 92.1728, loss: 4.5537
	step 5559:lm_loss: 4.5240, ppl: 92.2007, loss: 4.5541
	step 5560:lm_loss: 4.5240, ppl: 92.2071, loss: 4.5542
	step 5561:lm_loss: 4.5241, ppl: 92.2137, loss: 4.5543
	step 5562:lm_loss: 4.5238, ppl: 92.1864, loss: 4.5540
	step 5563:lm_loss: 4.5240, ppl: 92.2016, loss: 4.5542
	step 5564:lm_loss: 4.5237, ppl: 92.1788, loss: 4.5540
	step 5565:lm_loss: 4.5239, ppl: 92.1916, loss: 4.5541
	step 5566:lm_loss: 4.5237, ppl: 92.1773, loss: 4.5539
	step 5567:lm_loss: 4.5239, ppl: 92.1954, loss: 4.5541
	step 5568:lm_loss: 4.5240, ppl: 92.2050, loss: 4.5542
	step 5569:lm_loss: 4.5239, ppl: 92.1939, loss: 4.5541
	step 5570:lm_loss: 4.5238, ppl: 92.1852, loss: 4.5540
	step 5571:lm_loss: 4.5238, ppl: 92.1872, loss: 4.5540
	step 5572:lm_loss: 4.5238, ppl: 92.1851, loss: 4.5540
	step 5573:lm_loss: 4.5238, ppl: 92.1839, loss: 4.5540
	step 5574:lm_loss: 4.5237, ppl: 92.1768, loss: 4.5539
	step 5575:lm_loss: 4.5237, ppl: 92.1767, loss: 4.5538
	step 5576:lm_loss: 4.5238, ppl: 92.1807, loss: 4.5539
	step 5577:lm_loss: 4.5239, ppl: 92.1904, loss: 4.5540
	step 5578:lm_loss: 4.5237, ppl: 92.1799, loss: 4.5539
	step 5579:lm_loss: 4.5238, ppl: 92.1896, loss: 4.5540
	step 5580:lm_loss: 4.5238, ppl: 92.1815, loss: 4.5539
	step 5581:lm_loss: 4.5233, ppl: 92.1396, loss: 4.5536
	step 5582:lm_loss: 4.5234, ppl: 92.1530, loss: 4.5537
	step 5583:lm_loss: 4.5234, ppl: 92.1507, loss: 4.5537
	step 5584:lm_loss: 4.5235, ppl: 92.1555, loss: 4.5537
	step 5585:lm_loss: 4.5235, ppl: 92.1565, loss: 4.5537
	step 5586:lm_loss: 4.5234, ppl: 92.1497, loss: 4.5537
	step 5587:lm_loss: 4.5234, ppl: 92.1458, loss: 4.5536
	step 5588:lm_loss: 4.5233, ppl: 92.1431, loss: 4.5536
	step 5589:lm_loss: 4.5234, ppl: 92.1446, loss: 4.5536
	step 5590:lm_loss: 4.5233, ppl: 92.1400, loss: 4.5535
	step 5591:lm_loss: 4.5232, ppl: 92.1336, loss: 4.5534
	step 5592:lm_loss: 4.5230, ppl: 92.1101, loss: 4.5531
	step 5593:lm_loss: 4.5229, ppl: 92.1036, loss: 4.5530
	step 5594:lm_loss: 4.5230, ppl: 92.1091, loss: 4.5531
	step 5595:lm_loss: 4.5228, ppl: 92.0956, loss: 4.5529
	step 5596:lm_loss: 4.5229, ppl: 92.0987, loss: 4.5529
	step 5597:lm_loss: 4.5229, ppl: 92.1023, loss: 4.5529
	step 5598:lm_loss: 4.5230, ppl: 92.1076, loss: 4.5530
	step 5599:lm_loss: 4.5230, ppl: 92.1143, loss: 4.5531
	step 5600:lm_loss: 4.5230, ppl: 92.1115, loss: 4.5530
	step 5601:lm_loss: 4.5232, ppl: 92.1319, loss: 4.5533
	step 5602:lm_loss: 4.5232, ppl: 92.1330, loss: 4.5533
	step 5603:lm_loss: 4.5231, ppl: 92.1206, loss: 4.5531
	step 5604:lm_loss: 4.5231, ppl: 92.1187, loss: 4.5531
	step 5605:lm_loss: 4.5231, ppl: 92.1186, loss: 4.5531
	step 5606:lm_loss: 4.5231, ppl: 92.1170, loss: 4.5531
	step 5607:lm_loss: 4.5231, ppl: 92.1221, loss: 4.5531
	step 5608:lm_loss: 4.5232, ppl: 92.1278, loss: 4.5532
	step 5609:lm_loss: 4.5233, ppl: 92.1352, loss: 4.5533
	step 5610:lm_loss: 4.5235, ppl: 92.1574, loss: 4.5535
	step 5611:lm_loss: 4.5237, ppl: 92.1802, loss: 4.5538
	step 5612:lm_loss: 4.5236, ppl: 92.1700, loss: 4.5537
	step 5613:lm_loss: 4.5237, ppl: 92.1773, loss: 4.5537
	step 5614:lm_loss: 4.5234, ppl: 92.1507, loss: 4.5536
	step 5615:lm_loss: 4.5235, ppl: 92.1611, loss: 4.5537
	step 5616:lm_loss: 4.5234, ppl: 92.1520, loss: 4.5536
	step 5617:lm_loss: 4.5234, ppl: 92.1499, loss: 4.5536
	step 5618:lm_loss: 4.5234, ppl: 92.1442, loss: 4.5535
	step 5619:lm_loss: 4.5233, ppl: 92.1363, loss: 4.5534
	step 5620:lm_loss: 4.5232, ppl: 92.1332, loss: 4.5534
	step 5621:lm_loss: 4.5232, ppl: 92.1290, loss: 4.5533
	step 5622:lm_loss: 4.5233, ppl: 92.1367, loss: 4.5534
	step 5623:lm_loss: 4.5233, ppl: 92.1407, loss: 4.5534
	step 5624:lm_loss: 4.5233, ppl: 92.1354, loss: 4.5534
	step 5625:lm_loss: 4.5234, ppl: 92.1516, loss: 4.5535
	step 5626:lm_loss: 4.5234, ppl: 92.1508, loss: 4.5535
	step 5627:lm_loss: 4.5234, ppl: 92.1510, loss: 4.5535
	step 5628:lm_loss: 4.5234, ppl: 92.1530, loss: 4.5535
	step 5629:lm_loss: 4.5232, ppl: 92.1327, loss: 4.5533
	step 5630:lm_loss: 4.5233, ppl: 92.1384, loss: 4.5533
	step 5631:lm_loss: 4.5234, ppl: 92.1438, loss: 4.5534
	step 5632:lm_loss: 4.5234, ppl: 92.1518, loss: 4.5534
	step 5633:lm_loss: 4.5232, ppl: 92.1278, loss: 4.5532
	step 5634:lm_loss: 4.5232, ppl: 92.1303, loss: 4.5532
	step 5635:lm_loss: 4.5232, ppl: 92.1307, loss: 4.5532
	step 5636:lm_loss: 4.5235, ppl: 92.1547, loss: 4.5534
	step 5637:lm_loss: 4.5235, ppl: 92.1583, loss: 4.5534
	step 5638:lm_loss: 4.5236, ppl: 92.1627, loss: 4.5534
	step 5639:lm_loss: 4.5235, ppl: 92.1601, loss: 4.5534
	step 5640:lm_loss: 4.5236, ppl: 92.1644, loss: 4.5534
	step 5641:lm_loss: 4.5237, ppl: 92.1768, loss: 4.5535
	step 5642:lm_loss: 4.5237, ppl: 92.1795, loss: 4.5535
	step 5643:lm_loss: 4.5237, ppl: 92.1727, loss: 4.5534
	step 5644:lm_loss: 4.5235, ppl: 92.1556, loss: 4.5533
	step 5645:lm_loss: 4.5235, ppl: 92.1607, loss: 4.5533
	step 5646:lm_loss: 4.5235, ppl: 92.1542, loss: 4.5532
	step 5647:lm_loss: 4.5234, ppl: 92.1438, loss: 4.5531
	step 5648:lm_loss: 4.5234, ppl: 92.1472, loss: 4.5531
	step 5649:lm_loss: 4.5235, ppl: 92.1579, loss: 4.5533
	step 5650:lm_loss: 4.5237, ppl: 92.1733, loss: 4.5534
	step 5651:lm_loss: 4.5235, ppl: 92.1558, loss: 4.5533
	step 5652:lm_loss: 4.5233, ppl: 92.1411, loss: 4.5532
	step 5653:lm_loss: 4.5232, ppl: 92.1301, loss: 4.5530
	step 5654:lm_loss: 4.5230, ppl: 92.1158, loss: 4.5529
	step 5655:lm_loss: 4.5231, ppl: 92.1194, loss: 4.5529
	step 5656:lm_loss: 4.5230, ppl: 92.1099, loss: 4.5528
	step 5657:lm_loss: 4.5230, ppl: 92.1093, loss: 4.5528
	step 5658:lm_loss: 4.5229, ppl: 92.1045, loss: 4.5528
	step 5659:lm_loss: 4.5230, ppl: 92.1115, loss: 4.5528
	step 5660:lm_loss: 4.5231, ppl: 92.1214, loss: 4.5529
	step 5661:lm_loss: 4.5230, ppl: 92.1137, loss: 4.5529
	step 5662:lm_loss: 4.5231, ppl: 92.1181, loss: 4.5529
	step 5663:lm_loss: 4.5232, ppl: 92.1266, loss: 4.5530
	step 5664:lm_loss: 4.5229, ppl: 92.0982, loss: 4.5527
	step 5665:lm_loss: 4.5226, ppl: 92.0787, loss: 4.5525
	step 5666:lm_loss: 4.5227, ppl: 92.0834, loss: 4.5526
	step 5667:lm_loss: 4.5228, ppl: 92.0899, loss: 4.5526
	step 5668:lm_loss: 4.5229, ppl: 92.1044, loss: 4.5528
	step 5669:lm_loss: 4.5230, ppl: 92.1144, loss: 4.5529
	step 5670:lm_loss: 4.5229, ppl: 92.1060, loss: 4.5528
	step 5671:lm_loss: 4.5228, ppl: 92.0922, loss: 4.5526
	step 5672:lm_loss: 4.5228, ppl: 92.0962, loss: 4.5526
	step 5673:lm_loss: 4.5230, ppl: 92.1109, loss: 4.5528
	step 5674:lm_loss: 4.5228, ppl: 92.0912, loss: 4.5527
	step 5675:lm_loss: 4.5228, ppl: 92.0945, loss: 4.5527
	step 5676:lm_loss: 4.5227, ppl: 92.0848, loss: 4.5526
	step 5677:lm_loss: 4.5227, ppl: 92.0795, loss: 4.5525
	step 5678:lm_loss: 4.5227, ppl: 92.0876, loss: 4.5526
	step 5679:lm_loss: 4.5229, ppl: 92.1023, loss: 4.5527
	step 5680:lm_loss: 4.5230, ppl: 92.1128, loss: 4.5528
	step 5681:lm_loss: 4.5231, ppl: 92.1171, loss: 4.5529
	step 5682:lm_loss: 4.5233, ppl: 92.1406, loss: 4.5531
	step 5683:lm_loss: 4.5231, ppl: 92.1231, loss: 4.5528
	step 5684:lm_loss: 4.5222, ppl: 92.0379, loss: 4.5526
	step 5685:lm_loss: 4.5222, ppl: 92.0336, loss: 4.5525
	step 5686:lm_loss: 4.5224, ppl: 92.0571, loss: 4.5528
	step 5687:lm_loss: 4.5224, ppl: 92.0552, loss: 4.5528
	step 5688:lm_loss: 4.5225, ppl: 92.0646, loss: 4.5529
	step 5689:lm_loss: 4.5222, ppl: 92.0375, loss: 4.5527
	step 5690:lm_loss: 4.5222, ppl: 92.0355, loss: 4.5527
	step 5691:lm_loss: 4.5221, ppl: 92.0318, loss: 4.5527
	step 5692:lm_loss: 4.5220, ppl: 92.0193, loss: 4.5525
	step 5693:lm_loss: 4.5221, ppl: 92.0307, loss: 4.5527
	step 5694:lm_loss: 4.5222, ppl: 92.0394, loss: 4.5528
	step 5695:lm_loss: 4.5222, ppl: 92.0388, loss: 4.5528
	step 5696:lm_loss: 4.5222, ppl: 92.0392, loss: 4.5528
	step 5697:lm_loss: 4.5221, ppl: 92.0309, loss: 4.5527
	step 5698:lm_loss: 4.5223, ppl: 92.0428, loss: 4.5528
	step 5699:lm_loss: 4.5221, ppl: 92.0285, loss: 4.5527
	step 5700:lm_loss: 4.5221, ppl: 92.0283, loss: 4.5527
	step 5701:lm_loss: 4.5218, ppl: 92.0023, loss: 4.5524
	step 5702:lm_loss: 4.5220, ppl: 92.0168, loss: 4.5525
	step 5703:lm_loss: 4.5219, ppl: 92.0085, loss: 4.5524
	step 5704:lm_loss: 4.5217, ppl: 91.9959, loss: 4.5524
	step 5705:lm_loss: 4.5218, ppl: 92.0027, loss: 4.5525
	step 5706:lm_loss: 4.5218, ppl: 92.0051, loss: 4.5525
	step 5707:lm_loss: 4.5218, ppl: 91.9984, loss: 4.5524
	step 5708:lm_loss: 4.5217, ppl: 91.9933, loss: 4.5523
	step 5709:lm_loss: 4.5217, ppl: 91.9949, loss: 4.5523
	step 5710:lm_loss: 4.5213, ppl: 91.9591, loss: 4.5520
	step 5711:lm_loss: 4.5213, ppl: 91.9567, loss: 4.5519
	step 5712:lm_loss: 4.5212, ppl: 91.9486, loss: 4.5518
	step 5713:lm_loss: 4.5212, ppl: 91.9479, loss: 4.5518
	step 5714:lm_loss: 4.5213, ppl: 91.9509, loss: 4.5518
	step 5715:lm_loss: 4.5211, ppl: 91.9408, loss: 4.5518
	step 5716:lm_loss: 4.5211, ppl: 91.9388, loss: 4.5517
	step 5717:lm_loss: 4.5213, ppl: 91.9537, loss: 4.5518
	step 5718:lm_loss: 4.5215, ppl: 91.9753, loss: 4.5521
	step 5719:lm_loss: 4.5215, ppl: 91.9701, loss: 4.5521
	step 5720:lm_loss: 4.5215, ppl: 91.9748, loss: 4.5521
	step 5721:lm_loss: 4.5216, ppl: 91.9789, loss: 4.5521
	step 5722:lm_loss: 4.5215, ppl: 91.9758, loss: 4.5521
	step 5723:lm_loss: 4.5215, ppl: 91.9746, loss: 4.5521
	step 5724:lm_loss: 4.5216, ppl: 91.9834, loss: 4.5522
	step 5725:lm_loss: 4.5214, ppl: 91.9677, loss: 4.5520
	step 5726:lm_loss: 4.5214, ppl: 91.9669, loss: 4.5520
	step 5727:lm_loss: 4.5215, ppl: 91.9705, loss: 4.5521
	step 5728:lm_loss: 4.5214, ppl: 91.9644, loss: 4.5520
	step 5729:lm_loss: 4.5215, ppl: 91.9705, loss: 4.5521
	step 5730:lm_loss: 4.5217, ppl: 91.9913, loss: 4.5523
	step 5731:lm_loss: 4.5216, ppl: 91.9809, loss: 4.5521
	step 5732:lm_loss: 4.5217, ppl: 91.9882, loss: 4.5522
	step 5733:lm_loss: 4.5215, ppl: 91.9703, loss: 4.5520
	step 5734:lm_loss: 4.5216, ppl: 91.9795, loss: 4.5521
	step 5735:lm_loss: 4.5215, ppl: 91.9772, loss: 4.5521
	step 5736:lm_loss: 4.5216, ppl: 91.9858, loss: 4.5522
	step 5737:lm_loss: 4.5215, ppl: 91.9755, loss: 4.5520
	step 5738:lm_loss: 4.5211, ppl: 91.9375, loss: 4.5518
	step 5739:lm_loss: 4.5210, ppl: 91.9249, loss: 4.5516
	step 5740:lm_loss: 4.5211, ppl: 91.9377, loss: 4.5517
	step 5741:lm_loss: 4.5212, ppl: 91.9436, loss: 4.5518
	step 5742:lm_loss: 4.5211, ppl: 91.9383, loss: 4.5517
	step 5743:lm_loss: 4.5212, ppl: 91.9491, loss: 4.5519
	step 5744:lm_loss: 4.5215, ppl: 91.9703, loss: 4.5520
	step 5745:lm_loss: 4.5215, ppl: 91.9741, loss: 4.5521
	step 5746:lm_loss: 4.5215, ppl: 91.9760, loss: 4.5521
	step 5747:lm_loss: 4.5214, ppl: 91.9640, loss: 4.5520
	step 5748:lm_loss: 4.5213, ppl: 91.9586, loss: 4.5519
	step 5749:lm_loss: 4.5213, ppl: 91.9595, loss: 4.5519
	step 5750:lm_loss: 4.5213, ppl: 91.9584, loss: 4.5519
	step 5751:lm_loss: 4.5211, ppl: 91.9384, loss: 4.5517
	step 5752:lm_loss: 4.5212, ppl: 91.9419, loss: 4.5517
	step 5753:lm_loss: 4.5210, ppl: 91.9285, loss: 4.5516
	step 5754:lm_loss: 4.5209, ppl: 91.9220, loss: 4.5515
	step 5755:lm_loss: 4.5211, ppl: 91.9354, loss: 4.5517
	step 5756:lm_loss: 4.5212, ppl: 91.9431, loss: 4.5518
	step 5757:lm_loss: 4.5211, ppl: 91.9381, loss: 4.5517
	step 5758:lm_loss: 4.5212, ppl: 91.9431, loss: 4.5518
	step 5759:lm_loss: 4.5211, ppl: 91.9367, loss: 4.5517
	step 5760:lm_loss: 4.5212, ppl: 91.9426, loss: 4.5518
	step 5761:lm_loss: 4.5213, ppl: 91.9587, loss: 4.5519
	step 5762:lm_loss: 4.5215, ppl: 91.9709, loss: 4.5520
	step 5763:lm_loss: 4.5215, ppl: 91.9732, loss: 4.5520
	step 5764:lm_loss: 4.5215, ppl: 91.9726, loss: 4.5520
	step 5765:lm_loss: 4.5215, ppl: 91.9721, loss: 4.5520
	step 5766:lm_loss: 4.5214, ppl: 91.9685, loss: 4.5520
	step 5767:lm_loss: 4.5218, ppl: 91.9994, loss: 4.5523
	step 5768:lm_loss: 4.5215, ppl: 91.9763, loss: 4.5521
	step 5769:lm_loss: 4.5215, ppl: 91.9748, loss: 4.5520
	step 5770:lm_loss: 4.5214, ppl: 91.9684, loss: 4.5520
	step 5771:lm_loss: 4.5214, ppl: 91.9621, loss: 4.5518
	step 5772:lm_loss: 4.5214, ppl: 91.9628, loss: 4.5518
	step 5773:lm_loss: 4.5214, ppl: 91.9602, loss: 4.5518
	step 5774:lm_loss: 4.5214, ppl: 91.9632, loss: 4.5518
	step 5775:lm_loss: 4.5213, ppl: 91.9561, loss: 4.5517
	step 5776:lm_loss: 4.5214, ppl: 91.9598, loss: 4.5518
	step 5777:lm_loss: 4.5214, ppl: 91.9647, loss: 4.5518
	step 5778:lm_loss: 4.5213, ppl: 91.9542, loss: 4.5516
	step 5779:lm_loss: 4.5214, ppl: 91.9610, loss: 4.5517
	step 5780:lm_loss: 4.5215, ppl: 91.9689, loss: 4.5518
	step 5781:lm_loss: 4.5213, ppl: 91.9506, loss: 4.5517
	step 5782:lm_loss: 4.5212, ppl: 91.9481, loss: 4.5516
	step 5783:lm_loss: 4.5211, ppl: 91.9337, loss: 4.5515
	step 5784:lm_loss: 4.5210, ppl: 91.9296, loss: 4.5515
	step 5785:lm_loss: 4.5209, ppl: 91.9186, loss: 4.5513
	step 5786:lm_loss: 4.5209, ppl: 91.9228, loss: 4.5513
	step 5787:lm_loss: 4.5209, ppl: 91.9223, loss: 4.5513
	step 5788:lm_loss: 4.5210, ppl: 91.9270, loss: 4.5514
	step 5789:lm_loss: 4.5210, ppl: 91.9265, loss: 4.5514
	step 5790:lm_loss: 4.5209, ppl: 91.9219, loss: 4.5513
	step 5791:lm_loss: 4.5209, ppl: 91.9157, loss: 4.5512
	step 5792:lm_loss: 4.5208, ppl: 91.9112, loss: 4.5512
	step 5793:lm_loss: 4.5208, ppl: 91.9104, loss: 4.5512
	step 5794:lm_loss: 4.5209, ppl: 91.9167, loss: 4.5512
	step 5795:lm_loss: 4.5209, ppl: 91.9185, loss: 4.5512
	step 5796:lm_loss: 4.5210, ppl: 91.9314, loss: 4.5513
	step 5797:lm_loss: 4.5211, ppl: 91.9382, loss: 4.5514
	step 5798:lm_loss: 4.5210, ppl: 91.9309, loss: 4.5513
	step 5799:lm_loss: 4.5211, ppl: 91.9328, loss: 4.5513
	step 5800:lm_loss: 4.5208, ppl: 91.9131, loss: 4.5511
	step 5801:lm_loss: 4.5206, ppl: 91.8875, loss: 4.5509
	step 5802:lm_loss: 4.5206, ppl: 91.8909, loss: 4.5509
	step 5803:lm_loss: 4.5207, ppl: 91.9013, loss: 4.5510
	step 5804:lm_loss: 4.5207, ppl: 91.8975, loss: 4.5509
	step 5805:lm_loss: 4.5206, ppl: 91.8920, loss: 4.5509
	step 5806:lm_loss: 4.5203, ppl: 91.8669, loss: 4.5505
	step 5807:lm_loss: 4.5203, ppl: 91.8592, loss: 4.5504
	step 5808:lm_loss: 4.5204, ppl: 91.8764, loss: 4.5506
	step 5809:lm_loss: 4.5205, ppl: 91.8796, loss: 4.5507
	step 5810:lm_loss: 4.5205, ppl: 91.8789, loss: 4.5506
	step 5811:lm_loss: 4.5205, ppl: 91.8810, loss: 4.5507
	step 5812:lm_loss: 4.5204, ppl: 91.8767, loss: 4.5506
	step 5813:lm_loss: 4.5205, ppl: 91.8860, loss: 4.5507
	step 5814:lm_loss: 4.5208, ppl: 91.9115, loss: 4.5509
	step 5815:lm_loss: 4.5207, ppl: 91.9044, loss: 4.5508
	step 5816:lm_loss: 4.5207, ppl: 91.8960, loss: 4.5508
	step 5817:lm_loss: 4.5205, ppl: 91.8804, loss: 4.5506
	step 5818:lm_loss: 4.5205, ppl: 91.8843, loss: 4.5506
	step 5819:lm_loss: 4.5206, ppl: 91.8911, loss: 4.5507
	step 5820:lm_loss: 4.5206, ppl: 91.8940, loss: 4.5507
	step 5821:lm_loss: 4.5207, ppl: 91.9030, loss: 4.5509
	step 5822:lm_loss: 4.5208, ppl: 91.9060, loss: 4.5509
	step 5823:lm_loss: 4.5206, ppl: 91.8941, loss: 4.5507
	step 5824:lm_loss: 4.5207, ppl: 91.9014, loss: 4.5508
	step 5825:lm_loss: 4.5208, ppl: 91.9122, loss: 4.5509
	step 5826:lm_loss: 4.5209, ppl: 91.9159, loss: 4.5509
	step 5827:lm_loss: 4.5209, ppl: 91.9188, loss: 4.5510
	step 5828:lm_loss: 4.5208, ppl: 91.9047, loss: 4.5508
	step 5829:lm_loss: 4.5208, ppl: 91.9126, loss: 4.5509
	step 5830:lm_loss: 4.5208, ppl: 91.9068, loss: 4.5508
	step 5831:lm_loss: 4.5209, ppl: 91.9140, loss: 4.5509
	step 5832:lm_loss: 4.5209, ppl: 91.9228, loss: 4.5510
	step 5833:lm_loss: 4.5210, ppl: 91.9280, loss: 4.5510
	step 5834:lm_loss: 4.5210, ppl: 91.9283, loss: 4.5510
	step 5835:lm_loss: 4.5211, ppl: 91.9412, loss: 4.5512
	step 5836:lm_loss: 4.5209, ppl: 91.9211, loss: 4.5510
	step 5837:lm_loss: 4.5209, ppl: 91.9215, loss: 4.5510
	step 5838:lm_loss: 4.5210, ppl: 91.9265, loss: 4.5510
	step 5839:lm_loss: 4.5208, ppl: 91.9130, loss: 4.5509
	step 5840:lm_loss: 4.5209, ppl: 91.9224, loss: 4.5510
	step 5841:lm_loss: 4.5209, ppl: 91.9187, loss: 4.5510
	step 5842:lm_loss: 4.5210, ppl: 91.9294, loss: 4.5511
	step 5843:lm_loss: 4.5207, ppl: 91.9040, loss: 4.5508
	step 5844:lm_loss: 4.5208, ppl: 91.9050, loss: 4.5508
	step 5845:lm_loss: 4.5209, ppl: 91.9142, loss: 4.5509
	step 5846:lm_loss: 4.5208, ppl: 91.9101, loss: 4.5508
	step 5847:lm_loss: 4.5210, ppl: 91.9294, loss: 4.5509
	step 5848:lm_loss: 4.5212, ppl: 91.9486, loss: 4.5512
	step 5849:lm_loss: 4.5213, ppl: 91.9585, loss: 4.5513
	step 5850:lm_loss: 4.5213, ppl: 91.9580, loss: 4.5513
	step 5851:lm_loss: 4.5213, ppl: 91.9590, loss: 4.5513
	step 5852:lm_loss: 4.5211, ppl: 91.9339, loss: 4.5510
	step 5853:lm_loss: 4.5210, ppl: 91.9254, loss: 4.5509
	step 5854:lm_loss: 4.5209, ppl: 91.9187, loss: 4.5509
	step 5855:lm_loss: 4.5208, ppl: 91.9129, loss: 4.5508
	step 5856:lm_loss: 4.5208, ppl: 91.9071, loss: 4.5507
	step 5857:lm_loss: 4.5208, ppl: 91.9127, loss: 4.5508
	step 5858:lm_loss: 4.5208, ppl: 91.9115, loss: 4.5508
	step 5859:lm_loss: 4.5208, ppl: 91.9115, loss: 4.5507
	step 5860:lm_loss: 4.5211, ppl: 91.9355, loss: 4.5510
	step 5861:lm_loss: 4.5212, ppl: 91.9467, loss: 4.5512
	step 5862:lm_loss: 4.5212, ppl: 91.9427, loss: 4.5511
	step 5863:lm_loss: 4.5210, ppl: 91.9236, loss: 4.5509
	step 5864:lm_loss: 4.5209, ppl: 91.9164, loss: 4.5507
	step 5865:lm_loss: 4.5210, ppl: 91.9280, loss: 4.5508
	step 5866:lm_loss: 4.5211, ppl: 91.9330, loss: 4.5509
	step 5867:lm_loss: 4.5209, ppl: 91.9219, loss: 4.5508
	step 5868:lm_loss: 4.5210, ppl: 91.9262, loss: 4.5508
	step 5869:lm_loss: 4.5207, ppl: 91.8974, loss: 4.5506
	step 5870:lm_loss: 4.5206, ppl: 91.8886, loss: 4.5505
	step 5871:lm_loss: 4.5206, ppl: 91.8900, loss: 4.5505
	step 5872:lm_loss: 4.5205, ppl: 91.8803, loss: 4.5505
	step 5873:lm_loss: 4.5206, ppl: 91.8942, loss: 4.5506
	step 5874:lm_loss: 4.5207, ppl: 91.8978, loss: 4.5506
	step 5875:lm_loss: 4.5205, ppl: 91.8824, loss: 4.5504
	step 5876:lm_loss: 4.5206, ppl: 91.8937, loss: 4.5505
	step 5877:lm_loss: 4.5207, ppl: 91.9008, loss: 4.5506
	step 5878:lm_loss: 4.5209, ppl: 91.9153, loss: 4.5507
	step 5879:lm_loss: 4.5208, ppl: 91.9104, loss: 4.5507
	step 5880:lm_loss: 4.5209, ppl: 91.9184, loss: 4.5508
	step 5881:lm_loss: 4.5208, ppl: 91.9125, loss: 4.5507
	step 5882:lm_loss: 4.5209, ppl: 91.9175, loss: 4.5507
	step 5883:lm_loss: 4.5208, ppl: 91.9094, loss: 4.5506
	step 5884:lm_loss: 4.5206, ppl: 91.8925, loss: 4.5504
	step 5885:lm_loss: 4.5206, ppl: 91.8931, loss: 4.5504
	step 5886:lm_loss: 4.5206, ppl: 91.8909, loss: 4.5503
	step 5887:lm_loss: 4.5206, ppl: 91.8933, loss: 4.5504
	step 5888:lm_loss: 4.5206, ppl: 91.8892, loss: 4.5503
	step 5889:lm_loss: 4.5205, ppl: 91.8857, loss: 4.5502
	step 5890:lm_loss: 4.5205, ppl: 91.8845, loss: 4.5502
	step 5891:lm_loss: 4.5206, ppl: 91.8864, loss: 4.5502
	step 5892:lm_loss: 4.5205, ppl: 91.8805, loss: 4.5502
	step 5893:lm_loss: 4.5205, ppl: 91.8854, loss: 4.5502
	step 5894:lm_loss: 4.5207, ppl: 91.9011, loss: 4.5504
	step 5895:lm_loss: 4.5207, ppl: 91.9040, loss: 4.5504
	step 5896:lm_loss: 4.5208, ppl: 91.9066, loss: 4.5505
	step 5897:lm_loss: 4.5209, ppl: 91.9214, loss: 4.5506
	step 5898:lm_loss: 4.5210, ppl: 91.9265, loss: 4.5506
	step 5899:lm_loss: 4.5208, ppl: 91.9136, loss: 4.5505
	step 5900:lm_loss: 4.5209, ppl: 91.9229, loss: 4.5506
	step 5901:lm_loss: 4.5209, ppl: 91.9217, loss: 4.5506
	step 5902:lm_loss: 4.5209, ppl: 91.9152, loss: 4.5505
	step 5903:lm_loss: 4.5210, ppl: 91.9274, loss: 4.5506
	step 5904:lm_loss: 4.5211, ppl: 91.9399, loss: 4.5508
	step 5905:lm_loss: 4.5211, ppl: 91.9336, loss: 4.5507
	step 5906:lm_loss: 4.5211, ppl: 91.9408, loss: 4.5507
	step 5907:lm_loss: 4.5211, ppl: 91.9352, loss: 4.5507
	step 5908:lm_loss: 4.5212, ppl: 91.9424, loss: 4.5508
	step 5909:lm_loss: 4.5209, ppl: 91.9173, loss: 4.5505
	step 5910:lm_loss: 4.5210, ppl: 91.9312, loss: 4.5507
	step 5911:lm_loss: 4.5210, ppl: 91.9244, loss: 4.5506
	step 5912:lm_loss: 4.5211, ppl: 91.9392, loss: 4.5507
	step 5913:lm_loss: 4.5210, ppl: 91.9321, loss: 4.5506
	step 5914:lm_loss: 4.5211, ppl: 91.9360, loss: 4.5507
	step 5915:lm_loss: 4.5211, ppl: 91.9326, loss: 4.5506
	step 5916:lm_loss: 4.5209, ppl: 91.9226, loss: 4.5505
	step 5917:lm_loss: 4.5210, ppl: 91.9235, loss: 4.5505
	step 5918:lm_loss: 4.5209, ppl: 91.9185, loss: 4.5505
	step 5919:lm_loss: 4.5211, ppl: 91.9340, loss: 4.5506
	step 5920:lm_loss: 4.5210, ppl: 91.9274, loss: 4.5506
	step 5921:lm_loss: 4.5211, ppl: 91.9388, loss: 4.5507
	step 5922:lm_loss: 4.5213, ppl: 91.9542, loss: 4.5509
	step 5923:lm_loss: 4.5209, ppl: 91.9178, loss: 4.5506
	step 5924:lm_loss: 4.5209, ppl: 91.9182, loss: 4.5506
	step 5925:lm_loss: 4.5209, ppl: 91.9183, loss: 4.5506
	step 5926:lm_loss: 4.5212, ppl: 91.9450, loss: 4.5509
	step 5927:lm_loss: 4.5212, ppl: 91.9438, loss: 4.5509
	step 5928:lm_loss: 4.5211, ppl: 91.9383, loss: 4.5508
	step 5929:lm_loss: 4.5210, ppl: 91.9291, loss: 4.5507
	step 5930:lm_loss: 4.5211, ppl: 91.9376, loss: 4.5508
	step 5931:lm_loss: 4.5210, ppl: 91.9283, loss: 4.5507
	step 5932:lm_loss: 4.5208, ppl: 91.9131, loss: 4.5505
	step 5933:lm_loss: 4.5209, ppl: 91.9178, loss: 4.5506
	step 5934:lm_loss: 4.5207, ppl: 91.8960, loss: 4.5504
	step 5935:lm_loss: 4.5206, ppl: 91.8949, loss: 4.5503
	step 5936:lm_loss: 4.5207, ppl: 91.8958, loss: 4.5503
	step 5937:lm_loss: 4.5206, ppl: 91.8876, loss: 4.5503
	step 5938:lm_loss: 4.5205, ppl: 91.8795, loss: 4.5502
	step 5939:lm_loss: 4.5207, ppl: 91.9031, loss: 4.5504
	step 5940:lm_loss: 4.5206, ppl: 91.8910, loss: 4.5503
	step 5941:lm_loss: 4.5206, ppl: 91.8877, loss: 4.5503
	step 5942:lm_loss: 4.5206, ppl: 91.8897, loss: 4.5503
	step 5943:lm_loss: 4.5205, ppl: 91.8770, loss: 4.5501
	step 5944:lm_loss: 4.5204, ppl: 91.8744, loss: 4.5501
	step 5945:lm_loss: 4.5206, ppl: 91.8862, loss: 4.5502
	step 5946:lm_loss: 4.5207, ppl: 91.9018, loss: 4.5504
	step 5947:lm_loss: 4.5208, ppl: 91.9103, loss: 4.5504
	step 5948:lm_loss: 4.5207, ppl: 91.9021, loss: 4.5503
	step 5949:lm_loss: 4.5207, ppl: 91.8974, loss: 4.5502
	step 5950:lm_loss: 4.5206, ppl: 91.8862, loss: 4.5502
	step 5951:lm_loss: 4.5205, ppl: 91.8774, loss: 4.5501
	step 5952:lm_loss: 4.5204, ppl: 91.8732, loss: 4.5500
	step 5953:lm_loss: 4.5206, ppl: 91.8944, loss: 4.5503
	step 5954:lm_loss: 4.5206, ppl: 91.8878, loss: 4.5502
	step 5955:lm_loss: 4.5206, ppl: 91.8879, loss: 4.5502
	step 5956:lm_loss: 4.5205, ppl: 91.8855, loss: 4.5502
	step 5957:lm_loss: 4.5206, ppl: 91.8888, loss: 4.5502
	step 5958:lm_loss: 4.5207, ppl: 91.9013, loss: 4.5504
	step 5959:lm_loss: 4.5208, ppl: 91.9130, loss: 4.5505
	step 5960:lm_loss: 4.5207, ppl: 91.9031, loss: 4.5504
	step 5961:lm_loss: 4.5206, ppl: 91.8915, loss: 4.5502
	step 5962:lm_loss: 4.5207, ppl: 91.8954, loss: 4.5503
	step 5963:lm_loss: 4.5208, ppl: 91.9091, loss: 4.5504
	step 5964:lm_loss: 4.5209, ppl: 91.9141, loss: 4.5505
	step 5965:lm_loss: 4.5208, ppl: 91.9065, loss: 4.5504
	step 5966:lm_loss: 4.5209, ppl: 91.9204, loss: 4.5505
	step 5967:lm_loss: 4.5210, ppl: 91.9238, loss: 4.5505
	step 5968:lm_loss: 4.5209, ppl: 91.9168, loss: 4.5505
	step 5969:lm_loss: 4.5210, ppl: 91.9243, loss: 4.5505
	step 5970:lm_loss: 4.5208, ppl: 91.9118, loss: 4.5504
	step 5971:lm_loss: 4.5207, ppl: 91.8973, loss: 4.5502
	step 5972:lm_loss: 4.5208, ppl: 91.9127, loss: 4.5504
	step 5973:lm_loss: 4.5208, ppl: 91.9107, loss: 4.5504
	step 5974:lm_loss: 4.5208, ppl: 91.9123, loss: 4.5504
	step 5975:lm_loss: 4.5210, ppl: 91.9252, loss: 4.5504
	step 5976:lm_loss: 4.5210, ppl: 91.9276, loss: 4.5505
	step 5977:lm_loss: 4.5212, ppl: 91.9449, loss: 4.5506
	step 5978:lm_loss: 4.5213, ppl: 91.9524, loss: 4.5507
	step 5979:lm_loss: 4.5212, ppl: 91.9422, loss: 4.5506
	step 5980:lm_loss: 4.5211, ppl: 91.9389, loss: 4.5506
	step 5981:lm_loss: 4.5211, ppl: 91.9409, loss: 4.5506
	step 5982:lm_loss: 4.5212, ppl: 91.9424, loss: 4.5506
	step 5983:lm_loss: 4.5210, ppl: 91.9278, loss: 4.5505
	step 5984:lm_loss: 4.5211, ppl: 91.9399, loss: 4.5507
	step 5985:lm_loss: 4.5212, ppl: 91.9429, loss: 4.5507
	step 5986:lm_loss: 4.5210, ppl: 91.9312, loss: 4.5506
	step 5987:lm_loss: 4.5212, ppl: 91.9426, loss: 4.5507
	step 5988:lm_loss: 4.5209, ppl: 91.9209, loss: 4.5505
	step 5989:lm_loss: 4.5209, ppl: 91.9174, loss: 4.5505
	step 5990:lm_loss: 4.5208, ppl: 91.9072, loss: 4.5504
	step 5991:lm_loss: 4.5206, ppl: 91.8923, loss: 4.5502
	step 5992:lm_loss: 4.5204, ppl: 91.8684, loss: 4.5499
	step 5993:lm_loss: 4.5205, ppl: 91.8774, loss: 4.5500
	step 5994:lm_loss: 4.5205, ppl: 91.8858, loss: 4.5501
	step 5995:lm_loss: 4.5206, ppl: 91.8867, loss: 4.5501
	step 5996:lm_loss: 4.5206, ppl: 91.8910, loss: 4.5502
	step 5997:lm_loss: 4.5205, ppl: 91.8802, loss: 4.5501
	step 5998:lm_loss: 4.5199, ppl: 91.8301, loss: 4.5498
	step 5999:lm_loss: 4.5199, ppl: 91.8264, loss: 4.5497
	step 6000:lm_loss: 4.5199, ppl: 91.8243, loss: 4.5497
	step 6001:lm_loss: 4.5200, ppl: 91.8394, loss: 4.5500
	step 6002:lm_loss: 4.5201, ppl: 91.8411, loss: 4.5500
	step 6003:lm_loss: 4.5202, ppl: 91.8503, loss: 4.5501
	step 6004:lm_loss: 4.5199, ppl: 91.8284, loss: 4.5498
	step 6005:lm_loss: 4.5200, ppl: 91.8388, loss: 4.5500
	step 6006:lm_loss: 4.5201, ppl: 91.8476, loss: 4.5501
	step 6007:lm_loss: 4.5201, ppl: 91.8451, loss: 4.5500
	step 6008:lm_loss: 4.5202, ppl: 91.8519, loss: 4.5501
	step 6009:lm_loss: 4.5203, ppl: 91.8588, loss: 4.5502
	step 6010:lm_loss: 4.5202, ppl: 91.8539, loss: 4.5501
	step 6011:lm_loss: 4.5201, ppl: 91.8453, loss: 4.5500
	step 6012:lm_loss: 4.5202, ppl: 91.8579, loss: 4.5502
	step 6013:lm_loss: 4.5202, ppl: 91.8574, loss: 4.5502
	step 6014:lm_loss: 4.5203, ppl: 91.8630, loss: 4.5502
	step 6015:lm_loss: 4.5201, ppl: 91.8472, loss: 4.5501
	step 6016:lm_loss: 4.5201, ppl: 91.8466, loss: 4.5500
	step 6017:lm_loss: 4.5199, ppl: 91.8300, loss: 4.5499
	step 6018:lm_loss: 4.5201, ppl: 91.8432, loss: 4.5500
	step 6019:lm_loss: 4.5200, ppl: 91.8331, loss: 4.5499
	step 6020:lm_loss: 4.5200, ppl: 91.8348, loss: 4.5499
	step 6021:lm_loss: 4.5200, ppl: 91.8381, loss: 4.5499
	step 6022:lm_loss: 4.5202, ppl: 91.8521, loss: 4.5501
	step 6023:lm_loss: 4.5202, ppl: 91.8551, loss: 4.5501
	step 6024:lm_loss: 4.5203, ppl: 91.8658, loss: 4.5502
	step 6025:lm_loss: 4.5203, ppl: 91.8673, loss: 4.5503
	step 6026:lm_loss: 4.5203, ppl: 91.8674, loss: 4.5503
	step 6027:lm_loss: 4.5203, ppl: 91.8620, loss: 4.5502
	step 6028:lm_loss: 4.5203, ppl: 91.8614, loss: 4.5502
	step 6029:lm_loss: 4.5203, ppl: 91.8618, loss: 4.5502
	step 6030:lm_loss: 4.5202, ppl: 91.8516, loss: 4.5500
	step 6031:lm_loss: 4.5200, ppl: 91.8342, loss: 4.5499
	step 6032:lm_loss: 4.5199, ppl: 91.8259, loss: 4.5498
	step 6033:lm_loss: 4.5199, ppl: 91.8229, loss: 4.5498
	step 6034:lm_loss: 4.5199, ppl: 91.8218, loss: 4.5498
	step 6035:lm_loss: 4.5197, ppl: 91.8124, loss: 4.5497
	step 6036:lm_loss: 4.5197, ppl: 91.8051, loss: 4.5496
	step 6037:lm_loss: 4.5198, ppl: 91.8161, loss: 4.5498
	step 6038:lm_loss: 4.5199, ppl: 91.8249, loss: 4.5499
	step 6039:lm_loss: 4.5201, ppl: 91.8442, loss: 4.5501
	step 6040:lm_loss: 4.5201, ppl: 91.8414, loss: 4.5500
	step 6041:lm_loss: 4.5201, ppl: 91.8447, loss: 4.5501
	step 6042:lm_loss: 4.5202, ppl: 91.8556, loss: 4.5502
	step 6043:lm_loss: 4.5203, ppl: 91.8630, loss: 4.5502
	step 6044:lm_loss: 4.5204, ppl: 91.8703, loss: 4.5503
	step 6045:lm_loss: 4.5205, ppl: 91.8798, loss: 4.5504
	step 6046:lm_loss: 4.5204, ppl: 91.8717, loss: 4.5503
	step 6047:lm_loss: 4.5203, ppl: 91.8602, loss: 4.5501
	step 6048:lm_loss: 4.5204, ppl: 91.8707, loss: 4.5502
	step 6049:lm_loss: 4.5203, ppl: 91.8632, loss: 4.5502
	step 6050:lm_loss: 4.5202, ppl: 91.8577, loss: 4.5501
	step 6051:lm_loss: 4.5203, ppl: 91.8636, loss: 4.5501
	step 6052:lm_loss: 4.5202, ppl: 91.8568, loss: 4.5501
	step 6053:lm_loss: 4.5199, ppl: 91.8290, loss: 4.5500
	step 6054:lm_loss: 4.5196, ppl: 91.7949, loss: 4.5497
	step 6055:lm_loss: 4.5196, ppl: 91.7995, loss: 4.5498
	step 6056:lm_loss: 4.5195, ppl: 91.7879, loss: 4.5496
	step 6057:lm_loss: 4.5191, ppl: 91.7542, loss: 4.5492
	step 6058:lm_loss: 4.5192, ppl: 91.7593, loss: 4.5493
	step 6059:lm_loss: 4.5192, ppl: 91.7630, loss: 4.5493
	step 6060:lm_loss: 4.5190, ppl: 91.7478, loss: 4.5492
	step 6061:lm_loss: 4.5190, ppl: 91.7452, loss: 4.5492
	step 6062:lm_loss: 4.5190, ppl: 91.7483, loss: 4.5492
	step 6063:lm_loss: 4.5190, ppl: 91.7476, loss: 4.5492
	step 6064:lm_loss: 4.5191, ppl: 91.7499, loss: 4.5492
	step 6065:lm_loss: 4.5192, ppl: 91.7626, loss: 4.5493
	step 6066:lm_loss: 4.5192, ppl: 91.7647, loss: 4.5494
	step 6067:lm_loss: 4.5191, ppl: 91.7561, loss: 4.5493
	step 6068:lm_loss: 4.5191, ppl: 91.7554, loss: 4.5492
	step 6069:lm_loss: 4.5191, ppl: 91.7494, loss: 4.5492
	step 6070:lm_loss: 4.5189, ppl: 91.7332, loss: 4.5490
	step 6071:lm_loss: 4.5189, ppl: 91.7385, loss: 4.5491
	step 6072:lm_loss: 4.5191, ppl: 91.7513, loss: 4.5492
	step 6073:lm_loss: 4.5190, ppl: 91.7474, loss: 4.5492
	step 6074:lm_loss: 4.5191, ppl: 91.7522, loss: 4.5492
	step 6075:lm_loss: 4.5191, ppl: 91.7531, loss: 4.5492
	step 6076:lm_loss: 4.5192, ppl: 91.7597, loss: 4.5493
	step 6077:lm_loss: 4.5192, ppl: 91.7589, loss: 4.5493
	step 6078:lm_loss: 4.5192, ppl: 91.7608, loss: 4.5493
	step 6079:lm_loss: 4.5193, ppl: 91.7694, loss: 4.5494
	step 6080:lm_loss: 4.5191, ppl: 91.7522, loss: 4.5492
	step 6081:lm_loss: 4.5189, ppl: 91.7353, loss: 4.5490
	step 6082:lm_loss: 4.5190, ppl: 91.7455, loss: 4.5491
	step 6083:lm_loss: 4.5188, ppl: 91.7216, loss: 4.5488
	step 6084:lm_loss: 4.5188, ppl: 91.7261, loss: 4.5489
	step 6085:lm_loss: 4.5187, ppl: 91.7188, loss: 4.5488
	step 6086:lm_loss: 4.5191, ppl: 91.7520, loss: 4.5491
	step 6087:lm_loss: 4.5191, ppl: 91.7532, loss: 4.5491
	step 6088:lm_loss: 4.5191, ppl: 91.7533, loss: 4.5491
	step 6089:lm_loss: 4.5192, ppl: 91.7581, loss: 4.5492
	step 6090:lm_loss: 4.5192, ppl: 91.7595, loss: 4.5492
	step 6091:lm_loss: 4.5192, ppl: 91.7595, loss: 4.5492
	step 6092:lm_loss: 4.5193, ppl: 91.7669, loss: 4.5492
	step 6093:lm_loss: 4.5192, ppl: 91.7598, loss: 4.5492
	step 6094:lm_loss: 4.5192, ppl: 91.7631, loss: 4.5492
	step 6095:lm_loss: 4.5192, ppl: 91.7662, loss: 4.5492
	step 6096:lm_loss: 4.5194, ppl: 91.7785, loss: 4.5493
	step 6097:lm_loss: 4.5194, ppl: 91.7833, loss: 4.5493
	step 6098:lm_loss: 4.5195, ppl: 91.7852, loss: 4.5494
	step 6099:lm_loss: 4.5194, ppl: 91.7779, loss: 4.5493
	step 6100:lm_loss: 4.5193, ppl: 91.7711, loss: 4.5491
	step 6101:lm_loss: 4.5192, ppl: 91.7638, loss: 4.5490
	step 6102:lm_loss: 4.5193, ppl: 91.7748, loss: 4.5491
	step 6103:lm_loss: 4.5193, ppl: 91.7748, loss: 4.5491
	step 6104:lm_loss: 4.5192, ppl: 91.7635, loss: 4.5490
	step 6105:lm_loss: 4.5193, ppl: 91.7704, loss: 4.5491
	step 6106:lm_loss: 4.5193, ppl: 91.7703, loss: 4.5491
	step 6107:lm_loss: 4.5193, ppl: 91.7723, loss: 4.5491
	step 6108:lm_loss: 4.5193, ppl: 91.7669, loss: 4.5490
	step 6109:lm_loss: 4.5194, ppl: 91.7771, loss: 4.5492
	step 6110:lm_loss: 4.5194, ppl: 91.7795, loss: 4.5492
	step 6111:lm_loss: 4.5194, ppl: 91.7843, loss: 4.5492
	step 6112:lm_loss: 4.5196, ppl: 91.7948, loss: 4.5494
	step 6113:lm_loss: 4.5195, ppl: 91.7939, loss: 4.5494
	step 6114:lm_loss: 4.5197, ppl: 91.8080, loss: 4.5496
	step 6115:lm_loss: 4.5197, ppl: 91.8085, loss: 4.5496
	step 6116:lm_loss: 4.5196, ppl: 91.7979, loss: 4.5494
	step 6117:lm_loss: 4.5196, ppl: 91.7996, loss: 4.5494
	step 6118:lm_loss: 4.5192, ppl: 91.7662, loss: 4.5492
	step 6119:lm_loss: 4.5193, ppl: 91.7691, loss: 4.5492
	step 6120:lm_loss: 4.5193, ppl: 91.7698, loss: 4.5493
	step 6121:lm_loss: 4.5193, ppl: 91.7705, loss: 4.5493
	step 6122:lm_loss: 4.5192, ppl: 91.7587, loss: 4.5492
	step 6123:lm_loss: 4.5193, ppl: 91.7710, loss: 4.5493
	step 6124:lm_loss: 4.5194, ppl: 91.7799, loss: 4.5494
	step 6125:lm_loss: 4.5194, ppl: 91.7822, loss: 4.5494
	step 6126:lm_loss: 4.5193, ppl: 91.7737, loss: 4.5493
	step 6127:lm_loss: 4.5195, ppl: 91.7912, loss: 4.5495
	step 6128:lm_loss: 4.5195, ppl: 91.7882, loss: 4.5494
	step 6129:lm_loss: 4.5196, ppl: 91.7944, loss: 4.5495
	step 6130:lm_loss: 4.5196, ppl: 91.8013, loss: 4.5496
	step 6131:lm_loss: 4.5194, ppl: 91.7823, loss: 4.5493
	step 6132:lm_loss: 4.5196, ppl: 91.7962, loss: 4.5494
	step 6133:lm_loss: 4.5194, ppl: 91.7819, loss: 4.5492
	step 6134:lm_loss: 4.5195, ppl: 91.7913, loss: 4.5493
	step 6135:lm_loss: 4.5196, ppl: 91.7991, loss: 4.5494
	step 6136:lm_loss: 4.5196, ppl: 91.7995, loss: 4.5494
	step 6137:lm_loss: 4.5196, ppl: 91.8012, loss: 4.5494
	step 6138:lm_loss: 4.5197, ppl: 91.8114, loss: 4.5496
	step 6139:lm_loss: 4.5196, ppl: 91.8003, loss: 4.5494
	step 6140:lm_loss: 4.5195, ppl: 91.7926, loss: 4.5493
	step 6141:lm_loss: 4.5196, ppl: 91.8005, loss: 4.5493
	step 6142:lm_loss: 4.5197, ppl: 91.8056, loss: 4.5494
	step 6143:lm_loss: 4.5197, ppl: 91.8075, loss: 4.5494
	step 6144:lm_loss: 4.5198, ppl: 91.8141, loss: 4.5495
	step 6145:lm_loss: 4.5199, ppl: 91.8282, loss: 4.5496
	step 6146:lm_loss: 4.5199, ppl: 91.8270, loss: 4.5496
	step 6147:lm_loss: 4.5199, ppl: 91.8228, loss: 4.5496
	step 6148:lm_loss: 4.5198, ppl: 91.8214, loss: 4.5495
	step 6149:lm_loss: 4.5197, ppl: 91.8105, loss: 4.5494
	step 6150:lm_loss: 4.5199, ppl: 91.8255, loss: 4.5496
	step 6151:lm_loss: 4.5198, ppl: 91.8217, loss: 4.5496
	step 6152:lm_loss: 4.5198, ppl: 91.8175, loss: 4.5495
	step 6153:lm_loss: 4.5197, ppl: 91.8108, loss: 4.5494
	step 6154:lm_loss: 4.5199, ppl: 91.8245, loss: 4.5495
	step 6155:lm_loss: 4.5198, ppl: 91.8193, loss: 4.5495
	step 6156:lm_loss: 4.5198, ppl: 91.8181, loss: 4.5494
	step 6157:lm_loss: 4.5199, ppl: 91.8231, loss: 4.5495
	step 6158:lm_loss: 4.5199, ppl: 91.8249, loss: 4.5495
	step 6159:lm_loss: 4.5198, ppl: 91.8204, loss: 4.5495
	step 6160:lm_loss: 4.5197, ppl: 91.8085, loss: 4.5493
	step 6161:lm_loss: 4.5197, ppl: 91.8087, loss: 4.5493
	step 6162:lm_loss: 4.5196, ppl: 91.8030, loss: 4.5492
	step 6163:lm_loss: 4.5198, ppl: 91.8193, loss: 4.5494
	step 6164:lm_loss: 4.5198, ppl: 91.8198, loss: 4.5494
	step 6165:lm_loss: 4.5195, ppl: 91.7864, loss: 4.5493
	step 6166:lm_loss: 4.5192, ppl: 91.7616, loss: 4.5490
	step 6167:lm_loss: 4.5193, ppl: 91.7677, loss: 4.5491
	step 6168:lm_loss: 4.5192, ppl: 91.7581, loss: 4.5489
	step 6169:lm_loss: 4.5190, ppl: 91.7479, loss: 4.5488
	step 6170:lm_loss: 4.5188, ppl: 91.7265, loss: 4.5486
	step 6171:lm_loss: 4.5188, ppl: 91.7294, loss: 4.5487
	step 6172:lm_loss: 4.5188, ppl: 91.7300, loss: 4.5487
	step 6173:lm_loss: 4.5190, ppl: 91.7446, loss: 4.5488
	step 6174:lm_loss: 4.5190, ppl: 91.7428, loss: 4.5488
	step 6175:lm_loss: 4.5190, ppl: 91.7432, loss: 4.5488
	step 6176:lm_loss: 4.5191, ppl: 91.7497, loss: 4.5488
	step 6177:lm_loss: 4.5190, ppl: 91.7461, loss: 4.5488
	step 6178:lm_loss: 4.5190, ppl: 91.7437, loss: 4.5488
	step 6179:lm_loss: 4.5191, ppl: 91.7528, loss: 4.5488
	step 6180:lm_loss: 4.5190, ppl: 91.7474, loss: 4.5488
	step 6181:lm_loss: 4.5191, ppl: 91.7527, loss: 4.5488
	step 6182:lm_loss: 4.5190, ppl: 91.7456, loss: 4.5488
	step 6183:lm_loss: 4.5191, ppl: 91.7555, loss: 4.5489
	step 6184:lm_loss: 4.5193, ppl: 91.7726, loss: 4.5492
	step 6185:lm_loss: 4.5192, ppl: 91.7613, loss: 4.5490
	step 6186:lm_loss: 4.5192, ppl: 91.7641, loss: 4.5491
	step 6187:lm_loss: 4.5192, ppl: 91.7654, loss: 4.5491
	step 6188:lm_loss: 4.5192, ppl: 91.7667, loss: 4.5491
	step 6189:lm_loss: 4.5191, ppl: 91.7564, loss: 4.5489
	step 6190:lm_loss: 4.5193, ppl: 91.7704, loss: 4.5491
	step 6191:lm_loss: 4.5192, ppl: 91.7626, loss: 4.5490
	step 6192:lm_loss: 4.5192, ppl: 91.7576, loss: 4.5489
	step 6193:lm_loss: 4.5192, ppl: 91.7602, loss: 4.5489
	step 6194:lm_loss: 4.5192, ppl: 91.7636, loss: 4.5490
	step 6195:lm_loss: 4.5191, ppl: 91.7571, loss: 4.5489
	step 6196:lm_loss: 4.5191, ppl: 91.7493, loss: 4.5488
	step 6197:lm_loss: 4.5190, ppl: 91.7452, loss: 4.5487
	step 6198:lm_loss: 4.5189, ppl: 91.7308, loss: 4.5486
	step 6199:lm_loss: 4.5189, ppl: 91.7335, loss: 4.5486
	step 6200:lm_loss: 4.5191, ppl: 91.7549, loss: 4.5488
	step 6201:lm_loss: 4.5192, ppl: 91.7667, loss: 4.5490
	step 6202:lm_loss: 4.5193, ppl: 91.7715, loss: 4.5490
	step 6203:lm_loss: 4.5193, ppl: 91.7717, loss: 4.5490
	step 6204:lm_loss: 4.5195, ppl: 91.7916, loss: 4.5491
	step 6205:lm_loss: 4.5196, ppl: 91.7951, loss: 4.5492
	step 6206:lm_loss: 4.5196, ppl: 91.7989, loss: 4.5492
	step 6207:lm_loss: 4.5196, ppl: 91.8032, loss: 4.5493
	step 6208:lm_loss: 4.5197, ppl: 91.8045, loss: 4.5493
	step 6209:lm_loss: 4.5196, ppl: 91.7958, loss: 4.5492
	step 6210:lm_loss: 4.5195, ppl: 91.7927, loss: 4.5491
	step 6211:lm_loss: 4.5195, ppl: 91.7921, loss: 4.5491
	step 6212:lm_loss: 4.5197, ppl: 91.8078, loss: 4.5493
	step 6213:lm_loss: 4.5196, ppl: 91.7973, loss: 4.5492
	step 6214:lm_loss: 4.5197, ppl: 91.8084, loss: 4.5494
	step 6215:lm_loss: 4.5195, ppl: 91.7942, loss: 4.5493
	step 6216:lm_loss: 4.5197, ppl: 91.8126, loss: 4.5495
	step 6217:lm_loss: 4.5197, ppl: 91.8041, loss: 4.5494
	step 6218:lm_loss: 4.5194, ppl: 91.7759, loss: 4.5489
	step 6219:lm_loss: 4.5193, ppl: 91.7711, loss: 4.5489
	step 6220:lm_loss: 4.5193, ppl: 91.7694, loss: 4.5488
	step 6221:lm_loss: 4.5195, ppl: 91.7855, loss: 4.5490
	step 6222:lm_loss: 4.5193, ppl: 91.7751, loss: 4.5489
	step 6223:lm_loss: 4.5194, ppl: 91.7765, loss: 4.5489
	step 6224:lm_loss: 4.5194, ppl: 91.7782, loss: 4.5489
	step 6225:lm_loss: 4.5194, ppl: 91.7821, loss: 4.5490
	step 6226:lm_loss: 4.5195, ppl: 91.7885, loss: 4.5490
	step 6227:lm_loss: 4.5194, ppl: 91.7844, loss: 4.5490
	step 6228:lm_loss: 4.5196, ppl: 91.7987, loss: 4.5492
	step 6229:lm_loss: 4.5197, ppl: 91.8051, loss: 4.5492
	step 6230:lm_loss: 4.5197, ppl: 91.8053, loss: 4.5492
	step 6231:lm_loss: 4.5196, ppl: 91.8027, loss: 4.5492
	step 6232:lm_loss: 4.5194, ppl: 91.7843, loss: 4.5490
	step 6233:lm_loss: 4.5195, ppl: 91.7882, loss: 4.5490
	step 6234:lm_loss: 4.5195, ppl: 91.7870, loss: 4.5490
	step 6235:lm_loss: 4.5195, ppl: 91.7906, loss: 4.5490
	step 6236:lm_loss: 4.5193, ppl: 91.7732, loss: 4.5487
	step 6237:lm_loss: 4.5194, ppl: 91.7784, loss: 4.5487
	step 6238:lm_loss: 4.5193, ppl: 91.7684, loss: 4.5486
	step 6239:lm_loss: 4.5192, ppl: 91.7667, loss: 4.5486
	step 6240:lm_loss: 4.5192, ppl: 91.7584, loss: 4.5485
	step 6241:lm_loss: 4.5191, ppl: 91.7571, loss: 4.5485
	step 6242:lm_loss: 4.5192, ppl: 91.7585, loss: 4.5485
	step 6243:lm_loss: 4.5184, ppl: 91.6845, loss: 4.5483
	step 6244:lm_loss: 4.5184, ppl: 91.6877, loss: 4.5483
	step 6245:lm_loss: 4.5185, ppl: 91.6952, loss: 4.5484
	step 6246:lm_loss: 4.5185, ppl: 91.6957, loss: 4.5484
	step 6247:lm_loss: 4.5185, ppl: 91.6944, loss: 4.5484
	step 6248:lm_loss: 4.5183, ppl: 91.6832, loss: 4.5483
	step 6249:lm_loss: 4.5183, ppl: 91.6820, loss: 4.5482
	step 6250:lm_loss: 4.5182, ppl: 91.6683, loss: 4.5481
	step 6251:lm_loss: 4.5183, ppl: 91.6779, loss: 4.5481
	step 6252:lm_loss: 4.5182, ppl: 91.6710, loss: 4.5480
	step 6253:lm_loss: 4.5182, ppl: 91.6731, loss: 4.5481
	step 6254:lm_loss: 4.5183, ppl: 91.6774, loss: 4.5481
	step 6255:lm_loss: 4.5184, ppl: 91.6846, loss: 4.5482
	step 6256:lm_loss: 4.5184, ppl: 91.6872, loss: 4.5482
	step 6257:lm_loss: 4.5184, ppl: 91.6871, loss: 4.5482
	step 6258:lm_loss: 4.5184, ppl: 91.6863, loss: 4.5482
	step 6259:lm_loss: 4.5184, ppl: 91.6874, loss: 4.5482
	step 6260:lm_loss: 4.5180, ppl: 91.6530, loss: 4.5480
	step 6261:lm_loss: 4.5181, ppl: 91.6596, loss: 4.5481
	step 6262:lm_loss: 4.5178, ppl: 91.6292, loss: 4.5478
	step 6263:lm_loss: 4.5176, ppl: 91.6110, loss: 4.5476
	step 6264:lm_loss: 4.5175, ppl: 91.6103, loss: 4.5476
	step 6265:lm_loss: 4.5176, ppl: 91.6144, loss: 4.5477
	step 6266:lm_loss: 4.5173, ppl: 91.5856, loss: 4.5475
	step 6267:lm_loss: 4.5175, ppl: 91.6031, loss: 4.5477
	step 6268:lm_loss: 4.5175, ppl: 91.6048, loss: 4.5477
	step 6269:lm_loss: 4.5172, ppl: 91.5813, loss: 4.5474
	step 6270:lm_loss: 4.5173, ppl: 91.5912, loss: 4.5475
	step 6271:lm_loss: 4.5174, ppl: 91.5936, loss: 4.5475
	step 6272:lm_loss: 4.5172, ppl: 91.5752, loss: 4.5473
	step 6273:lm_loss: 4.5172, ppl: 91.5751, loss: 4.5473
	step 6274:lm_loss: 4.5171, ppl: 91.5668, loss: 4.5471
	step 6275:lm_loss: 4.5171, ppl: 91.5654, loss: 4.5471
	step 6276:lm_loss: 4.5172, ppl: 91.5764, loss: 4.5473
	step 6277:lm_loss: 4.5174, ppl: 91.5944, loss: 4.5476
	step 6278:lm_loss: 4.5173, ppl: 91.5873, loss: 4.5475
	step 6279:lm_loss: 4.5173, ppl: 91.5884, loss: 4.5475
	step 6280:lm_loss: 4.5174, ppl: 91.5926, loss: 4.5475
	step 6281:lm_loss: 4.5175, ppl: 91.6092, loss: 4.5477
	step 6282:lm_loss: 4.5176, ppl: 91.6161, loss: 4.5478
	step 6283:lm_loss: 4.5177, ppl: 91.6249, loss: 4.5479
	step 6284:lm_loss: 4.5177, ppl: 91.6209, loss: 4.5479
	step 6285:lm_loss: 4.5177, ppl: 91.6286, loss: 4.5480
	step 6286:lm_loss: 4.5177, ppl: 91.6202, loss: 4.5479
	step 6287:lm_loss: 4.5177, ppl: 91.6254, loss: 4.5479
	step 6288:lm_loss: 4.5179, ppl: 91.6406, loss: 4.5481
	step 6289:lm_loss: 4.5178, ppl: 91.6318, loss: 4.5480
	step 6290:lm_loss: 4.5179, ppl: 91.6446, loss: 4.5481
	step 6291:lm_loss: 4.5180, ppl: 91.6488, loss: 4.5481
	step 6292:lm_loss: 4.5179, ppl: 91.6465, loss: 4.5481
	step 6293:lm_loss: 4.5180, ppl: 91.6532, loss: 4.5482
	step 6294:lm_loss: 4.5181, ppl: 91.6614, loss: 4.5483
	step 6295:lm_loss: 4.5180, ppl: 91.6539, loss: 4.5482
	step 6296:lm_loss: 4.5181, ppl: 91.6588, loss: 4.5482
	step 6297:lm_loss: 4.5182, ppl: 91.6668, loss: 4.5483
	step 6298:lm_loss: 4.5181, ppl: 91.6642, loss: 4.5483
	step 6299:lm_loss: 4.5179, ppl: 91.6461, loss: 4.5481
	step 6300:lm_loss: 4.5179, ppl: 91.6460, loss: 4.5481
	step 6301:lm_loss: 4.5179, ppl: 91.6446, loss: 4.5480
	step 6302:lm_loss: 4.5179, ppl: 91.6396, loss: 4.5480
	step 6303:lm_loss: 4.5179, ppl: 91.6418, loss: 4.5480
	step 6304:lm_loss: 4.5178, ppl: 91.6366, loss: 4.5479
	step 6305:lm_loss: 4.5175, ppl: 91.6083, loss: 4.5478
	step 6306:lm_loss: 4.5175, ppl: 91.6024, loss: 4.5477
	step 6307:lm_loss: 4.5171, ppl: 91.5733, loss: 4.5475
	step 6308:lm_loss: 4.5171, ppl: 91.5693, loss: 4.5474
	step 6309:lm_loss: 4.5172, ppl: 91.5811, loss: 4.5476
	step 6310:lm_loss: 4.5172, ppl: 91.5758, loss: 4.5475
	step 6311:lm_loss: 4.5171, ppl: 91.5695, loss: 4.5475
	step 6312:lm_loss: 4.5168, ppl: 91.5428, loss: 4.5472
	step 6313:lm_loss: 4.5168, ppl: 91.5426, loss: 4.5472
	step 6314:lm_loss: 4.5167, ppl: 91.5338, loss: 4.5471
	step 6315:lm_loss: 4.5168, ppl: 91.5437, loss: 4.5472
	step 6316:lm_loss: 4.5167, ppl: 91.5299, loss: 4.5470
	step 6317:lm_loss: 4.5168, ppl: 91.5401, loss: 4.5471
	step 6318:lm_loss: 4.5166, ppl: 91.5273, loss: 4.5470
	step 6319:lm_loss: 4.5166, ppl: 91.5226, loss: 4.5469
	step 6320:lm_loss: 4.5166, ppl: 91.5218, loss: 4.5469
	step 6321:lm_loss: 4.5165, ppl: 91.5165, loss: 4.5468
	step 6322:lm_loss: 4.5164, ppl: 91.5089, loss: 4.5467
	step 6323:lm_loss: 4.5164, ppl: 91.5085, loss: 4.5467
	step 6324:lm_loss: 4.5164, ppl: 91.5093, loss: 4.5467
	step 6325:lm_loss: 4.5167, ppl: 91.5300, loss: 4.5469
	step 6326:lm_loss: 4.5165, ppl: 91.5127, loss: 4.5467
	step 6327:lm_loss: 4.5166, ppl: 91.5228, loss: 4.5469
	step 6328:lm_loss: 4.5166, ppl: 91.5269, loss: 4.5469
	step 6329:lm_loss: 4.5165, ppl: 91.5155, loss: 4.5468
	step 6330:lm_loss: 4.5163, ppl: 91.4959, loss: 4.5467
	step 6331:lm_loss: 4.5162, ppl: 91.4903, loss: 4.5466
	step 6332:lm_loss: 4.5162, ppl: 91.4897, loss: 4.5466
	step 6333:lm_loss: 4.5161, ppl: 91.4807, loss: 4.5465
	step 6334:lm_loss: 4.5161, ppl: 91.4766, loss: 4.5464
	step 6335:lm_loss: 4.5161, ppl: 91.4798, loss: 4.5464
	step 6336:lm_loss: 4.5162, ppl: 91.4884, loss: 4.5465
	step 6337:lm_loss: 4.5162, ppl: 91.4899, loss: 4.5465
	step 6338:lm_loss: 4.5163, ppl: 91.4959, loss: 4.5466
	step 6339:lm_loss: 4.5166, ppl: 91.5197, loss: 4.5469
	step 6340:lm_loss: 4.5167, ppl: 91.5306, loss: 4.5470
	step 6341:lm_loss: 4.5166, ppl: 91.5255, loss: 4.5470
	step 6342:lm_loss: 4.5165, ppl: 91.5174, loss: 4.5468
	step 6343:lm_loss: 4.5164, ppl: 91.5068, loss: 4.5467
	step 6344:lm_loss: 4.5164, ppl: 91.5060, loss: 4.5467
	step 6345:lm_loss: 4.5164, ppl: 91.5068, loss: 4.5467
	step 6346:lm_loss: 4.5162, ppl: 91.4879, loss: 4.5464
	step 6347:lm_loss: 4.5161, ppl: 91.4784, loss: 4.5464
	step 6348:lm_loss: 4.5157, ppl: 91.4397, loss: 4.5462
	step 6349:lm_loss: 4.5157, ppl: 91.4459, loss: 4.5462
	step 6350:lm_loss: 4.5158, ppl: 91.4518, loss: 4.5463
	step 6351:lm_loss: 4.5158, ppl: 91.4488, loss: 4.5462
	step 6352:lm_loss: 4.5158, ppl: 91.4509, loss: 4.5463
	step 6353:lm_loss: 4.5158, ppl: 91.4466, loss: 4.5462
	step 6354:lm_loss: 4.5157, ppl: 91.4376, loss: 4.5461
	step 6355:lm_loss: 4.5159, ppl: 91.4554, loss: 4.5462
	step 6356:lm_loss: 4.5158, ppl: 91.4522, loss: 4.5462
	step 6357:lm_loss: 4.5158, ppl: 91.4545, loss: 4.5462
	step 6358:lm_loss: 4.5158, ppl: 91.4525, loss: 4.5462
	step 6359:lm_loss: 4.5158, ppl: 91.4462, loss: 4.5462
	step 6360:lm_loss: 4.5158, ppl: 91.4499, loss: 4.5462
	step 6361:lm_loss: 4.5160, ppl: 91.4657, loss: 4.5463
	step 6362:lm_loss: 4.5159, ppl: 91.4624, loss: 4.5463
	step 6363:lm_loss: 4.5161, ppl: 91.4747, loss: 4.5464
	step 6364:lm_loss: 4.5162, ppl: 91.4858, loss: 4.5466
	step 6365:lm_loss: 4.5162, ppl: 91.4891, loss: 4.5466
	step 6366:lm_loss: 4.5163, ppl: 91.4967, loss: 4.5467
	step 6367:lm_loss: 4.5164, ppl: 91.5062, loss: 4.5468
	step 6368:lm_loss: 4.5165, ppl: 91.5133, loss: 4.5469
	step 6369:lm_loss: 4.5167, ppl: 91.5343, loss: 4.5472
	step 6370:lm_loss: 4.5169, ppl: 91.5468, loss: 4.5474
	step 6371:lm_loss: 4.5169, ppl: 91.5511, loss: 4.5474
	step 6372:lm_loss: 4.5171, ppl: 91.5669, loss: 4.5477
	step 6373:lm_loss: 4.5170, ppl: 91.5642, loss: 4.5476
	step 6374:lm_loss: 4.5171, ppl: 91.5739, loss: 4.5477
	step 6375:lm_loss: 4.5172, ppl: 91.5763, loss: 4.5477
	step 6376:lm_loss: 4.5171, ppl: 91.5733, loss: 4.5477
	step 6377:lm_loss: 4.5172, ppl: 91.5790, loss: 4.5477
	step 6378:lm_loss: 4.5171, ppl: 91.5689, loss: 4.5476
	step 6379:lm_loss: 4.5173, ppl: 91.5887, loss: 4.5479
	step 6380:lm_loss: 4.5174, ppl: 91.5990, loss: 4.5480
	step 6381:lm_loss: 4.5175, ppl: 91.6026, loss: 4.5480
	step 6382:lm_loss: 4.5176, ppl: 91.6152, loss: 4.5482
	step 6383:lm_loss: 4.5175, ppl: 91.6071, loss: 4.5481
	step 6384:lm_loss: 4.5175, ppl: 91.6042, loss: 4.5481
	step 6385:lm_loss: 4.5175, ppl: 91.6046, loss: 4.5481
	step 6386:lm_loss: 4.5176, ppl: 91.6142, loss: 4.5482
	step 6387:lm_loss: 4.5176, ppl: 91.6150, loss: 4.5482
	step 6388:lm_loss: 4.5176, ppl: 91.6188, loss: 4.5483
	step 6389:lm_loss: 4.5177, ppl: 91.6242, loss: 4.5483
	step 6390:lm_loss: 4.5177, ppl: 91.6217, loss: 4.5483
	step 6391:lm_loss: 4.5175, ppl: 91.6071, loss: 4.5482
	step 6392:lm_loss: 4.5173, ppl: 91.5884, loss: 4.5479
	step 6393:lm_loss: 4.5172, ppl: 91.5821, loss: 4.5478
	step 6394:lm_loss: 4.5171, ppl: 91.5678, loss: 4.5477
	step 6395:lm_loss: 4.5172, ppl: 91.5824, loss: 4.5479
	step 6396:lm_loss: 4.5174, ppl: 91.5978, loss: 4.5480
	step 6397:lm_loss: 4.5175, ppl: 91.6088, loss: 4.5481
	step 6398:lm_loss: 4.5176, ppl: 91.6151, loss: 4.5482
	step 6399:lm_loss: 4.5176, ppl: 91.6147, loss: 4.5481
	step 6400:lm_loss: 4.5177, ppl: 91.6217, loss: 4.5482
	step 6401:lm_loss: 4.5176, ppl: 91.6197, loss: 4.5482
	step 6402:lm_loss: 4.5178, ppl: 91.6305, loss: 4.5484
	step 6403:lm_loss: 4.5180, ppl: 91.6482, loss: 4.5485
	step 6404:lm_loss: 4.5179, ppl: 91.6474, loss: 4.5485
	step 6405:lm_loss: 4.5179, ppl: 91.6475, loss: 4.5485
	step 6406:lm_loss: 4.5181, ppl: 91.6567, loss: 4.5486
	step 6407:lm_loss: 4.5181, ppl: 91.6655, loss: 4.5487
	step 6408:lm_loss: 4.5182, ppl: 91.6716, loss: 4.5487
	step 6409:lm_loss: 4.5183, ppl: 91.6771, loss: 4.5488
	step 6410:lm_loss: 4.5182, ppl: 91.6739, loss: 4.5487
	step 6411:lm_loss: 4.5182, ppl: 91.6739, loss: 4.5487
	step 6412:lm_loss: 4.5183, ppl: 91.6841, loss: 4.5489
	step 6413:lm_loss: 4.5183, ppl: 91.6841, loss: 4.5489
	step 6414:lm_loss: 4.5182, ppl: 91.6707, loss: 4.5487
	step 6415:lm_loss: 4.5182, ppl: 91.6673, loss: 4.5487
	step 6416:lm_loss: 4.5181, ppl: 91.6620, loss: 4.5486
	step 6417:lm_loss: 4.5179, ppl: 91.6471, loss: 4.5485
	step 6418:lm_loss: 4.5179, ppl: 91.6445, loss: 4.5485
	step 6419:lm_loss: 4.5178, ppl: 91.6294, loss: 4.5483
	step 6420:lm_loss: 4.5177, ppl: 91.6244, loss: 4.5482
	step 6421:lm_loss: 4.5176, ppl: 91.6181, loss: 4.5481
	step 6422:lm_loss: 4.5177, ppl: 91.6254, loss: 4.5481
	step 6423:lm_loss: 4.5176, ppl: 91.6160, loss: 4.5480
	step 6424:lm_loss: 4.5177, ppl: 91.6251, loss: 4.5481
	step 6425:lm_loss: 4.5178, ppl: 91.6324, loss: 4.5482
	step 6426:lm_loss: 4.5177, ppl: 91.6254, loss: 4.5481
	step 6427:lm_loss: 4.5179, ppl: 91.6415, loss: 4.5483
	step 6428:lm_loss: 4.5179, ppl: 91.6440, loss: 4.5483
	step 6429:lm_loss: 4.5178, ppl: 91.6367, loss: 4.5483
	step 6430:lm_loss: 4.5177, ppl: 91.6261, loss: 4.5482
	step 6431:lm_loss: 4.5169, ppl: 91.5535, loss: 4.5478
	step 6432:lm_loss: 4.5170, ppl: 91.5602, loss: 4.5479
	step 6433:lm_loss: 4.5168, ppl: 91.5383, loss: 4.5477
	step 6434:lm_loss: 4.5169, ppl: 91.5484, loss: 4.5478
	step 6435:lm_loss: 4.5169, ppl: 91.5491, loss: 4.5478
	step 6436:lm_loss: 4.5168, ppl: 91.5446, loss: 4.5477
	step 6437:lm_loss: 4.5169, ppl: 91.5477, loss: 4.5477
	step 6438:lm_loss: 4.5167, ppl: 91.5343, loss: 4.5475
	step 6439:lm_loss: 4.5167, ppl: 91.5335, loss: 4.5475
	step 6440:lm_loss: 4.5166, ppl: 91.5225, loss: 4.5474
	step 6441:lm_loss: 4.5167, ppl: 91.5375, loss: 4.5476
	step 6442:lm_loss: 4.5167, ppl: 91.5367, loss: 4.5475
	step 6443:lm_loss: 4.5167, ppl: 91.5303, loss: 4.5475
	step 6444:lm_loss: 4.5166, ppl: 91.5245, loss: 4.5474
	step 6445:lm_loss: 4.5166, ppl: 91.5215, loss: 4.5474
	step 6446:lm_loss: 4.5166, ppl: 91.5238, loss: 4.5474
	step 6447:lm_loss: 4.5166, ppl: 91.5270, loss: 4.5474
	step 6448:lm_loss: 4.5166, ppl: 91.5208, loss: 4.5474
	step 6449:lm_loss: 4.5166, ppl: 91.5250, loss: 4.5474
	step 6450:lm_loss: 4.5166, ppl: 91.5211, loss: 4.5474
	step 6451:lm_loss: 4.5165, ppl: 91.5188, loss: 4.5474
	step 6452:lm_loss: 4.5167, ppl: 91.5349, loss: 4.5476
	step 6453:lm_loss: 4.5167, ppl: 91.5326, loss: 4.5476
	step 6454:lm_loss: 4.5169, ppl: 91.5529, loss: 4.5478
	step 6455:lm_loss: 4.5170, ppl: 91.5604, loss: 4.5479
	step 6456:lm_loss: 4.5169, ppl: 91.5488, loss: 4.5478
	step 6457:lm_loss: 4.5168, ppl: 91.5407, loss: 4.5478
	step 6458:lm_loss: 4.5167, ppl: 91.5303, loss: 4.5476
	step 6459:lm_loss: 4.5167, ppl: 91.5298, loss: 4.5476
	step 6460:lm_loss: 4.5166, ppl: 91.5216, loss: 4.5475
	step 6461:lm_loss: 4.5167, ppl: 91.5315, loss: 4.5477
	step 6462:lm_loss: 4.5168, ppl: 91.5382, loss: 4.5477
	step 6463:lm_loss: 4.5168, ppl: 91.5462, loss: 4.5478
	step 6464:lm_loss: 4.5169, ppl: 91.5554, loss: 4.5478
	step 6465:lm_loss: 4.5169, ppl: 91.5481, loss: 4.5478
	step 6466:lm_loss: 4.5169, ppl: 91.5529, loss: 4.5478
	step 6467:lm_loss: 4.5170, ppl: 91.5644, loss: 4.5479
	step 6468:lm_loss: 4.5169, ppl: 91.5471, loss: 4.5476
	step 6469:lm_loss: 4.5169, ppl: 91.5489, loss: 4.5477
	step 6470:lm_loss: 4.5169, ppl: 91.5492, loss: 4.5477
	step 6471:lm_loss: 4.5169, ppl: 91.5504, loss: 4.5477
	step 6472:lm_loss: 4.5170, ppl: 91.5637, loss: 4.5478
	step 6473:lm_loss: 4.5172, ppl: 91.5783, loss: 4.5480
	step 6474:lm_loss: 4.5172, ppl: 91.5763, loss: 4.5479
	step 6475:lm_loss: 4.5171, ppl: 91.5721, loss: 4.5479
	step 6476:lm_loss: 4.5172, ppl: 91.5810, loss: 4.5480
	step 6477:lm_loss: 4.5173, ppl: 91.5880, loss: 4.5481
	step 6478:lm_loss: 4.5173, ppl: 91.5889, loss: 4.5481
	step 6479:lm_loss: 4.5172, ppl: 91.5780, loss: 4.5480
	step 6480:lm_loss: 4.5174, ppl: 91.5962, loss: 4.5482
	step 6481:lm_loss: 4.5174, ppl: 91.5980, loss: 4.5483
	step 6482:lm_loss: 4.5173, ppl: 91.5888, loss: 4.5482
	step 6483:lm_loss: 4.5173, ppl: 91.5909, loss: 4.5482
	step 6484:lm_loss: 4.5174, ppl: 91.5985, loss: 4.5483
	step 6485:lm_loss: 4.5175, ppl: 91.6023, loss: 4.5483
	step 6486:lm_loss: 4.5177, ppl: 91.6252, loss: 4.5485
	step 6487:lm_loss: 4.5175, ppl: 91.6089, loss: 4.5483
	step 6488:lm_loss: 4.5175, ppl: 91.6021, loss: 4.5482
	step 6489:lm_loss: 4.5174, ppl: 91.5973, loss: 4.5481
	step 6490:lm_loss: 4.5175, ppl: 91.6023, loss: 4.5482
	step 6491:lm_loss: 4.5175, ppl: 91.6075, loss: 4.5482
	step 6492:lm_loss: 4.5175, ppl: 91.6034, loss: 4.5482
	step 6493:lm_loss: 4.5175, ppl: 91.6087, loss: 4.5482
	step 6494:lm_loss: 4.5176, ppl: 91.6111, loss: 4.5483
	step 6495:lm_loss: 4.5176, ppl: 91.6135, loss: 4.5483
	step 6496:lm_loss: 4.5176, ppl: 91.6197, loss: 4.5484
	step 6497:lm_loss: 4.5176, ppl: 91.6122, loss: 4.5483
	step 6498:lm_loss: 4.5176, ppl: 91.6147, loss: 4.5483
	step 6499:lm_loss: 4.5174, ppl: 91.5982, loss: 4.5481
	step 6500:lm_loss: 4.5176, ppl: 91.6170, loss: 4.5483
	step 6501:lm_loss: 4.5176, ppl: 91.6139, loss: 4.5483
	step 6502:lm_loss: 4.5175, ppl: 91.6049, loss: 4.5482
	step 6503:lm_loss: 4.5174, ppl: 91.6001, loss: 4.5481
	step 6504:lm_loss: 4.5174, ppl: 91.5946, loss: 4.5480
	step 6505:lm_loss: 4.5173, ppl: 91.5884, loss: 4.5479
	step 6506:lm_loss: 4.5173, ppl: 91.5855, loss: 4.5479
	step 6507:lm_loss: 4.5174, ppl: 91.5945, loss: 4.5480
	step 6508:lm_loss: 4.5173, ppl: 91.5902, loss: 4.5479
	step 6509:lm_loss: 4.5174, ppl: 91.6000, loss: 4.5480
	step 6510:lm_loss: 4.5174, ppl: 91.5943, loss: 4.5479
	step 6511:lm_loss: 4.5175, ppl: 91.6045, loss: 4.5481
	step 6512:lm_loss: 4.5176, ppl: 91.6158, loss: 4.5482
	step 6513:lm_loss: 4.5175, ppl: 91.6073, loss: 4.5481
	step 6514:lm_loss: 4.5176, ppl: 91.6188, loss: 4.5482
	step 6515:lm_loss: 4.5175, ppl: 91.6102, loss: 4.5481
	step 6516:lm_loss: 4.5175, ppl: 91.6061, loss: 4.5480
	step 6517:lm_loss: 4.5175, ppl: 91.6051, loss: 4.5480
	step 6518:lm_loss: 4.5174, ppl: 91.5980, loss: 4.5479
	step 6519:lm_loss: 4.5174, ppl: 91.5996, loss: 4.5479
	step 6520:lm_loss: 4.5175, ppl: 91.6060, loss: 4.5480
	step 6521:lm_loss: 4.5174, ppl: 91.5988, loss: 4.5479
	step 6522:lm_loss: 4.5174, ppl: 91.5992, loss: 4.5479
	step 6523:lm_loss: 4.5174, ppl: 91.5930, loss: 4.5478
	step 6524:lm_loss: 4.5174, ppl: 91.5954, loss: 4.5478
	step 6525:lm_loss: 4.5174, ppl: 91.6001, loss: 4.5478
	step 6526:lm_loss: 4.5174, ppl: 91.5971, loss: 4.5478
	step 6527:lm_loss: 4.5174, ppl: 91.5999, loss: 4.5478
	step 6528:lm_loss: 4.5175, ppl: 91.6020, loss: 4.5478
	step 6529:lm_loss: 4.5174, ppl: 91.5988, loss: 4.5478
	step 6530:lm_loss: 4.5176, ppl: 91.6114, loss: 4.5479
	step 6531:lm_loss: 4.5178, ppl: 91.6309, loss: 4.5482
	step 6532:lm_loss: 4.5179, ppl: 91.6404, loss: 4.5483
	step 6533:lm_loss: 4.5179, ppl: 91.6460, loss: 4.5484
	step 6534:lm_loss: 4.5179, ppl: 91.6453, loss: 4.5484
	step 6535:lm_loss: 4.5178, ppl: 91.6379, loss: 4.5483
	step 6536:lm_loss: 4.5176, ppl: 91.6171, loss: 4.5480
	step 6537:lm_loss: 4.5177, ppl: 91.6203, loss: 4.5480
	step 6538:lm_loss: 4.5177, ppl: 91.6201, loss: 4.5480
	step 6539:lm_loss: 4.5176, ppl: 91.6176, loss: 4.5480
	step 6540:lm_loss: 4.5177, ppl: 91.6233, loss: 4.5481
	step 6541:lm_loss: 4.5178, ppl: 91.6301, loss: 4.5481
	step 6542:lm_loss: 4.5177, ppl: 91.6228, loss: 4.5480
	step 6543:lm_loss: 4.5177, ppl: 91.6227, loss: 4.5480
	step 6544:lm_loss: 4.5178, ppl: 91.6327, loss: 4.5481
	step 6545:lm_loss: 4.5179, ppl: 91.6437, loss: 4.5482
	step 6546:lm_loss: 4.5180, ppl: 91.6513, loss: 4.5483
	step 6547:lm_loss: 4.5179, ppl: 91.6435, loss: 4.5482
	step 6548:lm_loss: 4.5179, ppl: 91.6449, loss: 4.5482
	step 6549:lm_loss: 4.5180, ppl: 91.6522, loss: 4.5483
	step 6550:lm_loss: 4.5181, ppl: 91.6606, loss: 4.5484
	step 6551:lm_loss: 4.5181, ppl: 91.6585, loss: 4.5483
	step 6552:lm_loss: 4.5181, ppl: 91.6621, loss: 4.5484
	step 6553:lm_loss: 4.5183, ppl: 91.6768, loss: 4.5485
	step 6554:lm_loss: 4.5183, ppl: 91.6760, loss: 4.5485
	step 6555:lm_loss: 4.5184, ppl: 91.6890, loss: 4.5487
	step 6556:lm_loss: 4.5183, ppl: 91.6787, loss: 4.5485
	step 6557:lm_loss: 4.5185, ppl: 91.6939, loss: 4.5486
	step 6558:lm_loss: 4.5184, ppl: 91.6863, loss: 4.5486
	step 6559:lm_loss: 4.5183, ppl: 91.6774, loss: 4.5485
	step 6560:lm_loss: 4.5182, ppl: 91.6670, loss: 4.5483
	step 6561:lm_loss: 4.5181, ppl: 91.6639, loss: 4.5483
	step 6562:lm_loss: 4.5181, ppl: 91.6635, loss: 4.5483
	step 6563:lm_loss: 4.5183, ppl: 91.6835, loss: 4.5485
	step 6564:lm_loss: 4.5184, ppl: 91.6903, loss: 4.5486
	step 6565:lm_loss: 4.5185, ppl: 91.6973, loss: 4.5487
	step 6566:lm_loss: 4.5184, ppl: 91.6920, loss: 4.5486
	step 6567:lm_loss: 4.5184, ppl: 91.6868, loss: 4.5485
	step 6568:lm_loss: 4.5182, ppl: 91.6721, loss: 4.5483
	step 6569:lm_loss: 4.5182, ppl: 91.6673, loss: 4.5483
	step 6570:lm_loss: 4.5183, ppl: 91.6784, loss: 4.5484
	step 6571:lm_loss: 4.5184, ppl: 91.6895, loss: 4.5485
	step 6572:lm_loss: 4.5184, ppl: 91.6930, loss: 4.5486
	step 6573:lm_loss: 4.5185, ppl: 91.7019, loss: 4.5486
	step 6574:lm_loss: 4.5186, ppl: 91.7033, loss: 4.5487
	step 6575:lm_loss: 4.5185, ppl: 91.6984, loss: 4.5486
	step 6576:lm_loss: 4.5182, ppl: 91.6729, loss: 4.5484
	step 6577:lm_loss: 4.5183, ppl: 91.6763, loss: 4.5484
	step 6578:lm_loss: 4.5184, ppl: 91.6895, loss: 4.5485
	step 6579:lm_loss: 4.5184, ppl: 91.6888, loss: 4.5485
	step 6580:lm_loss: 4.5184, ppl: 91.6877, loss: 4.5485
	step 6581:lm_loss: 4.5184, ppl: 91.6874, loss: 4.5485
	step 6582:lm_loss: 4.5184, ppl: 91.6880, loss: 4.5485
	step 6583:lm_loss: 4.5183, ppl: 91.6776, loss: 4.5484
	step 6584:lm_loss: 4.5182, ppl: 91.6706, loss: 4.5483
	step 6585:lm_loss: 4.5182, ppl: 91.6739, loss: 4.5484
	step 6586:lm_loss: 4.5183, ppl: 91.6800, loss: 4.5484
	step 6587:lm_loss: 4.5184, ppl: 91.6865, loss: 4.5485
	step 6588:lm_loss: 4.5185, ppl: 91.6969, loss: 4.5486
	step 6589:lm_loss: 4.5185, ppl: 91.6972, loss: 4.5486
	step 6590:lm_loss: 4.5185, ppl: 91.6996, loss: 4.5486
	step 6591:lm_loss: 4.5185, ppl: 91.6968, loss: 4.5486
	step 6592:lm_loss: 4.5185, ppl: 91.6983, loss: 4.5486
	step 6593:lm_loss: 4.5185, ppl: 91.6964, loss: 4.5486
	step 6594:lm_loss: 4.5184, ppl: 91.6859, loss: 4.5485
	step 6595:lm_loss: 4.5183, ppl: 91.6795, loss: 4.5484
	step 6596:lm_loss: 4.5182, ppl: 91.6736, loss: 4.5483
	step 6597:lm_loss: 4.5183, ppl: 91.6777, loss: 4.5484
	step 6598:lm_loss: 4.5183, ppl: 91.6831, loss: 4.5484
	step 6599:lm_loss: 4.5184, ppl: 91.6932, loss: 4.5485
	step 6600:lm_loss: 4.5185, ppl: 91.6975, loss: 4.5486
	step 6601:lm_loss: 4.5185, ppl: 91.7006, loss: 4.5486
	step 6602:lm_loss: 4.5186, ppl: 91.7072, loss: 4.5487
	step 6603:lm_loss: 4.5184, ppl: 91.6926, loss: 4.5486
	step 6604:lm_loss: 4.5183, ppl: 91.6772, loss: 4.5483
	step 6605:lm_loss: 4.5180, ppl: 91.6537, loss: 4.5481
	step 6606:lm_loss: 4.5180, ppl: 91.6487, loss: 4.5481
	step 6607:lm_loss: 4.5180, ppl: 91.6477, loss: 4.5481
	step 6608:lm_loss: 4.5180, ppl: 91.6486, loss: 4.5481
	step 6609:lm_loss: 4.5179, ppl: 91.6470, loss: 4.5481
	step 6610:lm_loss: 4.5179, ppl: 91.6414, loss: 4.5480
	step 6611:lm_loss: 4.5180, ppl: 91.6525, loss: 4.5481
	step 6612:lm_loss: 4.5181, ppl: 91.6614, loss: 4.5482
	step 6613:lm_loss: 4.5181, ppl: 91.6592, loss: 4.5481
	step 6614:lm_loss: 4.5180, ppl: 91.6527, loss: 4.5481
	step 6615:lm_loss: 4.5179, ppl: 91.6421, loss: 4.5479
	step 6616:lm_loss: 4.5177, ppl: 91.6281, loss: 4.5478
	step 6617:lm_loss: 4.5179, ppl: 91.6434, loss: 4.5479
	step 6618:lm_loss: 4.5180, ppl: 91.6526, loss: 4.5480
	step 6619:lm_loss: 4.5179, ppl: 91.6471, loss: 4.5479
	step 6620:lm_loss: 4.5182, ppl: 91.6659, loss: 4.5480
	step 6621:lm_loss: 4.5181, ppl: 91.6617, loss: 4.5479
	step 6622:lm_loss: 4.5182, ppl: 91.6700, loss: 4.5480
	step 6623:lm_loss: 4.5183, ppl: 91.6788, loss: 4.5481
	step 6624:lm_loss: 4.5185, ppl: 91.6962, loss: 4.5483
	step 6625:lm_loss: 4.5185, ppl: 91.6934, loss: 4.5483
	step 6626:lm_loss: 4.5185, ppl: 91.6985, loss: 4.5483
	step 6627:lm_loss: 4.5186, ppl: 91.7059, loss: 4.5484
	step 6628:lm_loss: 4.5187, ppl: 91.7161, loss: 4.5485
	step 6629:lm_loss: 4.5188, ppl: 91.7218, loss: 4.5486
	step 6630:lm_loss: 4.5188, ppl: 91.7229, loss: 4.5486
	step 6631:lm_loss: 4.5189, ppl: 91.7368, loss: 4.5487
	step 6632:lm_loss: 4.5189, ppl: 91.7386, loss: 4.5487
	step 6633:lm_loss: 4.5191, ppl: 91.7563, loss: 4.5489
	step 6634:lm_loss: 4.5191, ppl: 91.7566, loss: 4.5489
	step 6635:lm_loss: 4.5194, ppl: 91.7824, loss: 4.5492
	step 6636:lm_loss: 4.5194, ppl: 91.7793, loss: 4.5491
	step 6637:lm_loss: 4.5196, ppl: 91.7943, loss: 4.5493
	step 6638:lm_loss: 4.5195, ppl: 91.7937, loss: 4.5493
	step 6639:lm_loss: 4.5193, ppl: 91.7745, loss: 4.5491
	step 6640:lm_loss: 4.5193, ppl: 91.7745, loss: 4.5491
	step 6641:lm_loss: 4.5195, ppl: 91.7854, loss: 4.5492
	step 6642:lm_loss: 4.5195, ppl: 91.7906, loss: 4.5493
	step 6643:lm_loss: 4.5195, ppl: 91.7870, loss: 4.5492
	step 6644:lm_loss: 4.5194, ppl: 91.7783, loss: 4.5492
	step 6645:lm_loss: 4.5193, ppl: 91.7683, loss: 4.5490
	step 6646:lm_loss: 4.5192, ppl: 91.7639, loss: 4.5490
	step 6647:lm_loss: 4.5192, ppl: 91.7620, loss: 4.5489
	step 6648:lm_loss: 4.5192, ppl: 91.7634, loss: 4.5489
	step 6649:lm_loss: 4.5192, ppl: 91.7595, loss: 4.5489
	step 6650:lm_loss: 4.5191, ppl: 91.7541, loss: 4.5488
	step 6651:lm_loss: 4.5191, ppl: 91.7545, loss: 4.5488
	step 6652:lm_loss: 4.5192, ppl: 91.7605, loss: 4.5489
	step 6653:lm_loss: 4.5192, ppl: 91.7609, loss: 4.5489
	step 6654:lm_loss: 4.5192, ppl: 91.7640, loss: 4.5490
	step 6655:lm_loss: 4.5192, ppl: 91.7589, loss: 4.5489
	step 6656:lm_loss: 4.5192, ppl: 91.7640, loss: 4.5490
	step 6657:lm_loss: 4.5192, ppl: 91.7645, loss: 4.5490
	step 6658:lm_loss: 4.5192, ppl: 91.7618, loss: 4.5489
	step 6659:lm_loss: 4.5191, ppl: 91.7496, loss: 4.5488
	step 6660:lm_loss: 4.5191, ppl: 91.7549, loss: 4.5488
	step 6661:lm_loss: 4.5192, ppl: 91.7667, loss: 4.5489
	step 6662:lm_loss: 4.5193, ppl: 91.7691, loss: 4.5489
	step 6663:lm_loss: 4.5193, ppl: 91.7720, loss: 4.5490
	step 6664:lm_loss: 4.5194, ppl: 91.7841, loss: 4.5490
	step 6665:lm_loss: 4.5194, ppl: 91.7830, loss: 4.5490
	step 6666:lm_loss: 4.5194, ppl: 91.7850, loss: 4.5490
	step 6667:lm_loss: 4.5195, ppl: 91.7867, loss: 4.5490
	step 6668:lm_loss: 4.5195, ppl: 91.7893, loss: 4.5491
	step 6669:lm_loss: 4.5195, ppl: 91.7858, loss: 4.5490
	step 6670:lm_loss: 4.5196, ppl: 91.7977, loss: 4.5492
	step 6671:lm_loss: 4.5195, ppl: 91.7894, loss: 4.5491
	step 6672:lm_loss: 4.5196, ppl: 91.8021, loss: 4.5493
	step 6673:lm_loss: 4.5197, ppl: 91.8044, loss: 4.5493
	step 6674:lm_loss: 4.5196, ppl: 91.8008, loss: 4.5493
	step 6675:lm_loss: 4.5195, ppl: 91.7852, loss: 4.5491
	step 6676:lm_loss: 4.5195, ppl: 91.7888, loss: 4.5491
	step 6677:lm_loss: 4.5195, ppl: 91.7867, loss: 4.5491
	step 6678:lm_loss: 4.5195, ppl: 91.7902, loss: 4.5491
	step 6679:lm_loss: 4.5195, ppl: 91.7938, loss: 4.5492
	step 6680:lm_loss: 4.5196, ppl: 91.7997, loss: 4.5493
	step 6681:lm_loss: 4.5197, ppl: 91.8037, loss: 4.5493
	step 6682:lm_loss: 4.5197, ppl: 91.8050, loss: 4.5493
	step 6683:lm_loss: 4.5196, ppl: 91.8007, loss: 4.5493
	step 6684:lm_loss: 4.5197, ppl: 91.8121, loss: 4.5494
	step 6685:lm_loss: 4.5198, ppl: 91.8139, loss: 4.5494
	step 6686:lm_loss: 4.5197, ppl: 91.8038, loss: 4.5493
	step 6687:lm_loss: 4.5196, ppl: 91.8034, loss: 4.5493
	step 6688:lm_loss: 4.5197, ppl: 91.8077, loss: 4.5493
	step 6689:lm_loss: 4.5197, ppl: 91.8108, loss: 4.5494
	step 6690:lm_loss: 4.5197, ppl: 91.8100, loss: 4.5494
	step 6691:lm_loss: 4.5197, ppl: 91.8072, loss: 4.5493
	step 6692:lm_loss: 4.5197, ppl: 91.8116, loss: 4.5494
	step 6693:lm_loss: 4.5194, ppl: 91.7837, loss: 4.5490
	step 6694:lm_loss: 4.5193, ppl: 91.7748, loss: 4.5489
	step 6695:lm_loss: 4.5187, ppl: 91.7168, loss: 4.5488
	step 6696:lm_loss: 4.5188, ppl: 91.7242, loss: 4.5489
	step 6697:lm_loss: 4.5187, ppl: 91.7184, loss: 4.5488
	step 6698:lm_loss: 4.5188, ppl: 91.7247, loss: 4.5489
	step 6699:lm_loss: 4.5188, ppl: 91.7294, loss: 4.5489
	step 6700:lm_loss: 4.5189, ppl: 91.7341, loss: 4.5489
	step 6701:lm_loss: 4.5190, ppl: 91.7416, loss: 4.5490
	step 6702:lm_loss: 4.5191, ppl: 91.7517, loss: 4.5492
	step 6703:lm_loss: 4.5191, ppl: 91.7562, loss: 4.5492
	step 6704:lm_loss: 4.5189, ppl: 91.7356, loss: 4.5491
	step 6705:lm_loss: 4.5189, ppl: 91.7384, loss: 4.5491
	step 6706:lm_loss: 4.5189, ppl: 91.7368, loss: 4.5491
	step 6707:lm_loss: 4.5191, ppl: 91.7487, loss: 4.5491
	step 6708:lm_loss: 4.5191, ppl: 91.7509, loss: 4.5491
	step 6709:lm_loss: 4.5191, ppl: 91.7521, loss: 4.5492
	step 6710:lm_loss: 4.5190, ppl: 91.7404, loss: 4.5490
	step 6711:lm_loss: 4.5188, ppl: 91.7215, loss: 4.5488
	step 6712:lm_loss: 4.5186, ppl: 91.7111, loss: 4.5486
	step 6713:lm_loss: 4.5185, ppl: 91.6940, loss: 4.5484
	step 6714:lm_loss: 4.5185, ppl: 91.7018, loss: 4.5485
	step 6715:lm_loss: 4.5187, ppl: 91.7154, loss: 4.5487
	step 6716:lm_loss: 4.5186, ppl: 91.7073, loss: 4.5486
	step 6717:lm_loss: 4.5185, ppl: 91.7009, loss: 4.5485
	step 6718:lm_loss: 4.5186, ppl: 91.7035, loss: 4.5486
	step 6719:lm_loss: 4.5184, ppl: 91.6845, loss: 4.5483
	step 6720:lm_loss: 4.5184, ppl: 91.6880, loss: 4.5484
	step 6721:lm_loss: 4.5185, ppl: 91.6977, loss: 4.5484
	step 6722:lm_loss: 4.5182, ppl: 91.6664, loss: 4.5482
	step 6723:lm_loss: 4.5182, ppl: 91.6665, loss: 4.5482
	step 6724:lm_loss: 4.5182, ppl: 91.6700, loss: 4.5482
	step 6725:lm_loss: 4.5180, ppl: 91.6560, loss: 4.5481
	step 6726:lm_loss: 4.5181, ppl: 91.6591, loss: 4.5481
	step 6727:lm_loss: 4.5181, ppl: 91.6599, loss: 4.5481
	step 6728:lm_loss: 4.5178, ppl: 91.6343, loss: 4.5478
	step 6729:lm_loss: 4.5179, ppl: 91.6390, loss: 4.5478
	step 6730:lm_loss: 4.5178, ppl: 91.6328, loss: 4.5478
	step 6731:lm_loss: 4.5178, ppl: 91.6309, loss: 4.5477
	step 6732:lm_loss: 4.5178, ppl: 91.6309, loss: 4.5477
	step 6733:lm_loss: 4.5176, ppl: 91.6186, loss: 4.5476
	step 6734:lm_loss: 4.5177, ppl: 91.6253, loss: 4.5477
	step 6735:lm_loss: 4.5179, ppl: 91.6398, loss: 4.5480
	step 6736:lm_loss: 4.5179, ppl: 91.6435, loss: 4.5480
	step 6737:lm_loss: 4.5180, ppl: 91.6549, loss: 4.5481
	step 6738:lm_loss: 4.5182, ppl: 91.6708, loss: 4.5483
	step 6739:lm_loss: 4.5182, ppl: 91.6728, loss: 4.5483
	step 6740:lm_loss: 4.5182, ppl: 91.6709, loss: 4.5483
	step 6741:lm_loss: 4.5183, ppl: 91.6769, loss: 4.5483
	step 6742:lm_loss: 4.5184, ppl: 91.6901, loss: 4.5485
	step 6743:lm_loss: 4.5185, ppl: 91.6964, loss: 4.5486
	step 6744:lm_loss: 4.5186, ppl: 91.7091, loss: 4.5487
	step 6745:lm_loss: 4.5185, ppl: 91.6957, loss: 4.5485
	step 6746:lm_loss: 4.5186, ppl: 91.7027, loss: 4.5486
	step 6747:lm_loss: 4.5185, ppl: 91.6986, loss: 4.5485
	step 6748:lm_loss: 4.5186, ppl: 91.7066, loss: 4.5486
	step 6749:lm_loss: 4.5186, ppl: 91.7059, loss: 4.5486
	step 6750:lm_loss: 4.5186, ppl: 91.7069, loss: 4.5486
	step 6751:lm_loss: 4.5185, ppl: 91.6977, loss: 4.5484
	step 6752:lm_loss: 4.5185, ppl: 91.6972, loss: 4.5484
	step 6753:lm_loss: 4.5184, ppl: 91.6919, loss: 4.5484
	step 6754:lm_loss: 4.5185, ppl: 91.7022, loss: 4.5485
	step 6755:lm_loss: 4.5186, ppl: 91.7103, loss: 4.5485
	step 6756:lm_loss: 4.5185, ppl: 91.6974, loss: 4.5484
	step 6757:lm_loss: 4.5184, ppl: 91.6846, loss: 4.5481
	step 6758:lm_loss: 4.5183, ppl: 91.6823, loss: 4.5481
	step 6759:lm_loss: 4.5181, ppl: 91.6647, loss: 4.5480
	step 6760:lm_loss: 4.5181, ppl: 91.6636, loss: 4.5480
	step 6761:lm_loss: 4.5180, ppl: 91.6564, loss: 4.5479
	step 6762:lm_loss: 4.5179, ppl: 91.6439, loss: 4.5478
	step 6763:lm_loss: 4.5179, ppl: 91.6464, loss: 4.5478
	step 6764:lm_loss: 4.5180, ppl: 91.6562, loss: 4.5479
	step 6765:lm_loss: 4.5182, ppl: 91.6671, loss: 4.5480
	step 6766:lm_loss: 4.5182, ppl: 91.6716, loss: 4.5481
	step 6767:lm_loss: 4.5182, ppl: 91.6725, loss: 4.5481
	step 6768:lm_loss: 4.5182, ppl: 91.6750, loss: 4.5481
	step 6769:lm_loss: 4.5185, ppl: 91.6955, loss: 4.5484
	step 6770:lm_loss: 4.5185, ppl: 91.6951, loss: 4.5484
	step 6771:lm_loss: 4.5186, ppl: 91.7053, loss: 4.5485
	step 6772:lm_loss: 4.5188, ppl: 91.7229, loss: 4.5487
	step 6773:lm_loss: 4.5188, ppl: 91.7250, loss: 4.5487
	step 6774:lm_loss: 4.5187, ppl: 91.7189, loss: 4.5487
	step 6775:lm_loss: 4.5187, ppl: 91.7165, loss: 4.5486
	step 6776:lm_loss: 4.5185, ppl: 91.7009, loss: 4.5484
	step 6777:lm_loss: 4.5185, ppl: 91.6989, loss: 4.5484
	step 6778:lm_loss: 4.5186, ppl: 91.7055, loss: 4.5485
	step 6779:lm_loss: 4.5185, ppl: 91.7012, loss: 4.5484
	step 6780:lm_loss: 4.5186, ppl: 91.7035, loss: 4.5484
	step 6781:lm_loss: 4.5185, ppl: 91.6939, loss: 4.5484
	step 6782:lm_loss: 4.5185, ppl: 91.7011, loss: 4.5484
	step 6783:lm_loss: 4.5185, ppl: 91.6943, loss: 4.5484
	step 6784:lm_loss: 4.5185, ppl: 91.6971, loss: 4.5484
	step 6785:lm_loss: 4.5186, ppl: 91.7114, loss: 4.5485
	step 6786:lm_loss: 4.5185, ppl: 91.7008, loss: 4.5484
	step 6787:lm_loss: 4.5184, ppl: 91.6910, loss: 4.5483
	step 6788:lm_loss: 4.5184, ppl: 91.6881, loss: 4.5483
	step 6789:lm_loss: 4.5185, ppl: 91.6962, loss: 4.5484
	step 6790:lm_loss: 4.5185, ppl: 91.6996, loss: 4.5484
	step 6791:lm_loss: 4.5186, ppl: 91.7038, loss: 4.5485
	step 6792:lm_loss: 4.5185, ppl: 91.7004, loss: 4.5484
	step 6793:lm_loss: 4.5184, ppl: 91.6877, loss: 4.5483
	step 6794:lm_loss: 4.5184, ppl: 91.6934, loss: 4.5484
	step 6795:lm_loss: 4.5185, ppl: 91.6944, loss: 4.5484
	step 6796:lm_loss: 4.5185, ppl: 91.6971, loss: 4.5484
	step 6797:lm_loss: 4.5186, ppl: 91.7105, loss: 4.5485
	step 6798:lm_loss: 4.5186, ppl: 91.7093, loss: 4.5485
	step 6799:lm_loss: 4.5187, ppl: 91.7179, loss: 4.5486
	step 6800:lm_loss: 4.5186, ppl: 91.7109, loss: 4.5485
	step 6801:lm_loss: 4.5186, ppl: 91.7089, loss: 4.5484
	step 6802:lm_loss: 4.5187, ppl: 91.7183, loss: 4.5486
	step 6803:lm_loss: 4.5187, ppl: 91.7169, loss: 4.5485
	step 6804:lm_loss: 4.5187, ppl: 91.7117, loss: 4.5485
	step 6805:lm_loss: 4.5186, ppl: 91.7096, loss: 4.5484
	step 6806:lm_loss: 4.5188, ppl: 91.7240, loss: 4.5486
	step 6807:lm_loss: 4.5188, ppl: 91.7232, loss: 4.5486
	step 6808:lm_loss: 4.5188, ppl: 91.7252, loss: 4.5486
	step 6809:lm_loss: 4.5186, ppl: 91.7026, loss: 4.5484
	step 6810:lm_loss: 4.5185, ppl: 91.6969, loss: 4.5483
	step 6811:lm_loss: 4.5185, ppl: 91.6949, loss: 4.5483
	step 6812:lm_loss: 4.5187, ppl: 91.7162, loss: 4.5486
	step 6813:lm_loss: 4.5186, ppl: 91.7045, loss: 4.5484
	step 6814:lm_loss: 4.5188, ppl: 91.7283, loss: 4.5486
	step 6815:lm_loss: 4.5187, ppl: 91.7171, loss: 4.5485
	step 6816:lm_loss: 4.5187, ppl: 91.7164, loss: 4.5485
	step 6817:lm_loss: 4.5186, ppl: 91.7029, loss: 4.5484
	step 6818:lm_loss: 4.5186, ppl: 91.7057, loss: 4.5484
	step 6819:lm_loss: 4.5185, ppl: 91.6982, loss: 4.5483
	step 6820:lm_loss: 4.5185, ppl: 91.7015, loss: 4.5484
	step 6821:lm_loss: 4.5187, ppl: 91.7160, loss: 4.5485
	step 6822:lm_loss: 4.5187, ppl: 91.7197, loss: 4.5486
	step 6823:lm_loss: 4.5187, ppl: 91.7131, loss: 4.5485
	step 6824:lm_loss: 4.5186, ppl: 91.7062, loss: 4.5485
	step 6825:lm_loss: 4.5186, ppl: 91.7089, loss: 4.5485
	step 6826:lm_loss: 4.5185, ppl: 91.6987, loss: 4.5483
	step 6827:lm_loss: 4.5185, ppl: 91.7004, loss: 4.5483
	step 6828:lm_loss: 4.5185, ppl: 91.6990, loss: 4.5483
	step 6829:lm_loss: 4.5186, ppl: 91.7051, loss: 4.5484
	step 6830:lm_loss: 4.5186, ppl: 91.7078, loss: 4.5484
	step 6831:lm_loss: 4.5186, ppl: 91.7070, loss: 4.5484
	step 6832:lm_loss: 4.5187, ppl: 91.7169, loss: 4.5485
	step 6833:lm_loss: 4.5188, ppl: 91.7243, loss: 4.5486
	step 6834:lm_loss: 4.5186, ppl: 91.7111, loss: 4.5485
	step 6835:lm_loss: 4.5186, ppl: 91.7049, loss: 4.5484
	step 6836:lm_loss: 4.5186, ppl: 91.7100, loss: 4.5485
	step 6837:lm_loss: 4.5185, ppl: 91.7021, loss: 4.5484
	step 6838:lm_loss: 4.5185, ppl: 91.6955, loss: 4.5483
	step 6839:lm_loss: 4.5185, ppl: 91.7023, loss: 4.5485
	step 6840:lm_loss: 4.5183, ppl: 91.6827, loss: 4.5482
	step 6841:lm_loss: 4.5183, ppl: 91.6761, loss: 4.5482
	step 6842:lm_loss: 4.5183, ppl: 91.6761, loss: 4.5482
	step 6843:lm_loss: 4.5181, ppl: 91.6635, loss: 4.5480
	step 6844:lm_loss: 4.5181, ppl: 91.6655, loss: 4.5481
	step 6845:lm_loss: 4.5184, ppl: 91.6885, loss: 4.5482
	step 6846:lm_loss: 4.5184, ppl: 91.6929, loss: 4.5483
	step 6847:lm_loss: 4.5185, ppl: 91.7008, loss: 4.5484
	step 6848:lm_loss: 4.5183, ppl: 91.6841, loss: 4.5482
	step 6849:lm_loss: 4.5185, ppl: 91.6941, loss: 4.5483
	step 6850:lm_loss: 4.5186, ppl: 91.7072, loss: 4.5485
	step 6851:lm_loss: 4.5187, ppl: 91.7135, loss: 4.5485
	step 6852:lm_loss: 4.5188, ppl: 91.7233, loss: 4.5486
	step 6853:lm_loss: 4.5188, ppl: 91.7212, loss: 4.5486
	step 6854:lm_loss: 4.5188, ppl: 91.7231, loss: 4.5486
	step 6855:lm_loss: 4.5187, ppl: 91.7195, loss: 4.5486
	step 6856:lm_loss: 4.5187, ppl: 91.7164, loss: 4.5485
	step 6857:lm_loss: 4.5187, ppl: 91.7163, loss: 4.5485
	step 6858:lm_loss: 4.5187, ppl: 91.7182, loss: 4.5485
	step 6859:lm_loss: 4.5188, ppl: 91.7220, loss: 4.5486
	step 6860:lm_loss: 4.5190, ppl: 91.7458, loss: 4.5488
	step 6861:lm_loss: 4.5192, ppl: 91.7596, loss: 4.5489
	step 6862:lm_loss: 4.5192, ppl: 91.7621, loss: 4.5489
	step 6863:lm_loss: 4.5191, ppl: 91.7536, loss: 4.5488
	step 6864:lm_loss: 4.5191, ppl: 91.7555, loss: 4.5488
	step 6865:lm_loss: 4.5190, ppl: 91.7453, loss: 4.5487
	step 6866:lm_loss: 4.5189, ppl: 91.7389, loss: 4.5487
	step 6867:lm_loss: 4.5191, ppl: 91.7484, loss: 4.5488
	step 6868:lm_loss: 4.5191, ppl: 91.7497, loss: 4.5488
	step 6869:lm_loss: 4.5192, ppl: 91.7629, loss: 4.5488
	step 6870:lm_loss: 4.5192, ppl: 91.7662, loss: 4.5489
	step 6871:lm_loss: 4.5192, ppl: 91.7584, loss: 4.5488
	step 6872:lm_loss: 4.5192, ppl: 91.7586, loss: 4.5488
	step 6873:lm_loss: 4.5194, ppl: 91.7795, loss: 4.5489
	step 6874:lm_loss: 4.5195, ppl: 91.7894, loss: 4.5491
	step 6875:lm_loss: 4.5196, ppl: 91.7992, loss: 4.5492
	step 6876:lm_loss: 4.5197, ppl: 91.8060, loss: 4.5493
	step 6877:lm_loss: 4.5197, ppl: 91.8080, loss: 4.5493
	step 6878:lm_loss: 4.5197, ppl: 91.8059, loss: 4.5493
	step 6879:lm_loss: 4.5196, ppl: 91.8028, loss: 4.5492
	step 6880:lm_loss: 4.5196, ppl: 91.7997, loss: 4.5492
	step 6881:lm_loss: 4.5196, ppl: 91.8025, loss: 4.5492
	step 6882:lm_loss: 4.5197, ppl: 91.8058, loss: 4.5493
	step 6883:lm_loss: 4.5197, ppl: 91.8064, loss: 4.5493
	step 6884:lm_loss: 4.5197, ppl: 91.8075, loss: 4.5493
	step 6885:lm_loss: 4.5197, ppl: 91.8045, loss: 4.5493
	step 6886:lm_loss: 4.5197, ppl: 91.8093, loss: 4.5493
	step 6887:lm_loss: 4.5197, ppl: 91.8102, loss: 4.5493
	step 6888:lm_loss: 4.5197, ppl: 91.8067, loss: 4.5493
	step 6889:lm_loss: 4.5197, ppl: 91.8071, loss: 4.5493
	step 6890:lm_loss: 4.5197, ppl: 91.8105, loss: 4.5493
	step 6891:lm_loss: 4.5196, ppl: 91.7967, loss: 4.5491
	step 6892:lm_loss: 4.5194, ppl: 91.7762, loss: 4.5488
	step 6893:lm_loss: 4.5194, ppl: 91.7831, loss: 4.5489
	step 6894:lm_loss: 4.5195, ppl: 91.7908, loss: 4.5490
	step 6895:lm_loss: 4.5196, ppl: 91.8025, loss: 4.5491
	step 6896:lm_loss: 4.5194, ppl: 91.7803, loss: 4.5489
	step 6897:lm_loss: 4.5194, ppl: 91.7829, loss: 4.5489
	step 6898:lm_loss: 4.5194, ppl: 91.7773, loss: 4.5488
	step 6899:lm_loss: 4.5194, ppl: 91.7833, loss: 4.5489
	step 6900:lm_loss: 4.5195, ppl: 91.7888, loss: 4.5490
	step 6901:lm_loss: 4.5195, ppl: 91.7863, loss: 4.5490
	step 6902:lm_loss: 4.5192, ppl: 91.7610, loss: 4.5488
	step 6903:lm_loss: 4.5192, ppl: 91.7656, loss: 4.5488
	step 6904:lm_loss: 4.5193, ppl: 91.7727, loss: 4.5489
	step 6905:lm_loss: 4.5193, ppl: 91.7746, loss: 4.5490
	step 6906:lm_loss: 4.5194, ppl: 91.7766, loss: 4.5490
	step 6907:lm_loss: 4.5194, ppl: 91.7801, loss: 4.5490
	step 6908:lm_loss: 4.5195, ppl: 91.7852, loss: 4.5490
	step 6909:lm_loss: 4.5193, ppl: 91.7738, loss: 4.5489
	step 6910:lm_loss: 4.5194, ppl: 91.7772, loss: 4.5490
	step 6911:lm_loss: 4.5190, ppl: 91.7480, loss: 4.5488
	step 6912:lm_loss: 4.5191, ppl: 91.7504, loss: 4.5488
	step 6913:lm_loss: 4.5190, ppl: 91.7475, loss: 4.5488
	step 6914:lm_loss: 4.5190, ppl: 91.7444, loss: 4.5487
	step 6915:lm_loss: 4.5189, ppl: 91.7342, loss: 4.5486
	step 6916:lm_loss: 4.5188, ppl: 91.7263, loss: 4.5486
	step 6917:lm_loss: 4.5189, ppl: 91.7343, loss: 4.5486
	step 6918:lm_loss: 4.5189, ppl: 91.7361, loss: 4.5486
	step 6919:lm_loss: 4.5189, ppl: 91.7308, loss: 4.5486
	step 6920:lm_loss: 4.5190, ppl: 91.7443, loss: 4.5487
	step 6921:lm_loss: 4.5191, ppl: 91.7516, loss: 4.5488
	step 6922:lm_loss: 4.5190, ppl: 91.7463, loss: 4.5487
	step 6923:lm_loss: 4.5191, ppl: 91.7484, loss: 4.5487
	step 6924:lm_loss: 4.5191, ppl: 91.7551, loss: 4.5488
	step 6925:lm_loss: 4.5191, ppl: 91.7507, loss: 4.5487
	step 6926:lm_loss: 4.5193, ppl: 91.7705, loss: 4.5489
	step 6927:lm_loss: 4.5194, ppl: 91.7768, loss: 4.5489
	step 6928:lm_loss: 4.5193, ppl: 91.7758, loss: 4.5489
	step 6929:lm_loss: 4.5194, ppl: 91.7834, loss: 4.5490
	step 6930:lm_loss: 4.5195, ppl: 91.7929, loss: 4.5491
	step 6931:lm_loss: 4.5195, ppl: 91.7859, loss: 4.5490
	step 6932:lm_loss: 4.5194, ppl: 91.7808, loss: 4.5490
	step 6933:lm_loss: 4.5194, ppl: 91.7847, loss: 4.5490
	step 6934:lm_loss: 4.5195, ppl: 91.7854, loss: 4.5490
	step 6935:lm_loss: 4.5196, ppl: 91.7991, loss: 4.5492
	step 6936:lm_loss: 4.5196, ppl: 91.7957, loss: 4.5491
	step 6937:lm_loss: 4.5196, ppl: 91.8019, loss: 4.5492
	step 6938:lm_loss: 4.5196, ppl: 91.7957, loss: 4.5491
	step 6939:lm_loss: 4.5197, ppl: 91.8069, loss: 4.5492
	step 6940:lm_loss: 4.5194, ppl: 91.7848, loss: 4.5492
	step 6941:lm_loss: 4.5196, ppl: 91.7964, loss: 4.5493
	step 6942:lm_loss: 4.5196, ppl: 91.7945, loss: 4.5493
	step 6943:lm_loss: 4.5196, ppl: 91.7972, loss: 4.5493
	step 6944:lm_loss: 4.5196, ppl: 91.8025, loss: 4.5494
	step 6945:lm_loss: 4.5195, ppl: 91.7935, loss: 4.5493
	step 6946:lm_loss: 4.5193, ppl: 91.7719, loss: 4.5492
	step 6947:lm_loss: 4.5193, ppl: 91.7695, loss: 4.5491
	step 6948:lm_loss: 4.5190, ppl: 91.7415, loss: 4.5489
	step 6949:lm_loss: 4.5188, ppl: 91.7282, loss: 4.5488
	step 6950:lm_loss: 4.5187, ppl: 91.7191, loss: 4.5487
	step 6951:lm_loss: 4.5188, ppl: 91.7289, loss: 4.5488
	step 6952:lm_loss: 4.5189, ppl: 91.7324, loss: 4.5488
	step 6953:lm_loss: 4.5189, ppl: 91.7373, loss: 4.5489
	step 6954:lm_loss: 4.5189, ppl: 91.7314, loss: 4.5488
	step 6955:lm_loss: 4.5188, ppl: 91.7226, loss: 4.5487
	step 6956:lm_loss: 4.5186, ppl: 91.7037, loss: 4.5485
	step 6957:lm_loss: 4.5186, ppl: 91.7037, loss: 4.5485
	step 6958:lm_loss: 4.5185, ppl: 91.6995, loss: 4.5484
	step 6959:lm_loss: 4.5185, ppl: 91.6958, loss: 4.5484
	step 6960:lm_loss: 4.5184, ppl: 91.6916, loss: 4.5483
	step 6961:lm_loss: 4.5186, ppl: 91.7052, loss: 4.5484
	step 6962:lm_loss: 4.5186, ppl: 91.7084, loss: 4.5485
	step 6963:lm_loss: 4.5187, ppl: 91.7180, loss: 4.5486
	step 6964:lm_loss: 4.5187, ppl: 91.7157, loss: 4.5486
	step 6965:lm_loss: 4.5186, ppl: 91.7073, loss: 4.5485
	step 6966:lm_loss: 4.5187, ppl: 91.7126, loss: 4.5486
	step 6967:lm_loss: 4.5184, ppl: 91.6903, loss: 4.5484
	step 6968:lm_loss: 4.5183, ppl: 91.6820, loss: 4.5483
	step 6969:lm_loss: 4.5183, ppl: 91.6812, loss: 4.5483
	step 6970:lm_loss: 4.5183, ppl: 91.6800, loss: 4.5483
	step 6971:lm_loss: 4.5182, ppl: 91.6696, loss: 4.5482
	step 6972:lm_loss: 4.5182, ppl: 91.6710, loss: 4.5482
	step 6973:lm_loss: 4.5181, ppl: 91.6635, loss: 4.5481
	step 6974:lm_loss: 4.5177, ppl: 91.6269, loss: 4.5479
	step 6975:lm_loss: 4.5178, ppl: 91.6299, loss: 4.5479
	step 6976:lm_loss: 4.5178, ppl: 91.6375, loss: 4.5480
	step 6977:lm_loss: 4.5179, ppl: 91.6407, loss: 4.5480
	step 6978:lm_loss: 4.5179, ppl: 91.6387, loss: 4.5480
	step 6979:lm_loss: 4.5179, ppl: 91.6404, loss: 4.5480
	step 6980:lm_loss: 4.5179, ppl: 91.6394, loss: 4.5480
	step 6981:lm_loss: 4.5178, ppl: 91.6357, loss: 4.5480
	step 6982:lm_loss: 4.5178, ppl: 91.6335, loss: 4.5479
	step 6983:lm_loss: 4.5178, ppl: 91.6349, loss: 4.5480
	step 6984:lm_loss: 4.5178, ppl: 91.6331, loss: 4.5479
	step 6985:lm_loss: 4.5178, ppl: 91.6329, loss: 4.5479
	step 6986:lm_loss: 4.5178, ppl: 91.6344, loss: 4.5479
	step 6987:lm_loss: 4.5178, ppl: 91.6331, loss: 4.5479
	step 6988:lm_loss: 4.5177, ppl: 91.6231, loss: 4.5478
	step 6989:lm_loss: 4.5176, ppl: 91.6164, loss: 4.5477
	step 6990:lm_loss: 4.5176, ppl: 91.6198, loss: 4.5478
	step 6991:lm_loss: 4.5176, ppl: 91.6151, loss: 4.5477
	step 6992:lm_loss: 4.5178, ppl: 91.6361, loss: 4.5480
	step 6993:lm_loss: 4.5179, ppl: 91.6425, loss: 4.5481
	step 6994:lm_loss: 4.5179, ppl: 91.6454, loss: 4.5481
	step 6995:lm_loss: 4.5179, ppl: 91.6406, loss: 4.5480
	step 6996:lm_loss: 4.5179, ppl: 91.6391, loss: 4.5480
	step 6997:lm_loss: 4.5178, ppl: 91.6382, loss: 4.5480
	step 6998:lm_loss: 4.5177, ppl: 91.6258, loss: 4.5479
	step 6999:lm_loss: 4.5178, ppl: 91.6306, loss: 4.5480
	step 7000:lm_loss: 4.5178, ppl: 91.6372, loss: 4.5480
	step 7001:lm_loss: 4.5179, ppl: 91.6399, loss: 4.5481
	step 7002:lm_loss: 4.5178, ppl: 91.6343, loss: 4.5480
	step 7003:lm_loss: 4.5177, ppl: 91.6209, loss: 4.5478
	step 7004:lm_loss: 4.5176, ppl: 91.6141, loss: 4.5478
	step 7005:lm_loss: 4.5176, ppl: 91.6162, loss: 4.5478
	step 7006:lm_loss: 4.5177, ppl: 91.6253, loss: 4.5479
	step 7007:lm_loss: 4.5176, ppl: 91.6200, loss: 4.5478
	step 7008:lm_loss: 4.5177, ppl: 91.6267, loss: 4.5479
	step 7009:lm_loss: 4.5177, ppl: 91.6285, loss: 4.5479
	step 7010:lm_loss: 4.5178, ppl: 91.6295, loss: 4.5479
	step 7011:lm_loss: 4.5178, ppl: 91.6319, loss: 4.5480
	step 7012:lm_loss: 4.5180, ppl: 91.6508, loss: 4.5481
	step 7013:lm_loss: 4.5179, ppl: 91.6441, loss: 4.5480
	step 7014:lm_loss: 4.5180, ppl: 91.6503, loss: 4.5481
	step 7015:lm_loss: 4.5181, ppl: 91.6579, loss: 4.5481
	step 7016:lm_loss: 4.5180, ppl: 91.6540, loss: 4.5481
	step 7017:lm_loss: 4.5180, ppl: 91.6526, loss: 4.5481
	step 7018:lm_loss: 4.5179, ppl: 91.6401, loss: 4.5479
	step 7019:lm_loss: 4.5180, ppl: 91.6488, loss: 4.5481
	step 7020:lm_loss: 4.5180, ppl: 91.6501, loss: 4.5481
	step 7021:lm_loss: 4.5179, ppl: 91.6448, loss: 4.5480
	step 7022:lm_loss: 4.5176, ppl: 91.6191, loss: 4.5479
	step 7023:lm_loss: 4.5177, ppl: 91.6215, loss: 4.5479
	step 7024:lm_loss: 4.5177, ppl: 91.6251, loss: 4.5479
	step 7025:lm_loss: 4.5177, ppl: 91.6261, loss: 4.5479
	step 7026:lm_loss: 4.5178, ppl: 91.6305, loss: 4.5480
	step 7027:lm_loss: 4.5178, ppl: 91.6325, loss: 4.5480
	step 7028:lm_loss: 4.5178, ppl: 91.6365, loss: 4.5481
	step 7029:lm_loss: 4.5179, ppl: 91.6389, loss: 4.5481
	step 7030:lm_loss: 4.5179, ppl: 91.6410, loss: 4.5481
	step 7031:lm_loss: 4.5179, ppl: 91.6407, loss: 4.5481
	step 7032:lm_loss: 4.5176, ppl: 91.6154, loss: 4.5479
	step 7033:lm_loss: 4.5178, ppl: 91.6296, loss: 4.5481
	step 7034:lm_loss: 4.5176, ppl: 91.6161, loss: 4.5480
	step 7035:lm_loss: 4.5175, ppl: 91.6026, loss: 4.5478
	step 7036:lm_loss: 4.5175, ppl: 91.6019, loss: 4.5478
	step 7037:lm_loss: 4.5175, ppl: 91.6099, loss: 4.5479
	step 7038:lm_loss: 4.5175, ppl: 91.6082, loss: 4.5479
	step 7039:lm_loss: 4.5176, ppl: 91.6167, loss: 4.5480
	step 7040:lm_loss: 4.5176, ppl: 91.6161, loss: 4.5480
	step 7041:lm_loss: 4.5178, ppl: 91.6296, loss: 4.5481
	step 7042:lm_loss: 4.5178, ppl: 91.6367, loss: 4.5482
	step 7043:lm_loss: 4.5180, ppl: 91.6491, loss: 4.5484
	step 7044:lm_loss: 4.5181, ppl: 91.6585, loss: 4.5484
	step 7045:lm_loss: 4.5181, ppl: 91.6659, loss: 4.5485
	step 7046:lm_loss: 4.5182, ppl: 91.6711, loss: 4.5486
	step 7047:lm_loss: 4.5182, ppl: 91.6745, loss: 4.5486
	step 7048:lm_loss: 4.5181, ppl: 91.6656, loss: 4.5485
	step 7049:lm_loss: 4.5182, ppl: 91.6692, loss: 4.5486
	step 7050:lm_loss: 4.5180, ppl: 91.6557, loss: 4.5484
	step 7051:lm_loss: 4.5181, ppl: 91.6587, loss: 4.5485
	step 7052:lm_loss: 4.5178, ppl: 91.6363, loss: 4.5483
	step 7053:lm_loss: 4.5179, ppl: 91.6402, loss: 4.5484
	step 7054:lm_loss: 4.5180, ppl: 91.6511, loss: 4.5485
	step 7055:lm_loss: 4.5181, ppl: 91.6583, loss: 4.5486
	step 7056:lm_loss: 4.5182, ppl: 91.6675, loss: 4.5487
	step 7057:lm_loss: 4.5182, ppl: 91.6659, loss: 4.5487
	step 7058:lm_loss: 4.5181, ppl: 91.6631, loss: 4.5486
	step 7059:lm_loss: 4.5181, ppl: 91.6615, loss: 4.5486
	step 7060:lm_loss: 4.5182, ppl: 91.6661, loss: 4.5487
	step 7061:lm_loss: 4.5183, ppl: 91.6823, loss: 4.5488
	step 7062:lm_loss: 4.5183, ppl: 91.6815, loss: 4.5488
	step 7063:lm_loss: 4.5183, ppl: 91.6770, loss: 4.5487
	step 7064:lm_loss: 4.5182, ppl: 91.6668, loss: 4.5486
	step 7065:lm_loss: 4.5182, ppl: 91.6700, loss: 4.5486
	step 7066:lm_loss: 4.5182, ppl: 91.6696, loss: 4.5486
	step 7067:lm_loss: 4.5182, ppl: 91.6714, loss: 4.5486
	step 7068:lm_loss: 4.5183, ppl: 91.6774, loss: 4.5487
	step 7069:lm_loss: 4.5183, ppl: 91.6764, loss: 4.5487
	step 7070:lm_loss: 4.5183, ppl: 91.6763, loss: 4.5487
	step 7071:lm_loss: 4.5184, ppl: 91.6924, loss: 4.5488
	step 7072:lm_loss: 4.5186, ppl: 91.7097, loss: 4.5490
	step 7073:lm_loss: 4.5185, ppl: 91.6967, loss: 4.5488
	step 7074:lm_loss: 4.5185, ppl: 91.6997, loss: 4.5489
	step 7075:lm_loss: 4.5183, ppl: 91.6794, loss: 4.5487
	step 7076:lm_loss: 4.5182, ppl: 91.6686, loss: 4.5486
	step 7077:lm_loss: 4.5181, ppl: 91.6619, loss: 4.5485
	step 7078:lm_loss: 4.5181, ppl: 91.6612, loss: 4.5485
	step 7079:lm_loss: 4.5180, ppl: 91.6530, loss: 4.5484
	step 7080:lm_loss: 4.5181, ppl: 91.6635, loss: 4.5485
	step 7081:lm_loss: 4.5181, ppl: 91.6633, loss: 4.5485
	step 7082:lm_loss: 4.5181, ppl: 91.6636, loss: 4.5485
	step 7083:lm_loss: 4.5182, ppl: 91.6666, loss: 4.5485
	step 7084:lm_loss: 4.5181, ppl: 91.6629, loss: 4.5485
	step 7085:lm_loss: 4.5180, ppl: 91.6540, loss: 4.5483
	step 7086:lm_loss: 4.5179, ppl: 91.6426, loss: 4.5482
	step 7087:lm_loss: 4.5179, ppl: 91.6431, loss: 4.5482
	step 7088:lm_loss: 4.5179, ppl: 91.6433, loss: 4.5482
	step 7089:lm_loss: 4.5179, ppl: 91.6394, loss: 4.5481
	step 7090:lm_loss: 4.5178, ppl: 91.6339, loss: 4.5481
	step 7091:lm_loss: 4.5178, ppl: 91.6373, loss: 4.5481
	step 7092:lm_loss: 4.5179, ppl: 91.6427, loss: 4.5481
	step 7093:lm_loss: 4.5179, ppl: 91.6444, loss: 4.5482
	step 7094:lm_loss: 4.5178, ppl: 91.6359, loss: 4.5481
	step 7095:lm_loss: 4.5178, ppl: 91.6377, loss: 4.5481
	step 7096:lm_loss: 4.5178, ppl: 91.6330, loss: 4.5480
	step 7097:lm_loss: 4.5177, ppl: 91.6280, loss: 4.5480
	step 7098:lm_loss: 4.5178, ppl: 91.6312, loss: 4.5480
	step 7099:lm_loss: 4.5178, ppl: 91.6328, loss: 4.5480
	step 7100:lm_loss: 4.5178, ppl: 91.6336, loss: 4.5480
	step 7101:lm_loss: 4.5178, ppl: 91.6306, loss: 4.5480
	step 7102:lm_loss: 4.5179, ppl: 91.6399, loss: 4.5480
	step 7103:lm_loss: 4.5180, ppl: 91.6483, loss: 4.5481
	step 7104:lm_loss: 4.5179, ppl: 91.6459, loss: 4.5481
	step 7105:lm_loss: 4.5181, ppl: 91.6582, loss: 4.5482
	step 7106:lm_loss: 4.5181, ppl: 91.6604, loss: 4.5482
	step 7107:lm_loss: 4.5181, ppl: 91.6607, loss: 4.5482
	step 7108:lm_loss: 4.5181, ppl: 91.6622, loss: 4.5482
	step 7109:lm_loss: 4.5181, ppl: 91.6639, loss: 4.5483
	step 7110:lm_loss: 4.5182, ppl: 91.6679, loss: 4.5483
	step 7111:lm_loss: 4.5182, ppl: 91.6660, loss: 4.5483
	step 7112:lm_loss: 4.5182, ppl: 91.6675, loss: 4.5483
	step 7113:lm_loss: 4.5182, ppl: 91.6710, loss: 4.5483
	step 7114:lm_loss: 4.5182, ppl: 91.6738, loss: 4.5483
	step 7115:lm_loss: 4.5183, ppl: 91.6822, loss: 4.5485
	step 7116:lm_loss: 4.5183, ppl: 91.6827, loss: 4.5485
	step 7117:lm_loss: 4.5183, ppl: 91.6825, loss: 4.5485
	step 7118:lm_loss: 4.5184, ppl: 91.6846, loss: 4.5485
	step 7119:lm_loss: 4.5183, ppl: 91.6798, loss: 4.5484
	step 7120:lm_loss: 4.5183, ppl: 91.6782, loss: 4.5484
	step 7121:lm_loss: 4.5184, ppl: 91.6847, loss: 4.5484
	step 7122:lm_loss: 4.5183, ppl: 91.6839, loss: 4.5484
	step 7123:lm_loss: 4.5184, ppl: 91.6918, loss: 4.5485
	step 7124:lm_loss: 4.5182, ppl: 91.6727, loss: 4.5483
	step 7125:lm_loss: 4.5183, ppl: 91.6832, loss: 4.5484
	step 7126:lm_loss: 4.5183, ppl: 91.6808, loss: 4.5484
	step 7127:lm_loss: 4.5184, ppl: 91.6912, loss: 4.5486
	step 7128:lm_loss: 4.5183, ppl: 91.6800, loss: 4.5484
	step 7129:lm_loss: 4.5182, ppl: 91.6679, loss: 4.5483
	step 7130:lm_loss: 4.5181, ppl: 91.6619, loss: 4.5483
	step 7131:lm_loss: 4.5183, ppl: 91.6803, loss: 4.5484
	step 7132:lm_loss: 4.5182, ppl: 91.6668, loss: 4.5482
	step 7133:lm_loss: 4.5181, ppl: 91.6653, loss: 4.5482
	step 7134:lm_loss: 4.5181, ppl: 91.6602, loss: 4.5481
	step 7135:lm_loss: 4.5183, ppl: 91.6755, loss: 4.5483
	step 7136:lm_loss: 4.5183, ppl: 91.6825, loss: 4.5484
	step 7137:lm_loss: 4.5183, ppl: 91.6758, loss: 4.5483
	step 7138:lm_loss: 4.5183, ppl: 91.6794, loss: 4.5483
	step 7139:lm_loss: 4.5183, ppl: 91.6800, loss: 4.5483
	step 7140:lm_loss: 4.5184, ppl: 91.6924, loss: 4.5484
	step 7141:lm_loss: 4.5185, ppl: 91.7021, loss: 4.5485
	step 7142:lm_loss: 4.5187, ppl: 91.7164, loss: 4.5487
	step 7143:lm_loss: 4.5188, ppl: 91.7219, loss: 4.5487
	step 7144:lm_loss: 4.5188, ppl: 91.7259, loss: 4.5488
	step 7145:lm_loss: 4.5188, ppl: 91.7249, loss: 4.5488
	step 7146:lm_loss: 4.5187, ppl: 91.7165, loss: 4.5487
	step 7147:lm_loss: 4.5187, ppl: 91.7199, loss: 4.5487
	step 7148:lm_loss: 4.5188, ppl: 91.7230, loss: 4.5488
	step 7149:lm_loss: 4.5185, ppl: 91.7003, loss: 4.5486
	step 7150:lm_loss: 4.5186, ppl: 91.7065, loss: 4.5487
	step 7151:lm_loss: 4.5185, ppl: 91.6978, loss: 4.5486
	step 7152:lm_loss: 4.5185, ppl: 91.7010, loss: 4.5487
	step 7153:lm_loss: 4.5185, ppl: 91.7005, loss: 4.5487
	step 7154:lm_loss: 4.5186, ppl: 91.7052, loss: 4.5487
	step 7155:lm_loss: 4.5185, ppl: 91.6981, loss: 4.5487
	step 7156:lm_loss: 4.5186, ppl: 91.7031, loss: 4.5487
	step 7157:lm_loss: 4.5184, ppl: 91.6889, loss: 4.5485
	step 7158:lm_loss: 4.5184, ppl: 91.6858, loss: 4.5485
	step 7159:lm_loss: 4.5183, ppl: 91.6824, loss: 4.5485
	step 7160:lm_loss: 4.5183, ppl: 91.6808, loss: 4.5484
	step 7161:lm_loss: 4.5183, ppl: 91.6763, loss: 4.5484
	step 7162:lm_loss: 4.5183, ppl: 91.6818, loss: 4.5484
	step 7163:lm_loss: 4.5181, ppl: 91.6651, loss: 4.5483
	step 7164:lm_loss: 4.5181, ppl: 91.6610, loss: 4.5483
	step 7165:lm_loss: 4.5181, ppl: 91.6594, loss: 4.5482
	step 7166:lm_loss: 4.5181, ppl: 91.6585, loss: 4.5482
	step 7167:lm_loss: 4.5181, ppl: 91.6596, loss: 4.5482
	step 7168:lm_loss: 4.5181, ppl: 91.6587, loss: 4.5482
	step 7169:lm_loss: 4.5180, ppl: 91.6499, loss: 4.5482
	step 7170:lm_loss: 4.5180, ppl: 91.6509, loss: 4.5482
	step 7171:lm_loss: 4.5178, ppl: 91.6379, loss: 4.5480
	step 7172:lm_loss: 4.5178, ppl: 91.6366, loss: 4.5480
	step 7173:lm_loss: 4.5177, ppl: 91.6206, loss: 4.5479
	step 7174:lm_loss: 4.5176, ppl: 91.6195, loss: 4.5478
	step 7175:lm_loss: 4.5177, ppl: 91.6247, loss: 4.5479
	step 7176:lm_loss: 4.5177, ppl: 91.6250, loss: 4.5479
	step 7177:lm_loss: 4.5179, ppl: 91.6423, loss: 4.5481
	step 7178:lm_loss: 4.5180, ppl: 91.6508, loss: 4.5482
	step 7179:lm_loss: 4.5180, ppl: 91.6507, loss: 4.5482
	step 7180:lm_loss: 4.5180, ppl: 91.6485, loss: 4.5482
	step 7181:lm_loss: 4.5180, ppl: 91.6526, loss: 4.5482
	step 7182:lm_loss: 4.5181, ppl: 91.6568, loss: 4.5483
	step 7183:lm_loss: 4.5181, ppl: 91.6636, loss: 4.5483
	step 7184:lm_loss: 4.5182, ppl: 91.6688, loss: 4.5484
	step 7185:lm_loss: 4.5182, ppl: 91.6694, loss: 4.5484
	step 7186:lm_loss: 4.5183, ppl: 91.6815, loss: 4.5485
	step 7187:lm_loss: 4.5183, ppl: 91.6762, loss: 4.5484
	step 7188:lm_loss: 4.5183, ppl: 91.6756, loss: 4.5484
	step 7189:lm_loss: 4.5183, ppl: 91.6758, loss: 4.5484
	step 7190:lm_loss: 4.5183, ppl: 91.6751, loss: 4.5484
	step 7191:lm_loss: 4.5183, ppl: 91.6813, loss: 4.5485
	step 7192:lm_loss: 4.5183, ppl: 91.6829, loss: 4.5485
	step 7193:lm_loss: 4.5183, ppl: 91.6836, loss: 4.5485
	step 7194:lm_loss: 4.5183, ppl: 91.6821, loss: 4.5485
	step 7195:lm_loss: 4.5183, ppl: 91.6790, loss: 4.5485
	step 7196:lm_loss: 4.5182, ppl: 91.6729, loss: 4.5484
	step 7197:lm_loss: 4.5183, ppl: 91.6757, loss: 4.5484
	step 7198:lm_loss: 4.5183, ppl: 91.6762, loss: 4.5484
	step 7199:lm_loss: 4.5182, ppl: 91.6738, loss: 4.5484
	step 7200:lm_loss: 4.5184, ppl: 91.6849, loss: 4.5485
	step 7201:lm_loss: 4.5183, ppl: 91.6807, loss: 4.5484
	step 7202:lm_loss: 4.5183, ppl: 91.6784, loss: 4.5484
	step 7203:lm_loss: 4.5183, ppl: 91.6762, loss: 4.5484
	step 7204:lm_loss: 4.5181, ppl: 91.6594, loss: 4.5481
	step 7205:lm_loss: 4.5183, ppl: 91.6839, loss: 4.5484
	step 7206:lm_loss: 4.5184, ppl: 91.6888, loss: 4.5484
	step 7207:lm_loss: 4.5181, ppl: 91.6616, loss: 4.5483
	step 7208:lm_loss: 4.5182, ppl: 91.6714, loss: 4.5484
	step 7209:lm_loss: 4.5181, ppl: 91.6644, loss: 4.5483
	step 7210:lm_loss: 4.5182, ppl: 91.6698, loss: 4.5483
	step 7211:lm_loss: 4.5181, ppl: 91.6641, loss: 4.5482
	step 7212:lm_loss: 4.5181, ppl: 91.6609, loss: 4.5482
	step 7213:lm_loss: 4.5178, ppl: 91.6320, loss: 4.5481
	step 7214:lm_loss: 4.5179, ppl: 91.6386, loss: 4.5481
	step 7215:lm_loss: 4.5179, ppl: 91.6436, loss: 4.5482
	step 7216:lm_loss: 4.5179, ppl: 91.6460, loss: 4.5482
	step 7217:lm_loss: 4.5179, ppl: 91.6414, loss: 4.5481
	step 7218:lm_loss: 4.5179, ppl: 91.6414, loss: 4.5481
	step 7219:lm_loss: 4.5179, ppl: 91.6451, loss: 4.5482
	step 7220:lm_loss: 4.5179, ppl: 91.6458, loss: 4.5482
	step 7221:lm_loss: 4.5180, ppl: 91.6546, loss: 4.5483
	step 7222:lm_loss: 4.5181, ppl: 91.6640, loss: 4.5484
	step 7223:lm_loss: 4.5180, ppl: 91.6480, loss: 4.5483
	step 7224:lm_loss: 4.5180, ppl: 91.6537, loss: 4.5483
	step 7225:lm_loss: 4.5180, ppl: 91.6526, loss: 4.5483
	step 7226:lm_loss: 4.5179, ppl: 91.6407, loss: 4.5482
	step 7227:lm_loss: 4.5179, ppl: 91.6422, loss: 4.5482
	step 7228:lm_loss: 4.5179, ppl: 91.6443, loss: 4.5482
	step 7229:lm_loss: 4.5180, ppl: 91.6546, loss: 4.5483
	step 7230:lm_loss: 4.5181, ppl: 91.6589, loss: 4.5484
	step 7231:lm_loss: 4.5181, ppl: 91.6572, loss: 4.5483
	step 7232:lm_loss: 4.5181, ppl: 91.6634, loss: 4.5484
	step 7233:lm_loss: 4.5180, ppl: 91.6551, loss: 4.5483
	step 7234:lm_loss: 4.5180, ppl: 91.6553, loss: 4.5483
	step 7235:lm_loss: 4.5180, ppl: 91.6551, loss: 4.5483
	step 7236:lm_loss: 4.5183, ppl: 91.6762, loss: 4.5485
	step 7237:lm_loss: 4.5181, ppl: 91.6607, loss: 4.5484
	step 7238:lm_loss: 4.5182, ppl: 91.6702, loss: 4.5484
	step 7239:lm_loss: 4.5183, ppl: 91.6810, loss: 4.5485
	step 7240:lm_loss: 4.5185, ppl: 91.6969, loss: 4.5487
	step 7241:lm_loss: 4.5183, ppl: 91.6822, loss: 4.5486
	step 7242:lm_loss: 4.5183, ppl: 91.6838, loss: 4.5486
	step 7243:lm_loss: 4.5185, ppl: 91.6937, loss: 4.5487
	step 7244:lm_loss: 4.5185, ppl: 91.6949, loss: 4.5487
	step 7245:lm_loss: 4.5185, ppl: 91.7019, loss: 4.5488
	step 7246:lm_loss: 4.5182, ppl: 91.6704, loss: 4.5486
	step 7247:lm_loss: 4.5183, ppl: 91.6812, loss: 4.5487
	step 7248:lm_loss: 4.5183, ppl: 91.6827, loss: 4.5487
	step 7249:lm_loss: 4.5183, ppl: 91.6795, loss: 4.5486
	step 7250:lm_loss: 4.5183, ppl: 91.6821, loss: 4.5486
	step 7251:lm_loss: 4.5181, ppl: 91.6638, loss: 4.5485
	step 7252:lm_loss: 4.5181, ppl: 91.6641, loss: 4.5485
	step 7253:lm_loss: 4.5180, ppl: 91.6547, loss: 4.5484
	step 7254:lm_loss: 4.5180, ppl: 91.6547, loss: 4.5484
	step 7255:lm_loss: 4.5183, ppl: 91.6763, loss: 4.5485
	step 7256:lm_loss: 4.5183, ppl: 91.6781, loss: 4.5485
	step 7257:lm_loss: 4.5183, ppl: 91.6771, loss: 4.5485
	step 7258:lm_loss: 4.5183, ppl: 91.6769, loss: 4.5485
	step 7259:lm_loss: 4.5184, ppl: 91.6928, loss: 4.5486
	step 7260:lm_loss: 4.5184, ppl: 91.6860, loss: 4.5486
	step 7261:lm_loss: 4.5184, ppl: 91.6862, loss: 4.5486
	step 7262:lm_loss: 4.5184, ppl: 91.6879, loss: 4.5486
	step 7263:lm_loss: 4.5183, ppl: 91.6838, loss: 4.5485
	step 7264:lm_loss: 4.5183, ppl: 91.6775, loss: 4.5484
	step 7265:lm_loss: 4.5183, ppl: 91.6776, loss: 4.5484
	step 7266:lm_loss: 4.5182, ppl: 91.6733, loss: 4.5483
	step 7267:lm_loss: 4.5182, ppl: 91.6714, loss: 4.5483
	step 7268:lm_loss: 4.5182, ppl: 91.6670, loss: 4.5482
	step 7269:lm_loss: 4.5182, ppl: 91.6698, loss: 4.5483
	step 7270:lm_loss: 4.5183, ppl: 91.6798, loss: 4.5484
	step 7271:lm_loss: 4.5183, ppl: 91.6827, loss: 4.5484
	step 7272:lm_loss: 4.5182, ppl: 91.6679, loss: 4.5483
	step 7273:lm_loss: 4.5182, ppl: 91.6663, loss: 4.5483
	step 7274:lm_loss: 4.5183, ppl: 91.6815, loss: 4.5484
	step 7275:lm_loss: 4.5183, ppl: 91.6799, loss: 4.5484
	step 7276:lm_loss: 4.5183, ppl: 91.6759, loss: 4.5483
	step 7277:lm_loss: 4.5182, ppl: 91.6728, loss: 4.5483
	step 7278:lm_loss: 4.5182, ppl: 91.6684, loss: 4.5482
	step 7279:lm_loss: 4.5182, ppl: 91.6677, loss: 4.5482
	step 7280:lm_loss: 4.5184, ppl: 91.6885, loss: 4.5485
	step 7281:lm_loss: 4.5185, ppl: 91.6992, loss: 4.5486
	step 7282:lm_loss: 4.5186, ppl: 91.7056, loss: 4.5486
	step 7283:lm_loss: 4.5187, ppl: 91.7169, loss: 4.5487
	step 7284:lm_loss: 4.5187, ppl: 91.7160, loss: 4.5487
	step 7285:lm_loss: 4.5189, ppl: 91.7318, loss: 4.5489
	step 7286:lm_loss: 4.5188, ppl: 91.7276, loss: 4.5488
	step 7287:lm_loss: 4.5188, ppl: 91.7278, loss: 4.5488
	step 7288:lm_loss: 4.5188, ppl: 91.7263, loss: 4.5488
	step 7289:lm_loss: 4.5188, ppl: 91.7264, loss: 4.5488
	step 7290:lm_loss: 4.5189, ppl: 91.7309, loss: 4.5489
	step 7291:lm_loss: 4.5189, ppl: 91.7320, loss: 4.5489
	step 7292:lm_loss: 4.5187, ppl: 91.7200, loss: 4.5487
	step 7293:lm_loss: 4.5186, ppl: 91.7110, loss: 4.5486
	step 7294:lm_loss: 4.5188, ppl: 91.7231, loss: 4.5488
	step 7295:lm_loss: 4.5186, ppl: 91.7094, loss: 4.5487
	step 7296:lm_loss: 4.5186, ppl: 91.7082, loss: 4.5487
	step 7297:lm_loss: 4.5185, ppl: 91.6942, loss: 4.5485
	step 7298:lm_loss: 4.5185, ppl: 91.6951, loss: 4.5485
	step 7299:lm_loss: 4.5186, ppl: 91.7106, loss: 4.5487
	step 7300:lm_loss: 4.5186, ppl: 91.7072, loss: 4.5486
	step 7301:lm_loss: 4.5187, ppl: 91.7167, loss: 4.5487
	step 7302:lm_loss: 4.5188, ppl: 91.7256, loss: 4.5489
	step 7303:lm_loss: 4.5187, ppl: 91.7192, loss: 4.5487
	step 7304:lm_loss: 4.5187, ppl: 91.7145, loss: 4.5487
	step 7305:lm_loss: 4.5187, ppl: 91.7124, loss: 4.5486
	step 7306:lm_loss: 4.5185, ppl: 91.7017, loss: 4.5486
	step 7307:lm_loss: 4.5187, ppl: 91.7182, loss: 4.5487
	step 7308:lm_loss: 4.5186, ppl: 91.7074, loss: 4.5485
	step 7309:lm_loss: 4.5184, ppl: 91.6904, loss: 4.5483
	step 7310:lm_loss: 4.5184, ppl: 91.6892, loss: 4.5483
	step 7311:lm_loss: 4.5184, ppl: 91.6922, loss: 4.5484
	step 7312:lm_loss: 4.5186, ppl: 91.7050, loss: 4.5485
	step 7313:lm_loss: 4.5185, ppl: 91.6976, loss: 4.5484
	step 7314:lm_loss: 4.5185, ppl: 91.6935, loss: 4.5483
	step 7315:lm_loss: 4.5185, ppl: 91.6999, loss: 4.5483
	step 7316:lm_loss: 4.5185, ppl: 91.6998, loss: 4.5483
	step 7317:lm_loss: 4.5185, ppl: 91.7022, loss: 4.5484
	step 7318:lm_loss: 4.5184, ppl: 91.6843, loss: 4.5482
	step 7319:lm_loss: 4.5185, ppl: 91.6968, loss: 4.5483
	step 7320:lm_loss: 4.5185, ppl: 91.6972, loss: 4.5483
	step 7321:lm_loss: 4.5184, ppl: 91.6894, loss: 4.5482
	step 7322:lm_loss: 4.5184, ppl: 91.6882, loss: 4.5482
	step 7323:lm_loss: 4.5183, ppl: 91.6838, loss: 4.5482
	step 7324:lm_loss: 4.5185, ppl: 91.6958, loss: 4.5483
	step 7325:lm_loss: 4.5185, ppl: 91.6951, loss: 4.5483
	step 7326:lm_loss: 4.5186, ppl: 91.7082, loss: 4.5484
	step 7327:lm_loss: 4.5186, ppl: 91.7050, loss: 4.5484
	step 7328:lm_loss: 4.5185, ppl: 91.6961, loss: 4.5482
	step 7329:lm_loss: 4.5185, ppl: 91.6942, loss: 4.5482
	step 7330:lm_loss: 4.5186, ppl: 91.7031, loss: 4.5483
	step 7331:lm_loss: 4.5185, ppl: 91.7012, loss: 4.5483
	step 7332:lm_loss: 4.5185, ppl: 91.7017, loss: 4.5483
	step 7333:lm_loss: 4.5186, ppl: 91.7038, loss: 4.5483
	step 7334:lm_loss: 4.5185, ppl: 91.6962, loss: 4.5483
	step 7335:lm_loss: 4.5185, ppl: 91.7013, loss: 4.5483
	step 7336:lm_loss: 4.5187, ppl: 91.7184, loss: 4.5484
	step 7337:lm_loss: 4.5188, ppl: 91.7226, loss: 4.5485
	step 7338:lm_loss: 4.5186, ppl: 91.7030, loss: 4.5483
	step 7339:lm_loss: 4.5185, ppl: 91.7023, loss: 4.5483
	step 7340:lm_loss: 4.5185, ppl: 91.6983, loss: 4.5483
	step 7341:lm_loss: 4.5185, ppl: 91.6986, loss: 4.5483
	step 7342:lm_loss: 4.5183, ppl: 91.6833, loss: 4.5480
	step 7343:lm_loss: 4.5183, ppl: 91.6833, loss: 4.5480
	step 7344:lm_loss: 4.5183, ppl: 91.6770, loss: 4.5480
	step 7345:lm_loss: 4.5184, ppl: 91.6847, loss: 4.5480
	step 7346:lm_loss: 4.5183, ppl: 91.6756, loss: 4.5479
	step 7347:lm_loss: 4.5185, ppl: 91.6952, loss: 4.5481
	step 7348:lm_loss: 4.5186, ppl: 91.7115, loss: 4.5483
	step 7349:lm_loss: 4.5186, ppl: 91.7040, loss: 4.5482
	step 7350:lm_loss: 4.5186, ppl: 91.7032, loss: 4.5482
	step 7351:lm_loss: 4.5187, ppl: 91.7149, loss: 4.5483
	step 7352:lm_loss: 4.5186, ppl: 91.7099, loss: 4.5482
	step 7353:lm_loss: 4.5186, ppl: 91.7108, loss: 4.5482
	step 7354:lm_loss: 4.5187, ppl: 91.7175, loss: 4.5484
	step 7355:lm_loss: 4.5188, ppl: 91.7220, loss: 4.5484
	step 7356:lm_loss: 4.5188, ppl: 91.7267, loss: 4.5484
	step 7357:lm_loss: 4.5189, ppl: 91.7310, loss: 4.5485
	step 7358:lm_loss: 4.5188, ppl: 91.7277, loss: 4.5484
	step 7359:lm_loss: 4.5189, ppl: 91.7335, loss: 4.5485
	step 7360:lm_loss: 4.5189, ppl: 91.7317, loss: 4.5485
	step 7361:lm_loss: 4.5189, ppl: 91.7349, loss: 4.5485
	step 7362:lm_loss: 4.5189, ppl: 91.7352, loss: 4.5485
	step 7363:lm_loss: 4.5187, ppl: 91.7199, loss: 4.5484
	step 7364:lm_loss: 4.5188, ppl: 91.7296, loss: 4.5485
	step 7365:lm_loss: 4.5189, ppl: 91.7318, loss: 4.5485
	step 7366:lm_loss: 4.5187, ppl: 91.7197, loss: 4.5484
	step 7367:lm_loss: 4.5188, ppl: 91.7210, loss: 4.5484
	step 7368:lm_loss: 4.5188, ppl: 91.7217, loss: 4.5484
	step 7369:lm_loss: 4.5188, ppl: 91.7299, loss: 4.5484
	step 7370:lm_loss: 4.5188, ppl: 91.7290, loss: 4.5484
	step 7371:lm_loss: 4.5188, ppl: 91.7236, loss: 4.5484
	step 7372:lm_loss: 4.5189, ppl: 91.7317, loss: 4.5484
	step 7373:lm_loss: 4.5188, ppl: 91.7224, loss: 4.5483
	step 7374:lm_loss: 4.5188, ppl: 91.7283, loss: 4.5484
	step 7375:lm_loss: 4.5191, ppl: 91.7498, loss: 4.5486
	step 7376:lm_loss: 4.5191, ppl: 91.7487, loss: 4.5486
	step 7377:lm_loss: 4.5190, ppl: 91.7482, loss: 4.5486
	step 7378:lm_loss: 4.5190, ppl: 91.7464, loss: 4.5486
	step 7379:lm_loss: 4.5190, ppl: 91.7410, loss: 4.5485
	step 7380:lm_loss: 4.5190, ppl: 91.7444, loss: 4.5485
	step 7381:lm_loss: 4.5188, ppl: 91.7209, loss: 4.5483
	step 7382:lm_loss: 4.5189, ppl: 91.7378, loss: 4.5485
	step 7383:lm_loss: 4.5189, ppl: 91.7315, loss: 4.5484
	step 7384:lm_loss: 4.5189, ppl: 91.7328, loss: 4.5484
	step 7385:lm_loss: 4.5189, ppl: 91.7385, loss: 4.5485
	step 7386:lm_loss: 4.5190, ppl: 91.7472, loss: 4.5485
	step 7387:lm_loss: 4.5191, ppl: 91.7508, loss: 4.5486
	step 7388:lm_loss: 4.5192, ppl: 91.7632, loss: 4.5487
	step 7389:lm_loss: 4.5191, ppl: 91.7564, loss: 4.5486
	step 7390:lm_loss: 4.5192, ppl: 91.7620, loss: 4.5487
	step 7391:lm_loss: 4.5191, ppl: 91.7559, loss: 4.5486
	step 7392:lm_loss: 4.5192, ppl: 91.7661, loss: 4.5487
	step 7393:lm_loss: 4.5192, ppl: 91.7619, loss: 4.5486
	step 7394:lm_loss: 4.5193, ppl: 91.7722, loss: 4.5488
	step 7395:lm_loss: 4.5194, ppl: 91.7767, loss: 4.5488
	step 7396:lm_loss: 4.5194, ppl: 91.7822, loss: 4.5489
	step 7397:lm_loss: 4.5194, ppl: 91.7825, loss: 4.5489
	step 7398:lm_loss: 4.5192, ppl: 91.7619, loss: 4.5486
	step 7399:lm_loss: 4.5191, ppl: 91.7534, loss: 4.5485
	step 7400:lm_loss: 4.5191, ppl: 91.7534, loss: 4.5485
	step 7401:lm_loss: 4.5190, ppl: 91.7474, loss: 4.5484
	step 7402:lm_loss: 4.5191, ppl: 91.7528, loss: 4.5485
	step 7403:lm_loss: 4.5191, ppl: 91.7560, loss: 4.5485
	step 7404:lm_loss: 4.5193, ppl: 91.7733, loss: 4.5487
	step 7405:lm_loss: 4.5192, ppl: 91.7618, loss: 4.5486
	step 7406:lm_loss: 4.5193, ppl: 91.7705, loss: 4.5487
	step 7407:lm_loss: 4.5193, ppl: 91.7738, loss: 4.5488
	step 7408:lm_loss: 4.5193, ppl: 91.7698, loss: 4.5487
	step 7409:lm_loss: 4.5193, ppl: 91.7730, loss: 4.5488
	step 7410:lm_loss: 4.5193, ppl: 91.7738, loss: 4.5488
	step 7411:lm_loss: 4.5194, ppl: 91.7795, loss: 4.5488
	step 7412:lm_loss: 4.5195, ppl: 91.7917, loss: 4.5489
	step 7413:lm_loss: 4.5194, ppl: 91.7822, loss: 4.5488
	step 7414:lm_loss: 4.5195, ppl: 91.7941, loss: 4.5489
	step 7415:lm_loss: 4.5196, ppl: 91.7986, loss: 4.5489
	step 7416:lm_loss: 4.5193, ppl: 91.7684, loss: 4.5487
	step 7417:lm_loss: 4.5193, ppl: 91.7719, loss: 4.5487
	step 7418:lm_loss: 4.5192, ppl: 91.7617, loss: 4.5486
	step 7419:lm_loss: 4.5192, ppl: 91.7616, loss: 4.5486
	step 7420:lm_loss: 4.5193, ppl: 91.7738, loss: 4.5487
	step 7421:lm_loss: 4.5194, ppl: 91.7772, loss: 4.5487
	step 7422:lm_loss: 4.5194, ppl: 91.7832, loss: 4.5488
	step 7423:lm_loss: 4.5195, ppl: 91.7908, loss: 4.5488
	step 7424:lm_loss: 4.5196, ppl: 91.7989, loss: 4.5489
	step 7425:lm_loss: 4.5195, ppl: 91.7892, loss: 4.5488
	step 7426:lm_loss: 4.5195, ppl: 91.7907, loss: 4.5488
	step 7427:lm_loss: 4.5194, ppl: 91.7801, loss: 4.5487
	step 7428:lm_loss: 4.5194, ppl: 91.7807, loss: 4.5487
	step 7429:lm_loss: 4.5194, ppl: 91.7834, loss: 4.5487
	step 7430:lm_loss: 4.5195, ppl: 91.7910, loss: 4.5488
	step 7431:lm_loss: 4.5195, ppl: 91.7888, loss: 4.5488
	step 7432:lm_loss: 4.5195, ppl: 91.7884, loss: 4.5488
	step 7433:lm_loss: 4.5194, ppl: 91.7806, loss: 4.5486
	step 7434:lm_loss: 4.5194, ppl: 91.7772, loss: 4.5486
	step 7435:lm_loss: 4.5194, ppl: 91.7833, loss: 4.5486
	step 7436:lm_loss: 4.5195, ppl: 91.7904, loss: 4.5487
	step 7437:lm_loss: 4.5194, ppl: 91.7826, loss: 4.5486
	step 7438:lm_loss: 4.5195, ppl: 91.7889, loss: 4.5487
	step 7439:lm_loss: 4.5195, ppl: 91.7878, loss: 4.5486
	step 7440:lm_loss: 4.5195, ppl: 91.7882, loss: 4.5486
	step 7441:lm_loss: 4.5194, ppl: 91.7813, loss: 4.5486
	step 7442:lm_loss: 4.5191, ppl: 91.7562, loss: 4.5483
	step 7443:lm_loss: 4.5192, ppl: 91.7592, loss: 4.5484
	step 7444:lm_loss: 4.5193, ppl: 91.7753, loss: 4.5485
	step 7445:lm_loss: 4.5194, ppl: 91.7763, loss: 4.5485
	step 7446:lm_loss: 4.5194, ppl: 91.7820, loss: 4.5486
	step 7447:lm_loss: 4.5195, ppl: 91.7934, loss: 4.5487
	step 7448:lm_loss: 4.5194, ppl: 91.7851, loss: 4.5486
	step 7449:lm_loss: 4.5195, ppl: 91.7911, loss: 4.5487
	step 7450:lm_loss: 4.5196, ppl: 91.7992, loss: 4.5488
	step 7451:lm_loss: 4.5197, ppl: 91.8052, loss: 4.5488
	step 7452:lm_loss: 4.5193, ppl: 91.7751, loss: 4.5484
	step 7453:lm_loss: 4.5193, ppl: 91.7731, loss: 4.5484
	step 7454:lm_loss: 4.5194, ppl: 91.7817, loss: 4.5485
	step 7455:lm_loss: 4.5194, ppl: 91.7807, loss: 4.5485
	step 7456:lm_loss: 4.5195, ppl: 91.7879, loss: 4.5486
	step 7457:lm_loss: 4.5194, ppl: 91.7816, loss: 4.5485
	step 7458:lm_loss: 4.5195, ppl: 91.7920, loss: 4.5486
	step 7459:lm_loss: 4.5196, ppl: 91.7978, loss: 4.5487
	step 7460:lm_loss: 4.5195, ppl: 91.7860, loss: 4.5485
	step 7461:lm_loss: 4.5195, ppl: 91.7866, loss: 4.5485
	step 7462:lm_loss: 4.5196, ppl: 91.7980, loss: 4.5486
	step 7463:lm_loss: 4.5198, ppl: 91.8153, loss: 4.5488
	step 7464:lm_loss: 4.5197, ppl: 91.8090, loss: 4.5487
	step 7465:lm_loss: 4.5197, ppl: 91.8123, loss: 4.5487
	step 7466:lm_loss: 4.5196, ppl: 91.7944, loss: 4.5486
	step 7467:lm_loss: 4.5195, ppl: 91.7905, loss: 4.5485
	step 7468:lm_loss: 4.5195, ppl: 91.7927, loss: 4.5485
	step 7469:lm_loss: 4.5194, ppl: 91.7837, loss: 4.5485
	step 7470:lm_loss: 4.5194, ppl: 91.7835, loss: 4.5484
	step 7471:lm_loss: 4.5194, ppl: 91.7835, loss: 4.5484
	step 7472:lm_loss: 4.5194, ppl: 91.7802, loss: 4.5484
	step 7473:lm_loss: 4.5194, ppl: 91.7779, loss: 4.5484
	step 7474:lm_loss: 4.5193, ppl: 91.7758, loss: 4.5483
	step 7475:lm_loss: 4.5193, ppl: 91.7692, loss: 4.5483
	step 7476:lm_loss: 4.5191, ppl: 91.7570, loss: 4.5481
	step 7477:lm_loss: 4.5190, ppl: 91.7410, loss: 4.5479
	step 7478:lm_loss: 4.5188, ppl: 91.7251, loss: 4.5478
	step 7479:lm_loss: 4.5189, ppl: 91.7360, loss: 4.5480
	step 7480:lm_loss: 4.5189, ppl: 91.7355, loss: 4.5479
	step 7481:lm_loss: 4.5188, ppl: 91.7287, loss: 4.5478
	step 7482:lm_loss: 4.5188, ppl: 91.7241, loss: 4.5478
	step 7483:lm_loss: 4.5189, ppl: 91.7371, loss: 4.5479
	step 7484:lm_loss: 4.5189, ppl: 91.7332, loss: 4.5478
	step 7485:lm_loss: 4.5188, ppl: 91.7244, loss: 4.5477
	step 7486:lm_loss: 4.5189, ppl: 91.7374, loss: 4.5478
	step 7487:lm_loss: 4.5189, ppl: 91.7336, loss: 4.5478
	step 7488:lm_loss: 4.5190, ppl: 91.7406, loss: 4.5479
	step 7489:lm_loss: 4.5190, ppl: 91.7433, loss: 4.5479
	step 7490:lm_loss: 4.5189, ppl: 91.7351, loss: 4.5478
	step 7491:lm_loss: 4.5190, ppl: 91.7456, loss: 4.5479
	step 7492:lm_loss: 4.5190, ppl: 91.7476, loss: 4.5479
	step 7493:lm_loss: 4.5190, ppl: 91.7472, loss: 4.5479
	step 7494:lm_loss: 4.5190, ppl: 91.7453, loss: 4.5478
	step 7495:lm_loss: 4.5191, ppl: 91.7566, loss: 4.5479
	step 7496:lm_loss: 4.5192, ppl: 91.7638, loss: 4.5480
	step 7497:lm_loss: 4.5193, ppl: 91.7672, loss: 4.5480
	step 7498:lm_loss: 4.5193, ppl: 91.7752, loss: 4.5481
	step 7499:lm_loss: 4.5193, ppl: 91.7713, loss: 4.5480
	step 7500:lm_loss: 4.5194, ppl: 91.7781, loss: 4.5481
	step 7501:lm_loss: 4.5194, ppl: 91.7762, loss: 4.5481
	step 7502:lm_loss: 4.5193, ppl: 91.7731, loss: 4.5480
	step 7503:lm_loss: 4.5193, ppl: 91.7755, loss: 4.5481
	step 7504:lm_loss: 4.5195, ppl: 91.7883, loss: 4.5482
	step 7505:lm_loss: 4.5197, ppl: 91.8071, loss: 4.5484
	step 7506:lm_loss: 4.5197, ppl: 91.8091, loss: 4.5484
	step 7507:lm_loss: 4.5197, ppl: 91.8057, loss: 4.5483
	step 7508:lm_loss: 4.5197, ppl: 91.8089, loss: 4.5484
	step 7509:lm_loss: 4.5197, ppl: 91.8057, loss: 4.5483
	step 7510:lm_loss: 4.5198, ppl: 91.8144, loss: 4.5484
	step 7511:lm_loss: 4.5198, ppl: 91.8150, loss: 4.5484
	step 7512:lm_loss: 4.5198, ppl: 91.8213, loss: 4.5485
	step 7513:lm_loss: 4.5199, ppl: 91.8290, loss: 4.5486
	step 7514:lm_loss: 4.5200, ppl: 91.8374, loss: 4.5487
	step 7515:lm_loss: 4.5199, ppl: 91.8279, loss: 4.5485
	step 7516:lm_loss: 4.5196, ppl: 91.8027, loss: 4.5484
	step 7517:lm_loss: 4.5197, ppl: 91.8106, loss: 4.5484
	step 7518:lm_loss: 4.5198, ppl: 91.8155, loss: 4.5485
	step 7519:lm_loss: 4.5198, ppl: 91.8190, loss: 4.5486
	step 7520:lm_loss: 4.5198, ppl: 91.8142, loss: 4.5485
	step 7521:lm_loss: 4.5198, ppl: 91.8200, loss: 4.5485
	step 7522:lm_loss: 4.5195, ppl: 91.7940, loss: 4.5484
	step 7523:lm_loss: 4.5196, ppl: 91.7996, loss: 4.5484
	step 7524:lm_loss: 4.5198, ppl: 91.8143, loss: 4.5486
	step 7525:lm_loss: 4.5198, ppl: 91.8202, loss: 4.5487
	step 7526:lm_loss: 4.5200, ppl: 91.8342, loss: 4.5489
	step 7527:lm_loss: 4.5199, ppl: 91.8263, loss: 4.5488
	step 7528:lm_loss: 4.5199, ppl: 91.8307, loss: 4.5488
	step 7529:lm_loss: 4.5199, ppl: 91.8285, loss: 4.5488
	step 7530:lm_loss: 4.5199, ppl: 91.8277, loss: 4.5488
	step 7531:lm_loss: 4.5198, ppl: 91.8217, loss: 4.5487
	step 7532:lm_loss: 4.5200, ppl: 91.8317, loss: 4.5488
	step 7533:lm_loss: 4.5200, ppl: 91.8387, loss: 4.5489
	step 7534:lm_loss: 4.5200, ppl: 91.8365, loss: 4.5488
	step 7535:lm_loss: 4.5200, ppl: 91.8328, loss: 4.5488
	step 7536:lm_loss: 4.5200, ppl: 91.8323, loss: 4.5488
	step 7537:lm_loss: 4.5199, ppl: 91.8261, loss: 4.5487
	step 7538:lm_loss: 4.5199, ppl: 91.8266, loss: 4.5487
	step 7539:lm_loss: 4.5199, ppl: 91.8242, loss: 4.5487
	step 7540:lm_loss: 4.5199, ppl: 91.8263, loss: 4.5487
	step 7541:lm_loss: 4.5198, ppl: 91.8207, loss: 4.5487
	step 7542:lm_loss: 4.5199, ppl: 91.8309, loss: 4.5487
	step 7543:lm_loss: 4.5201, ppl: 91.8418, loss: 4.5488
	step 7544:lm_loss: 4.5200, ppl: 91.8317, loss: 4.5487
	step 7545:lm_loss: 4.5198, ppl: 91.8170, loss: 4.5485
	step 7546:lm_loss: 4.5199, ppl: 91.8292, loss: 4.5486
	step 7547:lm_loss: 4.5198, ppl: 91.8191, loss: 4.5485
	step 7548:lm_loss: 4.5199, ppl: 91.8241, loss: 4.5485
	step 7549:lm_loss: 4.5197, ppl: 91.8121, loss: 4.5484
	step 7550:lm_loss: 4.5196, ppl: 91.8026, loss: 4.5482
	step 7551:lm_loss: 4.5197, ppl: 91.8081, loss: 4.5483
	step 7552:lm_loss: 4.5197, ppl: 91.8113, loss: 4.5483
	step 7553:lm_loss: 4.5197, ppl: 91.8123, loss: 4.5483
	step 7554:lm_loss: 4.5197, ppl: 91.8076, loss: 4.5483
	step 7555:lm_loss: 4.5197, ppl: 91.8059, loss: 4.5483
	step 7556:lm_loss: 4.5197, ppl: 91.8072, loss: 4.5483
	step 7557:lm_loss: 4.5198, ppl: 91.8202, loss: 4.5484
	step 7558:lm_loss: 4.5198, ppl: 91.8184, loss: 4.5484
	step 7559:lm_loss: 4.5199, ppl: 91.8288, loss: 4.5485
	step 7560:lm_loss: 4.5201, ppl: 91.8420, loss: 4.5486
	step 7561:lm_loss: 4.5201, ppl: 91.8474, loss: 4.5486
	step 7562:lm_loss: 4.5200, ppl: 91.8372, loss: 4.5485
	step 7563:lm_loss: 4.5201, ppl: 91.8444, loss: 4.5486
	step 7564:lm_loss: 4.5190, ppl: 91.7430, loss: 4.5484
	step 7565:lm_loss: 4.5189, ppl: 91.7314, loss: 4.5483
	step 7566:lm_loss: 4.5188, ppl: 91.7234, loss: 4.5482
	step 7567:lm_loss: 4.5188, ppl: 91.7273, loss: 4.5483
	step 7568:lm_loss: 4.5189, ppl: 91.7332, loss: 4.5483
	step 7569:lm_loss: 4.5189, ppl: 91.7384, loss: 4.5484
	step 7570:lm_loss: 4.5188, ppl: 91.7293, loss: 4.5483
	step 7571:lm_loss: 4.5189, ppl: 91.7345, loss: 4.5483
	step 7572:lm_loss: 4.5189, ppl: 91.7303, loss: 4.5483
	step 7573:lm_loss: 4.5187, ppl: 91.7177, loss: 4.5482
	step 7574:lm_loss: 4.5187, ppl: 91.7175, loss: 4.5481
	step 7575:lm_loss: 4.5186, ppl: 91.7058, loss: 4.5481
	step 7576:lm_loss: 4.5187, ppl: 91.7172, loss: 4.5481
	step 7577:lm_loss: 4.5186, ppl: 91.7092, loss: 4.5481
	step 7578:lm_loss: 4.5186, ppl: 91.7077, loss: 4.5480
	step 7579:lm_loss: 4.5186, ppl: 91.7094, loss: 4.5480
	step 7580:lm_loss: 4.5187, ppl: 91.7157, loss: 4.5481
	step 7581:lm_loss: 4.5187, ppl: 91.7160, loss: 4.5481
	step 7582:lm_loss: 4.5187, ppl: 91.7153, loss: 4.5481
	step 7583:lm_loss: 4.5187, ppl: 91.7165, loss: 4.5481
	step 7584:lm_loss: 4.5187, ppl: 91.7149, loss: 4.5481
	step 7585:lm_loss: 4.5187, ppl: 91.7203, loss: 4.5481
	step 7586:lm_loss: 4.5188, ppl: 91.7249, loss: 4.5482
	step 7587:lm_loss: 4.5188, ppl: 91.7274, loss: 4.5482
	step 7588:lm_loss: 4.5189, ppl: 91.7340, loss: 4.5483
	step 7589:lm_loss: 4.5188, ppl: 91.7247, loss: 4.5482
	step 7590:lm_loss: 4.5186, ppl: 91.7076, loss: 4.5480
	step 7591:lm_loss: 4.5185, ppl: 91.6956, loss: 4.5478
	step 7592:lm_loss: 4.5183, ppl: 91.6800, loss: 4.5476
	step 7593:lm_loss: 4.5183, ppl: 91.6818, loss: 4.5477
	step 7594:lm_loss: 4.5185, ppl: 91.6937, loss: 4.5478
	step 7595:lm_loss: 4.5185, ppl: 91.6996, loss: 4.5479
	step 7596:lm_loss: 4.5187, ppl: 91.7182, loss: 4.5480
	step 7597:lm_loss: 4.5187, ppl: 91.7137, loss: 4.5480
	step 7598:lm_loss: 4.5186, ppl: 91.7080, loss: 4.5479
	step 7599:lm_loss: 4.5186, ppl: 91.7054, loss: 4.5478
	step 7600:lm_loss: 4.5187, ppl: 91.7191, loss: 4.5480
	step 7601:lm_loss: 4.5187, ppl: 91.7136, loss: 4.5479
	step 7602:lm_loss: 4.5188, ppl: 91.7221, loss: 4.5480
	step 7603:lm_loss: 4.5188, ppl: 91.7215, loss: 4.5480
	step 7604:lm_loss: 4.5187, ppl: 91.7158, loss: 4.5479
	step 7605:lm_loss: 4.5186, ppl: 91.7043, loss: 4.5478
	step 7606:lm_loss: 4.5186, ppl: 91.7078, loss: 4.5478
	step 7607:lm_loss: 4.5187, ppl: 91.7169, loss: 4.5479
	step 7608:lm_loss: 4.5188, ppl: 91.7210, loss: 4.5480
	step 7609:lm_loss: 4.5189, ppl: 91.7352, loss: 4.5481
	step 7610:lm_loss: 4.5190, ppl: 91.7415, loss: 4.5481
	step 7611:lm_loss: 4.5187, ppl: 91.7171, loss: 4.5480
	step 7612:lm_loss: 4.5187, ppl: 91.7118, loss: 4.5479
	step 7613:lm_loss: 4.5187, ppl: 91.7140, loss: 4.5479
	step 7614:lm_loss: 4.5187, ppl: 91.7119, loss: 4.5479
	step 7615:lm_loss: 4.5185, ppl: 91.6956, loss: 4.5478
	step 7616:lm_loss: 4.5184, ppl: 91.6869, loss: 4.5477
	step 7617:lm_loss: 4.5184, ppl: 91.6916, loss: 4.5477
	step 7618:lm_loss: 4.5184, ppl: 91.6930, loss: 4.5477
	step 7619:lm_loss: 4.5184, ppl: 91.6917, loss: 4.5477
	step 7620:lm_loss: 4.5186, ppl: 91.7077, loss: 4.5479
	step 7621:lm_loss: 4.5186, ppl: 91.7070, loss: 4.5479
	step 7622:lm_loss: 4.5186, ppl: 91.7028, loss: 4.5478
	step 7623:lm_loss: 4.5187, ppl: 91.7159, loss: 4.5479
	step 7624:lm_loss: 4.5188, ppl: 91.7229, loss: 4.5479
	step 7625:lm_loss: 4.5189, ppl: 91.7320, loss: 4.5480
	step 7626:lm_loss: 4.5186, ppl: 91.7067, loss: 4.5477
	step 7627:lm_loss: 4.5186, ppl: 91.7031, loss: 4.5477
	step 7628:lm_loss: 4.5185, ppl: 91.6953, loss: 4.5475
	step 7629:lm_loss: 4.5184, ppl: 91.6904, loss: 4.5474
	step 7630:lm_loss: 4.5185, ppl: 91.6942, loss: 4.5475
	step 7631:lm_loss: 4.5184, ppl: 91.6930, loss: 4.5475
	step 7632:lm_loss: 4.5184, ppl: 91.6924, loss: 4.5475
	step 7633:lm_loss: 4.5184, ppl: 91.6844, loss: 4.5474
	step 7634:lm_loss: 4.5185, ppl: 91.6987, loss: 4.5475
	step 7635:lm_loss: 4.5182, ppl: 91.6750, loss: 4.5473
	step 7636:lm_loss: 4.5182, ppl: 91.6686, loss: 4.5472
	step 7637:lm_loss: 4.5182, ppl: 91.6747, loss: 4.5472
	step 7638:lm_loss: 4.5184, ppl: 91.6844, loss: 4.5473
	step 7639:lm_loss: 4.5184, ppl: 91.6930, loss: 4.5474
	step 7640:lm_loss: 4.5186, ppl: 91.7039, loss: 4.5475
	step 7641:lm_loss: 4.5185, ppl: 91.6979, loss: 4.5474
	step 7642:lm_loss: 4.5184, ppl: 91.6929, loss: 4.5474
	step 7643:lm_loss: 4.5185, ppl: 91.6995, loss: 4.5475
	step 7644:lm_loss: 4.5184, ppl: 91.6915, loss: 4.5474
	step 7645:lm_loss: 4.5184, ppl: 91.6900, loss: 4.5474
	step 7646:lm_loss: 4.5185, ppl: 91.6938, loss: 4.5474
	step 7647:lm_loss: 4.5184, ppl: 91.6872, loss: 4.5474
	step 7648:lm_loss: 4.5181, ppl: 91.6584, loss: 4.5472
	step 7649:lm_loss: 4.5181, ppl: 91.6581, loss: 4.5472
	step 7650:lm_loss: 4.5181, ppl: 91.6589, loss: 4.5472
	step 7651:lm_loss: 4.5182, ppl: 91.6734, loss: 4.5474
	step 7652:lm_loss: 4.5181, ppl: 91.6572, loss: 4.5472
	step 7653:lm_loss: 4.5182, ppl: 91.6715, loss: 4.5473
	step 7654:lm_loss: 4.5182, ppl: 91.6699, loss: 4.5473
	step 7655:lm_loss: 4.5182, ppl: 91.6738, loss: 4.5474
	step 7656:lm_loss: 4.5182, ppl: 91.6748, loss: 4.5474
	step 7657:lm_loss: 4.5183, ppl: 91.6773, loss: 4.5474
	step 7658:lm_loss: 4.5184, ppl: 91.6917, loss: 4.5475
	step 7659:lm_loss: 4.5184, ppl: 91.6923, loss: 4.5475
	step 7660:lm_loss: 4.5185, ppl: 91.6950, loss: 4.5476
	step 7661:lm_loss: 4.5185, ppl: 91.6968, loss: 4.5476
	step 7662:lm_loss: 4.5183, ppl: 91.6800, loss: 4.5474
	step 7663:lm_loss: 4.5184, ppl: 91.6850, loss: 4.5475
	step 7664:lm_loss: 4.5182, ppl: 91.6728, loss: 4.5474
	step 7665:lm_loss: 4.5182, ppl: 91.6682, loss: 4.5473
	step 7666:lm_loss: 4.5180, ppl: 91.6485, loss: 4.5471
	step 7667:lm_loss: 4.5182, ppl: 91.6673, loss: 4.5473
	step 7668:lm_loss: 4.5182, ppl: 91.6660, loss: 4.5473
	step 7669:lm_loss: 4.5180, ppl: 91.6553, loss: 4.5472
	step 7670:lm_loss: 4.5180, ppl: 91.6511, loss: 4.5472
	step 7671:lm_loss: 4.5178, ppl: 91.6360, loss: 4.5469
	step 7672:lm_loss: 4.5176, ppl: 91.6179, loss: 4.5468
	step 7673:lm_loss: 4.5176, ppl: 91.6146, loss: 4.5467
	step 7674:lm_loss: 4.5175, ppl: 91.6077, loss: 4.5467
	step 7675:lm_loss: 4.5175, ppl: 91.6077, loss: 4.5467
	step 7676:lm_loss: 4.5176, ppl: 91.6109, loss: 4.5467
	step 7677:lm_loss: 4.5175, ppl: 91.6028, loss: 4.5466
	step 7678:lm_loss: 4.5175, ppl: 91.6067, loss: 4.5467
	step 7679:lm_loss: 4.5176, ppl: 91.6118, loss: 4.5467
	step 7680:lm_loss: 4.5177, ppl: 91.6222, loss: 4.5469
	step 7681:lm_loss: 4.5177, ppl: 91.6228, loss: 4.5469
	step 7682:lm_loss: 4.5177, ppl: 91.6211, loss: 4.5468
	step 7683:lm_loss: 4.5177, ppl: 91.6235, loss: 4.5469
	step 7684:lm_loss: 4.5177, ppl: 91.6216, loss: 4.5468
	step 7685:lm_loss: 4.5178, ppl: 91.6340, loss: 4.5470
	step 7686:lm_loss: 4.5180, ppl: 91.6485, loss: 4.5471
	step 7687:lm_loss: 4.5180, ppl: 91.6550, loss: 4.5472
	step 7688:lm_loss: 4.5182, ppl: 91.6664, loss: 4.5473
	step 7689:lm_loss: 4.5181, ppl: 91.6613, loss: 4.5473
	step 7690:lm_loss: 4.5181, ppl: 91.6639, loss: 4.5473
	step 7691:lm_loss: 4.5180, ppl: 91.6557, loss: 4.5472
	step 7692:lm_loss: 4.5182, ppl: 91.6708, loss: 4.5474
	step 7693:lm_loss: 4.5181, ppl: 91.6658, loss: 4.5473
	step 7694:lm_loss: 4.5180, ppl: 91.6540, loss: 4.5472
	step 7695:lm_loss: 4.5179, ppl: 91.6388, loss: 4.5471
	step 7696:lm_loss: 4.5180, ppl: 91.6483, loss: 4.5473
	step 7697:lm_loss: 4.5181, ppl: 91.6585, loss: 4.5474
	step 7698:lm_loss: 4.5181, ppl: 91.6574, loss: 4.5473
	step 7699:lm_loss: 4.5182, ppl: 91.6661, loss: 4.5474
	step 7700:lm_loss: 4.5182, ppl: 91.6689, loss: 4.5474
	step 7701:lm_loss: 4.5181, ppl: 91.6591, loss: 4.5473
	step 7702:lm_loss: 4.5180, ppl: 91.6523, loss: 4.5472
	step 7703:lm_loss: 4.5180, ppl: 91.6528, loss: 4.5472
	step 7704:lm_loss: 4.5180, ppl: 91.6494, loss: 4.5471
	step 7705:lm_loss: 4.5179, ppl: 91.6446, loss: 4.5471
	step 7706:lm_loss: 4.5181, ppl: 91.6585, loss: 4.5473
	step 7707:lm_loss: 4.5180, ppl: 91.6533, loss: 4.5472
	step 7708:lm_loss: 4.5180, ppl: 91.6552, loss: 4.5472
	step 7709:lm_loss: 4.5179, ppl: 91.6428, loss: 4.5472
	step 7710:lm_loss: 4.5179, ppl: 91.6401, loss: 4.5471
	step 7711:lm_loss: 4.5179, ppl: 91.6431, loss: 4.5471
	step 7712:lm_loss: 4.5179, ppl: 91.6423, loss: 4.5471
	step 7713:lm_loss: 4.5180, ppl: 91.6519, loss: 4.5473
	step 7714:lm_loss: 4.5180, ppl: 91.6555, loss: 4.5473
	step 7715:lm_loss: 4.5180, ppl: 91.6492, loss: 4.5472
	step 7716:lm_loss: 4.5179, ppl: 91.6436, loss: 4.5472
	step 7717:lm_loss: 4.5179, ppl: 91.6425, loss: 4.5472
	step 7718:lm_loss: 4.5179, ppl: 91.6387, loss: 4.5471
	step 7719:lm_loss: 4.5179, ppl: 91.6416, loss: 4.5471
	step 7720:lm_loss: 4.5178, ppl: 91.6320, loss: 4.5470
	step 7721:lm_loss: 4.5178, ppl: 91.6357, loss: 4.5470
	step 7722:lm_loss: 4.5179, ppl: 91.6418, loss: 4.5470
	step 7723:lm_loss: 4.5179, ppl: 91.6406, loss: 4.5470
	step 7724:lm_loss: 4.5178, ppl: 91.6342, loss: 4.5470
	step 7725:lm_loss: 4.5179, ppl: 91.6422, loss: 4.5470
	step 7726:lm_loss: 4.5178, ppl: 91.6361, loss: 4.5470
	step 7727:lm_loss: 4.5178, ppl: 91.6381, loss: 4.5470
	step 7728:lm_loss: 4.5179, ppl: 91.6446, loss: 4.5471
	step 7729:lm_loss: 4.5180, ppl: 91.6546, loss: 4.5472
	step 7730:lm_loss: 4.5181, ppl: 91.6595, loss: 4.5472
	step 7731:lm_loss: 4.5181, ppl: 91.6572, loss: 4.5472
	step 7732:lm_loss: 4.5181, ppl: 91.6628, loss: 4.5472
	step 7733:lm_loss: 4.5181, ppl: 91.6649, loss: 4.5473
	step 7734:lm_loss: 4.5181, ppl: 91.6590, loss: 4.5472
	step 7735:lm_loss: 4.5180, ppl: 91.6531, loss: 4.5471
	step 7736:lm_loss: 4.5180, ppl: 91.6478, loss: 4.5471
	step 7737:lm_loss: 4.5181, ppl: 91.6631, loss: 4.5472
	step 7738:lm_loss: 4.5180, ppl: 91.6566, loss: 4.5471
	step 7739:lm_loss: 4.5182, ppl: 91.6695, loss: 4.5473
	step 7740:lm_loss: 4.5182, ppl: 91.6713, loss: 4.5473
	step 7741:lm_loss: 4.5183, ppl: 91.6783, loss: 4.5473
	step 7742:lm_loss: 4.5184, ppl: 91.6918, loss: 4.5475
	step 7743:lm_loss: 4.5184, ppl: 91.6911, loss: 4.5475
	step 7744:lm_loss: 4.5184, ppl: 91.6866, loss: 4.5474
	step 7745:lm_loss: 4.5183, ppl: 91.6771, loss: 4.5473
	step 7746:lm_loss: 4.5183, ppl: 91.6793, loss: 4.5474
	step 7747:lm_loss: 4.5183, ppl: 91.6807, loss: 4.5474
	step 7748:lm_loss: 4.5183, ppl: 91.6751, loss: 4.5473
	step 7749:lm_loss: 4.5183, ppl: 91.6780, loss: 4.5473
	step 7750:lm_loss: 4.5184, ppl: 91.6845, loss: 4.5474
	step 7751:lm_loss: 4.5184, ppl: 91.6852, loss: 4.5474
	step 7752:lm_loss: 4.5184, ppl: 91.6845, loss: 4.5474
	step 7753:lm_loss: 4.5183, ppl: 91.6830, loss: 4.5473
	step 7754:lm_loss: 4.5183, ppl: 91.6816, loss: 4.5473
	step 7755:lm_loss: 4.5183, ppl: 91.6760, loss: 4.5473
	step 7756:lm_loss: 4.5183, ppl: 91.6824, loss: 4.5473
	step 7757:lm_loss: 4.5183, ppl: 91.6814, loss: 4.5473
	step 7758:lm_loss: 4.5183, ppl: 91.6781, loss: 4.5473
	step 7759:lm_loss: 4.5183, ppl: 91.6816, loss: 4.5473
	step 7760:lm_loss: 4.5183, ppl: 91.6837, loss: 4.5473
	step 7761:lm_loss: 4.5182, ppl: 91.6746, loss: 4.5472
	step 7762:lm_loss: 4.5183, ppl: 91.6789, loss: 4.5472
	step 7763:lm_loss: 4.5184, ppl: 91.6855, loss: 4.5473
	step 7764:lm_loss: 4.5185, ppl: 91.6970, loss: 4.5474
	step 7765:lm_loss: 4.5186, ppl: 91.7039, loss: 4.5474
	step 7766:lm_loss: 4.5186, ppl: 91.7027, loss: 4.5474
	step 7767:lm_loss: 4.5186, ppl: 91.7030, loss: 4.5474
	step 7768:lm_loss: 4.5186, ppl: 91.7031, loss: 4.5474
	step 7769:lm_loss: 4.5185, ppl: 91.7004, loss: 4.5474
	step 7770:lm_loss: 4.5186, ppl: 91.7090, loss: 4.5475
	step 7771:lm_loss: 4.5187, ppl: 91.7138, loss: 4.5475
	step 7772:lm_loss: 4.5187, ppl: 91.7120, loss: 4.5475
	step 7773:lm_loss: 4.5188, ppl: 91.7294, loss: 4.5477
	step 7774:lm_loss: 4.5189, ppl: 91.7329, loss: 4.5477
	step 7775:lm_loss: 4.5188, ppl: 91.7258, loss: 4.5476
	step 7776:lm_loss: 4.5188, ppl: 91.7285, loss: 4.5477
	step 7777:lm_loss: 4.5189, ppl: 91.7312, loss: 4.5477
	step 7778:lm_loss: 4.5190, ppl: 91.7413, loss: 4.5478
	step 7779:lm_loss: 4.5191, ppl: 91.7506, loss: 4.5480
	step 7780:lm_loss: 4.5191, ppl: 91.7490, loss: 4.5480
	step 7781:lm_loss: 4.5191, ppl: 91.7501, loss: 4.5480
	step 7782:lm_loss: 4.5192, ppl: 91.7605, loss: 4.5480
	step 7783:lm_loss: 4.5194, ppl: 91.7809, loss: 4.5482
	step 7784:lm_loss: 4.5193, ppl: 91.7677, loss: 4.5481
	step 7785:lm_loss: 4.5191, ppl: 91.7524, loss: 4.5478
	step 7786:lm_loss: 4.5191, ppl: 91.7542, loss: 4.5478
	step 7787:lm_loss: 4.5191, ppl: 91.7493, loss: 4.5478
	step 7788:lm_loss: 4.5190, ppl: 91.7448, loss: 4.5477
	step 7789:lm_loss: 4.5191, ppl: 91.7522, loss: 4.5478
	step 7790:lm_loss: 4.5189, ppl: 91.7376, loss: 4.5477
	step 7791:lm_loss: 4.5187, ppl: 91.7153, loss: 4.5475
	step 7792:lm_loss: 4.5187, ppl: 91.7184, loss: 4.5475
	step 7793:lm_loss: 4.5187, ppl: 91.7146, loss: 4.5475
	step 7794:lm_loss: 4.5188, ppl: 91.7287, loss: 4.5476
	step 7795:lm_loss: 4.5188, ppl: 91.7261, loss: 4.5476
	step 7796:lm_loss: 4.5189, ppl: 91.7312, loss: 4.5476
	step 7797:lm_loss: 4.5190, ppl: 91.7435, loss: 4.5477
	step 7798:lm_loss: 4.5190, ppl: 91.7462, loss: 4.5477
	step 7799:lm_loss: 4.5191, ppl: 91.7561, loss: 4.5478
	step 7800:lm_loss: 4.5191, ppl: 91.7499, loss: 4.5478
	step 7801:lm_loss: 4.5191, ppl: 91.7525, loss: 4.5478
	step 7802:lm_loss: 4.5193, ppl: 91.7670, loss: 4.5479
	step 7803:lm_loss: 4.5191, ppl: 91.7536, loss: 4.5477
	step 7804:lm_loss: 4.5191, ppl: 91.7566, loss: 4.5478
	step 7805:lm_loss: 4.5192, ppl: 91.7661, loss: 4.5479
	step 7806:lm_loss: 4.5193, ppl: 91.7751, loss: 4.5480
	step 7807:lm_loss: 4.5193, ppl: 91.7681, loss: 4.5479
	step 7808:lm_loss: 4.5193, ppl: 91.7749, loss: 4.5480
	step 7809:lm_loss: 4.5195, ppl: 91.7857, loss: 4.5481
	step 7810:lm_loss: 4.5195, ppl: 91.7876, loss: 4.5481
	step 7811:lm_loss: 4.5194, ppl: 91.7799, loss: 4.5480
	step 7812:lm_loss: 4.5194, ppl: 91.7763, loss: 4.5479
	step 7813:lm_loss: 4.5194, ppl: 91.7776, loss: 4.5480
	step 7814:lm_loss: 4.5191, ppl: 91.7538, loss: 4.5477
	step 7815:lm_loss: 4.5192, ppl: 91.7590, loss: 4.5477
	step 7816:lm_loss: 4.5193, ppl: 91.7720, loss: 4.5478
	step 7817:lm_loss: 4.5192, ppl: 91.7653, loss: 4.5478
	step 7818:lm_loss: 4.5193, ppl: 91.7707, loss: 4.5478
	step 7819:lm_loss: 4.5193, ppl: 91.7696, loss: 4.5478
	step 7820:lm_loss: 4.5194, ppl: 91.7762, loss: 4.5479
	step 7821:lm_loss: 4.5194, ppl: 91.7839, loss: 4.5479
	step 7822:lm_loss: 4.5194, ppl: 91.7806, loss: 4.5479
	step 7823:lm_loss: 4.5192, ppl: 91.7612, loss: 4.5477
	step 7824:lm_loss: 4.5192, ppl: 91.7596, loss: 4.5477
	step 7825:lm_loss: 4.5194, ppl: 91.7794, loss: 4.5480
	step 7826:lm_loss: 4.5194, ppl: 91.7810, loss: 4.5480
	step 7827:lm_loss: 4.5194, ppl: 91.7791, loss: 4.5479
	step 7828:lm_loss: 4.5193, ppl: 91.7743, loss: 4.5479
	step 7829:lm_loss: 4.5192, ppl: 91.7631, loss: 4.5478
	step 7830:lm_loss: 4.5191, ppl: 91.7547, loss: 4.5477
	step 7831:lm_loss: 4.5191, ppl: 91.7517, loss: 4.5477
	step 7832:lm_loss: 4.5192, ppl: 91.7663, loss: 4.5478
	step 7833:lm_loss: 4.5193, ppl: 91.7716, loss: 4.5479
	step 7834:lm_loss: 4.5193, ppl: 91.7706, loss: 4.5479
	step 7835:lm_loss: 4.5192, ppl: 91.7617, loss: 4.5478
	step 7836:lm_loss: 4.5192, ppl: 91.7592, loss: 4.5477
	step 7837:lm_loss: 4.5191, ppl: 91.7497, loss: 4.5477
	step 7838:lm_loss: 4.5191, ppl: 91.7509, loss: 4.5477
	step 7839:lm_loss: 4.5189, ppl: 91.7364, loss: 4.5475
	step 7840:lm_loss: 4.5188, ppl: 91.7210, loss: 4.5473
	step 7841:lm_loss: 4.5188, ppl: 91.7253, loss: 4.5474
	step 7842:lm_loss: 4.5188, ppl: 91.7247, loss: 4.5474
	step 7843:lm_loss: 4.5189, ppl: 91.7361, loss: 4.5475
	step 7844:lm_loss: 4.5189, ppl: 91.7355, loss: 4.5475
	step 7845:lm_loss: 4.5189, ppl: 91.7329, loss: 4.5474
	step 7846:lm_loss: 4.5189, ppl: 91.7320, loss: 4.5474
	step 7847:lm_loss: 4.5189, ppl: 91.7307, loss: 4.5474
	step 7848:lm_loss: 4.5189, ppl: 91.7351, loss: 4.5474
	step 7849:lm_loss: 4.5189, ppl: 91.7384, loss: 4.5475
	step 7850:lm_loss: 4.5188, ppl: 91.7224, loss: 4.5474
	step 7851:lm_loss: 4.5188, ppl: 91.7219, loss: 4.5474
	step 7852:lm_loss: 4.5188, ppl: 91.7248, loss: 4.5474
	step 7853:lm_loss: 4.5187, ppl: 91.7152, loss: 4.5473
	step 7854:lm_loss: 4.5187, ppl: 91.7124, loss: 4.5472
	step 7855:lm_loss: 4.5185, ppl: 91.7007, loss: 4.5471
	step 7856:lm_loss: 4.5186, ppl: 91.7101, loss: 4.5472
	step 7857:lm_loss: 4.5188, ppl: 91.7217, loss: 4.5473
	step 7858:lm_loss: 4.5189, ppl: 91.7330, loss: 4.5475
	step 7859:lm_loss: 4.5187, ppl: 91.7136, loss: 4.5473
	step 7860:lm_loss: 4.5186, ppl: 91.7066, loss: 4.5472
	step 7861:lm_loss: 4.5186, ppl: 91.7049, loss: 4.5472
	step 7862:lm_loss: 4.5187, ppl: 91.7164, loss: 4.5473
	step 7863:lm_loss: 4.5187, ppl: 91.7137, loss: 4.5473
	step 7864:lm_loss: 4.5187, ppl: 91.7159, loss: 4.5473
	step 7865:lm_loss: 4.5187, ppl: 91.7137, loss: 4.5473
	step 7866:lm_loss: 4.5187, ppl: 91.7143, loss: 4.5473
	step 7867:lm_loss: 4.5187, ppl: 91.7182, loss: 4.5473
	step 7868:lm_loss: 4.5185, ppl: 91.6982, loss: 4.5472
	step 7869:lm_loss: 4.5185, ppl: 91.7022, loss: 4.5472
	step 7870:lm_loss: 4.5185, ppl: 91.6980, loss: 4.5472
	step 7871:lm_loss: 4.5186, ppl: 91.7067, loss: 4.5472
	step 7872:lm_loss: 4.5186, ppl: 91.7072, loss: 4.5472
	step 7873:lm_loss: 4.5186, ppl: 91.7045, loss: 4.5472
	step 7874:lm_loss: 4.5186, ppl: 91.7079, loss: 4.5472
	step 7875:lm_loss: 4.5186, ppl: 91.7091, loss: 4.5472
	step 7876:lm_loss: 4.5187, ppl: 91.7124, loss: 4.5473
	step 7877:lm_loss: 4.5187, ppl: 91.7126, loss: 4.5473
	step 7878:lm_loss: 4.5186, ppl: 91.7039, loss: 4.5472
	step 7879:lm_loss: 4.5187, ppl: 91.7128, loss: 4.5473
	step 7880:lm_loss: 4.5188, ppl: 91.7280, loss: 4.5474
	step 7881:lm_loss: 4.5190, ppl: 91.7397, loss: 4.5475
	step 7882:lm_loss: 4.5189, ppl: 91.7374, loss: 4.5474
	step 7883:lm_loss: 4.5190, ppl: 91.7458, loss: 4.5475
	step 7884:lm_loss: 4.5190, ppl: 91.7421, loss: 4.5475
	step 7885:lm_loss: 4.5189, ppl: 91.7391, loss: 4.5474
	step 7886:lm_loss: 4.5189, ppl: 91.7336, loss: 4.5473
	step 7887:lm_loss: 4.5190, ppl: 91.7415, loss: 4.5474
	step 7888:lm_loss: 4.5189, ppl: 91.7357, loss: 4.5473
	step 7889:lm_loss: 4.5188, ppl: 91.7288, loss: 4.5473
	step 7890:lm_loss: 4.5189, ppl: 91.7351, loss: 4.5473
	step 7891:lm_loss: 4.5189, ppl: 91.7332, loss: 4.5473
	step 7892:lm_loss: 4.5190, ppl: 91.7427, loss: 4.5475
	step 7893:lm_loss: 4.5189, ppl: 91.7367, loss: 4.5474
	step 7894:lm_loss: 4.5189, ppl: 91.7306, loss: 4.5473
	step 7895:lm_loss: 4.5188, ppl: 91.7267, loss: 4.5472
	step 7896:lm_loss: 4.5189, ppl: 91.7326, loss: 4.5473
	step 7897:lm_loss: 4.5189, ppl: 91.7311, loss: 4.5473
	step 7898:lm_loss: 4.5187, ppl: 91.7166, loss: 4.5472
	step 7899:lm_loss: 4.5187, ppl: 91.7194, loss: 4.5472
	step 7900:lm_loss: 4.5187, ppl: 91.7141, loss: 4.5471
	step 7901:lm_loss: 4.5188, ppl: 91.7219, loss: 4.5472
	step 7902:lm_loss: 4.5189, ppl: 91.7327, loss: 4.5474
	step 7903:lm_loss: 4.5189, ppl: 91.7378, loss: 4.5474
	step 7904:lm_loss: 4.5189, ppl: 91.7314, loss: 4.5474
	step 7905:lm_loss: 4.5189, ppl: 91.7313, loss: 4.5474
	step 7906:lm_loss: 4.5189, ppl: 91.7372, loss: 4.5475
	step 7907:lm_loss: 4.5189, ppl: 91.7351, loss: 4.5474
	step 7908:lm_loss: 4.5190, ppl: 91.7447, loss: 4.5476
	step 7909:lm_loss: 4.5190, ppl: 91.7447, loss: 4.5476
	step 7910:lm_loss: 4.5192, ppl: 91.7613, loss: 4.5478
	step 7911:lm_loss: 4.5193, ppl: 91.7716, loss: 4.5479
	step 7912:lm_loss: 4.5193, ppl: 91.7681, loss: 4.5478
	step 7913:lm_loss: 4.5193, ppl: 91.7718, loss: 4.5479
	step 7914:lm_loss: 4.5193, ppl: 91.7675, loss: 4.5478
	step 7915:lm_loss: 4.5193, ppl: 91.7695, loss: 4.5478
	step 7916:lm_loss: 4.5193, ppl: 91.7690, loss: 4.5478
	step 7917:lm_loss: 4.5194, ppl: 91.7764, loss: 4.5479
	step 7918:lm_loss: 4.5193, ppl: 91.7724, loss: 4.5479
	step 7919:lm_loss: 4.5191, ppl: 91.7571, loss: 4.5477
	step 7920:lm_loss: 4.5192, ppl: 91.7618, loss: 4.5478
	step 7921:lm_loss: 4.5192, ppl: 91.7642, loss: 4.5478
	step 7922:lm_loss: 4.5192, ppl: 91.7592, loss: 4.5477
	step 7923:lm_loss: 4.5191, ppl: 91.7551, loss: 4.5477
	step 7924:lm_loss: 4.5191, ppl: 91.7515, loss: 4.5476
	step 7925:lm_loss: 4.5190, ppl: 91.7431, loss: 4.5475
	step 7926:lm_loss: 4.5190, ppl: 91.7431, loss: 4.5475
	step 7927:lm_loss: 4.5189, ppl: 91.7348, loss: 4.5474
	step 7928:lm_loss: 4.5188, ppl: 91.7241, loss: 4.5472
	step 7929:lm_loss: 4.5188, ppl: 91.7232, loss: 4.5472
	step 7930:lm_loss: 4.5186, ppl: 91.7054, loss: 4.5471
	step 7931:lm_loss: 4.5185, ppl: 91.6968, loss: 4.5470
	step 7932:lm_loss: 4.5184, ppl: 91.6928, loss: 4.5469
	step 7933:lm_loss: 4.5185, ppl: 91.6965, loss: 4.5470
	step 7934:lm_loss: 4.5185, ppl: 91.6954, loss: 4.5469
	step 7935:lm_loss: 4.5186, ppl: 91.7050, loss: 4.5470
	step 7936:lm_loss: 4.5186, ppl: 91.7046, loss: 4.5470
	step 7937:lm_loss: 4.5186, ppl: 91.7041, loss: 4.5470
	step 7938:lm_loss: 4.5182, ppl: 91.6730, loss: 4.5468
	step 7939:lm_loss: 4.5183, ppl: 91.6760, loss: 4.5468
	step 7940:lm_loss: 4.5182, ppl: 91.6692, loss: 4.5467
	step 7941:lm_loss: 4.5183, ppl: 91.6806, loss: 4.5468
	step 7942:lm_loss: 4.5184, ppl: 91.6885, loss: 4.5469
	step 7943:lm_loss: 4.5184, ppl: 91.6926, loss: 4.5469
	step 7944:lm_loss: 4.5186, ppl: 91.7099, loss: 4.5471
	step 7945:lm_loss: 4.5187, ppl: 91.7144, loss: 4.5471
	step 7946:lm_loss: 4.5185, ppl: 91.7020, loss: 4.5470
	step 7947:lm_loss: 4.5185, ppl: 91.7022, loss: 4.5470
	step 7948:lm_loss: 4.5187, ppl: 91.7189, loss: 4.5473
	step 7949:lm_loss: 4.5188, ppl: 91.7249, loss: 4.5474
	step 7950:lm_loss: 4.5187, ppl: 91.7152, loss: 4.5473
	step 7951:lm_loss: 4.5188, ppl: 91.7244, loss: 4.5474
	step 7952:lm_loss: 4.5187, ppl: 91.7140, loss: 4.5473
	step 7953:lm_loss: 4.5187, ppl: 91.7196, loss: 4.5474
	step 7954:lm_loss: 4.5187, ppl: 91.7120, loss: 4.5473
	step 7955:lm_loss: 4.5186, ppl: 91.7027, loss: 4.5472
	step 7956:lm_loss: 4.5185, ppl: 91.7024, loss: 4.5472
	step 7957:lm_loss: 4.5185, ppl: 91.7020, loss: 4.5471
	step 7958:lm_loss: 4.5185, ppl: 91.6972, loss: 4.5471
	step 7959:lm_loss: 4.5185, ppl: 91.6968, loss: 4.5471
	step 7960:lm_loss: 4.5185, ppl: 91.6975, loss: 4.5471
	step 7961:lm_loss: 4.5185, ppl: 91.6963, loss: 4.5471
	step 7962:lm_loss: 4.5183, ppl: 91.6750, loss: 4.5468
	step 7963:lm_loss: 4.5183, ppl: 91.6771, loss: 4.5468
	step 7964:lm_loss: 4.5184, ppl: 91.6914, loss: 4.5469
	step 7965:lm_loss: 4.5185, ppl: 91.7018, loss: 4.5471
	step 7966:lm_loss: 4.5185, ppl: 91.7012, loss: 4.5470
	step 7967:lm_loss: 4.5185, ppl: 91.6954, loss: 4.5470
	step 7968:lm_loss: 4.5184, ppl: 91.6903, loss: 4.5470
	step 7969:lm_loss: 4.5184, ppl: 91.6898, loss: 4.5469
	step 7970:lm_loss: 4.5184, ppl: 91.6858, loss: 4.5469
	step 7971:lm_loss: 4.5185, ppl: 91.6939, loss: 4.5469
	step 7972:lm_loss: 4.5184, ppl: 91.6887, loss: 4.5469
	step 7973:lm_loss: 4.5185, ppl: 91.6971, loss: 4.5469
	step 7974:lm_loss: 4.5186, ppl: 91.7031, loss: 4.5470
	step 7975:lm_loss: 4.5184, ppl: 91.6916, loss: 4.5469
	step 7976:lm_loss: 4.5185, ppl: 91.6993, loss: 4.5470
	step 7977:lm_loss: 4.5185, ppl: 91.7009, loss: 4.5470
	step 7978:lm_loss: 4.5186, ppl: 91.7092, loss: 4.5471
	step 7979:lm_loss: 4.5187, ppl: 91.7122, loss: 4.5471
	step 7980:lm_loss: 4.5186, ppl: 91.7047, loss: 4.5470
	step 7981:lm_loss: 4.5186, ppl: 91.7064, loss: 4.5470
	step 7982:lm_loss: 4.5185, ppl: 91.7003, loss: 4.5470
	step 7983:lm_loss: 4.5185, ppl: 91.6995, loss: 4.5469
	step 7984:lm_loss: 4.5185, ppl: 91.6975, loss: 4.5469
	step 7985:lm_loss: 4.5186, ppl: 91.7031, loss: 4.5470
	step 7986:lm_loss: 4.5186, ppl: 91.7066, loss: 4.5470
	step 7987:lm_loss: 4.5186, ppl: 91.7055, loss: 4.5470
	step 7988:lm_loss: 4.5186, ppl: 91.7060, loss: 4.5470
	step 7989:lm_loss: 4.5186, ppl: 91.7069, loss: 4.5470
	step 7990:lm_loss: 4.5188, ppl: 91.7224, loss: 4.5471
	step 7991:lm_loss: 4.5188, ppl: 91.7257, loss: 4.5472
	step 7992:lm_loss: 4.5187, ppl: 91.7174, loss: 4.5471
	step 7993:lm_loss: 4.5187, ppl: 91.7189, loss: 4.5471
	step 7994:lm_loss: 4.5186, ppl: 91.7108, loss: 4.5470
	step 7995:lm_loss: 4.5188, ppl: 91.7283, loss: 4.5471
	step 7996:lm_loss: 4.5187, ppl: 91.7197, loss: 4.5470
	step 7997:lm_loss: 4.5188, ppl: 91.7291, loss: 4.5471
	step 7998:lm_loss: 4.5187, ppl: 91.7151, loss: 4.5470
	step 7999:lm_loss: 4.5185, ppl: 91.7005, loss: 4.5469
	step 8000:lm_loss: 4.5186, ppl: 91.7030, loss: 4.5469
	step 8001:lm_loss: 4.5186, ppl: 91.7099, loss: 4.5470
	step 8002:lm_loss: 4.5186, ppl: 91.7089, loss: 4.5470
	step 8003:lm_loss: 4.5186, ppl: 91.7036, loss: 4.5469
	step 8004:lm_loss: 4.5184, ppl: 91.6925, loss: 4.5467
	step 8005:lm_loss: 4.5185, ppl: 91.6939, loss: 4.5468
	step 8006:lm_loss: 4.5185, ppl: 91.7003, loss: 4.5468
	step 8007:lm_loss: 4.5188, ppl: 91.7230, loss: 4.5471
	step 8008:lm_loss: 4.5188, ppl: 91.7281, loss: 4.5471
	step 8009:lm_loss: 4.5188, ppl: 91.7295, loss: 4.5471
	step 8010:lm_loss: 4.5189, ppl: 91.7322, loss: 4.5471
	step 8011:lm_loss: 4.5190, ppl: 91.7434, loss: 4.5472
	step 8012:lm_loss: 4.5190, ppl: 91.7419, loss: 4.5472
	step 8013:lm_loss: 4.5188, ppl: 91.7259, loss: 4.5471
	step 8014:lm_loss: 4.5189, ppl: 91.7343, loss: 4.5472
	step 8015:lm_loss: 4.5190, ppl: 91.7431, loss: 4.5473
	step 8016:lm_loss: 4.5189, ppl: 91.7387, loss: 4.5472
	step 8017:lm_loss: 4.5189, ppl: 91.7314, loss: 4.5472
	step 8018:lm_loss: 4.5189, ppl: 91.7387, loss: 4.5472
	step 8019:lm_loss: 4.5188, ppl: 91.7261, loss: 4.5471
	step 8020:lm_loss: 4.5189, ppl: 91.7344, loss: 4.5472
	step 8021:lm_loss: 4.5190, ppl: 91.7422, loss: 4.5472
	step 8022:lm_loss: 4.5190, ppl: 91.7396, loss: 4.5472
	step 8023:lm_loss: 4.5188, ppl: 91.7238, loss: 4.5470
	step 8024:lm_loss: 4.5188, ppl: 91.7258, loss: 4.5470
	step 8025:lm_loss: 4.5188, ppl: 91.7230, loss: 4.5470
	step 8026:lm_loss: 4.5189, ppl: 91.7336, loss: 4.5470
	step 8027:lm_loss: 4.5189, ppl: 91.7383, loss: 4.5471
	step 8028:lm_loss: 4.5190, ppl: 91.7424, loss: 4.5471
	step 8029:lm_loss: 4.5187, ppl: 91.7155, loss: 4.5469
	step 8030:lm_loss: 4.5187, ppl: 91.7170, loss: 4.5469
	step 8031:lm_loss: 4.5187, ppl: 91.7183, loss: 4.5470
	step 8032:lm_loss: 4.5187, ppl: 91.7147, loss: 4.5469
	step 8033:lm_loss: 4.5187, ppl: 91.7178, loss: 4.5470
	step 8034:lm_loss: 4.5186, ppl: 91.7084, loss: 4.5469
	step 8035:lm_loss: 4.5187, ppl: 91.7189, loss: 4.5471
	step 8036:lm_loss: 4.5187, ppl: 91.7142, loss: 4.5470
	step 8037:lm_loss: 4.5185, ppl: 91.6976, loss: 4.5468
	step 8038:lm_loss: 4.5185, ppl: 91.6982, loss: 4.5468
	step 8039:lm_loss: 4.5185, ppl: 91.6975, loss: 4.5468
	step 8040:lm_loss: 4.5186, ppl: 91.7059, loss: 4.5469
	step 8041:lm_loss: 4.5186, ppl: 91.7061, loss: 4.5469
	step 8042:lm_loss: 4.5185, ppl: 91.7017, loss: 4.5469
	step 8043:lm_loss: 4.5186, ppl: 91.7032, loss: 4.5469
	step 8044:lm_loss: 4.5186, ppl: 91.7098, loss: 4.5469
	step 8045:lm_loss: 4.5185, ppl: 91.7007, loss: 4.5468
	step 8046:lm_loss: 4.5185, ppl: 91.6984, loss: 4.5468
	step 8047:lm_loss: 4.5185, ppl: 91.7002, loss: 4.5468
	step 8048:lm_loss: 4.5185, ppl: 91.6992, loss: 4.5468
	step 8049:lm_loss: 4.5185, ppl: 91.6975, loss: 4.5468
	step 8050:lm_loss: 4.5184, ppl: 91.6880, loss: 4.5467
	step 8051:lm_loss: 4.5183, ppl: 91.6787, loss: 4.5465
	step 8052:lm_loss: 4.5183, ppl: 91.6838, loss: 4.5466
	step 8053:lm_loss: 4.5184, ppl: 91.6913, loss: 4.5466
	step 8054:lm_loss: 4.5186, ppl: 91.7073, loss: 4.5468
	step 8055:lm_loss: 4.5186, ppl: 91.7052, loss: 4.5468
	step 8056:lm_loss: 4.5186, ppl: 91.7052, loss: 4.5468
	step 8057:lm_loss: 4.5186, ppl: 91.7056, loss: 4.5468
	step 8058:lm_loss: 4.5185, ppl: 91.7000, loss: 4.5468
	step 8059:lm_loss: 4.5187, ppl: 91.7153, loss: 4.5470
	step 8060:lm_loss: 4.5186, ppl: 91.7030, loss: 4.5469
	step 8061:lm_loss: 4.5185, ppl: 91.6974, loss: 4.5469
	step 8062:lm_loss: 4.5184, ppl: 91.6876, loss: 4.5467
	step 8063:lm_loss: 4.5184, ppl: 91.6866, loss: 4.5467
	step 8064:lm_loss: 4.5185, ppl: 91.6942, loss: 4.5468
	step 8065:lm_loss: 4.5185, ppl: 91.6946, loss: 4.5468
	step 8066:lm_loss: 4.5186, ppl: 91.7028, loss: 4.5468
	step 8067:lm_loss: 4.5186, ppl: 91.7082, loss: 4.5469
	step 8068:lm_loss: 4.5187, ppl: 91.7133, loss: 4.5469
	step 8069:lm_loss: 4.5186, ppl: 91.7102, loss: 4.5469
	step 8070:lm_loss: 4.5186, ppl: 91.7067, loss: 4.5469
	step 8071:lm_loss: 4.5184, ppl: 91.6891, loss: 4.5467
	step 8072:lm_loss: 4.5185, ppl: 91.6961, loss: 4.5468
	step 8073:lm_loss: 4.5185, ppl: 91.6987, loss: 4.5468
	step 8074:lm_loss: 4.5185, ppl: 91.6990, loss: 4.5468
	step 8075:lm_loss: 4.5186, ppl: 91.7072, loss: 4.5469
	step 8076:lm_loss: 4.5186, ppl: 91.7067, loss: 4.5469
	step 8077:lm_loss: 4.5187, ppl: 91.7158, loss: 4.5470
	step 8078:lm_loss: 4.5187, ppl: 91.7205, loss: 4.5470
	step 8079:lm_loss: 4.5188, ppl: 91.7284, loss: 4.5471
	step 8080:lm_loss: 4.5189, ppl: 91.7326, loss: 4.5471
	step 8081:lm_loss: 4.5190, ppl: 91.7398, loss: 4.5471
	step 8082:lm_loss: 4.5191, ppl: 91.7521, loss: 4.5472
	step 8083:lm_loss: 4.5191, ppl: 91.7494, loss: 4.5472
	step 8084:lm_loss: 4.5191, ppl: 91.7573, loss: 4.5473
	step 8085:lm_loss: 4.5193, ppl: 91.7691, loss: 4.5474
	step 8086:lm_loss: 4.5194, ppl: 91.7771, loss: 4.5475
	step 8087:lm_loss: 4.5195, ppl: 91.7924, loss: 4.5476
	step 8088:lm_loss: 4.5196, ppl: 91.7980, loss: 4.5477
	step 8089:lm_loss: 4.5195, ppl: 91.7916, loss: 4.5476
	step 8090:lm_loss: 4.5196, ppl: 91.7985, loss: 4.5477
	step 8091:lm_loss: 4.5197, ppl: 91.8047, loss: 4.5478
	step 8092:lm_loss: 4.5196, ppl: 91.7943, loss: 4.5477
	step 8093:lm_loss: 4.5195, ppl: 91.7940, loss: 4.5477
	step 8094:lm_loss: 4.5196, ppl: 91.7989, loss: 4.5477
	step 8095:lm_loss: 4.5196, ppl: 91.8010, loss: 4.5478
	step 8096:lm_loss: 4.5197, ppl: 91.8050, loss: 4.5478
	step 8097:lm_loss: 4.5198, ppl: 91.8188, loss: 4.5479
	step 8098:lm_loss: 4.5200, ppl: 91.8313, loss: 4.5481
	step 8099:lm_loss: 4.5199, ppl: 91.8287, loss: 4.5480
	step 8100:lm_loss: 4.5197, ppl: 91.8106, loss: 4.5479
	step 8101:lm_loss: 4.5197, ppl: 91.8114, loss: 4.5479
	step 8102:lm_loss: 4.5197, ppl: 91.8110, loss: 4.5479
	step 8103:lm_loss: 4.5198, ppl: 91.8156, loss: 4.5479
	step 8104:lm_loss: 4.5198, ppl: 91.8183, loss: 4.5480
	step 8105:lm_loss: 4.5199, ppl: 91.8247, loss: 4.5481
	step 8106:lm_loss: 4.5197, ppl: 91.8094, loss: 4.5480
	step 8107:lm_loss: 4.5199, ppl: 91.8283, loss: 4.5482
	step 8108:lm_loss: 4.5201, ppl: 91.8417, loss: 4.5483
	step 8109:lm_loss: 4.5201, ppl: 91.8402, loss: 4.5483
	step 8110:lm_loss: 4.5201, ppl: 91.8427, loss: 4.5483
	step 8111:lm_loss: 4.5202, ppl: 91.8550, loss: 4.5484
	step 8112:lm_loss: 4.5202, ppl: 91.8562, loss: 4.5484
	step 8113:lm_loss: 4.5204, ppl: 91.8723, loss: 4.5486
	step 8114:lm_loss: 4.5203, ppl: 91.8676, loss: 4.5486
	step 8115:lm_loss: 4.5204, ppl: 91.8737, loss: 4.5487
	step 8116:lm_loss: 4.5205, ppl: 91.8791, loss: 4.5488
	step 8117:lm_loss: 4.5205, ppl: 91.8794, loss: 4.5488
	step 8118:lm_loss: 4.5204, ppl: 91.8769, loss: 4.5487
	step 8119:lm_loss: 4.5202, ppl: 91.8565, loss: 4.5486
	step 8120:lm_loss: 4.5203, ppl: 91.8595, loss: 4.5486
	step 8121:lm_loss: 4.5202, ppl: 91.8526, loss: 4.5486
	step 8122:lm_loss: 4.5201, ppl: 91.8433, loss: 4.5484
	step 8123:lm_loss: 4.5200, ppl: 91.8325, loss: 4.5483
	step 8124:lm_loss: 4.5199, ppl: 91.8304, loss: 4.5483
	step 8125:lm_loss: 4.5200, ppl: 91.8330, loss: 4.5483
	step 8126:lm_loss: 4.5201, ppl: 91.8430, loss: 4.5484
	step 8127:lm_loss: 4.5200, ppl: 91.8377, loss: 4.5484
	step 8128:lm_loss: 4.5202, ppl: 91.8499, loss: 4.5484
	step 8129:lm_loss: 4.5201, ppl: 91.8437, loss: 4.5484
	step 8130:lm_loss: 4.5201, ppl: 91.8425, loss: 4.5484
	step 8131:lm_loss: 4.5202, ppl: 91.8568, loss: 4.5485
	step 8132:lm_loss: 4.5203, ppl: 91.8635, loss: 4.5486
	step 8133:lm_loss: 4.5202, ppl: 91.8551, loss: 4.5485
	step 8134:lm_loss: 4.5202, ppl: 91.8561, loss: 4.5485
	step 8135:lm_loss: 4.5202, ppl: 91.8565, loss: 4.5485
	step 8136:lm_loss: 4.5202, ppl: 91.8565, loss: 4.5485
	step 8137:lm_loss: 4.5202, ppl: 91.8545, loss: 4.5485
	step 8138:lm_loss: 4.5201, ppl: 91.8489, loss: 4.5484
	step 8139:lm_loss: 4.5201, ppl: 91.8488, loss: 4.5484
	step 8140:lm_loss: 4.5201, ppl: 91.8489, loss: 4.5484
	step 8141:lm_loss: 4.5201, ppl: 91.8449, loss: 4.5484
	step 8142:lm_loss: 4.5202, ppl: 91.8528, loss: 4.5484
	step 8143:lm_loss: 4.5202, ppl: 91.8563, loss: 4.5485
	step 8144:lm_loss: 4.5202, ppl: 91.8569, loss: 4.5485
	step 8145:lm_loss: 4.5202, ppl: 91.8575, loss: 4.5485
	step 8146:lm_loss: 4.5203, ppl: 91.8631, loss: 4.5485
	step 8147:lm_loss: 4.5203, ppl: 91.8587, loss: 4.5484
	step 8148:lm_loss: 4.5204, ppl: 91.8688, loss: 4.5485
	step 8149:lm_loss: 4.5203, ppl: 91.8618, loss: 4.5484
	step 8150:lm_loss: 4.5203, ppl: 91.8591, loss: 4.5484
	step 8151:lm_loss: 4.5202, ppl: 91.8578, loss: 4.5484
	step 8152:lm_loss: 4.5202, ppl: 91.8529, loss: 4.5483
	step 8153:lm_loss: 4.5202, ppl: 91.8557, loss: 4.5483
	step 8154:lm_loss: 4.5203, ppl: 91.8638, loss: 4.5485
	step 8155:lm_loss: 4.5202, ppl: 91.8570, loss: 4.5484
	step 8156:lm_loss: 4.5203, ppl: 91.8665, loss: 4.5485
	step 8157:lm_loss: 4.5205, ppl: 91.8778, loss: 4.5486
	step 8158:lm_loss: 4.5204, ppl: 91.8763, loss: 4.5485
	step 8159:lm_loss: 4.5203, ppl: 91.8651, loss: 4.5485
	step 8160:lm_loss: 4.5205, ppl: 91.8783, loss: 4.5486
	step 8161:lm_loss: 4.5204, ppl: 91.8735, loss: 4.5485
	step 8162:lm_loss: 4.5204, ppl: 91.8714, loss: 4.5485
	step 8163:lm_loss: 4.5201, ppl: 91.8479, loss: 4.5483
	step 8164:lm_loss: 4.5201, ppl: 91.8489, loss: 4.5483
	step 8165:lm_loss: 4.5203, ppl: 91.8634, loss: 4.5484
	step 8166:lm_loss: 4.5202, ppl: 91.8516, loss: 4.5483
	step 8167:lm_loss: 4.5202, ppl: 91.8553, loss: 4.5483
	step 8168:lm_loss: 4.5202, ppl: 91.8532, loss: 4.5483
	step 8169:lm_loss: 4.5202, ppl: 91.8502, loss: 4.5482
	step 8170:lm_loss: 4.5202, ppl: 91.8570, loss: 4.5483
	step 8171:lm_loss: 4.5200, ppl: 91.8386, loss: 4.5481
	step 8172:lm_loss: 4.5200, ppl: 91.8360, loss: 4.5481
	step 8173:lm_loss: 4.5200, ppl: 91.8382, loss: 4.5481
	step 8174:lm_loss: 4.5200, ppl: 91.8340, loss: 4.5480
	step 8175:lm_loss: 4.5200, ppl: 91.8323, loss: 4.5480
	step 8176:lm_loss: 4.5201, ppl: 91.8437, loss: 4.5481
	step 8177:lm_loss: 4.5201, ppl: 91.8420, loss: 4.5481
	step 8178:lm_loss: 4.5201, ppl: 91.8488, loss: 4.5481
	step 8179:lm_loss: 4.5202, ppl: 91.8516, loss: 4.5482
	step 8180:lm_loss: 4.5204, ppl: 91.8690, loss: 4.5483
	step 8181:lm_loss: 4.5203, ppl: 91.8610, loss: 4.5482
	step 8182:lm_loss: 4.5203, ppl: 91.8649, loss: 4.5482
	step 8183:lm_loss: 4.5202, ppl: 91.8528, loss: 4.5481
	step 8184:lm_loss: 4.5200, ppl: 91.8311, loss: 4.5479
	step 8185:lm_loss: 4.5201, ppl: 91.8404, loss: 4.5479
	step 8186:lm_loss: 4.5201, ppl: 91.8402, loss: 4.5479
	step 8187:lm_loss: 4.5201, ppl: 91.8440, loss: 4.5480
	step 8188:lm_loss: 4.5201, ppl: 91.8482, loss: 4.5480
	step 8189:lm_loss: 4.5200, ppl: 91.8387, loss: 4.5479
	step 8190:lm_loss: 4.5200, ppl: 91.8347, loss: 4.5479
	step 8191:lm_loss: 4.5200, ppl: 91.8337, loss: 4.5478
	step 8192:lm_loss: 4.5198, ppl: 91.8156, loss: 4.5477
	step 8193:lm_loss: 4.5197, ppl: 91.8058, loss: 4.5476
	step 8194:lm_loss: 4.5197, ppl: 91.8067, loss: 4.5476
	step 8195:lm_loss: 4.5197, ppl: 91.8075, loss: 4.5476
	step 8196:lm_loss: 4.5197, ppl: 91.8124, loss: 4.5477
	step 8197:lm_loss: 4.5200, ppl: 91.8323, loss: 4.5478
	step 8198:lm_loss: 4.5200, ppl: 91.8317, loss: 4.5478
	step 8199:lm_loss: 4.5200, ppl: 91.8369, loss: 4.5479
	step 8200:lm_loss: 4.5200, ppl: 91.8378, loss: 4.5479
	step 8201:lm_loss: 4.5201, ppl: 91.8492, loss: 4.5481
	step 8202:lm_loss: 4.5202, ppl: 91.8506, loss: 4.5481
	step 8203:lm_loss: 4.5201, ppl: 91.8480, loss: 4.5481
	step 8204:lm_loss: 4.5202, ppl: 91.8571, loss: 4.5482
	step 8205:lm_loss: 4.5202, ppl: 91.8547, loss: 4.5482
	step 8206:lm_loss: 4.5202, ppl: 91.8516, loss: 4.5482
	step 8207:lm_loss: 4.5201, ppl: 91.8484, loss: 4.5481
	step 8208:lm_loss: 4.5202, ppl: 91.8499, loss: 4.5482
	step 8209:lm_loss: 4.5202, ppl: 91.8520, loss: 4.5482
	step 8210:lm_loss: 4.5202, ppl: 91.8527, loss: 4.5482
	step 8211:lm_loss: 4.5202, ppl: 91.8505, loss: 4.5481
	step 8212:lm_loss: 4.5202, ppl: 91.8541, loss: 4.5482
	step 8213:lm_loss: 4.5202, ppl: 91.8530, loss: 4.5482
	step 8214:lm_loss: 4.5202, ppl: 91.8527, loss: 4.5482
	step 8215:lm_loss: 4.5202, ppl: 91.8515, loss: 4.5482
	step 8216:lm_loss: 4.5203, ppl: 91.8642, loss: 4.5483
	step 8217:lm_loss: 4.5201, ppl: 91.8406, loss: 4.5482
	step 8218:lm_loss: 4.5201, ppl: 91.8422, loss: 4.5482
	step 8219:lm_loss: 4.5201, ppl: 91.8489, loss: 4.5483
	step 8220:lm_loss: 4.5201, ppl: 91.8493, loss: 4.5483
	step 8221:lm_loss: 4.5203, ppl: 91.8643, loss: 4.5484
	step 8222:lm_loss: 4.5203, ppl: 91.8629, loss: 4.5484
	step 8223:lm_loss: 4.5204, ppl: 91.8727, loss: 4.5486
	step 8224:lm_loss: 4.5202, ppl: 91.8568, loss: 4.5484
	step 8225:lm_loss: 4.5202, ppl: 91.8516, loss: 4.5483
	step 8226:lm_loss: 4.5202, ppl: 91.8530, loss: 4.5483
	step 8227:lm_loss: 4.5202, ppl: 91.8520, loss: 4.5483
	step 8228:lm_loss: 4.5200, ppl: 91.8395, loss: 4.5482
	step 8229:lm_loss: 4.5201, ppl: 91.8485, loss: 4.5483
	step 8230:lm_loss: 4.5202, ppl: 91.8500, loss: 4.5483
	step 8231:lm_loss: 4.5201, ppl: 91.8480, loss: 4.5483
	step 8232:lm_loss: 4.5201, ppl: 91.8409, loss: 4.5482
	step 8233:lm_loss: 4.5200, ppl: 91.8338, loss: 4.5481
	step 8234:lm_loss: 4.5200, ppl: 91.8384, loss: 4.5481
	step 8235:lm_loss: 4.5200, ppl: 91.8354, loss: 4.5481
	step 8236:lm_loss: 4.5199, ppl: 91.8239, loss: 4.5479
	step 8237:lm_loss: 4.5192, ppl: 91.7657, loss: 4.5478
	step 8238:lm_loss: 4.5194, ppl: 91.7787, loss: 4.5479
	step 8239:lm_loss: 4.5190, ppl: 91.7461, loss: 4.5478
	step 8240:lm_loss: 4.5190, ppl: 91.7393, loss: 4.5477
	step 8241:lm_loss: 4.5190, ppl: 91.7460, loss: 4.5477
	step 8242:lm_loss: 4.5190, ppl: 91.7469, loss: 4.5477
	step 8243:lm_loss: 4.5188, ppl: 91.7276, loss: 4.5476
	step 8244:lm_loss: 4.5190, ppl: 91.7402, loss: 4.5477
	step 8245:lm_loss: 4.5190, ppl: 91.7426, loss: 4.5477
	step 8246:lm_loss: 4.5189, ppl: 91.7335, loss: 4.5476
	step 8247:lm_loss: 4.5190, ppl: 91.7408, loss: 4.5477
	step 8248:lm_loss: 4.5190, ppl: 91.7409, loss: 4.5477
	step 8249:lm_loss: 4.5190, ppl: 91.7416, loss: 4.5477
	step 8250:lm_loss: 4.5190, ppl: 91.7442, loss: 4.5477
	step 8251:lm_loss: 4.5190, ppl: 91.7424, loss: 4.5477
	step 8252:lm_loss: 4.5187, ppl: 91.7195, loss: 4.5475
	step 8253:lm_loss: 4.5188, ppl: 91.7248, loss: 4.5476
	step 8254:lm_loss: 4.5188, ppl: 91.7261, loss: 4.5476
	step 8255:lm_loss: 4.5188, ppl: 91.7258, loss: 4.5476
	step 8256:lm_loss: 4.5186, ppl: 91.7040, loss: 4.5475
	step 8257:lm_loss: 4.5187, ppl: 91.7163, loss: 4.5476
	step 8258:lm_loss: 4.5188, ppl: 91.7226, loss: 4.5477
	step 8259:lm_loss: 4.5188, ppl: 91.7234, loss: 4.5477
	step 8260:lm_loss: 4.5188, ppl: 91.7239, loss: 4.5477
	step 8261:lm_loss: 4.5188, ppl: 91.7295, loss: 4.5478
	step 8262:lm_loss: 4.5188, ppl: 91.7294, loss: 4.5478
	step 8263:lm_loss: 4.5189, ppl: 91.7311, loss: 4.5478
	step 8264:lm_loss: 4.5187, ppl: 91.7191, loss: 4.5476
	step 8265:lm_loss: 4.5186, ppl: 91.7067, loss: 4.5475
	step 8266:lm_loss: 4.5187, ppl: 91.7126, loss: 4.5475
	step 8267:lm_loss: 4.5189, ppl: 91.7356, loss: 4.5477
	step 8268:lm_loss: 4.5189, ppl: 91.7357, loss: 4.5477
	step 8269:lm_loss: 4.5189, ppl: 91.7340, loss: 4.5477
	step 8270:lm_loss: 4.5190, ppl: 91.7444, loss: 4.5478
	step 8271:lm_loss: 4.5190, ppl: 91.7434, loss: 4.5478
	step 8272:lm_loss: 4.5191, ppl: 91.7504, loss: 4.5478
	step 8273:lm_loss: 4.5191, ppl: 91.7531, loss: 4.5479
	step 8274:lm_loss: 4.5192, ppl: 91.7624, loss: 4.5480
	step 8275:lm_loss: 4.5194, ppl: 91.7763, loss: 4.5481
	step 8276:lm_loss: 4.5194, ppl: 91.7773, loss: 4.5481
	step 8277:lm_loss: 4.5194, ppl: 91.7839, loss: 4.5482
	step 8278:lm_loss: 4.5194, ppl: 91.7831, loss: 4.5481
	step 8279:lm_loss: 4.5194, ppl: 91.7825, loss: 4.5481
	step 8280:lm_loss: 4.5195, ppl: 91.7857, loss: 4.5482
	step 8281:lm_loss: 4.5194, ppl: 91.7776, loss: 4.5481
	step 8282:lm_loss: 4.5194, ppl: 91.7791, loss: 4.5481
	step 8283:lm_loss: 4.5194, ppl: 91.7834, loss: 4.5481
	step 8284:lm_loss: 4.5194, ppl: 91.7836, loss: 4.5481
	step 8285:lm_loss: 4.5195, ppl: 91.7882, loss: 4.5482
	step 8286:lm_loss: 4.5193, ppl: 91.7735, loss: 4.5481
	step 8287:lm_loss: 4.5193, ppl: 91.7689, loss: 4.5480
	step 8288:lm_loss: 4.5193, ppl: 91.7689, loss: 4.5480
	step 8289:lm_loss: 4.5193, ppl: 91.7671, loss: 4.5480
	step 8290:lm_loss: 4.5188, ppl: 91.7277, loss: 4.5477
	step 8291:lm_loss: 4.5189, ppl: 91.7341, loss: 4.5478
	step 8292:lm_loss: 4.5189, ppl: 91.7317, loss: 4.5478
	step 8293:lm_loss: 4.5188, ppl: 91.7299, loss: 4.5477
	step 8294:lm_loss: 4.5189, ppl: 91.7385, loss: 4.5478
	step 8295:lm_loss: 4.5189, ppl: 91.7391, loss: 4.5479
	step 8296:lm_loss: 4.5189, ppl: 91.7348, loss: 4.5478
	step 8297:lm_loss: 4.5189, ppl: 91.7343, loss: 4.5478
	step 8298:lm_loss: 4.5189, ppl: 91.7388, loss: 4.5479
	step 8299:lm_loss: 4.5188, ppl: 91.7289, loss: 4.5478
	step 8300:lm_loss: 4.5189, ppl: 91.7382, loss: 4.5480
	step 8301:lm_loss: 4.5189, ppl: 91.7386, loss: 4.5480
	step 8302:lm_loss: 4.5187, ppl: 91.7143, loss: 4.5478
	step 8303:lm_loss: 4.5187, ppl: 91.7134, loss: 4.5478
	step 8304:lm_loss: 4.5186, ppl: 91.7091, loss: 4.5477
	step 8305:lm_loss: 4.5187, ppl: 91.7127, loss: 4.5477
	step 8306:lm_loss: 4.5186, ppl: 91.7043, loss: 4.5477
	step 8307:lm_loss: 4.5186, ppl: 91.7089, loss: 4.5477
	step 8308:lm_loss: 4.5187, ppl: 91.7149, loss: 4.5478
	step 8309:lm_loss: 4.5187, ppl: 91.7130, loss: 4.5478
	step 8310:lm_loss: 4.5186, ppl: 91.7116, loss: 4.5477
	step 8311:lm_loss: 4.5187, ppl: 91.7147, loss: 4.5478
	step 8312:lm_loss: 4.5188, ppl: 91.7230, loss: 4.5478
	step 8313:lm_loss: 4.5187, ppl: 91.7120, loss: 4.5477
	step 8314:lm_loss: 4.5186, ppl: 91.7116, loss: 4.5477
	step 8315:lm_loss: 4.5186, ppl: 91.7115, loss: 4.5477
	step 8316:lm_loss: 4.5186, ppl: 91.7072, loss: 4.5477
	step 8317:lm_loss: 4.5186, ppl: 91.7116, loss: 4.5477
	step 8318:lm_loss: 4.5187, ppl: 91.7175, loss: 4.5477
	step 8319:lm_loss: 4.5188, ppl: 91.7227, loss: 4.5478
	step 8320:lm_loss: 4.5188, ppl: 91.7212, loss: 4.5478
	step 8321:lm_loss: 4.5188, ppl: 91.7285, loss: 4.5478
	step 8322:lm_loss: 4.5189, ppl: 91.7304, loss: 4.5478
	step 8323:lm_loss: 4.5187, ppl: 91.7130, loss: 4.5477
	step 8324:lm_loss: 4.5187, ppl: 91.7200, loss: 4.5478
	step 8325:lm_loss: 4.5187, ppl: 91.7202, loss: 4.5478
	step 8326:lm_loss: 4.5189, ppl: 91.7318, loss: 4.5479
	step 8327:lm_loss: 4.5188, ppl: 91.7227, loss: 4.5478
	step 8328:lm_loss: 4.5188, ppl: 91.7231, loss: 4.5478
	step 8329:lm_loss: 4.5188, ppl: 91.7242, loss: 4.5478
	step 8330:lm_loss: 4.5188, ppl: 91.7296, loss: 4.5479
	step 8331:lm_loss: 4.5189, ppl: 91.7376, loss: 4.5480
	step 8332:lm_loss: 4.5189, ppl: 91.7361, loss: 4.5479
	step 8333:lm_loss: 4.5189, ppl: 91.7368, loss: 4.5480
	step 8334:lm_loss: 4.5189, ppl: 91.7384, loss: 4.5480
	step 8335:lm_loss: 4.5190, ppl: 91.7447, loss: 4.5480
	step 8336:lm_loss: 4.5190, ppl: 91.7422, loss: 4.5480
	step 8337:lm_loss: 4.5190, ppl: 91.7449, loss: 4.5480
	step 8338:lm_loss: 4.5189, ppl: 91.7309, loss: 4.5478
	step 8339:lm_loss: 4.5187, ppl: 91.7180, loss: 4.5477
	step 8340:lm_loss: 4.5188, ppl: 91.7265, loss: 4.5478
	step 8341:lm_loss: 4.5188, ppl: 91.7213, loss: 4.5477
	step 8342:lm_loss: 4.5188, ppl: 91.7257, loss: 4.5478
	step 8343:lm_loss: 4.5187, ppl: 91.7173, loss: 4.5477
	step 8344:lm_loss: 4.5187, ppl: 91.7155, loss: 4.5477
	step 8345:lm_loss: 4.5186, ppl: 91.7088, loss: 4.5476
	step 8346:lm_loss: 4.5187, ppl: 91.7159, loss: 4.5477
	step 8347:lm_loss: 4.5189, ppl: 91.7313, loss: 4.5478
	step 8348:lm_loss: 4.5188, ppl: 91.7300, loss: 4.5478
	step 8349:lm_loss: 4.5188, ppl: 91.7300, loss: 4.5478
	step 8350:lm_loss: 4.5189, ppl: 91.7361, loss: 4.5479
	step 8351:lm_loss: 4.5189, ppl: 91.7327, loss: 4.5479
	step 8352:lm_loss: 4.5188, ppl: 91.7250, loss: 4.5477
	step 8353:lm_loss: 4.5189, ppl: 91.7302, loss: 4.5478
	step 8354:lm_loss: 4.5188, ppl: 91.7276, loss: 4.5478
	step 8355:lm_loss: 4.5189, ppl: 91.7311, loss: 4.5478
	step 8356:lm_loss: 4.5186, ppl: 91.7061, loss: 4.5477
	step 8357:lm_loss: 4.5186, ppl: 91.7075, loss: 4.5477
	step 8358:lm_loss: 4.5185, ppl: 91.6976, loss: 4.5476
	step 8359:lm_loss: 4.5185, ppl: 91.7005, loss: 4.5477
	step 8360:lm_loss: 4.5185, ppl: 91.7019, loss: 4.5477
	step 8361:lm_loss: 4.5186, ppl: 91.7061, loss: 4.5477
	step 8362:lm_loss: 4.5184, ppl: 91.6879, loss: 4.5476
	step 8363:lm_loss: 4.5183, ppl: 91.6811, loss: 4.5475
	step 8364:lm_loss: 4.5183, ppl: 91.6780, loss: 4.5475
	step 8365:lm_loss: 4.5183, ppl: 91.6769, loss: 4.5475
	step 8366:lm_loss: 4.5182, ppl: 91.6686, loss: 4.5474
	step 8367:lm_loss: 4.5182, ppl: 91.6740, loss: 4.5475
	step 8368:lm_loss: 4.5181, ppl: 91.6641, loss: 4.5473
	step 8369:lm_loss: 4.5182, ppl: 91.6684, loss: 4.5474
	step 8370:lm_loss: 4.5181, ppl: 91.6656, loss: 4.5473
	step 8371:lm_loss: 4.5180, ppl: 91.6487, loss: 4.5472
	step 8372:lm_loss: 4.5178, ppl: 91.6323, loss: 4.5471
	step 8373:lm_loss: 4.5178, ppl: 91.6317, loss: 4.5471
	step 8374:lm_loss: 4.5178, ppl: 91.6316, loss: 4.5470
	step 8375:lm_loss: 4.5179, ppl: 91.6398, loss: 4.5471
	step 8376:lm_loss: 4.5179, ppl: 91.6442, loss: 4.5472
	step 8377:lm_loss: 4.5179, ppl: 91.6451, loss: 4.5472
	step 8378:lm_loss: 4.5180, ppl: 91.6562, loss: 4.5472
	step 8379:lm_loss: 4.5180, ppl: 91.6509, loss: 4.5471
	step 8380:lm_loss: 4.5180, ppl: 91.6476, loss: 4.5471
	step 8381:lm_loss: 4.5180, ppl: 91.6480, loss: 4.5471
	step 8382:lm_loss: 4.5181, ppl: 91.6627, loss: 4.5473
	step 8383:lm_loss: 4.5181, ppl: 91.6628, loss: 4.5473
	step 8384:lm_loss: 4.5180, ppl: 91.6502, loss: 4.5472
	step 8385:lm_loss: 4.5178, ppl: 91.6341, loss: 4.5471
	step 8386:lm_loss: 4.5178, ppl: 91.6370, loss: 4.5471
	step 8387:lm_loss: 4.5180, ppl: 91.6516, loss: 4.5473
	step 8388:lm_loss: 4.5179, ppl: 91.6403, loss: 4.5471
	step 8389:lm_loss: 4.5178, ppl: 91.6313, loss: 4.5470
	step 8390:lm_loss: 4.5178, ppl: 91.6296, loss: 4.5470
	step 8391:lm_loss: 4.5179, ppl: 91.6392, loss: 4.5471
	step 8392:lm_loss: 4.5179, ppl: 91.6451, loss: 4.5471
	step 8393:lm_loss: 4.5179, ppl: 91.6386, loss: 4.5471
	step 8394:lm_loss: 4.5177, ppl: 91.6290, loss: 4.5469
	step 8395:lm_loss: 4.5177, ppl: 91.6276, loss: 4.5469
	step 8396:lm_loss: 4.5176, ppl: 91.6193, loss: 4.5468
	step 8397:lm_loss: 4.5177, ppl: 91.6276, loss: 4.5469
	step 8398:lm_loss: 4.5178, ppl: 91.6331, loss: 4.5470
	step 8399:lm_loss: 4.5178, ppl: 91.6354, loss: 4.5470
	step 8400:lm_loss: 4.5179, ppl: 91.6466, loss: 4.5472
	step 8401:lm_loss: 4.5180, ppl: 91.6564, loss: 4.5473
	step 8402:lm_loss: 4.5181, ppl: 91.6576, loss: 4.5473
	step 8403:lm_loss: 4.5181, ppl: 91.6628, loss: 4.5473
	step 8404:lm_loss: 4.5179, ppl: 91.6436, loss: 4.5472
	step 8405:lm_loss: 4.5179, ppl: 91.6465, loss: 4.5472
	step 8406:lm_loss: 4.5180, ppl: 91.6490, loss: 4.5473
	step 8407:lm_loss: 4.5179, ppl: 91.6422, loss: 4.5472
	step 8408:lm_loss: 4.5179, ppl: 91.6395, loss: 4.5471
	step 8409:lm_loss: 4.5179, ppl: 91.6434, loss: 4.5471
	step 8410:lm_loss: 4.5177, ppl: 91.6269, loss: 4.5470
	step 8411:lm_loss: 4.5175, ppl: 91.6105, loss: 4.5468
	step 8412:lm_loss: 4.5175, ppl: 91.6055, loss: 4.5467
	step 8413:lm_loss: 4.5176, ppl: 91.6138, loss: 4.5468
	step 8414:lm_loss: 4.5176, ppl: 91.6169, loss: 4.5468
	step 8415:lm_loss: 4.5177, ppl: 91.6239, loss: 4.5469
	step 8416:lm_loss: 4.5175, ppl: 91.6083, loss: 4.5467
	step 8417:lm_loss: 4.5175, ppl: 91.6063, loss: 4.5467
	step 8418:lm_loss: 4.5176, ppl: 91.6149, loss: 4.5468
	step 8419:lm_loss: 4.5176, ppl: 91.6181, loss: 4.5469
	step 8420:lm_loss: 4.5175, ppl: 91.6069, loss: 4.5467
	step 8421:lm_loss: 4.5175, ppl: 91.6076, loss: 4.5467
	step 8422:lm_loss: 4.5176, ppl: 91.6153, loss: 4.5468
	step 8423:lm_loss: 4.5174, ppl: 91.5953, loss: 4.5466
	step 8424:lm_loss: 4.5174, ppl: 91.5996, loss: 4.5467
	step 8425:lm_loss: 4.5174, ppl: 91.5965, loss: 4.5466
	step 8426:lm_loss: 4.5174, ppl: 91.5968, loss: 4.5466
	step 8427:lm_loss: 4.5174, ppl: 91.5946, loss: 4.5466
	step 8428:lm_loss: 4.5174, ppl: 91.5987, loss: 4.5466
	step 8429:lm_loss: 4.5175, ppl: 91.6024, loss: 4.5467
	step 8430:lm_loss: 4.5175, ppl: 91.6064, loss: 4.5467
	step 8431:lm_loss: 4.5174, ppl: 91.5983, loss: 4.5466
	step 8432:lm_loss: 4.5174, ppl: 91.5936, loss: 4.5466
	step 8433:lm_loss: 4.5174, ppl: 91.5949, loss: 4.5466
	step 8434:lm_loss: 4.5174, ppl: 91.5927, loss: 4.5466
	step 8435:lm_loss: 4.5172, ppl: 91.5748, loss: 4.5464
	step 8436:lm_loss: 4.5170, ppl: 91.5590, loss: 4.5463
	step 8437:lm_loss: 4.5169, ppl: 91.5541, loss: 4.5462
	step 8438:lm_loss: 4.5169, ppl: 91.5543, loss: 4.5462
	step 8439:lm_loss: 4.5169, ppl: 91.5519, loss: 4.5462
	step 8440:lm_loss: 4.5169, ppl: 91.5483, loss: 4.5462
	step 8441:lm_loss: 4.5168, ppl: 91.5427, loss: 4.5461
	step 8442:lm_loss: 4.5167, ppl: 91.5327, loss: 4.5460
	step 8443:lm_loss: 4.5167, ppl: 91.5305, loss: 4.5460
	step 8444:lm_loss: 4.5166, ppl: 91.5247, loss: 4.5459
	step 8445:lm_loss: 4.5166, ppl: 91.5259, loss: 4.5459
	step 8446:lm_loss: 4.5166, ppl: 91.5276, loss: 4.5459
	step 8447:lm_loss: 4.5168, ppl: 91.5436, loss: 4.5462
	step 8448:lm_loss: 4.5169, ppl: 91.5497, loss: 4.5462
	step 8449:lm_loss: 4.5169, ppl: 91.5517, loss: 4.5463
	step 8450:lm_loss: 4.5170, ppl: 91.5598, loss: 4.5464
	step 8451:lm_loss: 4.5170, ppl: 91.5575, loss: 4.5463
	step 8452:lm_loss: 4.5170, ppl: 91.5570, loss: 4.5463
	step 8453:lm_loss: 4.5169, ppl: 91.5553, loss: 4.5463
	step 8454:lm_loss: 4.5170, ppl: 91.5575, loss: 4.5463
	step 8455:lm_loss: 4.5170, ppl: 91.5598, loss: 4.5464
	step 8456:lm_loss: 4.5170, ppl: 91.5612, loss: 4.5464
	step 8457:lm_loss: 4.5169, ppl: 91.5517, loss: 4.5463
	step 8458:lm_loss: 4.5169, ppl: 91.5529, loss: 4.5463
	step 8459:lm_loss: 4.5169, ppl: 91.5503, loss: 4.5462
	step 8460:lm_loss: 4.5168, ppl: 91.5410, loss: 4.5462
	step 8461:lm_loss: 4.5167, ppl: 91.5301, loss: 4.5460
	step 8462:lm_loss: 4.5168, ppl: 91.5453, loss: 4.5462
	step 8463:lm_loss: 4.5169, ppl: 91.5501, loss: 4.5463
	step 8464:lm_loss: 4.5167, ppl: 91.5357, loss: 4.5461
	step 8465:lm_loss: 4.5168, ppl: 91.5440, loss: 4.5462
	step 8466:lm_loss: 4.5168, ppl: 91.5401, loss: 4.5461
	step 8467:lm_loss: 4.5168, ppl: 91.5452, loss: 4.5462
	step 8468:lm_loss: 4.5169, ppl: 91.5503, loss: 4.5463
	step 8469:lm_loss: 4.5169, ppl: 91.5533, loss: 4.5463
	step 8470:lm_loss: 4.5170, ppl: 91.5572, loss: 4.5463
	step 8471:lm_loss: 4.5170, ppl: 91.5581, loss: 4.5463
	step 8472:lm_loss: 4.5168, ppl: 91.5431, loss: 4.5462
	step 8473:lm_loss: 4.5168, ppl: 91.5438, loss: 4.5462
	step 8474:lm_loss: 4.5170, ppl: 91.5560, loss: 4.5463
	step 8475:lm_loss: 4.5170, ppl: 91.5587, loss: 4.5464
	step 8476:lm_loss: 4.5171, ppl: 91.5740, loss: 4.5465
	step 8477:lm_loss: 4.5171, ppl: 91.5682, loss: 4.5464
	step 8478:lm_loss: 4.5172, ppl: 91.5746, loss: 4.5465
	step 8479:lm_loss: 4.5172, ppl: 91.5774, loss: 4.5465
	step 8480:lm_loss: 4.5172, ppl: 91.5751, loss: 4.5465
	step 8481:lm_loss: 4.5171, ppl: 91.5655, loss: 4.5463
	step 8482:lm_loss: 4.5170, ppl: 91.5578, loss: 4.5462
	step 8483:lm_loss: 4.5170, ppl: 91.5573, loss: 4.5462
	step 8484:lm_loss: 4.5171, ppl: 91.5675, loss: 4.5463
	step 8485:lm_loss: 4.5170, ppl: 91.5620, loss: 4.5462
	step 8486:lm_loss: 4.5170, ppl: 91.5646, loss: 4.5462
	step 8487:lm_loss: 4.5171, ppl: 91.5702, loss: 4.5463
	step 8488:lm_loss: 4.5170, ppl: 91.5577, loss: 4.5461
	step 8489:lm_loss: 4.5169, ppl: 91.5509, loss: 4.5461
	step 8490:lm_loss: 4.5169, ppl: 91.5469, loss: 4.5460
	step 8491:lm_loss: 4.5169, ppl: 91.5482, loss: 4.5460
	step 8492:lm_loss: 4.5167, ppl: 91.5360, loss: 4.5459
	step 8493:lm_loss: 4.5166, ppl: 91.5270, loss: 4.5458
	step 8494:lm_loss: 4.5165, ppl: 91.5192, loss: 4.5457
	step 8495:lm_loss: 4.5165, ppl: 91.5186, loss: 4.5456
	step 8496:lm_loss: 4.5165, ppl: 91.5192, loss: 4.5457
	step 8497:lm_loss: 4.5167, ppl: 91.5296, loss: 4.5457
	step 8498:lm_loss: 4.5166, ppl: 91.5283, loss: 4.5457
	step 8499:lm_loss: 4.5167, ppl: 91.5285, loss: 4.5457
	step 8500:lm_loss: 4.5166, ppl: 91.5252, loss: 4.5457
	step 8501:lm_loss: 4.5165, ppl: 91.5185, loss: 4.5456
	step 8502:lm_loss: 4.5165, ppl: 91.5142, loss: 4.5455
	step 8503:lm_loss: 4.5165, ppl: 91.5173, loss: 4.5456
	step 8504:lm_loss: 4.5165, ppl: 91.5149, loss: 4.5455
	step 8505:lm_loss: 4.5166, ppl: 91.5238, loss: 4.5456
	step 8506:lm_loss: 4.5168, ppl: 91.5387, loss: 4.5458
	step 8507:lm_loss: 4.5168, ppl: 91.5394, loss: 4.5458
	step 8508:lm_loss: 4.5168, ppl: 91.5377, loss: 4.5458
	step 8509:lm_loss: 4.5165, ppl: 91.5102, loss: 4.5457
	step 8510:lm_loss: 4.5164, ppl: 91.5042, loss: 4.5456
	step 8511:lm_loss: 4.5164, ppl: 91.5036, loss: 4.5456
	step 8512:lm_loss: 4.5164, ppl: 91.5044, loss: 4.5456
	step 8513:lm_loss: 4.5164, ppl: 91.5014, loss: 4.5456
	step 8514:lm_loss: 4.5163, ppl: 91.4956, loss: 4.5455
	step 8515:lm_loss: 4.5162, ppl: 91.4896, loss: 4.5455
	step 8516:lm_loss: 4.5164, ppl: 91.5078, loss: 4.5456
	step 8517:lm_loss: 4.5166, ppl: 91.5226, loss: 4.5458
	step 8518:lm_loss: 4.5166, ppl: 91.5205, loss: 4.5458
	step 8519:lm_loss: 4.5166, ppl: 91.5209, loss: 4.5458
	step 8520:lm_loss: 4.5165, ppl: 91.5185, loss: 4.5457
	step 8521:lm_loss: 4.5165, ppl: 91.5134, loss: 4.5457
	step 8522:lm_loss: 4.5166, ppl: 91.5195, loss: 4.5458
	step 8523:lm_loss: 4.5165, ppl: 91.5131, loss: 4.5457
	step 8524:lm_loss: 4.5166, ppl: 91.5240, loss: 4.5458
	step 8525:lm_loss: 4.5166, ppl: 91.5211, loss: 4.5458
	step 8526:lm_loss: 4.5166, ppl: 91.5268, loss: 4.5459
	step 8527:lm_loss: 4.5168, ppl: 91.5416, loss: 4.5461
	step 8528:lm_loss: 4.5166, ppl: 91.5240, loss: 4.5459
	step 8529:lm_loss: 4.5165, ppl: 91.5152, loss: 4.5458
	step 8530:lm_loss: 4.5165, ppl: 91.5155, loss: 4.5458
	step 8531:lm_loss: 4.5164, ppl: 91.5058, loss: 4.5457
	step 8532:lm_loss: 4.5164, ppl: 91.5056, loss: 4.5457
	step 8533:lm_loss: 4.5164, ppl: 91.5053, loss: 4.5457
	step 8534:lm_loss: 4.5165, ppl: 91.5107, loss: 4.5458
	step 8535:lm_loss: 4.5164, ppl: 91.5026, loss: 4.5457
	step 8536:lm_loss: 4.5163, ppl: 91.4938, loss: 4.5456
	step 8537:lm_loss: 4.5163, ppl: 91.4936, loss: 4.5456
	step 8538:lm_loss: 4.5162, ppl: 91.4845, loss: 4.5455
	step 8539:lm_loss: 4.5162, ppl: 91.4872, loss: 4.5455
	step 8540:lm_loss: 4.5163, ppl: 91.4954, loss: 4.5456
	step 8541:lm_loss: 4.5162, ppl: 91.4896, loss: 4.5455
	step 8542:lm_loss: 4.5163, ppl: 91.4945, loss: 4.5455
	step 8543:lm_loss: 4.5163, ppl: 91.4948, loss: 4.5456
	step 8544:lm_loss: 4.5162, ppl: 91.4914, loss: 4.5455
	step 8545:lm_loss: 4.5161, ppl: 91.4825, loss: 4.5455
	step 8546:lm_loss: 4.5162, ppl: 91.4839, loss: 4.5455
	step 8547:lm_loss: 4.5161, ppl: 91.4824, loss: 4.5454
	step 8548:lm_loss: 4.5161, ppl: 91.4741, loss: 4.5454
	step 8549:lm_loss: 4.5160, ppl: 91.4721, loss: 4.5454
	step 8550:lm_loss: 4.5161, ppl: 91.4776, loss: 4.5455
	step 8551:lm_loss: 4.5159, ppl: 91.4605, loss: 4.5453
	step 8552:lm_loss: 4.5159, ppl: 91.4566, loss: 4.5453
	step 8553:lm_loss: 4.5159, ppl: 91.4578, loss: 4.5453
	step 8554:lm_loss: 4.5159, ppl: 91.4583, loss: 4.5453
	step 8555:lm_loss: 4.5160, ppl: 91.4647, loss: 4.5454
	step 8556:lm_loss: 4.5160, ppl: 91.4693, loss: 4.5454
	step 8557:lm_loss: 4.5160, ppl: 91.4706, loss: 4.5454
	step 8558:lm_loss: 4.5160, ppl: 91.4665, loss: 4.5454
	step 8559:lm_loss: 4.5160, ppl: 91.4683, loss: 4.5454
	step 8560:lm_loss: 4.5159, ppl: 91.4615, loss: 4.5453
	step 8561:lm_loss: 4.5159, ppl: 91.4565, loss: 4.5453
	step 8562:lm_loss: 4.5158, ppl: 91.4548, loss: 4.5453
	step 8563:lm_loss: 4.5158, ppl: 91.4486, loss: 4.5452
	step 8564:lm_loss: 4.5157, ppl: 91.4440, loss: 4.5452
	step 8565:lm_loss: 4.5158, ppl: 91.4501, loss: 4.5452
	step 8566:lm_loss: 4.5159, ppl: 91.4596, loss: 4.5453
	step 8567:lm_loss: 4.5159, ppl: 91.4560, loss: 4.5453
	step 8568:lm_loss: 4.5160, ppl: 91.4687, loss: 4.5455
	step 8569:lm_loss: 4.5160, ppl: 91.4711, loss: 4.5455
	step 8570:lm_loss: 4.5160, ppl: 91.4725, loss: 4.5455
	step 8571:lm_loss: 4.5162, ppl: 91.4845, loss: 4.5456
	step 8572:lm_loss: 4.5161, ppl: 91.4739, loss: 4.5455
	step 8573:lm_loss: 4.5161, ppl: 91.4775, loss: 4.5456
	step 8574:lm_loss: 4.5162, ppl: 91.4872, loss: 4.5457
	step 8575:lm_loss: 4.5163, ppl: 91.5010, loss: 4.5459
	step 8576:lm_loss: 4.5164, ppl: 91.5018, loss: 4.5459
	step 8577:lm_loss: 4.5163, ppl: 91.4957, loss: 4.5458
	step 8578:lm_loss: 4.5164, ppl: 91.5055, loss: 4.5459
	step 8579:lm_loss: 4.5165, ppl: 91.5111, loss: 4.5460
	step 8580:lm_loss: 4.5165, ppl: 91.5145, loss: 4.5460
	step 8581:lm_loss: 4.5164, ppl: 91.5092, loss: 4.5460
	step 8582:lm_loss: 4.5165, ppl: 91.5172, loss: 4.5460
	step 8583:lm_loss: 4.5166, ppl: 91.5219, loss: 4.5461
	step 8584:lm_loss: 4.5166, ppl: 91.5197, loss: 4.5460
	step 8585:lm_loss: 4.5169, ppl: 91.5485, loss: 4.5463
	step 8586:lm_loss: 4.5169, ppl: 91.5534, loss: 4.5463
	step 8587:lm_loss: 4.5168, ppl: 91.5452, loss: 4.5462
	step 8588:lm_loss: 4.5170, ppl: 91.5565, loss: 4.5464
	step 8589:lm_loss: 4.5168, ppl: 91.5393, loss: 4.5462
	step 8590:lm_loss: 4.5168, ppl: 91.5458, loss: 4.5463
	step 8591:lm_loss: 4.5170, ppl: 91.5572, loss: 4.5464
	step 8592:lm_loss: 4.5170, ppl: 91.5559, loss: 4.5464
	step 8593:lm_loss: 4.5169, ppl: 91.5525, loss: 4.5463
	step 8594:lm_loss: 4.5170, ppl: 91.5569, loss: 4.5464
	step 8595:lm_loss: 4.5170, ppl: 91.5597, loss: 4.5464
	step 8596:lm_loss: 4.5170, ppl: 91.5650, loss: 4.5465
	step 8597:lm_loss: 4.5171, ppl: 91.5656, loss: 4.5465
	step 8598:lm_loss: 4.5171, ppl: 91.5660, loss: 4.5465
	step 8599:lm_loss: 4.5171, ppl: 91.5662, loss: 4.5465
	step 8600:lm_loss: 4.5170, ppl: 91.5628, loss: 4.5464
	step 8601:lm_loss: 4.5171, ppl: 91.5694, loss: 4.5465
	step 8602:lm_loss: 4.5172, ppl: 91.5743, loss: 4.5465
	step 8603:lm_loss: 4.5171, ppl: 91.5727, loss: 4.5465
	step 8604:lm_loss: 4.5169, ppl: 91.5551, loss: 4.5464
	step 8605:lm_loss: 4.5169, ppl: 91.5522, loss: 4.5464
	step 8606:lm_loss: 4.5170, ppl: 91.5570, loss: 4.5464
	step 8607:lm_loss: 4.5167, ppl: 91.5306, loss: 4.5462
	step 8608:lm_loss: 4.5167, ppl: 91.5338, loss: 4.5463
	step 8609:lm_loss: 4.5167, ppl: 91.5335, loss: 4.5463
	step 8610:lm_loss: 4.5168, ppl: 91.5388, loss: 4.5463
	step 8611:lm_loss: 4.5167, ppl: 91.5298, loss: 4.5462
	step 8612:lm_loss: 4.5166, ppl: 91.5227, loss: 4.5462
	step 8613:lm_loss: 4.5165, ppl: 91.5180, loss: 4.5461
	step 8614:lm_loss: 4.5165, ppl: 91.5175, loss: 4.5461
	step 8615:lm_loss: 4.5165, ppl: 91.5129, loss: 4.5461
	step 8616:lm_loss: 4.5166, ppl: 91.5216, loss: 4.5462
	step 8617:lm_loss: 4.5165, ppl: 91.5179, loss: 4.5462
	step 8618:lm_loss: 4.5166, ppl: 91.5211, loss: 4.5462
	step 8619:lm_loss: 4.5165, ppl: 91.5160, loss: 4.5461
	step 8620:lm_loss: 4.5165, ppl: 91.5189, loss: 4.5462
	step 8621:lm_loss: 4.5164, ppl: 91.5061, loss: 4.5461
	step 8622:lm_loss: 4.5163, ppl: 91.4986, loss: 4.5460
	step 8623:lm_loss: 4.5163, ppl: 91.4923, loss: 4.5459
	step 8624:lm_loss: 4.5164, ppl: 91.5038, loss: 4.5460
	step 8625:lm_loss: 4.5164, ppl: 91.5026, loss: 4.5460
	step 8626:lm_loss: 4.5164, ppl: 91.5055, loss: 4.5460
	step 8627:lm_loss: 4.5163, ppl: 91.4972, loss: 4.5460
	step 8628:lm_loss: 4.5163, ppl: 91.4957, loss: 4.5459
	step 8629:lm_loss: 4.5164, ppl: 91.5054, loss: 4.5460
	step 8630:lm_loss: 4.5164, ppl: 91.5042, loss: 4.5460
	step 8631:lm_loss: 4.5163, ppl: 91.4996, loss: 4.5460
	step 8632:lm_loss: 4.5165, ppl: 91.5179, loss: 4.5463
	step 8633:lm_loss: 4.5166, ppl: 91.5281, loss: 4.5464
	step 8634:lm_loss: 4.5167, ppl: 91.5292, loss: 4.5464
	step 8635:lm_loss: 4.5167, ppl: 91.5372, loss: 4.5465
	step 8636:lm_loss: 4.5168, ppl: 91.5430, loss: 4.5465
	step 8637:lm_loss: 4.5168, ppl: 91.5460, loss: 4.5466
	step 8638:lm_loss: 4.5169, ppl: 91.5504, loss: 4.5466
	step 8639:lm_loss: 4.5170, ppl: 91.5597, loss: 4.5467
	step 8640:lm_loss: 4.5166, ppl: 91.5275, loss: 4.5464
	step 8641:lm_loss: 4.5167, ppl: 91.5368, loss: 4.5466
	step 8642:lm_loss: 4.5167, ppl: 91.5370, loss: 4.5466
	step 8643:lm_loss: 4.5167, ppl: 91.5347, loss: 4.5465
	step 8644:lm_loss: 4.5167, ppl: 91.5337, loss: 4.5465
	step 8645:lm_loss: 4.5167, ppl: 91.5310, loss: 4.5465
	step 8646:lm_loss: 4.5168, ppl: 91.5410, loss: 4.5466
	step 8647:lm_loss: 4.5169, ppl: 91.5488, loss: 4.5467
	step 8648:lm_loss: 4.5169, ppl: 91.5534, loss: 4.5468
	step 8649:lm_loss: 4.5169, ppl: 91.5544, loss: 4.5468
	step 8650:lm_loss: 4.5169, ppl: 91.5501, loss: 4.5467
	step 8651:lm_loss: 4.5169, ppl: 91.5559, loss: 4.5468
	step 8652:lm_loss: 4.5170, ppl: 91.5576, loss: 4.5468
	step 8653:lm_loss: 4.5169, ppl: 91.5554, loss: 4.5468
	step 8654:lm_loss: 4.5169, ppl: 91.5520, loss: 4.5468
	step 8655:lm_loss: 4.5170, ppl: 91.5573, loss: 4.5468
	step 8656:lm_loss: 4.5170, ppl: 91.5590, loss: 4.5468
	step 8657:lm_loss: 4.5171, ppl: 91.5666, loss: 4.5469
	step 8658:lm_loss: 4.5171, ppl: 91.5681, loss: 4.5469
	step 8659:lm_loss: 4.5170, ppl: 91.5578, loss: 4.5468
	step 8660:lm_loss: 4.5169, ppl: 91.5531, loss: 4.5468
	step 8661:lm_loss: 4.5170, ppl: 91.5570, loss: 4.5468
	step 8662:lm_loss: 4.5169, ppl: 91.5495, loss: 4.5467
	step 8663:lm_loss: 4.5169, ppl: 91.5519, loss: 4.5467
	step 8664:lm_loss: 4.5170, ppl: 91.5620, loss: 4.5468
	step 8665:lm_loss: 4.5170, ppl: 91.5607, loss: 4.5468
	step 8666:lm_loss: 4.5169, ppl: 91.5519, loss: 4.5467
	step 8667:lm_loss: 4.5169, ppl: 91.5557, loss: 4.5468
	step 8668:lm_loss: 4.5170, ppl: 91.5624, loss: 4.5468
	step 8669:lm_loss: 4.5170, ppl: 91.5628, loss: 4.5468
	step 8670:lm_loss: 4.5171, ppl: 91.5661, loss: 4.5468
	step 8671:lm_loss: 4.5170, ppl: 91.5640, loss: 4.5468
	step 8672:lm_loss: 4.5171, ppl: 91.5710, loss: 4.5469
	step 8673:lm_loss: 4.5171, ppl: 91.5660, loss: 4.5468
	step 8674:lm_loss: 4.5170, ppl: 91.5588, loss: 4.5467
	step 8675:lm_loss: 4.5171, ppl: 91.5673, loss: 4.5468
	step 8676:lm_loss: 4.5170, ppl: 91.5575, loss: 4.5468
	step 8677:lm_loss: 4.5169, ppl: 91.5484, loss: 4.5466
	step 8678:lm_loss: 4.5170, ppl: 91.5595, loss: 4.5468
	step 8679:lm_loss: 4.5169, ppl: 91.5551, loss: 4.5467
	step 8680:lm_loss: 4.5170, ppl: 91.5587, loss: 4.5468
	step 8681:lm_loss: 4.5169, ppl: 91.5531, loss: 4.5467
	step 8682:lm_loss: 4.5169, ppl: 91.5511, loss: 4.5467
	step 8683:lm_loss: 4.5169, ppl: 91.5491, loss: 4.5466
	step 8684:lm_loss: 4.5169, ppl: 91.5488, loss: 4.5466
	step 8685:lm_loss: 4.5169, ppl: 91.5506, loss: 4.5467
	step 8686:lm_loss: 4.5170, ppl: 91.5611, loss: 4.5468
	step 8687:lm_loss: 4.5171, ppl: 91.5669, loss: 4.5468
	step 8688:lm_loss: 4.5172, ppl: 91.5766, loss: 4.5469
	step 8689:lm_loss: 4.5172, ppl: 91.5795, loss: 4.5470
	step 8690:lm_loss: 4.5171, ppl: 91.5718, loss: 4.5468
	step 8691:lm_loss: 4.5172, ppl: 91.5809, loss: 4.5470
	step 8692:lm_loss: 4.5173, ppl: 91.5849, loss: 4.5470
	step 8693:lm_loss: 4.5170, ppl: 91.5639, loss: 4.5468
	step 8694:lm_loss: 4.5170, ppl: 91.5601, loss: 4.5467
	step 8695:lm_loss: 4.5171, ppl: 91.5713, loss: 4.5468
	step 8696:lm_loss: 4.5170, ppl: 91.5649, loss: 4.5468
	step 8697:lm_loss: 4.5171, ppl: 91.5692, loss: 4.5468
	step 8698:lm_loss: 4.5173, ppl: 91.5839, loss: 4.5470
	step 8699:lm_loss: 4.5174, ppl: 91.5929, loss: 4.5471
	step 8700:lm_loss: 4.5175, ppl: 91.6084, loss: 4.5472
	step 8701:lm_loss: 4.5175, ppl: 91.6019, loss: 4.5471
	step 8702:lm_loss: 4.5175, ppl: 91.6059, loss: 4.5472
	step 8703:lm_loss: 4.5175, ppl: 91.6074, loss: 4.5472
	step 8704:lm_loss: 4.5175, ppl: 91.6107, loss: 4.5472
	step 8705:lm_loss: 4.5175, ppl: 91.6029, loss: 4.5471
	step 8706:lm_loss: 4.5175, ppl: 91.6064, loss: 4.5472
	step 8707:lm_loss: 4.5174, ppl: 91.5986, loss: 4.5471
	step 8708:lm_loss: 4.5175, ppl: 91.6023, loss: 4.5472
	step 8709:lm_loss: 4.5171, ppl: 91.5682, loss: 4.5469
	step 8710:lm_loss: 4.5170, ppl: 91.5576, loss: 4.5468
	step 8711:lm_loss: 4.5170, ppl: 91.5640, loss: 4.5469
	step 8712:lm_loss: 4.5169, ppl: 91.5518, loss: 4.5468
	step 8713:lm_loss: 4.5167, ppl: 91.5296, loss: 4.5466
	step 8714:lm_loss: 4.5166, ppl: 91.5265, loss: 4.5466
	step 8715:lm_loss: 4.5167, ppl: 91.5312, loss: 4.5467
	step 8716:lm_loss: 4.5168, ppl: 91.5394, loss: 4.5467
	step 8717:lm_loss: 4.5168, ppl: 91.5462, loss: 4.5468
	step 8718:lm_loss: 4.5168, ppl: 91.5467, loss: 4.5468
	step 8719:lm_loss: 4.5170, ppl: 91.5584, loss: 4.5469
	step 8720:lm_loss: 4.5170, ppl: 91.5611, loss: 4.5469
	step 8721:lm_loss: 4.5170, ppl: 91.5624, loss: 4.5469
	step 8722:lm_loss: 4.5170, ppl: 91.5625, loss: 4.5469
	step 8723:lm_loss: 4.5170, ppl: 91.5586, loss: 4.5469
	step 8724:lm_loss: 4.5170, ppl: 91.5579, loss: 4.5469
	step 8725:lm_loss: 4.5171, ppl: 91.5715, loss: 4.5470
	step 8726:lm_loss: 4.5172, ppl: 91.5769, loss: 4.5471
	step 8727:lm_loss: 4.5171, ppl: 91.5731, loss: 4.5470
	step 8728:lm_loss: 4.5170, ppl: 91.5623, loss: 4.5469
	step 8729:lm_loss: 4.5171, ppl: 91.5712, loss: 4.5470
	step 8730:lm_loss: 4.5171, ppl: 91.5709, loss: 4.5470
	step 8731:lm_loss: 4.5171, ppl: 91.5729, loss: 4.5470
	step 8732:lm_loss: 4.5172, ppl: 91.5778, loss: 4.5471
	step 8733:lm_loss: 4.5171, ppl: 91.5730, loss: 4.5471
	step 8734:lm_loss: 4.5171, ppl: 91.5712, loss: 4.5470
	step 8735:lm_loss: 4.5172, ppl: 91.5763, loss: 4.5471
	step 8736:lm_loss: 4.5171, ppl: 91.5742, loss: 4.5470
	step 8737:lm_loss: 4.5172, ppl: 91.5758, loss: 4.5470
	step 8738:lm_loss: 4.5172, ppl: 91.5776, loss: 4.5471
	step 8739:lm_loss: 4.5172, ppl: 91.5834, loss: 4.5471
	step 8740:lm_loss: 4.5173, ppl: 91.5846, loss: 4.5472
	step 8741:lm_loss: 4.5172, ppl: 91.5803, loss: 4.5471
	step 8742:lm_loss: 4.5171, ppl: 91.5738, loss: 4.5471
	step 8743:lm_loss: 4.5171, ppl: 91.5708, loss: 4.5470
	step 8744:lm_loss: 4.5171, ppl: 91.5704, loss: 4.5470
	step 8745:lm_loss: 4.5170, ppl: 91.5569, loss: 4.5468
	step 8746:lm_loss: 4.5168, ppl: 91.5406, loss: 4.5466
	step 8747:lm_loss: 4.5169, ppl: 91.5502, loss: 4.5468
	step 8748:lm_loss: 4.5169, ppl: 91.5551, loss: 4.5468
	step 8749:lm_loss: 4.5169, ppl: 91.5511, loss: 4.5468
	step 8750:lm_loss: 4.5169, ppl: 91.5481, loss: 4.5467
	step 8751:lm_loss: 4.5168, ppl: 91.5443, loss: 4.5467
	step 8752:lm_loss: 4.5169, ppl: 91.5516, loss: 4.5468
	step 8753:lm_loss: 4.5168, ppl: 91.5435, loss: 4.5467
	step 8754:lm_loss: 4.5167, ppl: 91.5353, loss: 4.5466
	step 8755:lm_loss: 4.5167, ppl: 91.5370, loss: 4.5466
	step 8756:lm_loss: 4.5167, ppl: 91.5285, loss: 4.5465
	step 8757:lm_loss: 4.5167, ppl: 91.5340, loss: 4.5466
	step 8758:lm_loss: 4.5167, ppl: 91.5331, loss: 4.5466
	step 8759:lm_loss: 4.5168, ppl: 91.5397, loss: 4.5467
	step 8760:lm_loss: 4.5168, ppl: 91.5385, loss: 4.5466
	step 8761:lm_loss: 4.5169, ppl: 91.5472, loss: 4.5467
	step 8762:lm_loss: 4.5168, ppl: 91.5413, loss: 4.5466
	step 8763:lm_loss: 4.5167, ppl: 91.5330, loss: 4.5465
	step 8764:lm_loss: 4.5168, ppl: 91.5409, loss: 4.5466
	step 8765:lm_loss: 4.5167, ppl: 91.5364, loss: 4.5466
	step 8766:lm_loss: 4.5168, ppl: 91.5417, loss: 4.5466
	step 8767:lm_loss: 4.5169, ppl: 91.5516, loss: 4.5467
	step 8768:lm_loss: 4.5169, ppl: 91.5485, loss: 4.5467
	step 8769:lm_loss: 4.5170, ppl: 91.5621, loss: 4.5468
	step 8770:lm_loss: 4.5170, ppl: 91.5645, loss: 4.5469
	step 8771:lm_loss: 4.5172, ppl: 91.5746, loss: 4.5470
	step 8772:lm_loss: 4.5173, ppl: 91.5907, loss: 4.5471
	step 8773:lm_loss: 4.5175, ppl: 91.6045, loss: 4.5474
	step 8774:lm_loss: 4.5175, ppl: 91.6089, loss: 4.5474
	step 8775:lm_loss: 4.5176, ppl: 91.6124, loss: 4.5474
	step 8776:lm_loss: 4.5176, ppl: 91.6132, loss: 4.5475
	step 8777:lm_loss: 4.5174, ppl: 91.6000, loss: 4.5473
	step 8778:lm_loss: 4.5174, ppl: 91.6010, loss: 4.5474
	step 8779:lm_loss: 4.5175, ppl: 91.6017, loss: 4.5474
	step 8780:lm_loss: 4.5175, ppl: 91.6059, loss: 4.5474
	step 8781:lm_loss: 4.5175, ppl: 91.6071, loss: 4.5474
	step 8782:lm_loss: 4.5176, ppl: 91.6130, loss: 4.5475
	step 8783:lm_loss: 4.5177, ppl: 91.6265, loss: 4.5476
	step 8784:lm_loss: 4.5178, ppl: 91.6341, loss: 4.5477
	step 8785:lm_loss: 4.5179, ppl: 91.6402, loss: 4.5477
	step 8786:lm_loss: 4.5178, ppl: 91.6325, loss: 4.5476
	step 8787:lm_loss: 4.5179, ppl: 91.6390, loss: 4.5477
	step 8788:lm_loss: 4.5178, ppl: 91.6318, loss: 4.5476
	step 8789:lm_loss: 4.5175, ppl: 91.6078, loss: 4.5475
	step 8790:lm_loss: 4.5176, ppl: 91.6125, loss: 4.5476
	step 8791:lm_loss: 4.5176, ppl: 91.6164, loss: 4.5476
	step 8792:lm_loss: 4.5177, ppl: 91.6286, loss: 4.5477
	step 8793:lm_loss: 4.5180, ppl: 91.6534, loss: 4.5478
	step 8794:lm_loss: 4.5181, ppl: 91.6582, loss: 4.5479
	step 8795:lm_loss: 4.5180, ppl: 91.6476, loss: 4.5477
	step 8796:lm_loss: 4.5179, ppl: 91.6437, loss: 4.5477
	step 8797:lm_loss: 4.5180, ppl: 91.6517, loss: 4.5478
	step 8798:lm_loss: 4.5180, ppl: 91.6538, loss: 4.5478
	step 8799:lm_loss: 4.5180, ppl: 91.6499, loss: 4.5478
	step 8800:lm_loss: 4.5180, ppl: 91.6500, loss: 4.5478
	step 8801:lm_loss: 4.5179, ppl: 91.6397, loss: 4.5477
	step 8802:lm_loss: 4.5178, ppl: 91.6345, loss: 4.5476
	step 8803:lm_loss: 4.5179, ppl: 91.6404, loss: 4.5477
	step 8804:lm_loss: 4.5179, ppl: 91.6388, loss: 4.5476
	step 8805:lm_loss: 4.5178, ppl: 91.6332, loss: 4.5476
	step 8806:lm_loss: 4.5178, ppl: 91.6370, loss: 4.5476
	step 8807:lm_loss: 4.5178, ppl: 91.6365, loss: 4.5476
	step 8808:lm_loss: 4.5179, ppl: 91.6402, loss: 4.5477
	step 8809:lm_loss: 4.5179, ppl: 91.6421, loss: 4.5477
	step 8810:lm_loss: 4.5179, ppl: 91.6434, loss: 4.5477
	step 8811:lm_loss: 4.5179, ppl: 91.6475, loss: 4.5477
	step 8812:lm_loss: 4.5180, ppl: 91.6478, loss: 4.5477
	step 8813:lm_loss: 4.5181, ppl: 91.6592, loss: 4.5478
	step 8814:lm_loss: 4.5181, ppl: 91.6615, loss: 4.5478
	step 8815:lm_loss: 4.5182, ppl: 91.6707, loss: 4.5479
	step 8816:lm_loss: 4.5182, ppl: 91.6698, loss: 4.5479
	step 8817:lm_loss: 4.5182, ppl: 91.6666, loss: 4.5479
	step 8818:lm_loss: 4.5181, ppl: 91.6639, loss: 4.5478
	step 8819:lm_loss: 4.5182, ppl: 91.6661, loss: 4.5478
	step 8820:lm_loss: 4.5182, ppl: 91.6715, loss: 4.5479
	step 8821:lm_loss: 4.5181, ppl: 91.6605, loss: 4.5478
	step 8822:lm_loss: 4.5183, ppl: 91.6766, loss: 4.5479
	step 8823:lm_loss: 4.5183, ppl: 91.6789, loss: 4.5479
	step 8824:lm_loss: 4.5182, ppl: 91.6671, loss: 4.5479
	step 8825:lm_loss: 4.5181, ppl: 91.6652, loss: 4.5478
	step 8826:lm_loss: 4.5181, ppl: 91.6614, loss: 4.5478
	step 8827:lm_loss: 4.5181, ppl: 91.6647, loss: 4.5478
	step 8828:lm_loss: 4.5180, ppl: 91.6563, loss: 4.5477
	step 8829:lm_loss: 4.5181, ppl: 91.6603, loss: 4.5478
	step 8830:lm_loss: 4.5182, ppl: 91.6674, loss: 4.5478
	step 8831:lm_loss: 4.5181, ppl: 91.6576, loss: 4.5477
	step 8832:lm_loss: 4.5181, ppl: 91.6650, loss: 4.5478
	step 8833:lm_loss: 4.5180, ppl: 91.6516, loss: 4.5476
	step 8834:lm_loss: 4.5180, ppl: 91.6558, loss: 4.5476
	step 8835:lm_loss: 4.5179, ppl: 91.6450, loss: 4.5475
	step 8836:lm_loss: 4.5179, ppl: 91.6461, loss: 4.5475
	step 8837:lm_loss: 4.5180, ppl: 91.6505, loss: 4.5476
	step 8838:lm_loss: 4.5180, ppl: 91.6487, loss: 4.5475
	step 8839:lm_loss: 4.5180, ppl: 91.6496, loss: 4.5476
	step 8840:lm_loss: 4.5181, ppl: 91.6621, loss: 4.5477
	step 8841:lm_loss: 4.5182, ppl: 91.6684, loss: 4.5478
	step 8842:lm_loss: 4.5182, ppl: 91.6711, loss: 4.5478
	step 8843:lm_loss: 4.5183, ppl: 91.6759, loss: 4.5478
	step 8844:lm_loss: 4.5183, ppl: 91.6823, loss: 4.5479
	step 8845:lm_loss: 4.5183, ppl: 91.6803, loss: 4.5479
	step 8846:lm_loss: 4.5184, ppl: 91.6878, loss: 4.5480
	step 8847:lm_loss: 4.5184, ppl: 91.6882, loss: 4.5480
	step 8848:lm_loss: 4.5185, ppl: 91.6993, loss: 4.5481
	step 8849:lm_loss: 4.5185, ppl: 91.6970, loss: 4.5481
	step 8850:lm_loss: 4.5184, ppl: 91.6852, loss: 4.5479
	step 8851:lm_loss: 4.5185, ppl: 91.6963, loss: 4.5480
	step 8852:lm_loss: 4.5185, ppl: 91.6994, loss: 4.5481
	step 8853:lm_loss: 4.5186, ppl: 91.7084, loss: 4.5482
	step 8854:lm_loss: 4.5187, ppl: 91.7130, loss: 4.5482
	step 8855:lm_loss: 4.5188, ppl: 91.7275, loss: 4.5484
	step 8856:lm_loss: 4.5189, ppl: 91.7324, loss: 4.5484
	step 8857:lm_loss: 4.5189, ppl: 91.7332, loss: 4.5484
	step 8858:lm_loss: 4.5188, ppl: 91.7281, loss: 4.5484
	step 8859:lm_loss: 4.5188, ppl: 91.7267, loss: 4.5483
	step 8860:lm_loss: 4.5189, ppl: 91.7334, loss: 4.5484
	step 8861:lm_loss: 4.5188, ppl: 91.7273, loss: 4.5483
	step 8862:lm_loss: 4.5188, ppl: 91.7270, loss: 4.5483
	step 8863:lm_loss: 4.5189, ppl: 91.7301, loss: 4.5483
	step 8864:lm_loss: 4.5189, ppl: 91.7353, loss: 4.5484
	step 8865:lm_loss: 4.5189, ppl: 91.7354, loss: 4.5484
	step 8866:lm_loss: 4.5189, ppl: 91.7335, loss: 4.5484
	step 8867:lm_loss: 4.5189, ppl: 91.7356, loss: 4.5484
	step 8868:lm_loss: 4.5190, ppl: 91.7453, loss: 4.5485
	step 8869:lm_loss: 4.5190, ppl: 91.7483, loss: 4.5485
	step 8870:lm_loss: 4.5189, ppl: 91.7311, loss: 4.5483
	step 8871:lm_loss: 4.5188, ppl: 91.7221, loss: 4.5483
	step 8872:lm_loss: 4.5187, ppl: 91.7142, loss: 4.5481
	step 8873:lm_loss: 4.5187, ppl: 91.7134, loss: 4.5481
	step 8874:lm_loss: 4.5188, ppl: 91.7229, loss: 4.5483
	step 8875:lm_loss: 4.5187, ppl: 91.7154, loss: 4.5482
	step 8876:lm_loss: 4.5188, ppl: 91.7279, loss: 4.5483
	step 8877:lm_loss: 4.5189, ppl: 91.7321, loss: 4.5484
	step 8878:lm_loss: 4.5188, ppl: 91.7289, loss: 4.5484
	step 8879:lm_loss: 4.5189, ppl: 91.7304, loss: 4.5484
	step 8880:lm_loss: 4.5188, ppl: 91.7286, loss: 4.5484
	step 8881:lm_loss: 4.5189, ppl: 91.7382, loss: 4.5484
	step 8882:lm_loss: 4.5190, ppl: 91.7444, loss: 4.5485
	step 8883:lm_loss: 4.5191, ppl: 91.7534, loss: 4.5486
	step 8884:lm_loss: 4.5190, ppl: 91.7429, loss: 4.5485
	step 8885:lm_loss: 4.5190, ppl: 91.7404, loss: 4.5485
	step 8886:lm_loss: 4.5191, ppl: 91.7510, loss: 4.5485
	step 8887:lm_loss: 4.5190, ppl: 91.7393, loss: 4.5484
	step 8888:lm_loss: 4.5189, ppl: 91.7326, loss: 4.5483
	step 8889:lm_loss: 4.5190, ppl: 91.7432, loss: 4.5484
	step 8890:lm_loss: 4.5190, ppl: 91.7480, loss: 4.5484
	step 8891:lm_loss: 4.5192, ppl: 91.7592, loss: 4.5485
	step 8892:lm_loss: 4.5192, ppl: 91.7639, loss: 4.5485
	step 8893:lm_loss: 4.5193, ppl: 91.7668, loss: 4.5486
	step 8894:lm_loss: 4.5191, ppl: 91.7575, loss: 4.5484
	step 8895:lm_loss: 4.5191, ppl: 91.7565, loss: 4.5484
	step 8896:lm_loss: 4.5191, ppl: 91.7566, loss: 4.5484
	step 8897:lm_loss: 4.5191, ppl: 91.7574, loss: 4.5484
	step 8898:lm_loss: 4.5192, ppl: 91.7644, loss: 4.5485
	step 8899:lm_loss: 4.5192, ppl: 91.7623, loss: 4.5485
	step 8900:lm_loss: 4.5191, ppl: 91.7554, loss: 4.5484
	step 8901:lm_loss: 4.5192, ppl: 91.7610, loss: 4.5485
	step 8902:lm_loss: 4.5192, ppl: 91.7647, loss: 4.5485
	step 8903:lm_loss: 4.5193, ppl: 91.7687, loss: 4.5486
	step 8904:lm_loss: 4.5192, ppl: 91.7662, loss: 4.5485
	step 8905:lm_loss: 4.5192, ppl: 91.7627, loss: 4.5485
	step 8906:lm_loss: 4.5191, ppl: 91.7572, loss: 4.5484
	step 8907:lm_loss: 4.5192, ppl: 91.7621, loss: 4.5485
	step 8908:lm_loss: 4.5192, ppl: 91.7656, loss: 4.5485
	step 8909:lm_loss: 4.5193, ppl: 91.7735, loss: 4.5486
	step 8910:lm_loss: 4.5193, ppl: 91.7749, loss: 4.5486
	step 8911:lm_loss: 4.5194, ppl: 91.7822, loss: 4.5487
	step 8912:lm_loss: 4.5195, ppl: 91.7903, loss: 4.5488
	step 8913:lm_loss: 4.5196, ppl: 91.7982, loss: 4.5489
	step 8914:lm_loss: 4.5195, ppl: 91.7895, loss: 4.5488
	step 8915:lm_loss: 4.5195, ppl: 91.7901, loss: 4.5488
	step 8916:lm_loss: 4.5195, ppl: 91.7883, loss: 4.5488
	step 8917:lm_loss: 4.5195, ppl: 91.7854, loss: 4.5487
	step 8918:lm_loss: 4.5196, ppl: 91.7943, loss: 4.5488
	step 8919:lm_loss: 4.5195, ppl: 91.7933, loss: 4.5488
	step 8920:lm_loss: 4.5195, ppl: 91.7892, loss: 4.5487
	step 8921:lm_loss: 4.5195, ppl: 91.7905, loss: 4.5487
	step 8922:lm_loss: 4.5195, ppl: 91.7872, loss: 4.5487
	step 8923:lm_loss: 4.5195, ppl: 91.7924, loss: 4.5488
	step 8924:lm_loss: 4.5196, ppl: 91.7961, loss: 4.5488
	step 8925:lm_loss: 4.5196, ppl: 91.7952, loss: 4.5488
	step 8926:lm_loss: 4.5195, ppl: 91.7917, loss: 4.5488
	step 8927:lm_loss: 4.5196, ppl: 91.7968, loss: 4.5488
	step 8928:lm_loss: 4.5196, ppl: 91.7997, loss: 4.5488
	step 8929:lm_loss: 4.5197, ppl: 91.8048, loss: 4.5489
	step 8930:lm_loss: 4.5197, ppl: 91.8125, loss: 4.5490
	step 8931:lm_loss: 4.5196, ppl: 91.8034, loss: 4.5489
	step 8932:lm_loss: 4.5197, ppl: 91.8047, loss: 4.5489
	step 8933:lm_loss: 4.5197, ppl: 91.8092, loss: 4.5489
	step 8934:lm_loss: 4.5197, ppl: 91.8074, loss: 4.5489
	step 8935:lm_loss: 4.5197, ppl: 91.8056, loss: 4.5489
	step 8936:lm_loss: 4.5198, ppl: 91.8192, loss: 4.5490
	step 8937:lm_loss: 4.5198, ppl: 91.8211, loss: 4.5490
	step 8938:lm_loss: 4.5198, ppl: 91.8197, loss: 4.5490
	step 8939:lm_loss: 4.5201, ppl: 91.8454, loss: 4.5493
	step 8940:lm_loss: 4.5201, ppl: 91.8489, loss: 4.5494
	step 8941:lm_loss: 4.5201, ppl: 91.8436, loss: 4.5493
	step 8942:lm_loss: 4.5202, ppl: 91.8533, loss: 4.5495
	step 8943:lm_loss: 4.5203, ppl: 91.8622, loss: 4.5496
	step 8944:lm_loss: 4.5203, ppl: 91.8598, loss: 4.5495
	step 8945:lm_loss: 4.5203, ppl: 91.8610, loss: 4.5495
	step 8946:lm_loss: 4.5201, ppl: 91.8472, loss: 4.5494
	step 8947:lm_loss: 4.5202, ppl: 91.8551, loss: 4.5495
	step 8948:lm_loss: 4.5201, ppl: 91.8465, loss: 4.5494
	step 8949:lm_loss: 4.5201, ppl: 91.8491, loss: 4.5494
	step 8950:lm_loss: 4.5202, ppl: 91.8552, loss: 4.5495
	step 8951:lm_loss: 4.5202, ppl: 91.8501, loss: 4.5495
	step 8952:lm_loss: 4.5202, ppl: 91.8583, loss: 4.5496
	step 8953:lm_loss: 4.5202, ppl: 91.8527, loss: 4.5495
	step 8954:lm_loss: 4.5202, ppl: 91.8516, loss: 4.5495
	step 8955:lm_loss: 4.5202, ppl: 91.8499, loss: 4.5495
	step 8956:lm_loss: 4.5201, ppl: 91.8463, loss: 4.5494
	step 8957:lm_loss: 4.5200, ppl: 91.8384, loss: 4.5494
	step 8958:lm_loss: 4.5201, ppl: 91.8423, loss: 4.5494
	step 8959:lm_loss: 4.5200, ppl: 91.8362, loss: 4.5493
	step 8960:lm_loss: 4.5199, ppl: 91.8298, loss: 4.5493
	step 8961:lm_loss: 4.5201, ppl: 91.8405, loss: 4.5495
	step 8962:lm_loss: 4.5202, ppl: 91.8499, loss: 4.5496
	step 8963:lm_loss: 4.5200, ppl: 91.8391, loss: 4.5495
	step 8964:lm_loss: 4.5201, ppl: 91.8472, loss: 4.5496
	step 8965:lm_loss: 4.5200, ppl: 91.8365, loss: 4.5494
	step 8966:lm_loss: 4.5200, ppl: 91.8352, loss: 4.5494
	step 8967:lm_loss: 4.5200, ppl: 91.8355, loss: 4.5494
	step 8968:lm_loss: 4.5199, ppl: 91.8263, loss: 4.5493
	step 8969:lm_loss: 4.5199, ppl: 91.8233, loss: 4.5493
	step 8970:lm_loss: 4.5198, ppl: 91.8177, loss: 4.5492
	step 8971:lm_loss: 4.5198, ppl: 91.8147, loss: 4.5492
	step 8972:lm_loss: 4.5197, ppl: 91.8078, loss: 4.5491
	step 8973:lm_loss: 4.5197, ppl: 91.8073, loss: 4.5491
	step 8974:lm_loss: 4.5198, ppl: 91.8160, loss: 4.5492
	step 8975:lm_loss: 4.5198, ppl: 91.8149, loss: 4.5491
	step 8976:lm_loss: 4.5196, ppl: 91.8034, loss: 4.5490
	step 8977:lm_loss: 4.5197, ppl: 91.8076, loss: 4.5490
	step 8978:lm_loss: 4.5198, ppl: 91.8135, loss: 4.5491
	step 8979:lm_loss: 4.5197, ppl: 91.8098, loss: 4.5490
	step 8980:lm_loss: 4.5198, ppl: 91.8150, loss: 4.5491
	step 8981:lm_loss: 4.5199, ppl: 91.8237, loss: 4.5492
	step 8982:lm_loss: 4.5200, ppl: 91.8317, loss: 4.5493
	step 8983:lm_loss: 4.5200, ppl: 91.8359, loss: 4.5493
	step 8984:lm_loss: 4.5199, ppl: 91.8304, loss: 4.5492
	step 8985:lm_loss: 4.5200, ppl: 91.8330, loss: 4.5493
	step 8986:lm_loss: 4.5201, ppl: 91.8423, loss: 4.5494
	step 8987:lm_loss: 4.5201, ppl: 91.8405, loss: 4.5493
	step 8988:lm_loss: 4.5200, ppl: 91.8373, loss: 4.5493
	step 8989:lm_loss: 4.5200, ppl: 91.8376, loss: 4.5493
	step 8990:lm_loss: 4.5200, ppl: 91.8377, loss: 4.5493
	step 8991:lm_loss: 4.5200, ppl: 91.8314, loss: 4.5492
	step 8992:lm_loss: 4.5199, ppl: 91.8292, loss: 4.5492
	step 8993:lm_loss: 4.5198, ppl: 91.8215, loss: 4.5491
	step 8994:lm_loss: 4.5198, ppl: 91.8212, loss: 4.5491
	step 8995:lm_loss: 4.5199, ppl: 91.8235, loss: 4.5491
	step 8996:lm_loss: 4.5198, ppl: 91.8130, loss: 4.5490
	step 8997:lm_loss: 4.5197, ppl: 91.8121, loss: 4.5490
	step 8998:lm_loss: 4.5199, ppl: 91.8269, loss: 4.5491
	step 8999:lm_loss: 4.5199, ppl: 91.8222, loss: 4.5490
	step 9000:lm_loss: 4.5199, ppl: 91.8271, loss: 4.5491
	step 9001:lm_loss: 4.5200, ppl: 91.8322, loss: 4.5492
	step 9002:lm_loss: 4.5200, ppl: 91.8314, loss: 4.5492
	step 9003:lm_loss: 4.5198, ppl: 91.8179, loss: 4.5489
	step 9004:lm_loss: 4.5198, ppl: 91.8158, loss: 4.5489
	step 9005:lm_loss: 4.5197, ppl: 91.8110, loss: 4.5489
	step 9006:lm_loss: 4.5197, ppl: 91.8116, loss: 4.5489
	step 9007:lm_loss: 4.5197, ppl: 91.8088, loss: 4.5488
	step 9008:lm_loss: 4.5198, ppl: 91.8129, loss: 4.5489
	step 9009:lm_loss: 4.5199, ppl: 91.8258, loss: 4.5490
	step 9010:lm_loss: 4.5199, ppl: 91.8275, loss: 4.5490
	step 9011:lm_loss: 4.5198, ppl: 91.8130, loss: 4.5489
	step 9012:lm_loss: 4.5197, ppl: 91.8114, loss: 4.5489
	step 9013:lm_loss: 4.5197, ppl: 91.8070, loss: 4.5488
	step 9014:lm_loss: 4.5198, ppl: 91.8140, loss: 4.5489
	step 9015:lm_loss: 4.5198, ppl: 91.8171, loss: 4.5489
	step 9016:lm_loss: 4.5198, ppl: 91.8200, loss: 4.5490
	step 9017:lm_loss: 4.5198, ppl: 91.8159, loss: 4.5489
	step 9018:lm_loss: 4.5198, ppl: 91.8140, loss: 4.5489
	step 9019:lm_loss: 4.5199, ppl: 91.8291, loss: 4.5491
	step 9020:lm_loss: 4.5199, ppl: 91.8235, loss: 4.5491
	step 9021:lm_loss: 4.5198, ppl: 91.8127, loss: 4.5490
	step 9022:lm_loss: 4.5198, ppl: 91.8196, loss: 4.5490
	step 9023:lm_loss: 4.5199, ppl: 91.8248, loss: 4.5491
	step 9024:lm_loss: 4.5199, ppl: 91.8256, loss: 4.5491
	step 9025:lm_loss: 4.5199, ppl: 91.8241, loss: 4.5491
	step 9026:lm_loss: 4.5200, ppl: 91.8314, loss: 4.5491
	step 9027:lm_loss: 4.5200, ppl: 91.8343, loss: 4.5492
	step 9028:lm_loss: 4.5198, ppl: 91.8148, loss: 4.5490
	step 9029:lm_loss: 4.5198, ppl: 91.8129, loss: 4.5489
	step 9030:lm_loss: 4.5197, ppl: 91.8116, loss: 4.5489
	step 9031:lm_loss: 4.5198, ppl: 91.8139, loss: 4.5489
	step 9032:lm_loss: 4.5199, ppl: 91.8225, loss: 4.5490
	step 9033:lm_loss: 4.5199, ppl: 91.8253, loss: 4.5491
	step 9034:lm_loss: 4.5201, ppl: 91.8443, loss: 4.5492
	step 9035:lm_loss: 4.5200, ppl: 91.8362, loss: 4.5491
	step 9036:lm_loss: 4.5201, ppl: 91.8427, loss: 4.5492
	step 9037:lm_loss: 4.5200, ppl: 91.8394, loss: 4.5492
	step 9038:lm_loss: 4.5201, ppl: 91.8429, loss: 4.5492
	step 9039:lm_loss: 4.5201, ppl: 91.8404, loss: 4.5492
	step 9040:lm_loss: 4.5201, ppl: 91.8408, loss: 4.5492
	step 9041:lm_loss: 4.5202, ppl: 91.8504, loss: 4.5493
	step 9042:lm_loss: 4.5201, ppl: 91.8452, loss: 4.5492
	step 9043:lm_loss: 4.5201, ppl: 91.8423, loss: 4.5491
	step 9044:lm_loss: 4.5200, ppl: 91.8367, loss: 4.5491
	step 9045:lm_loss: 4.5200, ppl: 91.8373, loss: 4.5491
	step 9046:lm_loss: 4.5200, ppl: 91.8355, loss: 4.5490
	step 9047:lm_loss: 4.5200, ppl: 91.8390, loss: 4.5491
	step 9048:lm_loss: 4.5201, ppl: 91.8411, loss: 4.5491
	step 9049:lm_loss: 4.5199, ppl: 91.8241, loss: 4.5489
	step 9050:lm_loss: 4.5198, ppl: 91.8168, loss: 4.5488
	step 9051:lm_loss: 4.5198, ppl: 91.8147, loss: 4.5488
	step 9052:lm_loss: 4.5198, ppl: 91.8197, loss: 4.5488
	step 9053:lm_loss: 4.5198, ppl: 91.8174, loss: 4.5488
	step 9054:lm_loss: 4.5198, ppl: 91.8135, loss: 4.5487
	step 9055:lm_loss: 4.5198, ppl: 91.8141, loss: 4.5487
	step 9056:lm_loss: 4.5198, ppl: 91.8143, loss: 4.5487
	step 9057:lm_loss: 4.5197, ppl: 91.8055, loss: 4.5486
	step 9058:lm_loss: 4.5198, ppl: 91.8206, loss: 4.5487
	step 9059:lm_loss: 4.5198, ppl: 91.8164, loss: 4.5487
	step 9060:lm_loss: 4.5198, ppl: 91.8138, loss: 4.5486
	step 9061:lm_loss: 4.5198, ppl: 91.8165, loss: 4.5487
	step 9062:lm_loss: 4.5197, ppl: 91.8118, loss: 4.5486
	step 9063:lm_loss: 4.5197, ppl: 91.8094, loss: 4.5486
	step 9064:lm_loss: 4.5196, ppl: 91.7994, loss: 4.5484
	step 9065:lm_loss: 4.5196, ppl: 91.8000, loss: 4.5484
	step 9066:lm_loss: 4.5197, ppl: 91.8044, loss: 4.5485
	step 9067:lm_loss: 4.5197, ppl: 91.8093, loss: 4.5485
	step 9068:lm_loss: 4.5198, ppl: 91.8172, loss: 4.5486
	step 9069:lm_loss: 4.5197, ppl: 91.8054, loss: 4.5485
	step 9070:lm_loss: 4.5195, ppl: 91.7864, loss: 4.5483
	step 9071:lm_loss: 4.5194, ppl: 91.7782, loss: 4.5482
	step 9072:lm_loss: 4.5194, ppl: 91.7766, loss: 4.5482
	step 9073:lm_loss: 4.5193, ppl: 91.7753, loss: 4.5482
	step 9074:lm_loss: 4.5195, ppl: 91.7859, loss: 4.5483
	step 9075:lm_loss: 4.5195, ppl: 91.7857, loss: 4.5483
	step 9076:lm_loss: 4.5195, ppl: 91.7906, loss: 4.5483
	step 9077:lm_loss: 4.5196, ppl: 91.7973, loss: 4.5484
	step 9078:lm_loss: 4.5197, ppl: 91.8067, loss: 4.5486
	step 9079:lm_loss: 4.5197, ppl: 91.8064, loss: 4.5486
	step 9080:lm_loss: 4.5196, ppl: 91.8017, loss: 4.5485
	step 9081:lm_loss: 4.5196, ppl: 91.7998, loss: 4.5485
	step 9082:lm_loss: 4.5197, ppl: 91.8049, loss: 4.5486
	step 9083:lm_loss: 4.5197, ppl: 91.8083, loss: 4.5486
	step 9084:lm_loss: 4.5196, ppl: 91.7965, loss: 4.5485
	step 9085:lm_loss: 4.5196, ppl: 91.7975, loss: 4.5485
	step 9086:lm_loss: 4.5196, ppl: 91.8029, loss: 4.5485
	step 9087:lm_loss: 4.5197, ppl: 91.8045, loss: 4.5485
	step 9088:lm_loss: 4.5197, ppl: 91.8078, loss: 4.5486
	step 9089:lm_loss: 4.5197, ppl: 91.8107, loss: 4.5486
	step 9090:lm_loss: 4.5197, ppl: 91.8115, loss: 4.5486
	step 9091:lm_loss: 4.5197, ppl: 91.8124, loss: 4.5486
	step 9092:lm_loss: 4.5198, ppl: 91.8129, loss: 4.5486
	step 9093:lm_loss: 4.5198, ppl: 91.8139, loss: 4.5486
	step 9094:lm_loss: 4.5198, ppl: 91.8131, loss: 4.5486
	step 9095:lm_loss: 4.5196, ppl: 91.7960, loss: 4.5485
	step 9096:lm_loss: 4.5196, ppl: 91.8009, loss: 4.5486
	step 9097:lm_loss: 4.5197, ppl: 91.8049, loss: 4.5486
	step 9098:lm_loss: 4.5198, ppl: 91.8132, loss: 4.5487
	step 9099:lm_loss: 4.5197, ppl: 91.8084, loss: 4.5487
	step 9100:lm_loss: 4.5197, ppl: 91.8081, loss: 4.5487
	step 9101:lm_loss: 4.5196, ppl: 91.7988, loss: 4.5486
	step 9102:lm_loss: 4.5196, ppl: 91.7955, loss: 4.5485
	step 9103:lm_loss: 4.5196, ppl: 91.7965, loss: 4.5485
	step 9104:lm_loss: 4.5196, ppl: 91.7954, loss: 4.5485
	step 9105:lm_loss: 4.5196, ppl: 91.7997, loss: 4.5486
	step 9106:lm_loss: 4.5196, ppl: 91.7993, loss: 4.5486
	step 9107:lm_loss: 4.5196, ppl: 91.8017, loss: 4.5486
	step 9108:lm_loss: 4.5195, ppl: 91.7915, loss: 4.5485
	step 9109:lm_loss: 4.5195, ppl: 91.7933, loss: 4.5485
	step 9110:lm_loss: 4.5195, ppl: 91.7894, loss: 4.5485
	step 9111:lm_loss: 4.5195, ppl: 91.7875, loss: 4.5484
	step 9112:lm_loss: 4.5195, ppl: 91.7922, loss: 4.5485
	step 9113:lm_loss: 4.5196, ppl: 91.7952, loss: 4.5485
	step 9114:lm_loss: 4.5196, ppl: 91.7983, loss: 4.5486
	step 9115:lm_loss: 4.5195, ppl: 91.7900, loss: 4.5485
	step 9116:lm_loss: 4.5195, ppl: 91.7911, loss: 4.5485
	step 9117:lm_loss: 4.5196, ppl: 91.7958, loss: 4.5485
	step 9118:lm_loss: 4.5195, ppl: 91.7858, loss: 4.5484
	step 9119:lm_loss: 4.5196, ppl: 91.7957, loss: 4.5485
	step 9120:lm_loss: 4.5195, ppl: 91.7861, loss: 4.5484
	step 9121:lm_loss: 4.5195, ppl: 91.7892, loss: 4.5484
	step 9122:lm_loss: 4.5197, ppl: 91.8040, loss: 4.5486
	step 9123:lm_loss: 4.5197, ppl: 91.8074, loss: 4.5486
	step 9124:lm_loss: 4.5196, ppl: 91.8011, loss: 4.5485
	step 9125:lm_loss: 4.5196, ppl: 91.7971, loss: 4.5485
	step 9126:lm_loss: 4.5196, ppl: 91.8017, loss: 4.5485
	step 9127:lm_loss: 4.5196, ppl: 91.7972, loss: 4.5485
	step 9128:lm_loss: 4.5196, ppl: 91.8003, loss: 4.5485
	step 9129:lm_loss: 4.5195, ppl: 91.7890, loss: 4.5483
	step 9130:lm_loss: 4.5194, ppl: 91.7818, loss: 4.5483
	step 9131:lm_loss: 4.5194, ppl: 91.7790, loss: 4.5482
	step 9132:lm_loss: 4.5193, ppl: 91.7718, loss: 4.5481
	step 9133:lm_loss: 4.5193, ppl: 91.7672, loss: 4.5481
	step 9134:lm_loss: 4.5193, ppl: 91.7678, loss: 4.5481
	step 9135:lm_loss: 4.5192, ppl: 91.7631, loss: 4.5480
	step 9136:lm_loss: 4.5192, ppl: 91.7649, loss: 4.5480
	step 9137:lm_loss: 4.5192, ppl: 91.7618, loss: 4.5480
	step 9138:lm_loss: 4.5192, ppl: 91.7576, loss: 4.5480
	step 9139:lm_loss: 4.5191, ppl: 91.7551, loss: 4.5479
	step 9140:lm_loss: 4.5190, ppl: 91.7449, loss: 4.5478
	step 9141:lm_loss: 4.5190, ppl: 91.7447, loss: 4.5478
	step 9142:lm_loss: 4.5190, ppl: 91.7469, loss: 4.5479
	step 9143:lm_loss: 4.5191, ppl: 91.7562, loss: 4.5480
	step 9144:lm_loss: 4.5191, ppl: 91.7491, loss: 4.5478
	step 9145:lm_loss: 4.5191, ppl: 91.7528, loss: 4.5479
	step 9146:lm_loss: 4.5191, ppl: 91.7553, loss: 4.5479
	step 9147:lm_loss: 4.5192, ppl: 91.7597, loss: 4.5480
	step 9148:lm_loss: 4.5192, ppl: 91.7606, loss: 4.5480
	step 9149:lm_loss: 4.5192, ppl: 91.7641, loss: 4.5480
	step 9150:lm_loss: 4.5193, ppl: 91.7707, loss: 4.5481
	step 9151:lm_loss: 4.5193, ppl: 91.7718, loss: 4.5481
	step 9152:lm_loss: 4.5193, ppl: 91.7723, loss: 4.5481
	step 9153:lm_loss: 4.5193, ppl: 91.7721, loss: 4.5481
	step 9154:lm_loss: 4.5194, ppl: 91.7848, loss: 4.5483
	step 9155:lm_loss: 4.5194, ppl: 91.7825, loss: 4.5482
	step 9156:lm_loss: 4.5193, ppl: 91.7698, loss: 4.5481
	step 9157:lm_loss: 4.5193, ppl: 91.7734, loss: 4.5482
	step 9158:lm_loss: 4.5193, ppl: 91.7752, loss: 4.5482
	step 9159:lm_loss: 4.5194, ppl: 91.7767, loss: 4.5482
	step 9160:lm_loss: 4.5196, ppl: 91.7973, loss: 4.5484
	step 9161:lm_loss: 4.5196, ppl: 91.7972, loss: 4.5484
	step 9162:lm_loss: 4.5196, ppl: 91.7975, loss: 4.5484
	step 9163:lm_loss: 4.5197, ppl: 91.8056, loss: 4.5485
	step 9164:lm_loss: 4.5198, ppl: 91.8192, loss: 4.5487
	step 9165:lm_loss: 4.5199, ppl: 91.8221, loss: 4.5487
	step 9166:lm_loss: 4.5200, ppl: 91.8315, loss: 4.5488
	step 9167:lm_loss: 4.5200, ppl: 91.8399, loss: 4.5489
	step 9168:lm_loss: 4.5200, ppl: 91.8382, loss: 4.5488
	step 9169:lm_loss: 4.5200, ppl: 91.8380, loss: 4.5488
	step 9170:lm_loss: 4.5200, ppl: 91.8398, loss: 4.5489
	step 9171:lm_loss: 4.5201, ppl: 91.8474, loss: 4.5490
	step 9172:lm_loss: 4.5201, ppl: 91.8431, loss: 4.5489
	step 9173:lm_loss: 4.5201, ppl: 91.8467, loss: 4.5490
	step 9174:lm_loss: 4.5201, ppl: 91.8478, loss: 4.5490
	step 9175:lm_loss: 4.5202, ppl: 91.8519, loss: 4.5490
	step 9176:lm_loss: 4.5202, ppl: 91.8521, loss: 4.5490
	step 9177:lm_loss: 4.5202, ppl: 91.8544, loss: 4.5491
	step 9178:lm_loss: 4.5202, ppl: 91.8570, loss: 4.5491
	step 9179:lm_loss: 4.5200, ppl: 91.8339, loss: 4.5489
	step 9180:lm_loss: 4.5200, ppl: 91.8328, loss: 4.5489
	step 9181:lm_loss: 4.5200, ppl: 91.8372, loss: 4.5489
	step 9182:lm_loss: 4.5202, ppl: 91.8530, loss: 4.5491
	step 9183:lm_loss: 4.5201, ppl: 91.8476, loss: 4.5490
	step 9184:lm_loss: 4.5201, ppl: 91.8409, loss: 4.5489
	step 9185:lm_loss: 4.5201, ppl: 91.8423, loss: 4.5489
	step 9186:lm_loss: 4.5201, ppl: 91.8439, loss: 4.5489
	step 9187:lm_loss: 4.5200, ppl: 91.8400, loss: 4.5489
	step 9188:lm_loss: 4.5201, ppl: 91.8420, loss: 4.5489
	step 9189:lm_loss: 4.5199, ppl: 91.8295, loss: 4.5488
	step 9190:lm_loss: 4.5200, ppl: 91.8323, loss: 4.5488
	step 9191:lm_loss: 4.5200, ppl: 91.8371, loss: 4.5488
	step 9192:lm_loss: 4.5198, ppl: 91.8203, loss: 4.5487
	step 9193:lm_loss: 4.5196, ppl: 91.8026, loss: 4.5485
	step 9194:lm_loss: 4.5198, ppl: 91.8131, loss: 4.5486
	step 9195:lm_loss: 4.5196, ppl: 91.8033, loss: 4.5485
	step 9196:lm_loss: 4.5197, ppl: 91.8060, loss: 4.5486
	step 9197:lm_loss: 4.5198, ppl: 91.8162, loss: 4.5487
	step 9198:lm_loss: 4.5197, ppl: 91.8120, loss: 4.5487
	step 9199:lm_loss: 4.5198, ppl: 91.8195, loss: 4.5487
	step 9200:lm_loss: 4.5198, ppl: 91.8178, loss: 4.5487
	step 9201:lm_loss: 4.5199, ppl: 91.8256, loss: 4.5488
	step 9202:lm_loss: 4.5198, ppl: 91.8170, loss: 4.5487
	step 9203:lm_loss: 4.5197, ppl: 91.8120, loss: 4.5487
	step 9204:lm_loss: 4.5196, ppl: 91.8020, loss: 4.5486
	step 9205:lm_loss: 4.5198, ppl: 91.8164, loss: 4.5487
	step 9206:lm_loss: 4.5199, ppl: 91.8234, loss: 4.5488
	step 9207:lm_loss: 4.5199, ppl: 91.8278, loss: 4.5488
	step 9208:lm_loss: 4.5200, ppl: 91.8356, loss: 4.5489
	step 9209:lm_loss: 4.5199, ppl: 91.8305, loss: 4.5489
	step 9210:lm_loss: 4.5200, ppl: 91.8380, loss: 4.5489
	step 9211:lm_loss: 4.5200, ppl: 91.8378, loss: 4.5489
	step 9212:lm_loss: 4.5201, ppl: 91.8417, loss: 4.5490
	step 9213:lm_loss: 4.5201, ppl: 91.8448, loss: 4.5490
	step 9214:lm_loss: 4.5200, ppl: 91.8345, loss: 4.5489
	step 9215:lm_loss: 4.5200, ppl: 91.8317, loss: 4.5488
	step 9216:lm_loss: 4.5200, ppl: 91.8314, loss: 4.5488
	step 9217:lm_loss: 4.5200, ppl: 91.8350, loss: 4.5489
	step 9218:lm_loss: 4.5198, ppl: 91.8137, loss: 4.5486
	step 9219:lm_loss: 4.5197, ppl: 91.8077, loss: 4.5485
	step 9220:lm_loss: 4.5197, ppl: 91.8086, loss: 4.5485
	step 9221:lm_loss: 4.5197, ppl: 91.8063, loss: 4.5485
	step 9222:lm_loss: 4.5197, ppl: 91.8056, loss: 4.5484
	step 9223:lm_loss: 4.5197, ppl: 91.8103, loss: 4.5485
	step 9224:lm_loss: 4.5198, ppl: 91.8196, loss: 4.5486
	step 9225:lm_loss: 4.5198, ppl: 91.8196, loss: 4.5486
	step 9226:lm_loss: 4.5198, ppl: 91.8186, loss: 4.5486
	step 9227:lm_loss: 4.5199, ppl: 91.8301, loss: 4.5487
	step 9228:lm_loss: 4.5201, ppl: 91.8429, loss: 4.5488
	step 9229:lm_loss: 4.5201, ppl: 91.8491, loss: 4.5488
	step 9230:lm_loss: 4.5201, ppl: 91.8437, loss: 4.5488
	step 9231:lm_loss: 4.5202, ppl: 91.8502, loss: 4.5488
	step 9232:lm_loss: 4.5202, ppl: 91.8545, loss: 4.5489
	step 9233:lm_loss: 4.5202, ppl: 91.8582, loss: 4.5489
	step 9234:lm_loss: 4.5202, ppl: 91.8558, loss: 4.5489
	step 9235:lm_loss: 4.5202, ppl: 91.8541, loss: 4.5489
	step 9236:lm_loss: 4.5202, ppl: 91.8519, loss: 4.5488
	step 9237:lm_loss: 4.5202, ppl: 91.8534, loss: 4.5489
	step 9238:lm_loss: 4.5203, ppl: 91.8619, loss: 4.5489
	step 9239:lm_loss: 4.5202, ppl: 91.8552, loss: 4.5489
	step 9240:lm_loss: 4.5202, ppl: 91.8561, loss: 4.5489
	step 9241:lm_loss: 4.5203, ppl: 91.8610, loss: 4.5489
	step 9242:lm_loss: 4.5203, ppl: 91.8636, loss: 4.5490
	step 9243:lm_loss: 4.5202, ppl: 91.8561, loss: 4.5489
	step 9244:lm_loss: 4.5202, ppl: 91.8520, loss: 4.5488
	step 9245:lm_loss: 4.5202, ppl: 91.8584, loss: 4.5489
	step 9246:lm_loss: 4.5202, ppl: 91.8571, loss: 4.5489
	step 9247:lm_loss: 4.5203, ppl: 91.8664, loss: 4.5490
	step 9248:lm_loss: 4.5203, ppl: 91.8672, loss: 4.5490
	step 9249:lm_loss: 4.5203, ppl: 91.8598, loss: 4.5489
	step 9250:lm_loss: 4.5203, ppl: 91.8676, loss: 4.5490
	step 9251:lm_loss: 4.5203, ppl: 91.8667, loss: 4.5490
	step 9252:lm_loss: 4.5203, ppl: 91.8588, loss: 4.5489
	step 9253:lm_loss: 4.5202, ppl: 91.8551, loss: 4.5489
	step 9254:lm_loss: 4.5204, ppl: 91.8683, loss: 4.5490
	step 9255:lm_loss: 4.5204, ppl: 91.8728, loss: 4.5491
	step 9256:lm_loss: 4.5205, ppl: 91.8860, loss: 4.5492
	step 9257:lm_loss: 4.5206, ppl: 91.8924, loss: 4.5493
	step 9258:lm_loss: 4.5207, ppl: 91.8991, loss: 4.5493
	step 9259:lm_loss: 4.5207, ppl: 91.9001, loss: 4.5494
	step 9260:lm_loss: 4.5208, ppl: 91.9067, loss: 4.5494
	step 9261:lm_loss: 4.5207, ppl: 91.9026, loss: 4.5494
	step 9262:lm_loss: 4.5208, ppl: 91.9099, loss: 4.5495
	step 9263:lm_loss: 4.5208, ppl: 91.9104, loss: 4.5495
	step 9264:lm_loss: 4.5208, ppl: 91.9075, loss: 4.5494
	step 9265:lm_loss: 4.5208, ppl: 91.9098, loss: 4.5495
	step 9266:lm_loss: 4.5209, ppl: 91.9153, loss: 4.5495
	step 9267:lm_loss: 4.5208, ppl: 91.9070, loss: 4.5494
	step 9268:lm_loss: 4.5209, ppl: 91.9145, loss: 4.5495
	step 9269:lm_loss: 4.5206, ppl: 91.8884, loss: 4.5492
	step 9270:lm_loss: 4.5207, ppl: 91.8981, loss: 4.5493
	step 9271:lm_loss: 4.5207, ppl: 91.8956, loss: 4.5493
	step 9272:lm_loss: 4.5206, ppl: 91.8942, loss: 4.5492
	step 9273:lm_loss: 4.5207, ppl: 91.9036, loss: 4.5493
	step 9274:lm_loss: 4.5207, ppl: 91.9021, loss: 4.5493
	step 9275:lm_loss: 4.5208, ppl: 91.9080, loss: 4.5494
	step 9276:lm_loss: 4.5208, ppl: 91.9083, loss: 4.5494
	step 9277:lm_loss: 4.5209, ppl: 91.9205, loss: 4.5496
	step 9278:lm_loss: 4.5209, ppl: 91.9203, loss: 4.5496
	step 9279:lm_loss: 4.5209, ppl: 91.9190, loss: 4.5496
	step 9280:lm_loss: 4.5210, ppl: 91.9235, loss: 4.5496
	step 9281:lm_loss: 4.5209, ppl: 91.9167, loss: 4.5495
	step 9282:lm_loss: 4.5209, ppl: 91.9186, loss: 4.5496
	step 9283:lm_loss: 4.5208, ppl: 91.9095, loss: 4.5494
	step 9284:lm_loss: 4.5208, ppl: 91.9102, loss: 4.5494
	step 9285:lm_loss: 4.5208, ppl: 91.9102, loss: 4.5494
	step 9286:lm_loss: 4.5208, ppl: 91.9054, loss: 4.5494
	step 9287:lm_loss: 4.5206, ppl: 91.8871, loss: 4.5493
	step 9288:lm_loss: 4.5206, ppl: 91.8888, loss: 4.5493
	step 9289:lm_loss: 4.5206, ppl: 91.8910, loss: 4.5493
	step 9290:lm_loss: 4.5206, ppl: 91.8947, loss: 4.5493
	step 9291:lm_loss: 4.5206, ppl: 91.8863, loss: 4.5493
	step 9292:lm_loss: 4.5206, ppl: 91.8906, loss: 4.5493
	step 9293:lm_loss: 4.5206, ppl: 91.8943, loss: 4.5493
	step 9294:lm_loss: 4.5206, ppl: 91.8882, loss: 4.5493
	step 9295:lm_loss: 4.5205, ppl: 91.8802, loss: 4.5491
	step 9296:lm_loss: 4.5205, ppl: 91.8826, loss: 4.5491
	step 9297:lm_loss: 4.5205, ppl: 91.8797, loss: 4.5491
	step 9298:lm_loss: 4.5205, ppl: 91.8817, loss: 4.5491
	step 9299:lm_loss: 4.5204, ppl: 91.8737, loss: 4.5490
	step 9300:lm_loss: 4.5201, ppl: 91.8426, loss: 4.5488
	step 9301:lm_loss: 4.5201, ppl: 91.8426, loss: 4.5488
	step 9302:lm_loss: 4.5201, ppl: 91.8407, loss: 4.5487
	step 9303:lm_loss: 4.5200, ppl: 91.8382, loss: 4.5487
	step 9304:lm_loss: 4.5200, ppl: 91.8397, loss: 4.5487
	step 9305:lm_loss: 4.5200, ppl: 91.8336, loss: 4.5487
	step 9306:lm_loss: 4.5201, ppl: 91.8435, loss: 4.5488
	step 9307:lm_loss: 4.5199, ppl: 91.8293, loss: 4.5486
	step 9308:lm_loss: 4.5199, ppl: 91.8283, loss: 4.5486
	step 9309:lm_loss: 4.5199, ppl: 91.8225, loss: 4.5485
	step 9310:lm_loss: 4.5200, ppl: 91.8361, loss: 4.5486
	step 9311:lm_loss: 4.5202, ppl: 91.8500, loss: 4.5488
	step 9312:lm_loss: 4.5202, ppl: 91.8507, loss: 4.5488
	step 9313:lm_loss: 4.5201, ppl: 91.8448, loss: 4.5487
	step 9314:lm_loss: 4.5201, ppl: 91.8463, loss: 4.5487
	step 9315:lm_loss: 4.5201, ppl: 91.8406, loss: 4.5487
	step 9316:lm_loss: 4.5201, ppl: 91.8444, loss: 4.5487
	step 9317:lm_loss: 4.5202, ppl: 91.8497, loss: 4.5488
	step 9318:lm_loss: 4.5201, ppl: 91.8433, loss: 4.5487
	step 9319:lm_loss: 4.5201, ppl: 91.8442, loss: 4.5487
	step 9320:lm_loss: 4.5200, ppl: 91.8357, loss: 4.5486
	step 9321:lm_loss: 4.5201, ppl: 91.8449, loss: 4.5487
	step 9322:lm_loss: 4.5199, ppl: 91.8250, loss: 4.5486
	step 9323:lm_loss: 4.5198, ppl: 91.8189, loss: 4.5485
	step 9324:lm_loss: 4.5198, ppl: 91.8155, loss: 4.5484
	step 9325:lm_loss: 4.5199, ppl: 91.8249, loss: 4.5485
	step 9326:lm_loss: 4.5200, ppl: 91.8358, loss: 4.5486
	step 9327:lm_loss: 4.5200, ppl: 91.8382, loss: 4.5486
	step 9328:lm_loss: 4.5200, ppl: 91.8364, loss: 4.5486
	step 9329:lm_loss: 4.5200, ppl: 91.8319, loss: 4.5486
	step 9330:lm_loss: 4.5201, ppl: 91.8446, loss: 4.5487
	step 9331:lm_loss: 4.5200, ppl: 91.8382, loss: 4.5486
	step 9332:lm_loss: 4.5201, ppl: 91.8464, loss: 4.5487
	step 9333:lm_loss: 4.5201, ppl: 91.8460, loss: 4.5487
	step 9334:lm_loss: 4.5200, ppl: 91.8366, loss: 4.5486
	step 9335:lm_loss: 4.5199, ppl: 91.8260, loss: 4.5485
	step 9336:lm_loss: 4.5199, ppl: 91.8308, loss: 4.5486
	step 9337:lm_loss: 4.5200, ppl: 91.8391, loss: 4.5486
	step 9338:lm_loss: 4.5201, ppl: 91.8448, loss: 4.5487
	step 9339:lm_loss: 4.5202, ppl: 91.8532, loss: 4.5487
	step 9340:lm_loss: 4.5202, ppl: 91.8533, loss: 4.5487
	step 9341:lm_loss: 4.5203, ppl: 91.8608, loss: 4.5489
	step 9342:lm_loss: 4.5203, ppl: 91.8634, loss: 4.5489
	step 9343:lm_loss: 4.5202, ppl: 91.8577, loss: 4.5488
	step 9344:lm_loss: 4.5202, ppl: 91.8555, loss: 4.5488
	step 9345:lm_loss: 4.5203, ppl: 91.8623, loss: 4.5489
	step 9346:lm_loss: 4.5204, ppl: 91.8729, loss: 4.5490
	step 9347:lm_loss: 4.5204, ppl: 91.8687, loss: 4.5489
	step 9348:lm_loss: 4.5204, ppl: 91.8706, loss: 4.5490
	step 9349:lm_loss: 4.5204, ppl: 91.8742, loss: 4.5490
	step 9350:lm_loss: 4.5204, ppl: 91.8745, loss: 4.5490
	step 9351:lm_loss: 4.5205, ppl: 91.8815, loss: 4.5491
	step 9352:lm_loss: 4.5205, ppl: 91.8817, loss: 4.5491
	step 9353:lm_loss: 4.5205, ppl: 91.8841, loss: 4.5491
	step 9354:lm_loss: 4.5206, ppl: 91.8904, loss: 4.5492
	step 9355:lm_loss: 4.5206, ppl: 91.8868, loss: 4.5492
	step 9356:lm_loss: 4.5206, ppl: 91.8897, loss: 4.5492
	step 9357:lm_loss: 4.5206, ppl: 91.8914, loss: 4.5492
	step 9358:lm_loss: 4.5205, ppl: 91.8832, loss: 4.5491
	step 9359:lm_loss: 4.5207, ppl: 91.8957, loss: 4.5493
	step 9360:lm_loss: 4.5206, ppl: 91.8927, loss: 4.5492
	step 9361:lm_loss: 4.5207, ppl: 91.9020, loss: 4.5493
	step 9362:lm_loss: 4.5207, ppl: 91.9021, loss: 4.5493
	step 9363:lm_loss: 4.5208, ppl: 91.9057, loss: 4.5494
	step 9364:lm_loss: 4.5207, ppl: 91.9019, loss: 4.5493
	step 9365:lm_loss: 4.5208, ppl: 91.9102, loss: 4.5494
	step 9366:lm_loss: 4.5207, ppl: 91.8959, loss: 4.5493
	step 9367:lm_loss: 4.5208, ppl: 91.9049, loss: 4.5494
	step 9368:lm_loss: 4.5208, ppl: 91.9073, loss: 4.5494
	step 9369:lm_loss: 4.5208, ppl: 91.9080, loss: 4.5494
	step 9370:lm_loss: 4.5208, ppl: 91.9067, loss: 4.5494
	step 9371:lm_loss: 4.5208, ppl: 91.9087, loss: 4.5494
	step 9372:lm_loss: 4.5208, ppl: 91.9074, loss: 4.5494
	step 9373:lm_loss: 4.5208, ppl: 91.9049, loss: 4.5494
	step 9374:lm_loss: 4.5208, ppl: 91.9081, loss: 4.5494
	step 9375:lm_loss: 4.5207, ppl: 91.9040, loss: 4.5493
	step 9376:lm_loss: 4.5207, ppl: 91.9014, loss: 4.5493
	step 9377:lm_loss: 4.5206, ppl: 91.8927, loss: 4.5492
	step 9378:lm_loss: 4.5205, ppl: 91.8851, loss: 4.5491
	step 9379:lm_loss: 4.5205, ppl: 91.8855, loss: 4.5491
	step 9380:lm_loss: 4.5205, ppl: 91.8817, loss: 4.5490
	step 9381:lm_loss: 4.5205, ppl: 91.8845, loss: 4.5490
	step 9382:lm_loss: 4.5206, ppl: 91.8907, loss: 4.5491
	step 9383:lm_loss: 4.5205, ppl: 91.8825, loss: 4.5490
	step 9384:lm_loss: 4.5205, ppl: 91.8793, loss: 4.5490
	step 9385:lm_loss: 4.5204, ppl: 91.8748, loss: 4.5489
	step 9386:lm_loss: 4.5203, ppl: 91.8618, loss: 4.5488
	step 9387:lm_loss: 4.5204, ppl: 91.8708, loss: 4.5489
	step 9388:lm_loss: 4.5204, ppl: 91.8734, loss: 4.5489
	step 9389:lm_loss: 4.5205, ppl: 91.8772, loss: 4.5489
	step 9390:lm_loss: 4.5205, ppl: 91.8844, loss: 4.5490
	step 9391:lm_loss: 4.5205, ppl: 91.8852, loss: 4.5490
	step 9392:lm_loss: 4.5207, ppl: 91.8971, loss: 4.5492
	step 9393:lm_loss: 4.5207, ppl: 91.8993, loss: 4.5492
	step 9394:lm_loss: 4.5207, ppl: 91.8960, loss: 4.5492
	step 9395:lm_loss: 4.5207, ppl: 91.8972, loss: 4.5492
	step 9396:lm_loss: 4.5206, ppl: 91.8944, loss: 4.5492
	step 9397:lm_loss: 4.5206, ppl: 91.8936, loss: 4.5491
	step 9398:lm_loss: 4.5208, ppl: 91.9068, loss: 4.5493
	step 9399:lm_loss: 4.5208, ppl: 91.9074, loss: 4.5493
	step 9400:lm_loss: 4.5208, ppl: 91.9082, loss: 4.5493
	step 9401:lm_loss: 4.5208, ppl: 91.9046, loss: 4.5492
	step 9402:lm_loss: 4.5209, ppl: 91.9168, loss: 4.5494
	step 9403:lm_loss: 4.5208, ppl: 91.9137, loss: 4.5493
	step 9404:lm_loss: 4.5208, ppl: 91.9055, loss: 4.5492
	step 9405:lm_loss: 4.5208, ppl: 91.9071, loss: 4.5492
	step 9406:lm_loss: 4.5207, ppl: 91.8974, loss: 4.5492
	step 9407:lm_loss: 4.5207, ppl: 91.8994, loss: 4.5492
	step 9408:lm_loss: 4.5207, ppl: 91.9032, loss: 4.5492
	step 9409:lm_loss: 4.5207, ppl: 91.8997, loss: 4.5491
	step 9410:lm_loss: 4.5206, ppl: 91.8886, loss: 4.5490
	step 9411:lm_loss: 4.5206, ppl: 91.8920, loss: 4.5491
	step 9412:lm_loss: 4.5204, ppl: 91.8749, loss: 4.5489
	step 9413:lm_loss: 4.5204, ppl: 91.8692, loss: 4.5489
	step 9414:lm_loss: 4.5202, ppl: 91.8565, loss: 4.5487
	step 9415:lm_loss: 4.5203, ppl: 91.8621, loss: 4.5488
	step 9416:lm_loss: 4.5202, ppl: 91.8583, loss: 4.5487
	step 9417:lm_loss: 4.5204, ppl: 91.8688, loss: 4.5488
	step 9418:lm_loss: 4.5204, ppl: 91.8686, loss: 4.5488
	step 9419:lm_loss: 4.5204, ppl: 91.8721, loss: 4.5488
	step 9420:lm_loss: 4.5204, ppl: 91.8732, loss: 4.5489
	step 9421:lm_loss: 4.5204, ppl: 91.8705, loss: 4.5488
	step 9422:lm_loss: 4.5203, ppl: 91.8675, loss: 4.5488
	step 9423:lm_loss: 4.5203, ppl: 91.8639, loss: 4.5487
	step 9424:lm_loss: 4.5203, ppl: 91.8605, loss: 4.5487
	step 9425:lm_loss: 4.5206, ppl: 91.8871, loss: 4.5490
	step 9426:lm_loss: 4.5206, ppl: 91.8901, loss: 4.5490
	step 9427:lm_loss: 4.5206, ppl: 91.8885, loss: 4.5490
	step 9428:lm_loss: 4.5205, ppl: 91.8801, loss: 4.5489
	step 9429:lm_loss: 4.5205, ppl: 91.8777, loss: 4.5489
	step 9430:lm_loss: 4.5205, ppl: 91.8820, loss: 4.5489
	step 9431:lm_loss: 4.5205, ppl: 91.8841, loss: 4.5490
	step 9432:lm_loss: 4.5205, ppl: 91.8842, loss: 4.5490
	step 9433:lm_loss: 4.5205, ppl: 91.8824, loss: 4.5489
	step 9434:lm_loss: 4.5205, ppl: 91.8792, loss: 4.5489
	step 9435:lm_loss: 4.5205, ppl: 91.8778, loss: 4.5489
	step 9436:lm_loss: 4.5205, ppl: 91.8807, loss: 4.5489
	step 9437:lm_loss: 4.5206, ppl: 91.8882, loss: 4.5490
	step 9438:lm_loss: 4.5206, ppl: 91.8908, loss: 4.5490
	step 9439:lm_loss: 4.5206, ppl: 91.8891, loss: 4.5490
	step 9440:lm_loss: 4.5205, ppl: 91.8788, loss: 4.5489
	step 9441:lm_loss: 4.5204, ppl: 91.8736, loss: 4.5489
	step 9442:lm_loss: 4.5205, ppl: 91.8779, loss: 4.5489
	step 9443:lm_loss: 4.5206, ppl: 91.8900, loss: 4.5490
	step 9444:lm_loss: 4.5206, ppl: 91.8864, loss: 4.5490
	step 9445:lm_loss: 4.5205, ppl: 91.8805, loss: 4.5489
	step 9446:lm_loss: 4.5205, ppl: 91.8774, loss: 4.5488
	step 9447:lm_loss: 4.5203, ppl: 91.8644, loss: 4.5487
	step 9448:lm_loss: 4.5203, ppl: 91.8673, loss: 4.5487
	step 9449:lm_loss: 4.5203, ppl: 91.8677, loss: 4.5487
	step 9450:lm_loss: 4.5203, ppl: 91.8636, loss: 4.5487
	step 9451:lm_loss: 4.5203, ppl: 91.8603, loss: 4.5486
	step 9452:lm_loss: 4.5202, ppl: 91.8534, loss: 4.5486
	step 9453:lm_loss: 4.5201, ppl: 91.8428, loss: 4.5484
	step 9454:lm_loss: 4.5201, ppl: 91.8424, loss: 4.5484
	step 9455:lm_loss: 4.5200, ppl: 91.8366, loss: 4.5484
	step 9456:lm_loss: 4.5201, ppl: 91.8492, loss: 4.5485
	step 9457:lm_loss: 4.5200, ppl: 91.8387, loss: 4.5484
	step 9458:lm_loss: 4.5200, ppl: 91.8367, loss: 4.5484
	step 9459:lm_loss: 4.5201, ppl: 91.8449, loss: 4.5484
	step 9460:lm_loss: 4.5201, ppl: 91.8412, loss: 4.5484
	step 9461:lm_loss: 4.5201, ppl: 91.8408, loss: 4.5484
	step 9462:lm_loss: 4.5201, ppl: 91.8478, loss: 4.5485
	step 9463:lm_loss: 4.5204, ppl: 91.8699, loss: 4.5486
	step 9464:lm_loss: 4.5203, ppl: 91.8610, loss: 4.5485
	step 9465:lm_loss: 4.5202, ppl: 91.8508, loss: 4.5484
	step 9466:lm_loss: 4.5202, ppl: 91.8517, loss: 4.5484
	step 9467:lm_loss: 4.5201, ppl: 91.8431, loss: 4.5483
	step 9468:lm_loss: 4.5201, ppl: 91.8419, loss: 4.5483
	step 9469:lm_loss: 4.5200, ppl: 91.8317, loss: 4.5482
	step 9470:lm_loss: 4.5199, ppl: 91.8282, loss: 4.5482
	step 9471:lm_loss: 4.5200, ppl: 91.8314, loss: 4.5482
	step 9472:lm_loss: 4.5199, ppl: 91.8288, loss: 4.5481
	step 9473:lm_loss: 4.5200, ppl: 91.8335, loss: 4.5482
	step 9474:lm_loss: 4.5201, ppl: 91.8476, loss: 4.5483
	step 9475:lm_loss: 4.5203, ppl: 91.8607, loss: 4.5485
	step 9476:lm_loss: 4.5202, ppl: 91.8557, loss: 4.5485
	step 9477:lm_loss: 4.5202, ppl: 91.8557, loss: 4.5485
	step 9478:lm_loss: 4.5202, ppl: 91.8576, loss: 4.5485
	step 9479:lm_loss: 4.5204, ppl: 91.8754, loss: 4.5487
	step 9480:lm_loss: 4.5205, ppl: 91.8789, loss: 4.5488
	step 9481:lm_loss: 4.5204, ppl: 91.8758, loss: 4.5487
	step 9482:lm_loss: 4.5204, ppl: 91.8709, loss: 4.5487
	step 9483:lm_loss: 4.5201, ppl: 91.8465, loss: 4.5486
	step 9484:lm_loss: 4.5202, ppl: 91.8518, loss: 4.5486
	step 9485:lm_loss: 4.5202, ppl: 91.8554, loss: 4.5487
	step 9486:lm_loss: 4.5203, ppl: 91.8611, loss: 4.5487
	step 9487:lm_loss: 4.5203, ppl: 91.8633, loss: 4.5488
	step 9488:lm_loss: 4.5201, ppl: 91.8439, loss: 4.5486
	step 9489:lm_loss: 4.5201, ppl: 91.8489, loss: 4.5487
	step 9490:lm_loss: 4.5202, ppl: 91.8534, loss: 4.5487
	step 9491:lm_loss: 4.5202, ppl: 91.8551, loss: 4.5487
	step 9492:lm_loss: 4.5203, ppl: 91.8632, loss: 4.5488
	step 9493:lm_loss: 4.5203, ppl: 91.8606, loss: 4.5488
	step 9494:lm_loss: 4.5202, ppl: 91.8556, loss: 4.5487
	step 9495:lm_loss: 4.5203, ppl: 91.8616, loss: 4.5488
	step 9496:lm_loss: 4.5202, ppl: 91.8569, loss: 4.5488
	step 9497:lm_loss: 4.5202, ppl: 91.8543, loss: 4.5487
	step 9498:lm_loss: 4.5202, ppl: 91.8528, loss: 4.5487
	step 9499:lm_loss: 4.5203, ppl: 91.8653, loss: 4.5489
	step 9500:lm_loss: 4.5203, ppl: 91.8661, loss: 4.5489
	step 9501:lm_loss: 4.5203, ppl: 91.8647, loss: 4.5489
	step 9502:lm_loss: 4.5202, ppl: 91.8572, loss: 4.5488
	step 9503:lm_loss: 4.5202, ppl: 91.8554, loss: 4.5487
	step 9504:lm_loss: 4.5201, ppl: 91.8468, loss: 4.5487
	step 9505:lm_loss: 4.5201, ppl: 91.8482, loss: 4.5487
	step 9506:lm_loss: 4.5201, ppl: 91.8473, loss: 4.5487
	step 9507:lm_loss: 4.5201, ppl: 91.8491, loss: 4.5487
	step 9508:lm_loss: 4.5202, ppl: 91.8509, loss: 4.5487
	step 9509:lm_loss: 4.5203, ppl: 91.8633, loss: 4.5489
	step 9510:lm_loss: 4.5203, ppl: 91.8626, loss: 4.5489
	step 9511:lm_loss: 4.5204, ppl: 91.8719, loss: 4.5490
	step 9512:lm_loss: 4.5204, ppl: 91.8762, loss: 4.5490
	step 9513:lm_loss: 4.5205, ppl: 91.8806, loss: 4.5491
	step 9514:lm_loss: 4.5204, ppl: 91.8739, loss: 4.5490
	step 9515:lm_loss: 4.5205, ppl: 91.8805, loss: 4.5491
	step 9516:lm_loss: 4.5205, ppl: 91.8832, loss: 4.5491
	step 9517:lm_loss: 4.5206, ppl: 91.8864, loss: 4.5491
	step 9518:lm_loss: 4.5205, ppl: 91.8818, loss: 4.5491
	step 9519:lm_loss: 4.5205, ppl: 91.8855, loss: 4.5491
	step 9520:lm_loss: 4.5205, ppl: 91.8848, loss: 4.5491
	step 9521:lm_loss: 4.5206, ppl: 91.8893, loss: 4.5492
	step 9522:lm_loss: 4.5205, ppl: 91.8855, loss: 4.5491
	step 9523:lm_loss: 4.5205, ppl: 91.8819, loss: 4.5491
	step 9524:lm_loss: 4.5205, ppl: 91.8825, loss: 4.5491
	step 9525:lm_loss: 4.5205, ppl: 91.8807, loss: 4.5491
	step 9526:lm_loss: 4.5203, ppl: 91.8618, loss: 4.5489
	step 9527:lm_loss: 4.5202, ppl: 91.8583, loss: 4.5488
	step 9528:lm_loss: 4.5204, ppl: 91.8717, loss: 4.5489
	step 9529:lm_loss: 4.5203, ppl: 91.8591, loss: 4.5488
	step 9530:lm_loss: 4.5204, ppl: 91.8703, loss: 4.5489
	step 9531:lm_loss: 4.5204, ppl: 91.8729, loss: 4.5489
	step 9532:lm_loss: 4.5205, ppl: 91.8840, loss: 4.5490
	step 9533:lm_loss: 4.5205, ppl: 91.8777, loss: 4.5489
	step 9534:lm_loss: 4.5204, ppl: 91.8755, loss: 4.5489
	step 9535:lm_loss: 4.5205, ppl: 91.8794, loss: 4.5489
	step 9536:lm_loss: 4.5205, ppl: 91.8829, loss: 4.5490
	step 9537:lm_loss: 4.5206, ppl: 91.8863, loss: 4.5490
	step 9538:lm_loss: 4.5206, ppl: 91.8910, loss: 4.5490
	step 9539:lm_loss: 4.5206, ppl: 91.8898, loss: 4.5490
	step 9540:lm_loss: 4.5206, ppl: 91.8902, loss: 4.5490
	step 9541:lm_loss: 4.5206, ppl: 91.8877, loss: 4.5490
	step 9542:lm_loss: 4.5204, ppl: 91.8761, loss: 4.5489
	step 9543:lm_loss: 4.5206, ppl: 91.8909, loss: 4.5490
	step 9544:lm_loss: 4.5206, ppl: 91.8905, loss: 4.5490
	step 9545:lm_loss: 4.5206, ppl: 91.8931, loss: 4.5490
	step 9546:lm_loss: 4.5206, ppl: 91.8914, loss: 4.5490
	step 9547:lm_loss: 4.5207, ppl: 91.8954, loss: 4.5490
	step 9548:lm_loss: 4.5207, ppl: 91.8981, loss: 4.5490
	step 9549:lm_loss: 4.5207, ppl: 91.9033, loss: 4.5491
	step 9550:lm_loss: 4.5207, ppl: 91.9003, loss: 4.5491
	step 9551:lm_loss: 4.5208, ppl: 91.9051, loss: 4.5491
	step 9552:lm_loss: 4.5208, ppl: 91.9122, loss: 4.5492
	step 9553:lm_loss: 4.5210, ppl: 91.9297, loss: 4.5494
	step 9554:lm_loss: 4.5213, ppl: 91.9510, loss: 4.5497
	step 9555:lm_loss: 4.5213, ppl: 91.9573, loss: 4.5497
	step 9556:lm_loss: 4.5214, ppl: 91.9677, loss: 4.5498
	step 9557:lm_loss: 4.5215, ppl: 91.9701, loss: 4.5498
	step 9558:lm_loss: 4.5214, ppl: 91.9637, loss: 4.5498
	step 9559:lm_loss: 4.5214, ppl: 91.9667, loss: 4.5498
	step 9560:lm_loss: 4.5214, ppl: 91.9631, loss: 4.5498
	step 9561:lm_loss: 4.5214, ppl: 91.9664, loss: 4.5498
	step 9562:lm_loss: 4.5214, ppl: 91.9643, loss: 4.5498
	step 9563:lm_loss: 4.5214, ppl: 91.9612, loss: 4.5498
	step 9564:lm_loss: 4.5214, ppl: 91.9631, loss: 4.5498
	step 9565:lm_loss: 4.5214, ppl: 91.9676, loss: 4.5498
	step 9566:lm_loss: 4.5214, ppl: 91.9672, loss: 4.5498
	step 9567:lm_loss: 4.5214, ppl: 91.9642, loss: 4.5498
	step 9568:lm_loss: 4.5213, ppl: 91.9532, loss: 4.5497
	step 9569:lm_loss: 4.5213, ppl: 91.9524, loss: 4.5497
	step 9570:lm_loss: 4.5213, ppl: 91.9556, loss: 4.5497
	step 9571:lm_loss: 4.5213, ppl: 91.9551, loss: 4.5497
	step 9572:lm_loss: 4.5213, ppl: 91.9541, loss: 4.5497
	step 9573:lm_loss: 4.5211, ppl: 91.9346, loss: 4.5495
	step 9574:lm_loss: 4.5212, ppl: 91.9450, loss: 4.5496
	step 9575:lm_loss: 4.5212, ppl: 91.9495, loss: 4.5496
	step 9576:lm_loss: 4.5212, ppl: 91.9459, loss: 4.5496
	step 9577:lm_loss: 4.5212, ppl: 91.9436, loss: 4.5495
	step 9578:lm_loss: 4.5213, ppl: 91.9576, loss: 4.5497
	step 9579:lm_loss: 4.5213, ppl: 91.9551, loss: 4.5497
	step 9580:lm_loss: 4.5213, ppl: 91.9530, loss: 4.5496
	step 9581:lm_loss: 4.5213, ppl: 91.9525, loss: 4.5496
	step 9582:lm_loss: 4.5213, ppl: 91.9584, loss: 4.5497
	step 9583:lm_loss: 4.5213, ppl: 91.9545, loss: 4.5496
	step 9584:lm_loss: 4.5211, ppl: 91.9409, loss: 4.5495
	step 9585:lm_loss: 4.5211, ppl: 91.9335, loss: 4.5494
	step 9586:lm_loss: 4.5210, ppl: 91.9289, loss: 4.5493
	step 9587:lm_loss: 4.5210, ppl: 91.9287, loss: 4.5493
	step 9588:lm_loss: 4.5211, ppl: 91.9360, loss: 4.5494
	step 9589:lm_loss: 4.5211, ppl: 91.9321, loss: 4.5493
	step 9590:lm_loss: 4.5209, ppl: 91.9223, loss: 4.5492
	step 9591:lm_loss: 4.5209, ppl: 91.9162, loss: 4.5492
	step 9592:lm_loss: 4.5208, ppl: 91.9055, loss: 4.5491
	step 9593:lm_loss: 4.5207, ppl: 91.8986, loss: 4.5490
	step 9594:lm_loss: 4.5207, ppl: 91.9043, loss: 4.5491
	step 9595:lm_loss: 4.5208, ppl: 91.9085, loss: 4.5491
	step 9596:lm_loss: 4.5207, ppl: 91.9015, loss: 4.5490
	step 9597:lm_loss: 4.5207, ppl: 91.9001, loss: 4.5490
	step 9598:lm_loss: 4.5207, ppl: 91.8962, loss: 4.5489
	step 9599:lm_loss: 4.5207, ppl: 91.9003, loss: 4.5490
	step 9600:lm_loss: 4.5207, ppl: 91.9019, loss: 4.5490
	step 9601:lm_loss: 4.5207, ppl: 91.8974, loss: 4.5489
	step 9602:lm_loss: 4.5207, ppl: 91.8983, loss: 4.5489
	step 9603:lm_loss: 4.5208, ppl: 91.9082, loss: 4.5491
	step 9604:lm_loss: 4.5208, ppl: 91.9069, loss: 4.5491
	step 9605:lm_loss: 4.5208, ppl: 91.9069, loss: 4.5491
	step 9606:lm_loss: 4.5208, ppl: 91.9086, loss: 4.5491
	step 9607:lm_loss: 4.5208, ppl: 91.9113, loss: 4.5491
	step 9608:lm_loss: 4.5208, ppl: 91.9058, loss: 4.5490
	step 9609:lm_loss: 4.5208, ppl: 91.9064, loss: 4.5490
	step 9610:lm_loss: 4.5206, ppl: 91.8938, loss: 4.5489
	step 9611:lm_loss: 4.5206, ppl: 91.8889, loss: 4.5489
	step 9612:lm_loss: 4.5207, ppl: 91.8975, loss: 4.5490
	step 9613:lm_loss: 4.5209, ppl: 91.9182, loss: 4.5492
	step 9614:lm_loss: 4.5209, ppl: 91.9170, loss: 4.5492
	step 9615:lm_loss: 4.5207, ppl: 91.9044, loss: 4.5491
	step 9616:lm_loss: 4.5207, ppl: 91.8992, loss: 4.5490
	step 9617:lm_loss: 4.5208, ppl: 91.9078, loss: 4.5491
	step 9618:lm_loss: 4.5209, ppl: 91.9160, loss: 4.5492
	step 9619:lm_loss: 4.5210, ppl: 91.9242, loss: 4.5493
	step 9620:lm_loss: 4.5210, ppl: 91.9291, loss: 4.5493
	step 9621:lm_loss: 4.5211, ppl: 91.9332, loss: 4.5494
	step 9622:lm_loss: 4.5210, ppl: 91.9279, loss: 4.5493
	step 9623:lm_loss: 4.5210, ppl: 91.9297, loss: 4.5493
	step 9624:lm_loss: 4.5211, ppl: 91.9401, loss: 4.5495
	step 9625:lm_loss: 4.5211, ppl: 91.9372, loss: 4.5494
	step 9626:lm_loss: 4.5211, ppl: 91.9375, loss: 4.5494
	step 9627:lm_loss: 4.5210, ppl: 91.9298, loss: 4.5493
	step 9628:lm_loss: 4.5210, ppl: 91.9240, loss: 4.5492
	step 9629:lm_loss: 4.5208, ppl: 91.9130, loss: 4.5491
	step 9630:lm_loss: 4.5209, ppl: 91.9138, loss: 4.5492
	step 9631:lm_loss: 4.5208, ppl: 91.9129, loss: 4.5491
	step 9632:lm_loss: 4.5209, ppl: 91.9158, loss: 4.5492
	step 9633:lm_loss: 4.5208, ppl: 91.9116, loss: 4.5491
	step 9634:lm_loss: 4.5209, ppl: 91.9178, loss: 4.5492
	step 9635:lm_loss: 4.5209, ppl: 91.9186, loss: 4.5492
	step 9636:lm_loss: 4.5209, ppl: 91.9223, loss: 4.5492
	step 9637:lm_loss: 4.5208, ppl: 91.9109, loss: 4.5491
	step 9638:lm_loss: 4.5209, ppl: 91.9160, loss: 4.5492
	step 9639:lm_loss: 4.5208, ppl: 91.9101, loss: 4.5491
	step 9640:lm_loss: 4.5208, ppl: 91.9046, loss: 4.5490
	step 9641:lm_loss: 4.5207, ppl: 91.8991, loss: 4.5490
	step 9642:lm_loss: 4.5206, ppl: 91.8913, loss: 4.5489
	step 9643:lm_loss: 4.5206, ppl: 91.8925, loss: 4.5489
	step 9644:lm_loss: 4.5207, ppl: 91.8986, loss: 4.5490
	step 9645:lm_loss: 4.5206, ppl: 91.8948, loss: 4.5489
	step 9646:lm_loss: 4.5208, ppl: 91.9115, loss: 4.5490
	step 9647:lm_loss: 4.5209, ppl: 91.9175, loss: 4.5491
	step 9648:lm_loss: 4.5209, ppl: 91.9176, loss: 4.5491
	step 9649:lm_loss: 4.5209, ppl: 91.9150, loss: 4.5490
	step 9650:lm_loss: 4.5209, ppl: 91.9174, loss: 4.5491
	step 9651:lm_loss: 4.5209, ppl: 91.9184, loss: 4.5491
	step 9652:lm_loss: 4.5210, ppl: 91.9241, loss: 4.5491
	step 9653:lm_loss: 4.5210, ppl: 91.9246, loss: 4.5491
	step 9654:lm_loss: 4.5210, ppl: 91.9293, loss: 4.5492
	step 9655:lm_loss: 4.5210, ppl: 91.9261, loss: 4.5492
	step 9656:lm_loss: 4.5210, ppl: 91.9310, loss: 4.5492
	step 9657:lm_loss: 4.5210, ppl: 91.9292, loss: 4.5492
	step 9658:lm_loss: 4.5210, ppl: 91.9231, loss: 4.5491
	step 9659:lm_loss: 4.5210, ppl: 91.9270, loss: 4.5491
	step 9660:lm_loss: 4.5211, ppl: 91.9340, loss: 4.5492
	step 9661:lm_loss: 4.5211, ppl: 91.9407, loss: 4.5493
	step 9662:lm_loss: 4.5212, ppl: 91.9451, loss: 4.5494
	step 9663:lm_loss: 4.5212, ppl: 91.9451, loss: 4.5494
	step 9664:lm_loss: 4.5212, ppl: 91.9428, loss: 4.5493
	step 9665:lm_loss: 4.5212, ppl: 91.9452, loss: 4.5493
	step 9666:lm_loss: 4.5212, ppl: 91.9496, loss: 4.5494
	step 9667:lm_loss: 4.5213, ppl: 91.9509, loss: 4.5494
	step 9668:lm_loss: 4.5214, ppl: 91.9645, loss: 4.5495
	step 9669:lm_loss: 4.5215, ppl: 91.9745, loss: 4.5496
	step 9670:lm_loss: 4.5215, ppl: 91.9711, loss: 4.5495
	step 9671:lm_loss: 4.5216, ppl: 91.9801, loss: 4.5496
	step 9672:lm_loss: 4.5216, ppl: 91.9809, loss: 4.5496
	step 9673:lm_loss: 4.5216, ppl: 91.9866, loss: 4.5497
	step 9674:lm_loss: 4.5215, ppl: 91.9780, loss: 4.5496
	step 9675:lm_loss: 4.5216, ppl: 91.9796, loss: 4.5496
	step 9676:lm_loss: 4.5216, ppl: 91.9834, loss: 4.5497
	step 9677:lm_loss: 4.5217, ppl: 91.9926, loss: 4.5498
	step 9678:lm_loss: 4.5217, ppl: 91.9935, loss: 4.5498
	step 9679:lm_loss: 4.5217, ppl: 91.9964, loss: 4.5498
	step 9680:lm_loss: 4.5218, ppl: 91.9997, loss: 4.5499
	step 9681:lm_loss: 4.5218, ppl: 92.0026, loss: 4.5499
	step 9682:lm_loss: 4.5219, ppl: 92.0076, loss: 4.5499
	step 9683:lm_loss: 4.5218, ppl: 92.0008, loss: 4.5498
	step 9684:lm_loss: 4.5219, ppl: 92.0066, loss: 4.5499
	step 9685:lm_loss: 4.5219, ppl: 92.0077, loss: 4.5499
	step 9686:lm_loss: 4.5219, ppl: 92.0090, loss: 4.5499
	step 9687:lm_loss: 4.5219, ppl: 92.0124, loss: 4.5499
	step 9688:lm_loss: 4.5219, ppl: 92.0122, loss: 4.5499
	step 9689:lm_loss: 4.5220, ppl: 92.0161, loss: 4.5500
	step 9690:lm_loss: 4.5220, ppl: 92.0200, loss: 4.5500
	step 9691:lm_loss: 4.5220, ppl: 92.0234, loss: 4.5500
	step 9692:lm_loss: 4.5221, ppl: 92.0242, loss: 4.5501
	step 9693:lm_loss: 4.5220, ppl: 92.0219, loss: 4.5500
	step 9694:lm_loss: 4.5220, ppl: 92.0193, loss: 4.5500
	step 9695:lm_loss: 4.5221, ppl: 92.0265, loss: 4.5501
	step 9696:lm_loss: 4.5221, ppl: 92.0246, loss: 4.5500
	step 9697:lm_loss: 4.5221, ppl: 92.0312, loss: 4.5501
	step 9698:lm_loss: 4.5221, ppl: 92.0264, loss: 4.5501
	step 9699:lm_loss: 4.5220, ppl: 92.0239, loss: 4.5501
	step 9700:lm_loss: 4.5220, ppl: 92.0231, loss: 4.5501
	step 9701:lm_loss: 4.5221, ppl: 92.0282, loss: 4.5501
	step 9702:lm_loss: 4.5220, ppl: 92.0222, loss: 4.5500
	step 9703:lm_loss: 4.5221, ppl: 92.0294, loss: 4.5501
	step 9704:lm_loss: 4.5220, ppl: 92.0216, loss: 4.5500
	step 9705:lm_loss: 4.5221, ppl: 92.0283, loss: 4.5501
	step 9706:lm_loss: 4.5222, ppl: 92.0336, loss: 4.5501
	step 9707:lm_loss: 4.5221, ppl: 92.0326, loss: 4.5501
	step 9708:lm_loss: 4.5221, ppl: 92.0279, loss: 4.5501
	step 9709:lm_loss: 4.5221, ppl: 92.0277, loss: 4.5501
	step 9710:lm_loss: 4.5221, ppl: 92.0315, loss: 4.5501
	step 9711:lm_loss: 4.5221, ppl: 92.0332, loss: 4.5501
	step 9712:lm_loss: 4.5222, ppl: 92.0358, loss: 4.5502
	step 9713:lm_loss: 4.5221, ppl: 92.0311, loss: 4.5501
	step 9714:lm_loss: 4.5219, ppl: 92.0079, loss: 4.5499
	step 9715:lm_loss: 4.5219, ppl: 92.0100, loss: 4.5500
	step 9716:lm_loss: 4.5219, ppl: 92.0114, loss: 4.5500
	step 9717:lm_loss: 4.5218, ppl: 92.0036, loss: 4.5500
	step 9718:lm_loss: 4.5217, ppl: 91.9912, loss: 4.5498
	step 9719:lm_loss: 4.5217, ppl: 91.9964, loss: 4.5498
	step 9720:lm_loss: 4.5218, ppl: 92.0041, loss: 4.5499
	step 9721:lm_loss: 4.5219, ppl: 92.0094, loss: 4.5500
	step 9722:lm_loss: 4.5219, ppl: 92.0099, loss: 4.5500
	step 9723:lm_loss: 4.5219, ppl: 92.0081, loss: 4.5500
	step 9724:lm_loss: 4.5220, ppl: 92.0225, loss: 4.5501
	step 9725:lm_loss: 4.5221, ppl: 92.0269, loss: 4.5502
	step 9726:lm_loss: 4.5220, ppl: 92.0166, loss: 4.5501
	step 9727:lm_loss: 4.5220, ppl: 92.0212, loss: 4.5501
	step 9728:lm_loss: 4.5220, ppl: 92.0219, loss: 4.5501
	step 9729:lm_loss: 4.5222, ppl: 92.0348, loss: 4.5503
	step 9730:lm_loss: 4.5222, ppl: 92.0361, loss: 4.5503
	step 9731:lm_loss: 4.5222, ppl: 92.0342, loss: 4.5502
	step 9732:lm_loss: 4.5221, ppl: 92.0302, loss: 4.5502
	step 9733:lm_loss: 4.5223, ppl: 92.0437, loss: 4.5504
	step 9734:lm_loss: 4.5223, ppl: 92.0454, loss: 4.5504
	step 9735:lm_loss: 4.5223, ppl: 92.0512, loss: 4.5504
	step 9736:lm_loss: 4.5222, ppl: 92.0394, loss: 4.5504
	step 9737:lm_loss: 4.5222, ppl: 92.0355, loss: 4.5503
	step 9738:lm_loss: 4.5222, ppl: 92.0393, loss: 4.5503
	step 9739:lm_loss: 4.5222, ppl: 92.0385, loss: 4.5503
	step 9740:lm_loss: 4.5223, ppl: 92.0457, loss: 4.5504
	step 9741:lm_loss: 4.5222, ppl: 92.0370, loss: 4.5503
	step 9742:lm_loss: 4.5222, ppl: 92.0381, loss: 4.5504
	step 9743:lm_loss: 4.5223, ppl: 92.0439, loss: 4.5504
	step 9744:lm_loss: 4.5222, ppl: 92.0409, loss: 4.5504
	step 9745:lm_loss: 4.5222, ppl: 92.0408, loss: 4.5504
	step 9746:lm_loss: 4.5224, ppl: 92.0524, loss: 4.5505
	step 9747:lm_loss: 4.5224, ppl: 92.0546, loss: 4.5506
	step 9748:lm_loss: 4.5224, ppl: 92.0571, loss: 4.5506
	step 9749:lm_loss: 4.5224, ppl: 92.0607, loss: 4.5506
	step 9750:lm_loss: 4.5225, ppl: 92.0623, loss: 4.5507
	step 9751:lm_loss: 4.5226, ppl: 92.0731, loss: 4.5508
	step 9752:lm_loss: 4.5225, ppl: 92.0655, loss: 4.5507
	step 9753:lm_loss: 4.5225, ppl: 92.0667, loss: 4.5507
	step 9754:lm_loss: 4.5225, ppl: 92.0664, loss: 4.5507
	step 9755:lm_loss: 4.5226, ppl: 92.0757, loss: 4.5508
	step 9756:lm_loss: 4.5224, ppl: 92.0601, loss: 4.5506
	step 9757:lm_loss: 4.5225, ppl: 92.0623, loss: 4.5507
	step 9758:lm_loss: 4.5225, ppl: 92.0667, loss: 4.5507
	step 9759:lm_loss: 4.5224, ppl: 92.0570, loss: 4.5506
	step 9760:lm_loss: 4.5224, ppl: 92.0543, loss: 4.5506
	step 9761:lm_loss: 4.5225, ppl: 92.0645, loss: 4.5507
	step 9762:lm_loss: 4.5225, ppl: 92.0619, loss: 4.5507
	step 9763:lm_loss: 4.5226, ppl: 92.0744, loss: 4.5508
	step 9764:lm_loss: 4.5224, ppl: 92.0562, loss: 4.5506
	step 9765:lm_loss: 4.5223, ppl: 92.0462, loss: 4.5506
	step 9766:lm_loss: 4.5225, ppl: 92.0628, loss: 4.5507
	step 9767:lm_loss: 4.5225, ppl: 92.0628, loss: 4.5507
	step 9768:lm_loss: 4.5224, ppl: 92.0600, loss: 4.5506
	step 9769:lm_loss: 4.5222, ppl: 92.0418, loss: 4.5505
	step 9770:lm_loss: 4.5224, ppl: 92.0517, loss: 4.5506
	step 9771:lm_loss: 4.5224, ppl: 92.0563, loss: 4.5506
	step 9772:lm_loss: 4.5224, ppl: 92.0529, loss: 4.5506
	step 9773:lm_loss: 4.5221, ppl: 92.0320, loss: 4.5505
	step 9774:lm_loss: 4.5221, ppl: 92.0319, loss: 4.5505
	step 9775:lm_loss: 4.5220, ppl: 92.0201, loss: 4.5503
	step 9776:lm_loss: 4.5220, ppl: 92.0182, loss: 4.5503
	step 9777:lm_loss: 4.5220, ppl: 92.0158, loss: 4.5503
	step 9778:lm_loss: 4.5220, ppl: 92.0223, loss: 4.5503
	step 9779:lm_loss: 4.5220, ppl: 92.0223, loss: 4.5503
	step 9780:lm_loss: 4.5220, ppl: 92.0171, loss: 4.5503
	step 9781:lm_loss: 4.5219, ppl: 92.0147, loss: 4.5502
	step 9782:lm_loss: 4.5220, ppl: 92.0174, loss: 4.5503
	step 9783:lm_loss: 4.5217, ppl: 91.9963, loss: 4.5501
	step 9784:lm_loss: 4.5218, ppl: 91.9979, loss: 4.5502
	step 9785:lm_loss: 4.5217, ppl: 91.9882, loss: 4.5501
	step 9786:lm_loss: 4.5215, ppl: 91.9777, loss: 4.5500
	step 9787:lm_loss: 4.5216, ppl: 91.9841, loss: 4.5500
	step 9788:lm_loss: 4.5216, ppl: 91.9808, loss: 4.5500
	step 9789:lm_loss: 4.5216, ppl: 91.9860, loss: 4.5501
	step 9790:lm_loss: 4.5214, ppl: 91.9638, loss: 4.5500
	step 9791:lm_loss: 4.5213, ppl: 91.9508, loss: 4.5499
	step 9792:lm_loss: 4.5214, ppl: 91.9614, loss: 4.5499
	step 9793:lm_loss: 4.5213, ppl: 91.9538, loss: 4.5498
	step 9794:lm_loss: 4.5214, ppl: 91.9637, loss: 4.5500
	step 9795:lm_loss: 4.5214, ppl: 91.9631, loss: 4.5499
	step 9796:lm_loss: 4.5214, ppl: 91.9680, loss: 4.5500
	step 9797:lm_loss: 4.5214, ppl: 91.9677, loss: 4.5500
	step 9798:lm_loss: 4.5214, ppl: 91.9632, loss: 4.5499
	step 9799:lm_loss: 4.5214, ppl: 91.9650, loss: 4.5499
	step 9800:lm_loss: 4.5214, ppl: 91.9633, loss: 4.5499
	step 9801:lm_loss: 4.5214, ppl: 91.9683, loss: 4.5500
	step 9802:lm_loss: 4.5212, ppl: 91.9504, loss: 4.5498
	step 9803:lm_loss: 4.5213, ppl: 91.9560, loss: 4.5498
	step 9804:lm_loss: 4.5213, ppl: 91.9520, loss: 4.5498
	step 9805:lm_loss: 4.5213, ppl: 91.9534, loss: 4.5498
	step 9806:lm_loss: 4.5213, ppl: 91.9539, loss: 4.5498
	step 9807:lm_loss: 4.5214, ppl: 91.9603, loss: 4.5499
	step 9808:lm_loss: 4.5213, ppl: 91.9547, loss: 4.5498
	step 9809:lm_loss: 4.5211, ppl: 91.9387, loss: 4.5496
	step 9810:lm_loss: 4.5211, ppl: 91.9349, loss: 4.5496
	step 9811:lm_loss: 4.5210, ppl: 91.9305, loss: 4.5495
	step 9812:lm_loss: 4.5209, ppl: 91.9222, loss: 4.5495
	step 9813:lm_loss: 4.5209, ppl: 91.9164, loss: 4.5494
	step 9814:lm_loss: 4.5209, ppl: 91.9146, loss: 4.5494
	step 9815:lm_loss: 4.5209, ppl: 91.9190, loss: 4.5494
	step 9816:lm_loss: 4.5209, ppl: 91.9202, loss: 4.5495
	step 9817:lm_loss: 4.5210, ppl: 91.9295, loss: 4.5496
	step 9818:lm_loss: 4.5210, ppl: 91.9303, loss: 4.5496
	step 9819:lm_loss: 4.5210, ppl: 91.9301, loss: 4.5496
	step 9820:lm_loss: 4.5211, ppl: 91.9322, loss: 4.5496
	step 9821:lm_loss: 4.5211, ppl: 91.9349, loss: 4.5496
	step 9822:lm_loss: 4.5209, ppl: 91.9214, loss: 4.5495
	step 9823:lm_loss: 4.5209, ppl: 91.9156, loss: 4.5494
	step 9824:lm_loss: 4.5208, ppl: 91.9103, loss: 4.5494
	step 9825:lm_loss: 4.5208, ppl: 91.9122, loss: 4.5494
	step 9826:lm_loss: 4.5208, ppl: 91.9102, loss: 4.5494
	step 9827:lm_loss: 4.5208, ppl: 91.9101, loss: 4.5494
	step 9828:lm_loss: 4.5208, ppl: 91.9072, loss: 4.5493
	step 9829:lm_loss: 4.5209, ppl: 91.9145, loss: 4.5494
	step 9830:lm_loss: 4.5208, ppl: 91.9115, loss: 4.5494
	step 9831:lm_loss: 4.5209, ppl: 91.9140, loss: 4.5494
	step 9832:lm_loss: 4.5208, ppl: 91.9137, loss: 4.5494
	step 9833:lm_loss: 4.5208, ppl: 91.9108, loss: 4.5493
	step 9834:lm_loss: 4.5208, ppl: 91.9132, loss: 4.5494
	step 9835:lm_loss: 4.5208, ppl: 91.9112, loss: 4.5494
	step 9836:lm_loss: 4.5209, ppl: 91.9159, loss: 4.5494
	step 9837:lm_loss: 4.5208, ppl: 91.9121, loss: 4.5494
	step 9838:lm_loss: 4.5208, ppl: 91.9075, loss: 4.5493
	step 9839:lm_loss: 4.5208, ppl: 91.9056, loss: 4.5493
	step 9840:lm_loss: 4.5208, ppl: 91.9081, loss: 4.5493
	step 9841:lm_loss: 4.5208, ppl: 91.9099, loss: 4.5493
	step 9842:lm_loss: 4.5208, ppl: 91.9073, loss: 4.5493
	step 9843:lm_loss: 4.5207, ppl: 91.9012, loss: 4.5492
	step 9844:lm_loss: 4.5207, ppl: 91.8987, loss: 4.5492
	step 9845:lm_loss: 4.5207, ppl: 91.9020, loss: 4.5493
	step 9846:lm_loss: 4.5207, ppl: 91.8991, loss: 4.5492
	step 9847:lm_loss: 4.5207, ppl: 91.8995, loss: 4.5492
	step 9848:lm_loss: 4.5206, ppl: 91.8916, loss: 4.5491
	step 9849:lm_loss: 4.5206, ppl: 91.8911, loss: 4.5491
	step 9850:lm_loss: 4.5206, ppl: 91.8924, loss: 4.5491
	step 9851:lm_loss: 4.5207, ppl: 91.9005, loss: 4.5491
	step 9852:lm_loss: 4.5207, ppl: 91.9020, loss: 4.5492
	step 9853:lm_loss: 4.5207, ppl: 91.9008, loss: 4.5491
	step 9854:lm_loss: 4.5207, ppl: 91.9016, loss: 4.5492
	step 9855:lm_loss: 4.5207, ppl: 91.9016, loss: 4.5492
	step 9856:lm_loss: 4.5208, ppl: 91.9061, loss: 4.5492
	step 9857:lm_loss: 4.5208, ppl: 91.9082, loss: 4.5492
	step 9858:lm_loss: 4.5208, ppl: 91.9078, loss: 4.5492
	step 9859:lm_loss: 4.5207, ppl: 91.9035, loss: 4.5491
	step 9860:lm_loss: 4.5208, ppl: 91.9064, loss: 4.5492
	step 9861:lm_loss: 4.5207, ppl: 91.8976, loss: 4.5491
	step 9862:lm_loss: 4.5207, ppl: 91.8976, loss: 4.5491
	step 9863:lm_loss: 4.5206, ppl: 91.8946, loss: 4.5490
	step 9864:lm_loss: 4.5206, ppl: 91.8931, loss: 4.5490
	step 9865:lm_loss: 4.5206, ppl: 91.8937, loss: 4.5490
	step 9866:lm_loss: 4.5207, ppl: 91.9019, loss: 4.5491
	step 9867:lm_loss: 4.5207, ppl: 91.9024, loss: 4.5491
	step 9868:lm_loss: 4.5208, ppl: 91.9109, loss: 4.5492
	step 9869:lm_loss: 4.5208, ppl: 91.9127, loss: 4.5492
	step 9870:lm_loss: 4.5209, ppl: 91.9148, loss: 4.5493
	step 9871:lm_loss: 4.5209, ppl: 91.9203, loss: 4.5493
	step 9872:lm_loss: 4.5209, ppl: 91.9221, loss: 4.5493
	step 9873:lm_loss: 4.5208, ppl: 91.9098, loss: 4.5492
	step 9874:lm_loss: 4.5208, ppl: 91.9070, loss: 4.5491
	step 9875:lm_loss: 4.5208, ppl: 91.9046, loss: 4.5491
	step 9876:lm_loss: 4.5208, ppl: 91.9092, loss: 4.5492
	step 9877:lm_loss: 4.5209, ppl: 91.9147, loss: 4.5492
	step 9878:lm_loss: 4.5208, ppl: 91.9114, loss: 4.5492
	step 9879:lm_loss: 4.5208, ppl: 91.9101, loss: 4.5492
	step 9880:lm_loss: 4.5208, ppl: 91.9086, loss: 4.5491
	step 9881:lm_loss: 4.5207, ppl: 91.9036, loss: 4.5491
	step 9882:lm_loss: 4.5208, ppl: 91.9057, loss: 4.5491
	step 9883:lm_loss: 4.5209, ppl: 91.9146, loss: 4.5492
	step 9884:lm_loss: 4.5208, ppl: 91.9121, loss: 4.5492
	step 9885:lm_loss: 4.5209, ppl: 91.9176, loss: 4.5492
	step 9886:lm_loss: 4.5208, ppl: 91.9113, loss: 4.5491
	step 9887:lm_loss: 4.5208, ppl: 91.9086, loss: 4.5491
	step 9888:lm_loss: 4.5209, ppl: 91.9138, loss: 4.5492
	step 9889:lm_loss: 4.5209, ppl: 91.9138, loss: 4.5492
	step 9890:lm_loss: 4.5209, ppl: 91.9181, loss: 4.5492
	step 9891:lm_loss: 4.5209, ppl: 91.9207, loss: 4.5493
	step 9892:lm_loss: 4.5208, ppl: 91.9094, loss: 4.5492
	step 9893:lm_loss: 4.5209, ppl: 91.9190, loss: 4.5493
	step 9894:lm_loss: 4.5210, ppl: 91.9265, loss: 4.5494
	step 9895:lm_loss: 4.5210, ppl: 91.9266, loss: 4.5494
	step 9896:lm_loss: 4.5208, ppl: 91.9056, loss: 4.5492
	step 9897:lm_loss: 4.5206, ppl: 91.8953, loss: 4.5491
	step 9898:lm_loss: 4.5206, ppl: 91.8874, loss: 4.5489
	step 9899:lm_loss: 4.5207, ppl: 91.9013, loss: 4.5491
	step 9900:lm_loss: 4.5208, ppl: 91.9128, loss: 4.5493
	step 9901:lm_loss: 4.5209, ppl: 91.9171, loss: 4.5493
	step 9902:lm_loss: 4.5208, ppl: 91.9120, loss: 4.5492
	step 9903:lm_loss: 4.5209, ppl: 91.9196, loss: 4.5493
	step 9904:lm_loss: 4.5208, ppl: 91.9072, loss: 4.5492
	step 9905:lm_loss: 4.5207, ppl: 91.9044, loss: 4.5491
	step 9906:lm_loss: 4.5207, ppl: 91.8963, loss: 4.5491
	step 9907:lm_loss: 4.5207, ppl: 91.8983, loss: 4.5491
	step 9908:lm_loss: 4.5206, ppl: 91.8926, loss: 4.5490
	step 9909:lm_loss: 4.5207, ppl: 91.8978, loss: 4.5491
	step 9910:lm_loss: 4.5207, ppl: 91.8986, loss: 4.5491
	step 9911:lm_loss: 4.5207, ppl: 91.8988, loss: 4.5491
	step 9912:lm_loss: 4.5207, ppl: 91.8964, loss: 4.5490
	step 9913:lm_loss: 4.5205, ppl: 91.8842, loss: 4.5490
	step 9914:lm_loss: 4.5204, ppl: 91.8702, loss: 4.5488
	step 9915:lm_loss: 4.5203, ppl: 91.8629, loss: 4.5487
	step 9916:lm_loss: 4.5203, ppl: 91.8611, loss: 4.5487
	step 9917:lm_loss: 4.5202, ppl: 91.8553, loss: 4.5486
	step 9918:lm_loss: 4.5200, ppl: 91.8349, loss: 4.5483
	step 9919:lm_loss: 4.5201, ppl: 91.8407, loss: 4.5484
	step 9920:lm_loss: 4.5201, ppl: 91.8411, loss: 4.5484
	step 9921:lm_loss: 4.5199, ppl: 91.8301, loss: 4.5483
	step 9922:lm_loss: 4.5201, ppl: 91.8423, loss: 4.5484
	step 9923:lm_loss: 4.5202, ppl: 91.8505, loss: 4.5485
	step 9924:lm_loss: 4.5202, ppl: 91.8579, loss: 4.5486
	step 9925:lm_loss: 4.5202, ppl: 91.8571, loss: 4.5486
	step 9926:lm_loss: 4.5203, ppl: 91.8676, loss: 4.5487
	step 9927:lm_loss: 4.5203, ppl: 91.8629, loss: 4.5487
	step 9928:lm_loss: 4.5202, ppl: 91.8579, loss: 4.5486
	step 9929:lm_loss: 4.5203, ppl: 91.8653, loss: 4.5487
	step 9930:lm_loss: 4.5204, ppl: 91.8684, loss: 4.5487
	step 9931:lm_loss: 4.5203, ppl: 91.8642, loss: 4.5487
	step 9932:lm_loss: 4.5203, ppl: 91.8603, loss: 4.5486
	step 9933:lm_loss: 4.5201, ppl: 91.8425, loss: 4.5485
	step 9934:lm_loss: 4.5201, ppl: 91.8426, loss: 4.5485
	step 9935:lm_loss: 4.5201, ppl: 91.8476, loss: 4.5486
	step 9936:lm_loss: 4.5201, ppl: 91.8478, loss: 4.5486
	step 9937:lm_loss: 4.5201, ppl: 91.8465, loss: 4.5485
	step 9938:lm_loss: 4.5199, ppl: 91.8297, loss: 4.5484
	step 9939:lm_loss: 4.5201, ppl: 91.8431, loss: 4.5486
	step 9940:lm_loss: 4.5201, ppl: 91.8466, loss: 4.5486
	step 9941:lm_loss: 4.5201, ppl: 91.8480, loss: 4.5487
	step 9942:lm_loss: 4.5202, ppl: 91.8512, loss: 4.5487
	step 9943:lm_loss: 4.5202, ppl: 91.8556, loss: 4.5488
	step 9944:lm_loss: 4.5202, ppl: 91.8529, loss: 4.5487
	step 9945:lm_loss: 4.5202, ppl: 91.8536, loss: 4.5487
	step 9946:lm_loss: 4.5204, ppl: 91.8681, loss: 4.5489
	step 9947:lm_loss: 4.5203, ppl: 91.8629, loss: 4.5488
	step 9948:lm_loss: 4.5203, ppl: 91.8638, loss: 4.5488
	step 9949:lm_loss: 4.5203, ppl: 91.8639, loss: 4.5488
	step 9950:lm_loss: 4.5203, ppl: 91.8652, loss: 4.5488
	step 9951:lm_loss: 4.5203, ppl: 91.8628, loss: 4.5488
	step 9952:lm_loss: 4.5203, ppl: 91.8636, loss: 4.5488
	step 9953:lm_loss: 4.5203, ppl: 91.8662, loss: 4.5488
	step 9954:lm_loss: 4.5203, ppl: 91.8649, loss: 4.5488
	step 9955:lm_loss: 4.5203, ppl: 91.8667, loss: 4.5488
	step 9956:lm_loss: 4.5203, ppl: 91.8643, loss: 4.5488
	step 9957:lm_loss: 4.5203, ppl: 91.8593, loss: 4.5487
	step 9958:lm_loss: 4.5203, ppl: 91.8649, loss: 4.5488
	step 9959:lm_loss: 4.5203, ppl: 91.8637, loss: 4.5488
	step 9960:lm_loss: 4.5204, ppl: 91.8684, loss: 4.5488
	step 9961:lm_loss: 4.5203, ppl: 91.8641, loss: 4.5488
	step 9962:lm_loss: 4.5204, ppl: 91.8716, loss: 4.5488
	step 9963:lm_loss: 4.5204, ppl: 91.8761, loss: 4.5489
	step 9964:lm_loss: 4.5204, ppl: 91.8683, loss: 4.5488
	step 9965:lm_loss: 4.5204, ppl: 91.8716, loss: 4.5488
	step 9966:lm_loss: 4.5204, ppl: 91.8705, loss: 4.5488
	step 9967:lm_loss: 4.5204, ppl: 91.8750, loss: 4.5488
	step 9968:lm_loss: 4.5204, ppl: 91.8747, loss: 4.5488
	step 9969:lm_loss: 4.5204, ppl: 91.8690, loss: 4.5487
	step 9970:lm_loss: 4.5204, ppl: 91.8724, loss: 4.5488
	step 9971:lm_loss: 4.5204, ppl: 91.8747, loss: 4.5488
	step 9972:lm_loss: 4.5204, ppl: 91.8755, loss: 4.5488
	step 9973:lm_loss: 4.5204, ppl: 91.8708, loss: 4.5488
	step 9974:lm_loss: 4.5202, ppl: 91.8573, loss: 4.5486
	step 9975:lm_loss: 4.5202, ppl: 91.8580, loss: 4.5486
	step 9976:lm_loss: 4.5202, ppl: 91.8552, loss: 4.5486
	step 9977:lm_loss: 4.5201, ppl: 91.8469, loss: 4.5484
	step 9978:lm_loss: 4.5201, ppl: 91.8432, loss: 4.5484
	step 9979:lm_loss: 4.5200, ppl: 91.8368, loss: 4.5483
	step 9980:lm_loss: 4.5200, ppl: 91.8316, loss: 4.5483
	step 9981:lm_loss: 4.5200, ppl: 91.8402, loss: 4.5484
	step 9982:lm_loss: 4.5200, ppl: 91.8386, loss: 4.5484
	step 9983:lm_loss: 4.5199, ppl: 91.8283, loss: 4.5483
	step 9984:lm_loss: 4.5199, ppl: 91.8268, loss: 4.5483
	step 9985:lm_loss: 4.5200, ppl: 91.8319, loss: 4.5483
	step 9986:lm_loss: 4.5200, ppl: 91.8360, loss: 4.5483
	step 9987:lm_loss: 4.5201, ppl: 91.8426, loss: 4.5484
	step 9988:lm_loss: 4.5202, ppl: 91.8560, loss: 4.5486
	step 9989:lm_loss: 4.5202, ppl: 91.8545, loss: 4.5486
	step 9990:lm_loss: 4.5203, ppl: 91.8586, loss: 4.5486
	step 9991:lm_loss: 4.5204, ppl: 91.8704, loss: 4.5487
	step 9992:lm_loss: 4.5204, ppl: 91.8715, loss: 4.5487
	step 9993:lm_loss: 4.5204, ppl: 91.8678, loss: 4.5487
	step 9994:lm_loss: 4.5203, ppl: 91.8627, loss: 4.5486
	step 9995:lm_loss: 4.5203, ppl: 91.8660, loss: 4.5486
	step 9996:lm_loss: 4.5203, ppl: 91.8644, loss: 4.5486
	step 9997:lm_loss: 4.5203, ppl: 91.8598, loss: 4.5486
	step 9998:lm_loss: 4.5203, ppl: 91.8641, loss: 4.5486
	step 9999:lm_loss: 4.5203, ppl: 91.8602, loss: 4.5485
	step 10000:lm_loss: 4.5203, ppl: 91.8586, loss: 4.5485
	step 10001:lm_loss: 4.5203, ppl: 91.8632, loss: 4.5486
	step 10002:lm_loss: 4.5204, ppl: 91.8709, loss: 4.5486
	step 10003:lm_loss: 4.5203, ppl: 91.8669, loss: 4.5485
	step 10004:lm_loss: 4.5203, ppl: 91.8663, loss: 4.5485
	step 10005:lm_loss: 4.5202, ppl: 91.8549, loss: 4.5484
	step 10006:lm_loss: 4.5202, ppl: 91.8567, loss: 4.5485
	step 10007:lm_loss: 4.5203, ppl: 91.8615, loss: 4.5485
	step 10008:lm_loss: 4.5203, ppl: 91.8676, loss: 4.5486
	step 10009:lm_loss: 4.5205, ppl: 91.8784, loss: 4.5487
	step 10010:lm_loss: 4.5204, ppl: 91.8737, loss: 4.5487
	step 10011:lm_loss: 4.5203, ppl: 91.8610, loss: 4.5486
	step 10012:lm_loss: 4.5202, ppl: 91.8572, loss: 4.5486
	step 10013:lm_loss: 4.5202, ppl: 91.8581, loss: 4.5486
	step 10014:lm_loss: 4.5203, ppl: 91.8622, loss: 4.5486
	step 10015:lm_loss: 4.5202, ppl: 91.8541, loss: 4.5486
	step 10016:lm_loss: 4.5201, ppl: 91.8468, loss: 4.5485
	step 10017:lm_loss: 4.5201, ppl: 91.8409, loss: 4.5484
	step 10018:lm_loss: 4.5199, ppl: 91.8234, loss: 4.5482
	step 10019:lm_loss: 4.5198, ppl: 91.8187, loss: 4.5482
	step 10020:lm_loss: 4.5198, ppl: 91.8164, loss: 4.5481
	step 10021:lm_loss: 4.5198, ppl: 91.8173, loss: 4.5482
	step 10022:lm_loss: 4.5199, ppl: 91.8246, loss: 4.5482
	step 10023:lm_loss: 4.5200, ppl: 91.8325, loss: 4.5483
	step 10024:lm_loss: 4.5199, ppl: 91.8260, loss: 4.5482
	step 10025:lm_loss: 4.5200, ppl: 91.8348, loss: 4.5483
	step 10026:lm_loss: 4.5199, ppl: 91.8300, loss: 4.5483
	step 10027:lm_loss: 4.5200, ppl: 91.8372, loss: 4.5483
	step 10028:lm_loss: 4.5200, ppl: 91.8369, loss: 4.5483
	step 10029:lm_loss: 4.5200, ppl: 91.8386, loss: 4.5483
	step 10030:lm_loss: 4.5200, ppl: 91.8313, loss: 4.5482
	step 10031:lm_loss: 4.5201, ppl: 91.8445, loss: 4.5484
	step 10032:lm_loss: 4.5201, ppl: 91.8492, loss: 4.5485
	step 10033:lm_loss: 4.5202, ppl: 91.8524, loss: 4.5485
	step 10034:lm_loss: 4.5202, ppl: 91.8571, loss: 4.5486
	step 10035:lm_loss: 4.5203, ppl: 91.8587, loss: 4.5486
	step 10036:lm_loss: 4.5203, ppl: 91.8624, loss: 4.5486
	step 10037:lm_loss: 4.5203, ppl: 91.8600, loss: 4.5486
	step 10038:lm_loss: 4.5203, ppl: 91.8617, loss: 4.5486
	step 10039:lm_loss: 4.5203, ppl: 91.8670, loss: 4.5487
	step 10040:lm_loss: 4.5205, ppl: 91.8796, loss: 4.5488
	step 10041:lm_loss: 4.5204, ppl: 91.8754, loss: 4.5487
	step 10042:lm_loss: 4.5204, ppl: 91.8708, loss: 4.5486
	step 10043:lm_loss: 4.5203, ppl: 91.8605, loss: 4.5485
	step 10044:lm_loss: 4.5202, ppl: 91.8511, loss: 4.5485
	step 10045:lm_loss: 4.5202, ppl: 91.8529, loss: 4.5485
	step 10046:lm_loss: 4.5202, ppl: 91.8553, loss: 4.5485
	step 10047:lm_loss: 4.5203, ppl: 91.8650, loss: 4.5486
	step 10048:lm_loss: 4.5203, ppl: 91.8665, loss: 4.5486
	step 10049:lm_loss: 4.5203, ppl: 91.8601, loss: 4.5486
	step 10050:lm_loss: 4.5203, ppl: 91.8664, loss: 4.5487
	step 10051:lm_loss: 4.5202, ppl: 91.8542, loss: 4.5485
	step 10052:lm_loss: 4.5202, ppl: 91.8576, loss: 4.5486
	step 10053:lm_loss: 4.5202, ppl: 91.8535, loss: 4.5485
	step 10054:lm_loss: 4.5203, ppl: 91.8627, loss: 4.5486
	step 10055:lm_loss: 4.5202, ppl: 91.8581, loss: 4.5486
	step 10056:lm_loss: 4.5202, ppl: 91.8579, loss: 4.5486
	step 10057:lm_loss: 4.5203, ppl: 91.8602, loss: 4.5486
	step 10058:lm_loss: 4.5203, ppl: 91.8594, loss: 4.5486
	step 10059:lm_loss: 4.5203, ppl: 91.8644, loss: 4.5487
	step 10060:lm_loss: 4.5202, ppl: 91.8532, loss: 4.5486
	step 10061:lm_loss: 4.5203, ppl: 91.8605, loss: 4.5486
	step 10062:lm_loss: 4.5203, ppl: 91.8646, loss: 4.5486
	step 10063:lm_loss: 4.5203, ppl: 91.8644, loss: 4.5486
	step 10064:lm_loss: 4.5204, ppl: 91.8719, loss: 4.5487
	step 10065:lm_loss: 4.5204, ppl: 91.8766, loss: 4.5487
	step 10066:lm_loss: 4.5205, ppl: 91.8787, loss: 4.5487
	step 10067:lm_loss: 4.5205, ppl: 91.8774, loss: 4.5487
	step 10068:lm_loss: 4.5205, ppl: 91.8801, loss: 4.5487
	step 10069:lm_loss: 4.5205, ppl: 91.8823, loss: 4.5487
	step 10070:lm_loss: 4.5203, ppl: 91.8670, loss: 4.5486
	step 10071:lm_loss: 4.5203, ppl: 91.8627, loss: 4.5485
	step 10072:lm_loss: 4.5204, ppl: 91.8693, loss: 4.5486
	step 10073:lm_loss: 4.5203, ppl: 91.8655, loss: 4.5485
	step 10074:lm_loss: 4.5203, ppl: 91.8652, loss: 4.5485
	step 10075:lm_loss: 4.5203, ppl: 91.8629, loss: 4.5485
	step 10076:lm_loss: 4.5203, ppl: 91.8650, loss: 4.5485
	step 10077:lm_loss: 4.5204, ppl: 91.8763, loss: 4.5487
	step 10078:lm_loss: 4.5206, ppl: 91.8867, loss: 4.5489
	step 10079:lm_loss: 4.5206, ppl: 91.8932, loss: 4.5490
	step 10080:lm_loss: 4.5206, ppl: 91.8941, loss: 4.5490
	step 10081:lm_loss: 4.5207, ppl: 91.9006, loss: 4.5491
	step 10082:lm_loss: 4.5208, ppl: 91.9056, loss: 4.5492
	step 10083:lm_loss: 4.5209, ppl: 91.9194, loss: 4.5493
	step 10084:lm_loss: 4.5209, ppl: 91.9171, loss: 4.5492
	step 10085:lm_loss: 4.5210, ppl: 91.9250, loss: 4.5494
	step 10086:lm_loss: 4.5209, ppl: 91.9146, loss: 4.5493
	step 10087:lm_loss: 4.5209, ppl: 91.9151, loss: 4.5493
	step 10088:lm_loss: 4.5210, ppl: 91.9259, loss: 4.5494
	step 10089:lm_loss: 4.5210, ppl: 91.9236, loss: 4.5494
	step 10090:lm_loss: 4.5211, ppl: 91.9327, loss: 4.5494
	step 10091:lm_loss: 4.5211, ppl: 91.9379, loss: 4.5495
	step 10092:lm_loss: 4.5212, ppl: 91.9448, loss: 4.5495
	step 10093:lm_loss: 4.5212, ppl: 91.9430, loss: 4.5495
	step 10094:lm_loss: 4.5211, ppl: 91.9412, loss: 4.5495
	step 10095:lm_loss: 4.5211, ppl: 91.9386, loss: 4.5494
	step 10096:lm_loss: 4.5212, ppl: 91.9463, loss: 4.5495
	step 10097:lm_loss: 4.5211, ppl: 91.9393, loss: 4.5494
	step 10098:lm_loss: 4.5212, ppl: 91.9497, loss: 4.5495
	step 10099:lm_loss: 4.5212, ppl: 91.9414, loss: 4.5494
	step 10100:lm_loss: 4.5211, ppl: 91.9390, loss: 4.5494
	step 10101:lm_loss: 4.5210, ppl: 91.9316, loss: 4.5493
	step 10102:lm_loss: 4.5210, ppl: 91.9314, loss: 4.5493
	step 10103:lm_loss: 4.5211, ppl: 91.9340, loss: 4.5493
	step 10104:lm_loss: 4.5210, ppl: 91.9302, loss: 4.5493
	step 10105:lm_loss: 4.5210, ppl: 91.9313, loss: 4.5493
	step 10106:lm_loss: 4.5211, ppl: 91.9355, loss: 4.5493
	step 10107:lm_loss: 4.5212, ppl: 91.9492, loss: 4.5495
	step 10108:lm_loss: 4.5212, ppl: 91.9462, loss: 4.5494
	step 10109:lm_loss: 4.5211, ppl: 91.9361, loss: 4.5492
	step 10110:lm_loss: 4.5212, ppl: 91.9440, loss: 4.5493
	step 10111:lm_loss: 4.5213, ppl: 91.9514, loss: 4.5494
	step 10112:lm_loss: 4.5213, ppl: 91.9573, loss: 4.5495
	step 10113:lm_loss: 4.5213, ppl: 91.9513, loss: 4.5494
	step 10114:lm_loss: 4.5213, ppl: 91.9557, loss: 4.5495
	step 10115:lm_loss: 4.5213, ppl: 91.9513, loss: 4.5494
	step 10116:lm_loss: 4.5213, ppl: 91.9506, loss: 4.5494
	step 10117:lm_loss: 4.5213, ppl: 91.9505, loss: 4.5494
	step 10118:lm_loss: 4.5213, ppl: 91.9537, loss: 4.5494
	step 10119:lm_loss: 4.5213, ppl: 91.9584, loss: 4.5495
	step 10120:lm_loss: 4.5213, ppl: 91.9569, loss: 4.5495
	step 10121:lm_loss: 4.5213, ppl: 91.9569, loss: 4.5495
	step 10122:lm_loss: 4.5214, ppl: 91.9646, loss: 4.5496
	step 10123:lm_loss: 4.5214, ppl: 91.9673, loss: 4.5496
	step 10124:lm_loss: 4.5215, ppl: 91.9700, loss: 4.5496
	step 10125:lm_loss: 4.5214, ppl: 91.9651, loss: 4.5496
	step 10126:lm_loss: 4.5214, ppl: 91.9619, loss: 4.5496
	step 10127:lm_loss: 4.5214, ppl: 91.9626, loss: 4.5496
	step 10128:lm_loss: 4.5214, ppl: 91.9653, loss: 4.5496
	step 10129:lm_loss: 4.5214, ppl: 91.9652, loss: 4.5496
	step 10130:lm_loss: 4.5214, ppl: 91.9629, loss: 4.5495
	step 10131:lm_loss: 4.5213, ppl: 91.9524, loss: 4.5494
	step 10132:lm_loss: 4.5213, ppl: 91.9505, loss: 4.5494
	step 10133:lm_loss: 4.5213, ppl: 91.9523, loss: 4.5494
	step 10134:lm_loss: 4.5211, ppl: 91.9328, loss: 4.5493
	step 10135:lm_loss: 4.5211, ppl: 91.9329, loss: 4.5493
	step 10136:lm_loss: 4.5210, ppl: 91.9303, loss: 4.5492
	step 10137:lm_loss: 4.5211, ppl: 91.9345, loss: 4.5493
	step 10138:lm_loss: 4.5211, ppl: 91.9341, loss: 4.5493
	step 10139:lm_loss: 4.5211, ppl: 91.9340, loss: 4.5493
	step 10140:lm_loss: 4.5211, ppl: 91.9366, loss: 4.5493
	step 10141:lm_loss: 4.5211, ppl: 91.9337, loss: 4.5493
	step 10142:lm_loss: 4.5210, ppl: 91.9311, loss: 4.5492
	step 10143:lm_loss: 4.5210, ppl: 91.9285, loss: 4.5492
	step 10144:lm_loss: 4.5210, ppl: 91.9317, loss: 4.5492
	step 10145:lm_loss: 4.5211, ppl: 91.9361, loss: 4.5493
	step 10146:lm_loss: 4.5212, ppl: 91.9478, loss: 4.5494
	step 10147:lm_loss: 4.5211, ppl: 91.9378, loss: 4.5493
	step 10148:lm_loss: 4.5210, ppl: 91.9290, loss: 4.5492
	step 10149:lm_loss: 4.5211, ppl: 91.9323, loss: 4.5492
	step 10150:lm_loss: 4.5210, ppl: 91.9234, loss: 4.5492
	step 10151:lm_loss: 4.5208, ppl: 91.9136, loss: 4.5491
	step 10152:lm_loss: 4.5208, ppl: 91.9061, loss: 4.5490
	step 10153:lm_loss: 4.5208, ppl: 91.9113, loss: 4.5490
	step 10154:lm_loss: 4.5209, ppl: 91.9185, loss: 4.5491
	step 10155:lm_loss: 4.5209, ppl: 91.9166, loss: 4.5491
	step 10156:lm_loss: 4.5209, ppl: 91.9182, loss: 4.5491
	step 10157:lm_loss: 4.5210, ppl: 91.9235, loss: 4.5491
	step 10158:lm_loss: 4.5210, ppl: 91.9284, loss: 4.5492
	step 10159:lm_loss: 4.5210, ppl: 91.9292, loss: 4.5492
	step 10160:lm_loss: 4.5210, ppl: 91.9309, loss: 4.5492
	step 10161:lm_loss: 4.5210, ppl: 91.9291, loss: 4.5492
	step 10162:lm_loss: 4.5210, ppl: 91.9261, loss: 4.5491
	step 10163:lm_loss: 4.5210, ppl: 91.9283, loss: 4.5492
	step 10164:lm_loss: 4.5210, ppl: 91.9239, loss: 4.5491
	step 10165:lm_loss: 4.5210, ppl: 91.9243, loss: 4.5491
	step 10166:lm_loss: 4.5210, ppl: 91.9247, loss: 4.5491
	step 10167:lm_loss: 4.5210, ppl: 91.9244, loss: 4.5491
	step 10168:lm_loss: 4.5210, ppl: 91.9281, loss: 4.5491
	step 10169:lm_loss: 4.5210, ppl: 91.9265, loss: 4.5491
	step 10170:lm_loss: 4.5211, ppl: 91.9374, loss: 4.5492
	step 10171:lm_loss: 4.5212, ppl: 91.9433, loss: 4.5493
	step 10172:lm_loss: 4.5212, ppl: 91.9422, loss: 4.5493
	step 10173:lm_loss: 4.5212, ppl: 91.9435, loss: 4.5493
	step 10174:lm_loss: 4.5211, ppl: 91.9411, loss: 4.5493
	step 10175:lm_loss: 4.5212, ppl: 91.9475, loss: 4.5494
	step 10176:lm_loss: 4.5213, ppl: 91.9545, loss: 4.5494
	step 10177:lm_loss: 4.5214, ppl: 91.9613, loss: 4.5495
	step 10178:lm_loss: 4.5213, ppl: 91.9512, loss: 4.5494
	step 10179:lm_loss: 4.5213, ppl: 91.9555, loss: 4.5495
	step 10180:lm_loss: 4.5213, ppl: 91.9585, loss: 4.5495
	step 10181:lm_loss: 4.5213, ppl: 91.9559, loss: 4.5495
	step 10182:lm_loss: 4.5212, ppl: 91.9491, loss: 4.5494
	step 10183:lm_loss: 4.5212, ppl: 91.9488, loss: 4.5494
	step 10184:lm_loss: 4.5213, ppl: 91.9532, loss: 4.5494
	step 10185:lm_loss: 4.5212, ppl: 91.9471, loss: 4.5493
	step 10186:lm_loss: 4.5212, ppl: 91.9426, loss: 4.5493
	step 10187:lm_loss: 4.5211, ppl: 91.9346, loss: 4.5492
	step 10188:lm_loss: 4.5210, ppl: 91.9290, loss: 4.5491
	step 10189:lm_loss: 4.5210, ppl: 91.9298, loss: 4.5491
	step 10190:lm_loss: 4.5209, ppl: 91.9219, loss: 4.5490
	step 10191:lm_loss: 4.5210, ppl: 91.9252, loss: 4.5491
	step 10192:lm_loss: 4.5209, ppl: 91.9192, loss: 4.5490
	step 10193:lm_loss: 4.5209, ppl: 91.9199, loss: 4.5490
	step 10194:lm_loss: 4.5207, ppl: 91.9037, loss: 4.5489
	step 10195:lm_loss: 4.5206, ppl: 91.8902, loss: 4.5488
	step 10196:lm_loss: 4.5206, ppl: 91.8923, loss: 4.5488
	step 10197:lm_loss: 4.5205, ppl: 91.8850, loss: 4.5487
	step 10198:lm_loss: 4.5205, ppl: 91.8847, loss: 4.5487
	step 10199:lm_loss: 4.5205, ppl: 91.8820, loss: 4.5487
	step 10200:lm_loss: 4.5205, ppl: 91.8858, loss: 4.5487
	step 10201:lm_loss: 4.5205, ppl: 91.8853, loss: 4.5487
	step 10202:lm_loss: 4.5206, ppl: 91.8896, loss: 4.5488
	step 10203:lm_loss: 4.5207, ppl: 91.9003, loss: 4.5489
	step 10204:lm_loss: 4.5206, ppl: 91.8935, loss: 4.5487
	step 10205:lm_loss: 4.5205, ppl: 91.8806, loss: 4.5486
	step 10206:lm_loss: 4.5205, ppl: 91.8837, loss: 4.5486
	step 10207:lm_loss: 4.5205, ppl: 91.8849, loss: 4.5486
	step 10208:lm_loss: 4.5206, ppl: 91.8895, loss: 4.5487
	step 10209:lm_loss: 4.5206, ppl: 91.8874, loss: 4.5487
	step 10210:lm_loss: 4.5205, ppl: 91.8835, loss: 4.5486
	step 10211:lm_loss: 4.5206, ppl: 91.8875, loss: 4.5487
	step 10212:lm_loss: 4.5206, ppl: 91.8909, loss: 4.5487
	step 10213:lm_loss: 4.5204, ppl: 91.8763, loss: 4.5486
	step 10214:lm_loss: 4.5206, ppl: 91.8865, loss: 4.5487
	step 10215:lm_loss: 4.5205, ppl: 91.8857, loss: 4.5487
	step 10216:lm_loss: 4.5206, ppl: 91.8919, loss: 4.5488
	step 10217:lm_loss: 4.5206, ppl: 91.8910, loss: 4.5487
	step 10218:lm_loss: 4.5206, ppl: 91.8879, loss: 4.5487
	step 10219:lm_loss: 4.5207, ppl: 91.8966, loss: 4.5488
	step 10220:lm_loss: 4.5207, ppl: 91.8984, loss: 4.5488
	step 10221:lm_loss: 4.5208, ppl: 91.9057, loss: 4.5489
	step 10222:lm_loss: 4.5207, ppl: 91.9042, loss: 4.5489
	step 10223:lm_loss: 4.5207, ppl: 91.8973, loss: 4.5488
	step 10224:lm_loss: 4.5206, ppl: 91.8944, loss: 4.5488
	step 10225:lm_loss: 4.5207, ppl: 91.9012, loss: 4.5488
	step 10226:lm_loss: 4.5207, ppl: 91.8999, loss: 4.5488
	step 10227:lm_loss: 4.5206, ppl: 91.8896, loss: 4.5487
	step 10228:lm_loss: 4.5206, ppl: 91.8903, loss: 4.5487
	step 10229:lm_loss: 4.5207, ppl: 91.8979, loss: 4.5488
	step 10230:lm_loss: 4.5208, ppl: 91.9102, loss: 4.5490
	step 10231:lm_loss: 4.5208, ppl: 91.9117, loss: 4.5490
	step 10232:lm_loss: 4.5209, ppl: 91.9140, loss: 4.5490
	step 10233:lm_loss: 4.5209, ppl: 91.9208, loss: 4.5491
	step 10234:lm_loss: 4.5209, ppl: 91.9221, loss: 4.5491
	step 10235:lm_loss: 4.5209, ppl: 91.9214, loss: 4.5491
	step 10236:lm_loss: 4.5210, ppl: 91.9231, loss: 4.5491
	step 10237:lm_loss: 4.5209, ppl: 91.9212, loss: 4.5491
	step 10238:lm_loss: 4.5209, ppl: 91.9221, loss: 4.5491
	step 10239:lm_loss: 4.5210, ppl: 91.9249, loss: 4.5491
	step 10240:lm_loss: 4.5210, ppl: 91.9306, loss: 4.5492
	step 10241:lm_loss: 4.5211, ppl: 91.9326, loss: 4.5492
	step 10242:lm_loss: 4.5210, ppl: 91.9309, loss: 4.5492
	step 10243:lm_loss: 4.5210, ppl: 91.9273, loss: 4.5492
	step 10244:lm_loss: 4.5210, ppl: 91.9240, loss: 4.5491
	step 10245:lm_loss: 4.5209, ppl: 91.9190, loss: 4.5490
	step 10246:lm_loss: 4.5209, ppl: 91.9164, loss: 4.5490
	step 10247:lm_loss: 4.5209, ppl: 91.9139, loss: 4.5490
	step 10248:lm_loss: 4.5208, ppl: 91.9110, loss: 4.5489
	step 10249:lm_loss: 4.5208, ppl: 91.9115, loss: 4.5489
	step 10250:lm_loss: 4.5207, ppl: 91.9016, loss: 4.5488
	step 10251:lm_loss: 4.5208, ppl: 91.9054, loss: 4.5489
	step 10252:lm_loss: 4.5209, ppl: 91.9149, loss: 4.5489
	step 10253:lm_loss: 4.5209, ppl: 91.9181, loss: 4.5490
	step 10254:lm_loss: 4.5209, ppl: 91.9229, loss: 4.5490
	step 10255:lm_loss: 4.5210, ppl: 91.9298, loss: 4.5491
	step 10256:lm_loss: 4.5210, ppl: 91.9294, loss: 4.5491
	step 10257:lm_loss: 4.5208, ppl: 91.9121, loss: 4.5489
	step 10258:lm_loss: 4.5209, ppl: 91.9139, loss: 4.5489
	step 10259:lm_loss: 4.5208, ppl: 91.9119, loss: 4.5489
	step 10260:lm_loss: 4.5208, ppl: 91.9118, loss: 4.5489
	step 10261:lm_loss: 4.5208, ppl: 91.9115, loss: 4.5489
	step 10262:lm_loss: 4.5209, ppl: 91.9146, loss: 4.5489
	step 10263:lm_loss: 4.5208, ppl: 91.9133, loss: 4.5489
	step 10264:lm_loss: 4.5208, ppl: 91.9111, loss: 4.5489
	step 10265:lm_loss: 4.5208, ppl: 91.9122, loss: 4.5489
	step 10266:lm_loss: 4.5208, ppl: 91.9123, loss: 4.5489
	step 10267:lm_loss: 4.5209, ppl: 91.9156, loss: 4.5489
	step 10268:lm_loss: 4.5209, ppl: 91.9171, loss: 4.5489
	step 10269:lm_loss: 4.5208, ppl: 91.9128, loss: 4.5489
	step 10270:lm_loss: 4.5209, ppl: 91.9162, loss: 4.5489
	step 10271:lm_loss: 4.5209, ppl: 91.9172, loss: 4.5489
	step 10272:lm_loss: 4.5209, ppl: 91.9221, loss: 4.5490
	step 10273:lm_loss: 4.5210, ppl: 91.9260, loss: 4.5490
	step 10274:lm_loss: 4.5210, ppl: 91.9276, loss: 4.5490
	step 10275:lm_loss: 4.5208, ppl: 91.9049, loss: 4.5489
	step 10276:lm_loss: 4.5207, ppl: 91.9041, loss: 4.5489
	step 10277:lm_loss: 4.5207, ppl: 91.9017, loss: 4.5488
	step 10278:lm_loss: 4.5208, ppl: 91.9069, loss: 4.5489
	step 10279:lm_loss: 4.5207, ppl: 91.9044, loss: 4.5489
	step 10280:lm_loss: 4.5208, ppl: 91.9047, loss: 4.5489
	step 10281:lm_loss: 4.5207, ppl: 91.9032, loss: 4.5489
	step 10282:lm_loss: 4.5208, ppl: 91.9112, loss: 4.5489
	step 10283:lm_loss: 4.5209, ppl: 91.9190, loss: 4.5490
	step 10284:lm_loss: 4.5209, ppl: 91.9199, loss: 4.5490
	step 10285:lm_loss: 4.5209, ppl: 91.9145, loss: 4.5489
	step 10286:lm_loss: 4.5208, ppl: 91.9084, loss: 4.5489
	step 10287:lm_loss: 4.5207, ppl: 91.9014, loss: 4.5487
	step 10288:lm_loss: 4.5206, ppl: 91.8940, loss: 4.5487
	step 10289:lm_loss: 4.5207, ppl: 91.9037, loss: 4.5487
	step 10290:lm_loss: 4.5208, ppl: 91.9096, loss: 4.5488
	step 10291:lm_loss: 4.5207, ppl: 91.9044, loss: 4.5487
	step 10292:lm_loss: 4.5207, ppl: 91.8999, loss: 4.5486
	step 10293:lm_loss: 4.5207, ppl: 91.8982, loss: 4.5486
	step 10294:lm_loss: 4.5206, ppl: 91.8925, loss: 4.5486
	step 10295:lm_loss: 4.5205, ppl: 91.8809, loss: 4.5484
	step 10296:lm_loss: 4.5205, ppl: 91.8836, loss: 4.5485
	step 10297:lm_loss: 4.5206, ppl: 91.8864, loss: 4.5485
	step 10298:lm_loss: 4.5206, ppl: 91.8868, loss: 4.5485
	step 10299:lm_loss: 4.5205, ppl: 91.8855, loss: 4.5485
	step 10300:lm_loss: 4.5205, ppl: 91.8825, loss: 4.5485
	step 10301:lm_loss: 4.5205, ppl: 91.8802, loss: 4.5484
	step 10302:lm_loss: 4.5205, ppl: 91.8773, loss: 4.5484
	step 10303:lm_loss: 4.5205, ppl: 91.8805, loss: 4.5484
	step 10304:lm_loss: 4.5205, ppl: 91.8848, loss: 4.5485
	step 10305:lm_loss: 4.5206, ppl: 91.8865, loss: 4.5485
	step 10306:lm_loss: 4.5207, ppl: 91.8978, loss: 4.5486
	step 10307:lm_loss: 4.5206, ppl: 91.8948, loss: 4.5486
	step 10308:lm_loss: 4.5207, ppl: 91.8974, loss: 4.5486
	step 10309:lm_loss: 4.5207, ppl: 91.9044, loss: 4.5487
	step 10310:lm_loss: 4.5207, ppl: 91.9026, loss: 4.5487
	step 10311:lm_loss: 4.5207, ppl: 91.8982, loss: 4.5487
	step 10312:lm_loss: 4.5207, ppl: 91.8958, loss: 4.5486
	step 10313:lm_loss: 4.5207, ppl: 91.9024, loss: 4.5487
	step 10314:lm_loss: 4.5208, ppl: 91.9061, loss: 4.5488
	step 10315:lm_loss: 4.5208, ppl: 91.9059, loss: 4.5488
	step 10316:lm_loss: 4.5208, ppl: 91.9063, loss: 4.5488
	step 10317:lm_loss: 4.5209, ppl: 91.9166, loss: 4.5489
	step 10318:lm_loss: 4.5209, ppl: 91.9143, loss: 4.5488
	step 10319:lm_loss: 4.5209, ppl: 91.9146, loss: 4.5488
	step 10320:lm_loss: 4.5208, ppl: 91.9099, loss: 4.5488
	step 10321:lm_loss: 4.5207, ppl: 91.8992, loss: 4.5487
	step 10322:lm_loss: 4.5208, ppl: 91.9051, loss: 4.5487
	step 10323:lm_loss: 4.5208, ppl: 91.9080, loss: 4.5488
	step 10324:lm_loss: 4.5208, ppl: 91.9073, loss: 4.5488
	step 10325:lm_loss: 4.5206, ppl: 91.8930, loss: 4.5486
	step 10326:lm_loss: 4.5205, ppl: 91.8822, loss: 4.5485
	step 10327:lm_loss: 4.5206, ppl: 91.8933, loss: 4.5486
	step 10328:lm_loss: 4.5206, ppl: 91.8889, loss: 4.5486
	step 10329:lm_loss: 4.5205, ppl: 91.8809, loss: 4.5485
	step 10330:lm_loss: 4.5205, ppl: 91.8796, loss: 4.5485
	step 10331:lm_loss: 4.5205, ppl: 91.8828, loss: 4.5485
	step 10332:lm_loss: 4.5206, ppl: 91.8872, loss: 4.5486
	step 10333:lm_loss: 4.5205, ppl: 91.8788, loss: 4.5485
	step 10334:lm_loss: 4.5204, ppl: 91.8754, loss: 4.5484
	step 10335:lm_loss: 4.5205, ppl: 91.8791, loss: 4.5485
	step 10336:lm_loss: 4.5205, ppl: 91.8801, loss: 4.5485
	step 10337:lm_loss: 4.5205, ppl: 91.8849, loss: 4.5486
	step 10338:lm_loss: 4.5205, ppl: 91.8839, loss: 4.5486
	step 10339:lm_loss: 4.5205, ppl: 91.8828, loss: 4.5485
	step 10340:lm_loss: 4.5205, ppl: 91.8773, loss: 4.5485
	step 10341:lm_loss: 4.5204, ppl: 91.8703, loss: 4.5484
	step 10342:lm_loss: 4.5204, ppl: 91.8708, loss: 4.5484
	step 10343:lm_loss: 4.5204, ppl: 91.8716, loss: 4.5484
	step 10344:lm_loss: 4.5204, ppl: 91.8723, loss: 4.5484
	step 10345:lm_loss: 4.5204, ppl: 91.8696, loss: 4.5484
	step 10346:lm_loss: 4.5204, ppl: 91.8680, loss: 4.5483
	step 10347:lm_loss: 4.5205, ppl: 91.8861, loss: 4.5485
	step 10348:lm_loss: 4.5206, ppl: 91.8905, loss: 4.5485
	step 10349:lm_loss: 4.5206, ppl: 91.8929, loss: 4.5485
	step 10350:lm_loss: 4.5207, ppl: 91.8994, loss: 4.5486
	step 10351:lm_loss: 4.5206, ppl: 91.8922, loss: 4.5485
	step 10352:lm_loss: 4.5206, ppl: 91.8909, loss: 4.5485
	step 10353:lm_loss: 4.5206, ppl: 91.8910, loss: 4.5485
	step 10354:lm_loss: 4.5207, ppl: 91.8964, loss: 4.5485
	step 10355:lm_loss: 4.5206, ppl: 91.8938, loss: 4.5485
	step 10356:lm_loss: 4.5206, ppl: 91.8904, loss: 4.5485
	step 10357:lm_loss: 4.5207, ppl: 91.9006, loss: 4.5486
	step 10358:lm_loss: 4.5208, ppl: 91.9057, loss: 4.5486
	step 10359:lm_loss: 4.5207, ppl: 91.8991, loss: 4.5485
	step 10360:lm_loss: 4.5207, ppl: 91.8963, loss: 4.5485
	step 10361:lm_loss: 4.5207, ppl: 91.8965, loss: 4.5485
	step 10362:lm_loss: 4.5206, ppl: 91.8927, loss: 4.5485
	step 10363:lm_loss: 4.5205, ppl: 91.8853, loss: 4.5484
	step 10364:lm_loss: 4.5205, ppl: 91.8850, loss: 4.5484
	step 10365:lm_loss: 4.5207, ppl: 91.8986, loss: 4.5485
	step 10366:lm_loss: 4.5207, ppl: 91.9032, loss: 4.5486
	step 10367:lm_loss: 4.5207, ppl: 91.9032, loss: 4.5486
	step 10368:lm_loss: 4.5207, ppl: 91.9041, loss: 4.5486
	step 10369:lm_loss: 4.5208, ppl: 91.9059, loss: 4.5486
	step 10370:lm_loss: 4.5208, ppl: 91.9072, loss: 4.5486
	step 10371:lm_loss: 4.5209, ppl: 91.9217, loss: 4.5487
	step 10372:lm_loss: 4.5208, ppl: 91.9107, loss: 4.5487
	step 10373:lm_loss: 4.5208, ppl: 91.9099, loss: 4.5486
	step 10374:lm_loss: 4.5208, ppl: 91.9096, loss: 4.5486
	step 10375:lm_loss: 4.5208, ppl: 91.9107, loss: 4.5486
	step 10376:lm_loss: 4.5207, ppl: 91.8994, loss: 4.5485
	step 10377:lm_loss: 4.5207, ppl: 91.9002, loss: 4.5485
	step 10378:lm_loss: 4.5206, ppl: 91.8945, loss: 4.5484
	step 10379:lm_loss: 4.5207, ppl: 91.8990, loss: 4.5485
	step 10380:lm_loss: 4.5207, ppl: 91.8963, loss: 4.5484
	step 10381:lm_loss: 4.5207, ppl: 91.8960, loss: 4.5484
	step 10382:lm_loss: 4.5207, ppl: 91.8956, loss: 4.5484
	step 10383:lm_loss: 4.5207, ppl: 91.8969, loss: 4.5484
	step 10384:lm_loss: 4.5207, ppl: 91.8982, loss: 4.5484
	step 10385:lm_loss: 4.5206, ppl: 91.8948, loss: 4.5484
	step 10386:lm_loss: 4.5206, ppl: 91.8950, loss: 4.5484
	step 10387:lm_loss: 4.5206, ppl: 91.8931, loss: 4.5484
	step 10388:lm_loss: 4.5206, ppl: 91.8937, loss: 4.5484
	step 10389:lm_loss: 4.5206, ppl: 91.8948, loss: 4.5484
	step 10390:lm_loss: 4.5205, ppl: 91.8832, loss: 4.5482
	step 10391:lm_loss: 4.5205, ppl: 91.8827, loss: 4.5482
	step 10392:lm_loss: 4.5205, ppl: 91.8816, loss: 4.5482
	step 10393:lm_loss: 4.5205, ppl: 91.8806, loss: 4.5482
	step 10394:lm_loss: 4.5205, ppl: 91.8808, loss: 4.5482
	step 10395:lm_loss: 4.5205, ppl: 91.8778, loss: 4.5481
	step 10396:lm_loss: 4.5204, ppl: 91.8750, loss: 4.5481
	step 10397:lm_loss: 4.5204, ppl: 91.8743, loss: 4.5481
	step 10398:lm_loss: 4.5204, ppl: 91.8758, loss: 4.5481
	step 10399:lm_loss: 4.5204, ppl: 91.8689, loss: 4.5481
	step 10400:lm_loss: 4.5203, ppl: 91.8648, loss: 4.5480
	step 10401:lm_loss: 4.5202, ppl: 91.8582, loss: 4.5479
	step 10402:lm_loss: 4.5203, ppl: 91.8591, loss: 4.5480
	step 10403:lm_loss: 4.5200, ppl: 91.8343, loss: 4.5478
	step 10404:lm_loss: 4.5200, ppl: 91.8354, loss: 4.5478
	step 10405:lm_loss: 4.5200, ppl: 91.8393, loss: 4.5478
	step 10406:lm_loss: 4.5201, ppl: 91.8446, loss: 4.5479
	step 10407:lm_loss: 4.5202, ppl: 91.8582, loss: 4.5480
	step 10408:lm_loss: 4.5202, ppl: 91.8529, loss: 4.5480
	step 10409:lm_loss: 4.5202, ppl: 91.8557, loss: 4.5480
	step 10410:lm_loss: 4.5203, ppl: 91.8605, loss: 4.5480
	step 10411:lm_loss: 4.5203, ppl: 91.8607, loss: 4.5480
	step 10412:lm_loss: 4.5203, ppl: 91.8635, loss: 4.5481
	step 10413:lm_loss: 4.5203, ppl: 91.8654, loss: 4.5481
	step 10414:lm_loss: 4.5203, ppl: 91.8608, loss: 4.5480
	step 10415:lm_loss: 4.5202, ppl: 91.8544, loss: 4.5480
	step 10416:lm_loss: 4.5204, ppl: 91.8679, loss: 4.5481
	step 10417:lm_loss: 4.5203, ppl: 91.8644, loss: 4.5481
	step 10418:lm_loss: 4.5204, ppl: 91.8700, loss: 4.5481
	step 10419:lm_loss: 4.5204, ppl: 91.8720, loss: 4.5481
	step 10420:lm_loss: 4.5204, ppl: 91.8739, loss: 4.5482
	step 10421:lm_loss: 4.5204, ppl: 91.8728, loss: 4.5481
	step 10422:lm_loss: 4.5203, ppl: 91.8674, loss: 4.5481
	step 10423:lm_loss: 4.5204, ppl: 91.8716, loss: 4.5482
	step 10424:lm_loss: 4.5203, ppl: 91.8670, loss: 4.5481
	step 10425:lm_loss: 4.5204, ppl: 91.8693, loss: 4.5481
	step 10426:lm_loss: 4.5204, ppl: 91.8703, loss: 4.5481
	step 10427:lm_loss: 4.5204, ppl: 91.8721, loss: 4.5481
	step 10428:lm_loss: 4.5203, ppl: 91.8671, loss: 4.5481
	step 10429:lm_loss: 4.5204, ppl: 91.8693, loss: 4.5481
	step 10430:lm_loss: 4.5204, ppl: 91.8691, loss: 4.5481
	step 10431:lm_loss: 4.5204, ppl: 91.8693, loss: 4.5481
	step 10432:lm_loss: 4.5203, ppl: 91.8638, loss: 4.5481
	step 10433:lm_loss: 4.5203, ppl: 91.8614, loss: 4.5480
	step 10434:lm_loss: 4.5203, ppl: 91.8621, loss: 4.5480
	step 10435:lm_loss: 4.5203, ppl: 91.8668, loss: 4.5481
	step 10436:lm_loss: 4.5205, ppl: 91.8815, loss: 4.5482
	step 10437:lm_loss: 4.5206, ppl: 91.8938, loss: 4.5483
	step 10438:lm_loss: 4.5206, ppl: 91.8888, loss: 4.5483
	step 10439:lm_loss: 4.5206, ppl: 91.8904, loss: 4.5483
	step 10440:lm_loss: 4.5205, ppl: 91.8829, loss: 4.5482
	step 10441:lm_loss: 4.5205, ppl: 91.8842, loss: 4.5482
	step 10442:lm_loss: 4.5205, ppl: 91.8859, loss: 4.5482
	step 10443:lm_loss: 4.5205, ppl: 91.8831, loss: 4.5482
	step 10444:lm_loss: 4.5206, ppl: 91.8920, loss: 4.5484
	step 10445:lm_loss: 4.5206, ppl: 91.8928, loss: 4.5484
	step 10446:lm_loss: 4.5207, ppl: 91.9007, loss: 4.5484
	step 10447:lm_loss: 4.5207, ppl: 91.9010, loss: 4.5484
	step 10448:lm_loss: 4.5207, ppl: 91.8979, loss: 4.5484
	step 10449:lm_loss: 4.5207, ppl: 91.9010, loss: 4.5484
	step 10450:lm_loss: 4.5208, ppl: 91.9048, loss: 4.5484
	step 10451:lm_loss: 4.5208, ppl: 91.9113, loss: 4.5485
	step 10452:lm_loss: 4.5209, ppl: 91.9141, loss: 4.5486
	step 10453:lm_loss: 4.5208, ppl: 91.9119, loss: 4.5485
	step 10454:lm_loss: 4.5209, ppl: 91.9208, loss: 4.5486
	step 10455:lm_loss: 4.5209, ppl: 91.9221, loss: 4.5486
	step 10456:lm_loss: 4.5209, ppl: 91.9215, loss: 4.5486
	step 10457:lm_loss: 4.5209, ppl: 91.9166, loss: 4.5486
	step 10458:lm_loss: 4.5208, ppl: 91.9123, loss: 4.5485
	step 10459:lm_loss: 4.5209, ppl: 91.9170, loss: 4.5486
	step 10460:lm_loss: 4.5209, ppl: 91.9198, loss: 4.5486
	step 10461:lm_loss: 4.5209, ppl: 91.9198, loss: 4.5486
	step 10462:lm_loss: 4.5209, ppl: 91.9228, loss: 4.5487
	step 10463:lm_loss: 4.5209, ppl: 91.9150, loss: 4.5486
	step 10464:lm_loss: 4.5209, ppl: 91.9180, loss: 4.5486
	step 10465:lm_loss: 4.5209, ppl: 91.9198, loss: 4.5486
	step 10466:lm_loss: 4.5209, ppl: 91.9202, loss: 4.5486
	step 10467:lm_loss: 4.5209, ppl: 91.9163, loss: 4.5486
	step 10468:lm_loss: 4.5209, ppl: 91.9157, loss: 4.5486
	step 10469:lm_loss: 4.5209, ppl: 91.9203, loss: 4.5486
	step 10470:lm_loss: 4.5209, ppl: 91.9222, loss: 4.5486
	step 10471:lm_loss: 4.5210, ppl: 91.9261, loss: 4.5487
	step 10472:lm_loss: 4.5212, ppl: 91.9460, loss: 4.5489
	step 10473:lm_loss: 4.5210, ppl: 91.9305, loss: 4.5488
	step 10474:lm_loss: 4.5210, ppl: 91.9302, loss: 4.5488
	step 10475:lm_loss: 4.5210, ppl: 91.9307, loss: 4.5488
	step 10476:lm_loss: 4.5210, ppl: 91.9263, loss: 4.5487
	step 10477:lm_loss: 4.5210, ppl: 91.9309, loss: 4.5488
	step 10478:lm_loss: 4.5211, ppl: 91.9321, loss: 4.5488
	step 10479:lm_loss: 4.5211, ppl: 91.9327, loss: 4.5488
	step 10480:lm_loss: 4.5211, ppl: 91.9372, loss: 4.5488
	step 10481:lm_loss: 4.5211, ppl: 91.9378, loss: 4.5488
	step 10482:lm_loss: 4.5213, ppl: 91.9507, loss: 4.5490
	step 10483:lm_loss: 4.5213, ppl: 91.9575, loss: 4.5490
	step 10484:lm_loss: 4.5214, ppl: 91.9610, loss: 4.5491
	step 10485:lm_loss: 4.5215, ppl: 91.9719, loss: 4.5491
	step 10486:lm_loss: 4.5215, ppl: 91.9707, loss: 4.5491
	step 10487:lm_loss: 4.5213, ppl: 91.9544, loss: 4.5490
	step 10488:lm_loss: 4.5213, ppl: 91.9540, loss: 4.5490
	step 10489:lm_loss: 4.5212, ppl: 91.9501, loss: 4.5489
	step 10490:lm_loss: 4.5211, ppl: 91.9403, loss: 4.5488
	step 10491:lm_loss: 4.5212, ppl: 91.9441, loss: 4.5489
	step 10492:lm_loss: 4.5211, ppl: 91.9366, loss: 4.5488
	step 10493:lm_loss: 4.5211, ppl: 91.9387, loss: 4.5488
	step 10494:lm_loss: 4.5212, ppl: 91.9419, loss: 4.5489
	step 10495:lm_loss: 4.5211, ppl: 91.9404, loss: 4.5488
	step 10496:lm_loss: 4.5213, ppl: 91.9513, loss: 4.5490
	step 10497:lm_loss: 4.5213, ppl: 91.9541, loss: 4.5490
	step 10498:lm_loss: 4.5213, ppl: 91.9542, loss: 4.5490
	step 10499:lm_loss: 4.5214, ppl: 91.9616, loss: 4.5491
	step 10500:lm_loss: 4.5213, ppl: 91.9584, loss: 4.5490
	step 10501:lm_loss: 4.5214, ppl: 91.9613, loss: 4.5490
	step 10502:lm_loss: 4.5214, ppl: 91.9647, loss: 4.5491
	step 10503:lm_loss: 4.5215, ppl: 91.9698, loss: 4.5491
	step 10504:lm_loss: 4.5215, ppl: 91.9733, loss: 4.5492
	step 10505:lm_loss: 4.5214, ppl: 91.9651, loss: 4.5491
	step 10506:lm_loss: 4.5215, ppl: 91.9693, loss: 4.5492
	step 10507:lm_loss: 4.5216, ppl: 91.9830, loss: 4.5493
	step 10508:lm_loss: 4.5217, ppl: 91.9877, loss: 4.5494
	step 10509:lm_loss: 4.5217, ppl: 91.9915, loss: 4.5494
	step 10510:lm_loss: 4.5217, ppl: 91.9913, loss: 4.5494
	step 10511:lm_loss: 4.5217, ppl: 91.9928, loss: 4.5494
	step 10512:lm_loss: 4.5217, ppl: 91.9923, loss: 4.5494
	step 10513:lm_loss: 4.5219, ppl: 92.0064, loss: 4.5496
	step 10514:lm_loss: 4.5218, ppl: 92.0053, loss: 4.5496
	step 10515:lm_loss: 4.5219, ppl: 92.0076, loss: 4.5496
	step 10516:lm_loss: 4.5220, ppl: 92.0201, loss: 4.5497
	step 10517:lm_loss: 4.5220, ppl: 92.0220, loss: 4.5497
	step 10518:lm_loss: 4.5221, ppl: 92.0243, loss: 4.5497
	step 10519:lm_loss: 4.5221, ppl: 92.0252, loss: 4.5497
	step 10520:lm_loss: 4.5222, ppl: 92.0359, loss: 4.5498
	step 10521:lm_loss: 4.5222, ppl: 92.0373, loss: 4.5498
	step 10522:lm_loss: 4.5220, ppl: 92.0173, loss: 4.5497
	step 10523:lm_loss: 4.5219, ppl: 92.0129, loss: 4.5496
	step 10524:lm_loss: 4.5219, ppl: 92.0078, loss: 4.5496
	step 10525:lm_loss: 4.5219, ppl: 92.0076, loss: 4.5496
	step 10526:lm_loss: 4.5219, ppl: 92.0068, loss: 4.5495
	step 10527:lm_loss: 4.5219, ppl: 92.0081, loss: 4.5496
	step 10528:lm_loss: 4.5220, ppl: 92.0155, loss: 4.5497
	step 10529:lm_loss: 4.5219, ppl: 92.0134, loss: 4.5496
	step 10530:lm_loss: 4.5218, ppl: 92.0055, loss: 4.5496
	step 10531:lm_loss: 4.5220, ppl: 92.0154, loss: 4.5497
	step 10532:lm_loss: 4.5219, ppl: 92.0080, loss: 4.5496
	step 10533:lm_loss: 4.5219, ppl: 92.0132, loss: 4.5497
	step 10534:lm_loss: 4.5221, ppl: 92.0241, loss: 4.5498
	step 10535:lm_loss: 4.5220, ppl: 92.0236, loss: 4.5498
	step 10536:lm_loss: 4.5220, ppl: 92.0179, loss: 4.5497
	step 10537:lm_loss: 4.5221, ppl: 92.0302, loss: 4.5498
	step 10538:lm_loss: 4.5221, ppl: 92.0315, loss: 4.5498
	step 10539:lm_loss: 4.5221, ppl: 92.0310, loss: 4.5498
	step 10540:lm_loss: 4.5223, ppl: 92.0457, loss: 4.5501
	step 10541:lm_loss: 4.5223, ppl: 92.0451, loss: 4.5501
	step 10542:lm_loss: 4.5222, ppl: 92.0420, loss: 4.5500
	step 10543:lm_loss: 4.5223, ppl: 92.0455, loss: 4.5500
	step 10544:lm_loss: 4.5222, ppl: 92.0424, loss: 4.5500
	step 10545:lm_loss: 4.5222, ppl: 92.0363, loss: 4.5499
	step 10546:lm_loss: 4.5222, ppl: 92.0337, loss: 4.5499
	step 10547:lm_loss: 4.5222, ppl: 92.0382, loss: 4.5499
	step 10548:lm_loss: 4.5222, ppl: 92.0384, loss: 4.5499
	step 10549:lm_loss: 4.5222, ppl: 92.0335, loss: 4.5499
	step 10550:lm_loss: 4.5223, ppl: 92.0435, loss: 4.5500
	step 10551:lm_loss: 4.5223, ppl: 92.0436, loss: 4.5500
	step 10552:lm_loss: 4.5223, ppl: 92.0503, loss: 4.5501
	step 10553:lm_loss: 4.5223, ppl: 92.0467, loss: 4.5500
	step 10554:lm_loss: 4.5224, ppl: 92.0557, loss: 4.5502
	step 10555:lm_loss: 4.5224, ppl: 92.0562, loss: 4.5502
	step 10556:lm_loss: 4.5224, ppl: 92.0574, loss: 4.5502
	step 10557:lm_loss: 4.5225, ppl: 92.0676, loss: 4.5503
	step 10558:lm_loss: 4.5224, ppl: 92.0562, loss: 4.5502
	step 10559:lm_loss: 4.5223, ppl: 92.0483, loss: 4.5501
	step 10560:lm_loss: 4.5223, ppl: 92.0473, loss: 4.5501
	step 10561:lm_loss: 4.5222, ppl: 92.0424, loss: 4.5501
	step 10562:lm_loss: 4.5223, ppl: 92.0480, loss: 4.5501
	step 10563:lm_loss: 4.5224, ppl: 92.0549, loss: 4.5502
	step 10564:lm_loss: 4.5223, ppl: 92.0444, loss: 4.5501
	step 10565:lm_loss: 4.5222, ppl: 92.0419, loss: 4.5501
	step 10566:lm_loss: 4.5223, ppl: 92.0490, loss: 4.5502
	step 10567:lm_loss: 4.5224, ppl: 92.0569, loss: 4.5503
	step 10568:lm_loss: 4.5224, ppl: 92.0578, loss: 4.5503
	step 10569:lm_loss: 4.5223, ppl: 92.0480, loss: 4.5501
	step 10570:lm_loss: 4.5222, ppl: 92.0393, loss: 4.5501
	step 10571:lm_loss: 4.5222, ppl: 92.0372, loss: 4.5500
	step 10572:lm_loss: 4.5222, ppl: 92.0417, loss: 4.5501
	step 10573:lm_loss: 4.5223, ppl: 92.0498, loss: 4.5502
	step 10574:lm_loss: 4.5224, ppl: 92.0591, loss: 4.5503
	step 10575:lm_loss: 4.5224, ppl: 92.0564, loss: 4.5502
	step 10576:lm_loss: 4.5224, ppl: 92.0590, loss: 4.5502
	step 10577:lm_loss: 4.5224, ppl: 92.0590, loss: 4.5502
	step 10578:lm_loss: 4.5225, ppl: 92.0616, loss: 4.5503
	step 10579:lm_loss: 4.5225, ppl: 92.0694, loss: 4.5503
	step 10580:lm_loss: 4.5226, ppl: 92.0787, loss: 4.5504
	step 10581:lm_loss: 4.5227, ppl: 92.0839, loss: 4.5505
	step 10582:lm_loss: 4.5228, ppl: 92.0891, loss: 4.5506
	step 10583:lm_loss: 4.5227, ppl: 92.0875, loss: 4.5505
	step 10584:lm_loss: 4.5227, ppl: 92.0863, loss: 4.5505
	step 10585:lm_loss: 4.5227, ppl: 92.0880, loss: 4.5505
	step 10586:lm_loss: 4.5227, ppl: 92.0851, loss: 4.5505
	step 10587:lm_loss: 4.5227, ppl: 92.0884, loss: 4.5506
	step 10588:lm_loss: 4.5227, ppl: 92.0843, loss: 4.5505
	step 10589:lm_loss: 4.5227, ppl: 92.0821, loss: 4.5505
	step 10590:lm_loss: 4.5227, ppl: 92.0802, loss: 4.5505
	step 10591:lm_loss: 4.5226, ppl: 92.0773, loss: 4.5504
	step 10592:lm_loss: 4.5226, ppl: 92.0765, loss: 4.5504
	step 10593:lm_loss: 4.5226, ppl: 92.0744, loss: 4.5504
	step 10594:lm_loss: 4.5226, ppl: 92.0752, loss: 4.5504
	step 10595:lm_loss: 4.5225, ppl: 92.0626, loss: 4.5503
	step 10596:lm_loss: 4.5226, ppl: 92.0703, loss: 4.5503
	step 10597:lm_loss: 4.5225, ppl: 92.0671, loss: 4.5503
	step 10598:lm_loss: 4.5225, ppl: 92.0658, loss: 4.5503
	step 10599:lm_loss: 4.5226, ppl: 92.0724, loss: 4.5504
	step 10600:lm_loss: 4.5226, ppl: 92.0767, loss: 4.5504
	step 10601:lm_loss: 4.5225, ppl: 92.0682, loss: 4.5503
	step 10602:lm_loss: 4.5225, ppl: 92.0693, loss: 4.5503
	step 10603:lm_loss: 4.5226, ppl: 92.0793, loss: 4.5504
	step 10604:lm_loss: 4.5226, ppl: 92.0782, loss: 4.5504
	step 10605:lm_loss: 4.5227, ppl: 92.0799, loss: 4.5504
	step 10606:lm_loss: 4.5227, ppl: 92.0817, loss: 4.5505
	step 10607:lm_loss: 4.5227, ppl: 92.0841, loss: 4.5505
	step 10608:lm_loss: 4.5227, ppl: 92.0876, loss: 4.5505
	step 10609:lm_loss: 4.5228, ppl: 92.0951, loss: 4.5506
	step 10610:lm_loss: 4.5228, ppl: 92.0972, loss: 4.5506
	step 10611:lm_loss: 4.5228, ppl: 92.0951, loss: 4.5506
	step 10612:lm_loss: 4.5228, ppl: 92.0910, loss: 4.5506
	step 10613:lm_loss: 4.5227, ppl: 92.0869, loss: 4.5505
	step 10614:lm_loss: 4.5226, ppl: 92.0716, loss: 4.5504
	step 10615:lm_loss: 4.5227, ppl: 92.0805, loss: 4.5505
	step 10616:lm_loss: 4.5226, ppl: 92.0790, loss: 4.5505
	step 10617:lm_loss: 4.5227, ppl: 92.0876, loss: 4.5506
	step 10618:lm_loss: 4.5228, ppl: 92.0916, loss: 4.5506
	step 10619:lm_loss: 4.5229, ppl: 92.1028, loss: 4.5508
	step 10620:lm_loss: 4.5230, ppl: 92.1105, loss: 4.5509
	step 10621:lm_loss: 4.5230, ppl: 92.1139, loss: 4.5509
	step 10622:lm_loss: 4.5229, ppl: 92.1066, loss: 4.5508
	step 10623:lm_loss: 4.5229, ppl: 92.1031, loss: 4.5508
	step 10624:lm_loss: 4.5228, ppl: 92.0914, loss: 4.5507
	step 10625:lm_loss: 4.5228, ppl: 92.0938, loss: 4.5507
	step 10626:lm_loss: 4.5229, ppl: 92.1029, loss: 4.5508
	step 10627:lm_loss: 4.5229, ppl: 92.1027, loss: 4.5508
	step 10628:lm_loss: 4.5229, ppl: 92.1041, loss: 4.5508
	step 10629:lm_loss: 4.5230, ppl: 92.1077, loss: 4.5509
	step 10630:lm_loss: 4.5229, ppl: 92.0982, loss: 4.5508
	step 10631:lm_loss: 4.5229, ppl: 92.0998, loss: 4.5508
	step 10632:lm_loss: 4.5229, ppl: 92.1069, loss: 4.5509
	step 10633:lm_loss: 4.5229, ppl: 92.1055, loss: 4.5509
	step 10634:lm_loss: 4.5230, ppl: 92.1073, loss: 4.5509
	step 10635:lm_loss: 4.5229, ppl: 92.1050, loss: 4.5509
	step 10636:lm_loss: 4.5229, ppl: 92.1044, loss: 4.5508
	step 10637:lm_loss: 4.5229, ppl: 92.1042, loss: 4.5508
	step 10638:lm_loss: 4.5229, ppl: 92.1057, loss: 4.5509
	step 10639:lm_loss: 4.5230, ppl: 92.1124, loss: 4.5509
	step 10640:lm_loss: 4.5231, ppl: 92.1178, loss: 4.5510
	step 10641:lm_loss: 4.5230, ppl: 92.1154, loss: 4.5510
	step 10642:lm_loss: 4.5231, ppl: 92.1165, loss: 4.5510
	step 10643:lm_loss: 4.5230, ppl: 92.1148, loss: 4.5510
	step 10644:lm_loss: 4.5231, ppl: 92.1182, loss: 4.5510
	step 10645:lm_loss: 4.5231, ppl: 92.1243, loss: 4.5511
	step 10646:lm_loss: 4.5232, ppl: 92.1254, loss: 4.5511
	step 10647:lm_loss: 4.5231, ppl: 92.1245, loss: 4.5511
	step 10648:lm_loss: 4.5231, ppl: 92.1243, loss: 4.5511
	step 10649:lm_loss: 4.5232, ppl: 92.1256, loss: 4.5511
	step 10650:lm_loss: 4.5232, ppl: 92.1316, loss: 4.5511
	step 10651:lm_loss: 4.5232, ppl: 92.1329, loss: 4.5511
	step 10652:lm_loss: 4.5232, ppl: 92.1297, loss: 4.5511
	step 10653:lm_loss: 4.5232, ppl: 92.1344, loss: 4.5511
	step 10654:lm_loss: 4.5233, ppl: 92.1350, loss: 4.5511
	step 10655:lm_loss: 4.5233, ppl: 92.1369, loss: 4.5512
	step 10656:lm_loss: 4.5234, ppl: 92.1480, loss: 4.5513
	step 10657:lm_loss: 4.5234, ppl: 92.1520, loss: 4.5513
	step 10658:lm_loss: 4.5234, ppl: 92.1449, loss: 4.5513
	step 10659:lm_loss: 4.5234, ppl: 92.1464, loss: 4.5513
	step 10660:lm_loss: 4.5234, ppl: 92.1452, loss: 4.5513
	step 10661:lm_loss: 4.5233, ppl: 92.1419, loss: 4.5512
	step 10662:lm_loss: 4.5233, ppl: 92.1387, loss: 4.5512
	step 10663:lm_loss: 4.5233, ppl: 92.1375, loss: 4.5512
	step 10664:lm_loss: 4.5232, ppl: 92.1295, loss: 4.5511
	step 10665:lm_loss: 4.5231, ppl: 92.1253, loss: 4.5511
	step 10666:lm_loss: 4.5231, ppl: 92.1252, loss: 4.5511
	step 10667:lm_loss: 4.5233, ppl: 92.1371, loss: 4.5512
	step 10668:lm_loss: 4.5231, ppl: 92.1193, loss: 4.5511
	step 10669:lm_loss: 4.5231, ppl: 92.1207, loss: 4.5511
	step 10670:lm_loss: 4.5232, ppl: 92.1287, loss: 4.5512
	step 10671:lm_loss: 4.5234, ppl: 92.1451, loss: 4.5513
	step 10672:lm_loss: 4.5234, ppl: 92.1455, loss: 4.5513
	step 10673:lm_loss: 4.5234, ppl: 92.1466, loss: 4.5513
	step 10674:lm_loss: 4.5234, ppl: 92.1452, loss: 4.5513
	step 10675:lm_loss: 4.5233, ppl: 92.1425, loss: 4.5513
	step 10676:lm_loss: 4.5233, ppl: 92.1385, loss: 4.5512
	step 10677:lm_loss: 4.5233, ppl: 92.1410, loss: 4.5512
	step 10678:lm_loss: 4.5234, ppl: 92.1453, loss: 4.5513
	step 10679:lm_loss: 4.5233, ppl: 92.1396, loss: 4.5512
	step 10680:lm_loss: 4.5234, ppl: 92.1458, loss: 4.5513
	step 10681:lm_loss: 4.5235, ppl: 92.1568, loss: 4.5513
	step 10682:lm_loss: 4.5232, ppl: 92.1344, loss: 4.5512
	step 10683:lm_loss: 4.5233, ppl: 92.1352, loss: 4.5512
	step 10684:lm_loss: 4.5233, ppl: 92.1391, loss: 4.5513
	step 10685:lm_loss: 4.5234, ppl: 92.1442, loss: 4.5513
	step 10686:lm_loss: 4.5234, ppl: 92.1488, loss: 4.5514
	step 10687:lm_loss: 4.5233, ppl: 92.1411, loss: 4.5513
	step 10688:lm_loss: 4.5232, ppl: 92.1274, loss: 4.5511
	step 10689:lm_loss: 4.5232, ppl: 92.1324, loss: 4.5511
	step 10690:lm_loss: 4.5234, ppl: 92.1444, loss: 4.5513
	step 10691:lm_loss: 4.5233, ppl: 92.1430, loss: 4.5512
	step 10692:lm_loss: 4.5234, ppl: 92.1442, loss: 4.5513
	step 10693:lm_loss: 4.5234, ppl: 92.1451, loss: 4.5513
	step 10694:lm_loss: 4.5232, ppl: 92.1332, loss: 4.5511
	step 10695:lm_loss: 4.5233, ppl: 92.1369, loss: 4.5512
	step 10696:lm_loss: 4.5232, ppl: 92.1322, loss: 4.5511
	step 10697:lm_loss: 4.5233, ppl: 92.1382, loss: 4.5512
	step 10698:lm_loss: 4.5232, ppl: 92.1326, loss: 4.5511
	step 10699:lm_loss: 4.5232, ppl: 92.1324, loss: 4.5511
	step 10700:lm_loss: 4.5232, ppl: 92.1339, loss: 4.5511
	step 10701:lm_loss: 4.5233, ppl: 92.1386, loss: 4.5512
	step 10702:lm_loss: 4.5234, ppl: 92.1473, loss: 4.5513
	step 10703:lm_loss: 4.5234, ppl: 92.1454, loss: 4.5513
	step 10704:lm_loss: 4.5235, ppl: 92.1575, loss: 4.5514
	step 10705:lm_loss: 4.5234, ppl: 92.1466, loss: 4.5513
	step 10706:lm_loss: 4.5234, ppl: 92.1459, loss: 4.5513
	step 10707:lm_loss: 4.5233, ppl: 92.1423, loss: 4.5512
	step 10708:lm_loss: 4.5233, ppl: 92.1413, loss: 4.5512
	step 10709:lm_loss: 4.5233, ppl: 92.1409, loss: 4.5512
	step 10710:lm_loss: 4.5234, ppl: 92.1461, loss: 4.5513
	step 10711:lm_loss: 4.5234, ppl: 92.1492, loss: 4.5513
	step 10712:lm_loss: 4.5233, ppl: 92.1430, loss: 4.5512
	step 10713:lm_loss: 4.5234, ppl: 92.1440, loss: 4.5512
	step 10714:lm_loss: 4.5234, ppl: 92.1483, loss: 4.5513
	step 10715:lm_loss: 4.5234, ppl: 92.1476, loss: 4.5513
	step 10716:lm_loss: 4.5234, ppl: 92.1491, loss: 4.5513
	step 10717:lm_loss: 4.5234, ppl: 92.1482, loss: 4.5513
	step 10718:lm_loss: 4.5234, ppl: 92.1481, loss: 4.5513
	step 10719:lm_loss: 4.5234, ppl: 92.1469, loss: 4.5512
	step 10720:lm_loss: 4.5234, ppl: 92.1505, loss: 4.5513
	step 10721:lm_loss: 4.5234, ppl: 92.1507, loss: 4.5513
	step 10722:lm_loss: 4.5235, ppl: 92.1547, loss: 4.5513
	step 10723:lm_loss: 4.5235, ppl: 92.1602, loss: 4.5514
	step 10724:lm_loss: 4.5235, ppl: 92.1589, loss: 4.5514
	step 10725:lm_loss: 4.5237, ppl: 92.1721, loss: 4.5515
	step 10726:lm_loss: 4.5237, ppl: 92.1737, loss: 4.5515
	step 10727:lm_loss: 4.5238, ppl: 92.1851, loss: 4.5516
	step 10728:lm_loss: 4.5239, ppl: 92.1916, loss: 4.5517
	step 10729:lm_loss: 4.5240, ppl: 92.2033, loss: 4.5517
	step 10730:lm_loss: 4.5238, ppl: 92.1882, loss: 4.5516
	step 10731:lm_loss: 4.5239, ppl: 92.1909, loss: 4.5517
	step 10732:lm_loss: 4.5239, ppl: 92.1963, loss: 4.5518
	step 10733:lm_loss: 4.5239, ppl: 92.1971, loss: 4.5518
	step 10734:lm_loss: 4.5240, ppl: 92.2013, loss: 4.5518
	step 10735:lm_loss: 4.5239, ppl: 92.1967, loss: 4.5518
	step 10736:lm_loss: 4.5239, ppl: 92.1980, loss: 4.5518
	step 10737:lm_loss: 4.5241, ppl: 92.2098, loss: 4.5519
	step 10738:lm_loss: 4.5241, ppl: 92.2143, loss: 4.5519
	step 10739:lm_loss: 4.5240, ppl: 92.2045, loss: 4.5519
	step 10740:lm_loss: 4.5238, ppl: 92.1888, loss: 4.5517
	step 10741:lm_loss: 4.5238, ppl: 92.1817, loss: 4.5516
	step 10742:lm_loss: 4.5238, ppl: 92.1833, loss: 4.5516
	step 10743:lm_loss: 4.5238, ppl: 92.1837, loss: 4.5517
	step 10744:lm_loss: 4.5239, ppl: 92.1908, loss: 4.5517
	step 10745:lm_loss: 4.5237, ppl: 92.1781, loss: 4.5516
	step 10746:lm_loss: 4.5238, ppl: 92.1818, loss: 4.5517
	step 10747:lm_loss: 4.5236, ppl: 92.1650, loss: 4.5515
	step 10748:lm_loss: 4.5236, ppl: 92.1658, loss: 4.5515
	step 10749:lm_loss: 4.5236, ppl: 92.1646, loss: 4.5515
	step 10750:lm_loss: 4.5236, ppl: 92.1625, loss: 4.5515
	step 10751:lm_loss: 4.5235, ppl: 92.1587, loss: 4.5514
	step 10752:lm_loss: 4.5235, ppl: 92.1599, loss: 4.5514
	step 10753:lm_loss: 4.5236, ppl: 92.1682, loss: 4.5516
	step 10754:lm_loss: 4.5235, ppl: 92.1621, loss: 4.5515
	step 10755:lm_loss: 4.5236, ppl: 92.1657, loss: 4.5516
	step 10756:lm_loss: 4.5236, ppl: 92.1634, loss: 4.5515
	step 10757:lm_loss: 4.5236, ppl: 92.1646, loss: 4.5515
	step 10758:lm_loss: 4.5235, ppl: 92.1590, loss: 4.5515
	step 10759:lm_loss: 4.5235, ppl: 92.1583, loss: 4.5515
	step 10760:lm_loss: 4.5234, ppl: 92.1516, loss: 4.5514
	step 10761:lm_loss: 4.5235, ppl: 92.1531, loss: 4.5514
	step 10762:lm_loss: 4.5235, ppl: 92.1604, loss: 4.5515
	step 10763:lm_loss: 4.5235, ppl: 92.1550, loss: 4.5514
	step 10764:lm_loss: 4.5235, ppl: 92.1534, loss: 4.5514
	step 10765:lm_loss: 4.5235, ppl: 92.1530, loss: 4.5514
	step 10766:lm_loss: 4.5234, ppl: 92.1524, loss: 4.5514
	step 10767:lm_loss: 4.5235, ppl: 92.1561, loss: 4.5514
	step 10768:lm_loss: 4.5236, ppl: 92.1626, loss: 4.5515
	step 10769:lm_loss: 4.5236, ppl: 92.1700, loss: 4.5516
	step 10770:lm_loss: 4.5236, ppl: 92.1636, loss: 4.5515
	step 10771:lm_loss: 4.5236, ppl: 92.1627, loss: 4.5515
	step 10772:lm_loss: 4.5235, ppl: 92.1610, loss: 4.5515
	step 10773:lm_loss: 4.5236, ppl: 92.1650, loss: 4.5515
	step 10774:lm_loss: 4.5236, ppl: 92.1689, loss: 4.5515
	step 10775:lm_loss: 4.5236, ppl: 92.1668, loss: 4.5515
	step 10776:lm_loss: 4.5236, ppl: 92.1666, loss: 4.5515
	step 10777:lm_loss: 4.5236, ppl: 92.1648, loss: 4.5515
	step 10778:lm_loss: 4.5235, ppl: 92.1612, loss: 4.5515
	step 10779:lm_loss: 4.5235, ppl: 92.1564, loss: 4.5514
	step 10780:lm_loss: 4.5235, ppl: 92.1584, loss: 4.5514
	step 10781:lm_loss: 4.5235, ppl: 92.1605, loss: 4.5515
	step 10782:lm_loss: 4.5235, ppl: 92.1614, loss: 4.5515
	step 10783:lm_loss: 4.5235, ppl: 92.1572, loss: 4.5514
	step 10784:lm_loss: 4.5234, ppl: 92.1523, loss: 4.5513
	step 10785:lm_loss: 4.5235, ppl: 92.1534, loss: 4.5514
	step 10786:lm_loss: 4.5235, ppl: 92.1563, loss: 4.5514
	step 10787:lm_loss: 4.5233, ppl: 92.1401, loss: 4.5512
	step 10788:lm_loss: 4.5233, ppl: 92.1407, loss: 4.5513
	step 10789:lm_loss: 4.5233, ppl: 92.1363, loss: 4.5512
	step 10790:lm_loss: 4.5232, ppl: 92.1337, loss: 4.5512
	step 10791:lm_loss: 4.5233, ppl: 92.1358, loss: 4.5512
	step 10792:lm_loss: 4.5233, ppl: 92.1425, loss: 4.5513
	step 10793:lm_loss: 4.5233, ppl: 92.1399, loss: 4.5512
	step 10794:lm_loss: 4.5233, ppl: 92.1421, loss: 4.5513
	step 10795:lm_loss: 4.5233, ppl: 92.1399, loss: 4.5512
	step 10796:lm_loss: 4.5233, ppl: 92.1434, loss: 4.5513
	step 10797:lm_loss: 4.5234, ppl: 92.1439, loss: 4.5513
	step 10798:lm_loss: 4.5232, ppl: 92.1290, loss: 4.5511
	step 10799:lm_loss: 4.5232, ppl: 92.1331, loss: 4.5512
	step 10800:lm_loss: 4.5231, ppl: 92.1247, loss: 4.5511
	step 10801:lm_loss: 4.5232, ppl: 92.1318, loss: 4.5512
	step 10802:lm_loss: 4.5233, ppl: 92.1422, loss: 4.5513
	step 10803:lm_loss: 4.5234, ppl: 92.1441, loss: 4.5513
	step 10804:lm_loss: 4.5233, ppl: 92.1416, loss: 4.5513
	step 10805:lm_loss: 4.5233, ppl: 92.1365, loss: 4.5512
	step 10806:lm_loss: 4.5231, ppl: 92.1213, loss: 4.5511
	step 10807:lm_loss: 4.5231, ppl: 92.1226, loss: 4.5511
	step 10808:lm_loss: 4.5231, ppl: 92.1211, loss: 4.5511
	step 10809:lm_loss: 4.5231, ppl: 92.1228, loss: 4.5511
	step 10810:lm_loss: 4.5231, ppl: 92.1246, loss: 4.5511
	step 10811:lm_loss: 4.5231, ppl: 92.1247, loss: 4.5511
	step 10812:lm_loss: 4.5231, ppl: 92.1213, loss: 4.5511
	step 10813:lm_loss: 4.5230, ppl: 92.1113, loss: 4.5510
	step 10814:lm_loss: 4.5230, ppl: 92.1119, loss: 4.5510
	step 10815:lm_loss: 4.5232, ppl: 92.1259, loss: 4.5511
	step 10816:lm_loss: 4.5231, ppl: 92.1234, loss: 4.5510
	step 10817:lm_loss: 4.5231, ppl: 92.1190, loss: 4.5510
	step 10818:lm_loss: 4.5230, ppl: 92.1115, loss: 4.5509
	step 10819:lm_loss: 4.5231, ppl: 92.1181, loss: 4.5510
	step 10820:lm_loss: 4.5231, ppl: 92.1231, loss: 4.5511
	step 10821:lm_loss: 4.5231, ppl: 92.1231, loss: 4.5511
	step 10822:lm_loss: 4.5230, ppl: 92.1148, loss: 4.5510
	step 10823:lm_loss: 4.5230, ppl: 92.1110, loss: 4.5510
	step 10824:lm_loss: 4.5231, ppl: 92.1164, loss: 4.5510
	step 10825:lm_loss: 4.5230, ppl: 92.1117, loss: 4.5510
	step 10826:lm_loss: 4.5230, ppl: 92.1092, loss: 4.5509
	step 10827:lm_loss: 4.5230, ppl: 92.1084, loss: 4.5509
	step 10828:lm_loss: 4.5230, ppl: 92.1122, loss: 4.5509
	step 10829:lm_loss: 4.5230, ppl: 92.1144, loss: 4.5510
	step 10830:lm_loss: 4.5230, ppl: 92.1160, loss: 4.5510
	step 10831:lm_loss: 4.5231, ppl: 92.1246, loss: 4.5511
	step 10832:lm_loss: 4.5231, ppl: 92.1235, loss: 4.5511
	step 10833:lm_loss: 4.5232, ppl: 92.1312, loss: 4.5512
	step 10834:lm_loss: 4.5233, ppl: 92.1350, loss: 4.5512
	step 10835:lm_loss: 4.5232, ppl: 92.1316, loss: 4.5512
	step 10836:lm_loss: 4.5232, ppl: 92.1316, loss: 4.5512
	step 10837:lm_loss: 4.5232, ppl: 92.1342, loss: 4.5512
	step 10838:lm_loss: 4.5233, ppl: 92.1427, loss: 4.5513
	step 10839:lm_loss: 4.5234, ppl: 92.1439, loss: 4.5513
	step 10840:lm_loss: 4.5233, ppl: 92.1404, loss: 4.5513
	step 10841:lm_loss: 4.5233, ppl: 92.1390, loss: 4.5512
	step 10842:lm_loss: 4.5233, ppl: 92.1399, loss: 4.5513
	step 10843:lm_loss: 4.5233, ppl: 92.1425, loss: 4.5513
	step 10844:lm_loss: 4.5234, ppl: 92.1481, loss: 4.5513
	step 10845:lm_loss: 4.5234, ppl: 92.1452, loss: 4.5513
	step 10846:lm_loss: 4.5234, ppl: 92.1489, loss: 4.5513
	step 10847:lm_loss: 4.5233, ppl: 92.1347, loss: 4.5512
	step 10848:lm_loss: 4.5232, ppl: 92.1297, loss: 4.5511
	step 10849:lm_loss: 4.5231, ppl: 92.1248, loss: 4.5511
	step 10850:lm_loss: 4.5232, ppl: 92.1310, loss: 4.5511
	step 10851:lm_loss: 4.5233, ppl: 92.1354, loss: 4.5512
	step 10852:lm_loss: 4.5233, ppl: 92.1353, loss: 4.5512
	step 10853:lm_loss: 4.5231, ppl: 92.1212, loss: 4.5510
	step 10854:lm_loss: 4.5231, ppl: 92.1196, loss: 4.5510
	step 10855:lm_loss: 4.5232, ppl: 92.1264, loss: 4.5511
	step 10856:lm_loss: 4.5232, ppl: 92.1332, loss: 4.5512
	step 10857:lm_loss: 4.5232, ppl: 92.1280, loss: 4.5511
	step 10858:lm_loss: 4.5232, ppl: 92.1342, loss: 4.5512
	step 10859:lm_loss: 4.5232, ppl: 92.1311, loss: 4.5512
	step 10860:lm_loss: 4.5232, ppl: 92.1299, loss: 4.5511
	step 10861:lm_loss: 4.5230, ppl: 92.1150, loss: 4.5510
	step 10862:lm_loss: 4.5231, ppl: 92.1242, loss: 4.5511
	step 10863:lm_loss: 4.5231, ppl: 92.1211, loss: 4.5510
	step 10864:lm_loss: 4.5231, ppl: 92.1216, loss: 4.5511
	step 10865:lm_loss: 4.5231, ppl: 92.1246, loss: 4.5511
	step 10866:lm_loss: 4.5232, ppl: 92.1293, loss: 4.5511
	step 10867:lm_loss: 4.5232, ppl: 92.1293, loss: 4.5511
	step 10868:lm_loss: 4.5232, ppl: 92.1318, loss: 4.5511
	step 10869:lm_loss: 4.5232, ppl: 92.1278, loss: 4.5511
	step 10870:lm_loss: 4.5232, ppl: 92.1316, loss: 4.5511
	step 10871:lm_loss: 4.5231, ppl: 92.1235, loss: 4.5510
	step 10872:lm_loss: 4.5232, ppl: 92.1298, loss: 4.5511
	step 10873:lm_loss: 4.5232, ppl: 92.1300, loss: 4.5511
	step 10874:lm_loss: 4.5231, ppl: 92.1186, loss: 4.5510
	step 10875:lm_loss: 4.5231, ppl: 92.1169, loss: 4.5510
	step 10876:lm_loss: 4.5231, ppl: 92.1182, loss: 4.5510
	step 10877:lm_loss: 4.5230, ppl: 92.1122, loss: 4.5509
	step 10878:lm_loss: 4.5230, ppl: 92.1140, loss: 4.5509
	step 10879:lm_loss: 4.5230, ppl: 92.1095, loss: 4.5509
	step 10880:lm_loss: 4.5230, ppl: 92.1112, loss: 4.5509
	step 10881:lm_loss: 4.5230, ppl: 92.1160, loss: 4.5510
	step 10882:lm_loss: 4.5231, ppl: 92.1164, loss: 4.5510
	step 10883:lm_loss: 4.5231, ppl: 92.1207, loss: 4.5510
	step 10884:lm_loss: 4.5230, ppl: 92.1159, loss: 4.5510
	step 10885:lm_loss: 4.5232, ppl: 92.1262, loss: 4.5511
	step 10886:lm_loss: 4.5232, ppl: 92.1326, loss: 4.5511
	step 10887:lm_loss: 4.5232, ppl: 92.1290, loss: 4.5511
	step 10888:lm_loss: 4.5232, ppl: 92.1317, loss: 4.5511
	step 10889:lm_loss: 4.5233, ppl: 92.1403, loss: 4.5512
	step 10890:lm_loss: 4.5234, ppl: 92.1460, loss: 4.5512
	step 10891:lm_loss: 4.5234, ppl: 92.1439, loss: 4.5512
	step 10892:lm_loss: 4.5234, ppl: 92.1529, loss: 4.5513
	step 10893:lm_loss: 4.5234, ppl: 92.1484, loss: 4.5513
	step 10894:lm_loss: 4.5234, ppl: 92.1473, loss: 4.5512
	step 10895:lm_loss: 4.5234, ppl: 92.1480, loss: 4.5512
	step 10896:lm_loss: 4.5234, ppl: 92.1514, loss: 4.5513
	step 10897:lm_loss: 4.5234, ppl: 92.1492, loss: 4.5513
	step 10898:lm_loss: 4.5234, ppl: 92.1488, loss: 4.5513
	step 10899:lm_loss: 4.5234, ppl: 92.1447, loss: 4.5512
	step 10900:lm_loss: 4.5234, ppl: 92.1467, loss: 4.5512
	step 10901:lm_loss: 4.5233, ppl: 92.1432, loss: 4.5512
	step 10902:lm_loss: 4.5232, ppl: 92.1331, loss: 4.5511
	step 10903:lm_loss: 4.5232, ppl: 92.1317, loss: 4.5510
	step 10904:lm_loss: 4.5233, ppl: 92.1426, loss: 4.5511
	step 10905:lm_loss: 4.5232, ppl: 92.1334, loss: 4.5510
	step 10906:lm_loss: 4.5233, ppl: 92.1382, loss: 4.5511
	step 10907:lm_loss: 4.5233, ppl: 92.1387, loss: 4.5511
	step 10908:lm_loss: 4.5233, ppl: 92.1368, loss: 4.5510
	step 10909:lm_loss: 4.5233, ppl: 92.1371, loss: 4.5510
	step 10910:lm_loss: 4.5233, ppl: 92.1404, loss: 4.5511
	step 10911:lm_loss: 4.5234, ppl: 92.1488, loss: 4.5512
	step 10912:lm_loss: 4.5235, ppl: 92.1565, loss: 4.5512
	step 10913:lm_loss: 4.5234, ppl: 92.1516, loss: 4.5511
	step 10914:lm_loss: 4.5235, ppl: 92.1553, loss: 4.5512
	step 10915:lm_loss: 4.5233, ppl: 92.1432, loss: 4.5511
	step 10916:lm_loss: 4.5233, ppl: 92.1390, loss: 4.5510
	step 10917:lm_loss: 4.5233, ppl: 92.1366, loss: 4.5510
	step 10918:lm_loss: 4.5232, ppl: 92.1336, loss: 4.5509
	step 10919:lm_loss: 4.5232, ppl: 92.1296, loss: 4.5509
	step 10920:lm_loss: 4.5231, ppl: 92.1206, loss: 4.5508
	step 10921:lm_loss: 4.5232, ppl: 92.1325, loss: 4.5509
	step 10922:lm_loss: 4.5233, ppl: 92.1361, loss: 4.5510
	step 10923:lm_loss: 4.5232, ppl: 92.1336, loss: 4.5509
	step 10924:lm_loss: 4.5232, ppl: 92.1319, loss: 4.5509
	step 10925:lm_loss: 4.5232, ppl: 92.1304, loss: 4.5509
	step 10926:lm_loss: 4.5231, ppl: 92.1240, loss: 4.5508
	step 10927:lm_loss: 4.5231, ppl: 92.1196, loss: 4.5508
	step 10928:lm_loss: 4.5231, ppl: 92.1179, loss: 4.5507
	step 10929:lm_loss: 4.5231, ppl: 92.1189, loss: 4.5508
	step 10930:lm_loss: 4.5231, ppl: 92.1201, loss: 4.5508
	step 10931:lm_loss: 4.5231, ppl: 92.1236, loss: 4.5508
	step 10932:lm_loss: 4.5231, ppl: 92.1188, loss: 4.5508
	step 10933:lm_loss: 4.5230, ppl: 92.1135, loss: 4.5507
	step 10934:lm_loss: 4.5230, ppl: 92.1082, loss: 4.5506
	step 10935:lm_loss: 4.5231, ppl: 92.1170, loss: 4.5507
	step 10936:lm_loss: 4.5229, ppl: 92.1018, loss: 4.5506
	step 10937:lm_loss: 4.5229, ppl: 92.1005, loss: 4.5506
	step 10938:lm_loss: 4.5228, ppl: 92.0948, loss: 4.5505
	step 10939:lm_loss: 4.5228, ppl: 92.0903, loss: 4.5505
	step 10940:lm_loss: 4.5229, ppl: 92.1028, loss: 4.5506
	step 10941:lm_loss: 4.5230, ppl: 92.1090, loss: 4.5507
	step 10942:lm_loss: 4.5227, ppl: 92.0859, loss: 4.5506
	step 10943:lm_loss: 4.5227, ppl: 92.0884, loss: 4.5506
	step 10944:lm_loss: 4.5227, ppl: 92.0844, loss: 4.5506
	step 10945:lm_loss: 4.5228, ppl: 92.0886, loss: 4.5506
	step 10946:lm_loss: 4.5227, ppl: 92.0852, loss: 4.5506
	step 10947:lm_loss: 4.5226, ppl: 92.0731, loss: 4.5505
	step 10948:lm_loss: 4.5226, ppl: 92.0769, loss: 4.5505
	step 10949:lm_loss: 4.5227, ppl: 92.0855, loss: 4.5506
	step 10950:lm_loss: 4.5227, ppl: 92.0832, loss: 4.5506
	step 10951:lm_loss: 4.5227, ppl: 92.0881, loss: 4.5507
	step 10952:lm_loss: 4.5228, ppl: 92.0906, loss: 4.5507
	step 10953:lm_loss: 4.5228, ppl: 92.0901, loss: 4.5507
	step 10954:lm_loss: 4.5228, ppl: 92.0908, loss: 4.5507
	step 10955:lm_loss: 4.5227, ppl: 92.0833, loss: 4.5507
	step 10956:lm_loss: 4.5226, ppl: 92.0741, loss: 4.5506
	step 10957:lm_loss: 4.5226, ppl: 92.0780, loss: 4.5506
	step 10958:lm_loss: 4.5227, ppl: 92.0816, loss: 4.5506
	step 10959:lm_loss: 4.5226, ppl: 92.0706, loss: 4.5506
	step 10960:lm_loss: 4.5225, ppl: 92.0679, loss: 4.5505
	step 10961:lm_loss: 4.5227, ppl: 92.0801, loss: 4.5507
	step 10962:lm_loss: 4.5227, ppl: 92.0824, loss: 4.5507
	step 10963:lm_loss: 4.5227, ppl: 92.0814, loss: 4.5507
	step 10964:lm_loss: 4.5227, ppl: 92.0805, loss: 4.5507
	step 10965:lm_loss: 4.5226, ppl: 92.0764, loss: 4.5506
	step 10966:lm_loss: 4.5226, ppl: 92.0779, loss: 4.5506
	step 10967:lm_loss: 4.5226, ppl: 92.0737, loss: 4.5506
	step 10968:lm_loss: 4.5226, ppl: 92.0786, loss: 4.5506
	step 10969:lm_loss: 4.5227, ppl: 92.0845, loss: 4.5507
	step 10970:lm_loss: 4.5228, ppl: 92.0962, loss: 4.5509
	step 10971:lm_loss: 4.5228, ppl: 92.0942, loss: 4.5508
	step 10972:lm_loss: 4.5228, ppl: 92.0925, loss: 4.5508
	step 10973:lm_loss: 4.5228, ppl: 92.0970, loss: 4.5509
	step 10974:lm_loss: 4.5228, ppl: 92.0897, loss: 4.5507
	step 10975:lm_loss: 4.5228, ppl: 92.0933, loss: 4.5508
	step 10976:lm_loss: 4.5229, ppl: 92.1021, loss: 4.5509
	step 10977:lm_loss: 4.5230, ppl: 92.1120, loss: 4.5510
	step 10978:lm_loss: 4.5230, ppl: 92.1070, loss: 4.5509
	step 10979:lm_loss: 4.5230, ppl: 92.1131, loss: 4.5510
	step 10980:lm_loss: 4.5230, ppl: 92.1136, loss: 4.5510
	step 10981:lm_loss: 4.5230, ppl: 92.1126, loss: 4.5510
	step 10982:lm_loss: 4.5230, ppl: 92.1156, loss: 4.5510
	step 10983:lm_loss: 4.5231, ppl: 92.1195, loss: 4.5511
	step 10984:lm_loss: 4.5231, ppl: 92.1170, loss: 4.5510
	step 10985:lm_loss: 4.5230, ppl: 92.1116, loss: 4.5510
	step 10986:lm_loss: 4.5230, ppl: 92.1080, loss: 4.5509
	step 10987:lm_loss: 4.5229, ppl: 92.1069, loss: 4.5509
	step 10988:lm_loss: 4.5230, ppl: 92.1160, loss: 4.5511
	step 10989:lm_loss: 4.5231, ppl: 92.1192, loss: 4.5511
	step 10990:lm_loss: 4.5231, ppl: 92.1188, loss: 4.5511
	step 10991:lm_loss: 4.5231, ppl: 92.1251, loss: 4.5511
	step 10992:lm_loss: 4.5232, ppl: 92.1280, loss: 4.5512
	step 10993:lm_loss: 4.5232, ppl: 92.1308, loss: 4.5512
	step 10994:lm_loss: 4.5233, ppl: 92.1382, loss: 4.5513
	step 10995:lm_loss: 4.5234, ppl: 92.1520, loss: 4.5514
	step 10996:lm_loss: 4.5235, ppl: 92.1593, loss: 4.5515
	step 10997:lm_loss: 4.5235, ppl: 92.1593, loss: 4.5515
	step 10998:lm_loss: 4.5235, ppl: 92.1612, loss: 4.5515
	step 10999:lm_loss: 4.5235, ppl: 92.1608, loss: 4.5515
	step 11000:lm_loss: 4.5236, ppl: 92.1668, loss: 4.5516
	step 11001:lm_loss: 4.5236, ppl: 92.1682, loss: 4.5516
	step 11002:lm_loss: 4.5235, ppl: 92.1560, loss: 4.5515
	step 11003:lm_loss: 4.5235, ppl: 92.1610, loss: 4.5515
	step 11004:lm_loss: 4.5235, ppl: 92.1610, loss: 4.5515
	step 11005:lm_loss: 4.5235, ppl: 92.1616, loss: 4.5515
	step 11006:lm_loss: 4.5235, ppl: 92.1577, loss: 4.5514
	step 11007:lm_loss: 4.5236, ppl: 92.1671, loss: 4.5515
	step 11008:lm_loss: 4.5236, ppl: 92.1633, loss: 4.5515
	step 11009:lm_loss: 4.5236, ppl: 92.1665, loss: 4.5515
	step 11010:lm_loss: 4.5236, ppl: 92.1656, loss: 4.5515
	step 11011:lm_loss: 4.5236, ppl: 92.1673, loss: 4.5515
	step 11012:lm_loss: 4.5236, ppl: 92.1673, loss: 4.5515
	step 11013:lm_loss: 4.5236, ppl: 92.1686, loss: 4.5515
	step 11014:lm_loss: 4.5236, ppl: 92.1672, loss: 4.5515
	step 11015:lm_loss: 4.5235, ppl: 92.1541, loss: 4.5514
	step 11016:lm_loss: 4.5234, ppl: 92.1524, loss: 4.5514
	step 11017:lm_loss: 4.5235, ppl: 92.1530, loss: 4.5514
	step 11018:lm_loss: 4.5235, ppl: 92.1590, loss: 4.5515
	step 11019:lm_loss: 4.5235, ppl: 92.1595, loss: 4.5515
	step 11020:lm_loss: 4.5236, ppl: 92.1644, loss: 4.5515
	step 11021:lm_loss: 4.5234, ppl: 92.1525, loss: 4.5514
	step 11022:lm_loss: 4.5234, ppl: 92.1467, loss: 4.5513
	step 11023:lm_loss: 4.5234, ppl: 92.1451, loss: 4.5513
	step 11024:lm_loss: 4.5234, ppl: 92.1459, loss: 4.5513
	step 11025:lm_loss: 4.5232, ppl: 92.1341, loss: 4.5512
	step 11026:lm_loss: 4.5233, ppl: 92.1426, loss: 4.5513
	step 11027:lm_loss: 4.5232, ppl: 92.1285, loss: 4.5512
	step 11028:lm_loss: 4.5232, ppl: 92.1289, loss: 4.5512
	step 11029:lm_loss: 4.5233, ppl: 92.1403, loss: 4.5513
	step 11030:lm_loss: 4.5233, ppl: 92.1354, loss: 4.5512
	step 11031:lm_loss: 4.5233, ppl: 92.1357, loss: 4.5512
	step 11032:lm_loss: 4.5233, ppl: 92.1420, loss: 4.5513
	step 11033:lm_loss: 4.5233, ppl: 92.1437, loss: 4.5513
	step 11034:lm_loss: 4.5233, ppl: 92.1435, loss: 4.5513
	step 11035:lm_loss: 4.5234, ppl: 92.1457, loss: 4.5513
	step 11036:lm_loss: 4.5234, ppl: 92.1518, loss: 4.5514
	step 11037:lm_loss: 4.5234, ppl: 92.1494, loss: 4.5514
	step 11038:lm_loss: 4.5234, ppl: 92.1443, loss: 4.5513
	step 11039:lm_loss: 4.5233, ppl: 92.1375, loss: 4.5512
	step 11040:lm_loss: 4.5232, ppl: 92.1343, loss: 4.5512
	step 11041:lm_loss: 4.5232, ppl: 92.1318, loss: 4.5511
	step 11042:lm_loss: 4.5233, ppl: 92.1364, loss: 4.5512
	step 11043:lm_loss: 4.5233, ppl: 92.1426, loss: 4.5513
	step 11044:lm_loss: 4.5234, ppl: 92.1481, loss: 4.5513
	step 11045:lm_loss: 4.5233, ppl: 92.1411, loss: 4.5513
	step 11046:lm_loss: 4.5234, ppl: 92.1444, loss: 4.5513
	step 11047:lm_loss: 4.5234, ppl: 92.1490, loss: 4.5514
	step 11048:lm_loss: 4.5234, ppl: 92.1517, loss: 4.5514
	step 11049:lm_loss: 4.5235, ppl: 92.1540, loss: 4.5514
	step 11050:lm_loss: 4.5235, ppl: 92.1590, loss: 4.5514
	step 11051:lm_loss: 4.5235, ppl: 92.1621, loss: 4.5515
	step 11052:lm_loss: 4.5237, ppl: 92.1770, loss: 4.5516
	step 11053:lm_loss: 4.5237, ppl: 92.1780, loss: 4.5516
	step 11054:lm_loss: 4.5237, ppl: 92.1734, loss: 4.5516
	step 11055:lm_loss: 4.5237, ppl: 92.1793, loss: 4.5517
	step 11056:lm_loss: 4.5236, ppl: 92.1690, loss: 4.5515
	step 11057:lm_loss: 4.5236, ppl: 92.1680, loss: 4.5515
	step 11058:lm_loss: 4.5236, ppl: 92.1680, loss: 4.5515
	step 11059:lm_loss: 4.5235, ppl: 92.1533, loss: 4.5514
	step 11060:lm_loss: 4.5234, ppl: 92.1492, loss: 4.5513
	step 11061:lm_loss: 4.5234, ppl: 92.1514, loss: 4.5513
	step 11062:lm_loss: 4.5233, ppl: 92.1435, loss: 4.5512
	step 11063:lm_loss: 4.5233, ppl: 92.1405, loss: 4.5512
	step 11064:lm_loss: 4.5233, ppl: 92.1383, loss: 4.5512
	step 11065:lm_loss: 4.5233, ppl: 92.1385, loss: 4.5512
	step 11066:lm_loss: 4.5232, ppl: 92.1320, loss: 4.5511
	step 11067:lm_loss: 4.5233, ppl: 92.1416, loss: 4.5513
	step 11068:lm_loss: 4.5234, ppl: 92.1453, loss: 4.5513
	step 11069:lm_loss: 4.5234, ppl: 92.1466, loss: 4.5513
	step 11070:lm_loss: 4.5234, ppl: 92.1504, loss: 4.5513
	step 11071:lm_loss: 4.5234, ppl: 92.1491, loss: 4.5513
	step 11072:lm_loss: 4.5235, ppl: 92.1564, loss: 4.5514
	step 11073:lm_loss: 4.5234, ppl: 92.1481, loss: 4.5513
	step 11074:lm_loss: 4.5234, ppl: 92.1460, loss: 4.5513
	step 11075:lm_loss: 4.5234, ppl: 92.1462, loss: 4.5513
	step 11076:lm_loss: 4.5234, ppl: 92.1476, loss: 4.5513
	step 11077:lm_loss: 4.5234, ppl: 92.1527, loss: 4.5514
	step 11078:lm_loss: 4.5236, ppl: 92.1663, loss: 4.5516
	step 11079:lm_loss: 4.5237, ppl: 92.1763, loss: 4.5517
	step 11080:lm_loss: 4.5236, ppl: 92.1633, loss: 4.5516
	step 11081:lm_loss: 4.5236, ppl: 92.1679, loss: 4.5516
	step 11082:lm_loss: 4.5237, ppl: 92.1723, loss: 4.5517
	step 11083:lm_loss: 4.5236, ppl: 92.1708, loss: 4.5517
	step 11084:lm_loss: 4.5236, ppl: 92.1650, loss: 4.5516
	step 11085:lm_loss: 4.5235, ppl: 92.1585, loss: 4.5515
	step 11086:lm_loss: 4.5236, ppl: 92.1658, loss: 4.5516
	step 11087:lm_loss: 4.5237, ppl: 92.1771, loss: 4.5517
	step 11088:lm_loss: 4.5236, ppl: 92.1689, loss: 4.5516
	step 11089:lm_loss: 4.5237, ppl: 92.1740, loss: 4.5517
	step 11090:lm_loss: 4.5237, ppl: 92.1741, loss: 4.5517
	step 11091:lm_loss: 4.5237, ppl: 92.1737, loss: 4.5517
	step 11092:lm_loss: 4.5236, ppl: 92.1711, loss: 4.5517
	step 11093:lm_loss: 4.5237, ppl: 92.1755, loss: 4.5517
	step 11094:lm_loss: 4.5237, ppl: 92.1799, loss: 4.5517
	step 11095:lm_loss: 4.5238, ppl: 92.1848, loss: 4.5518
	step 11096:lm_loss: 4.5238, ppl: 92.1852, loss: 4.5518
	step 11097:lm_loss: 4.5238, ppl: 92.1832, loss: 4.5518
	step 11098:lm_loss: 4.5238, ppl: 92.1891, loss: 4.5519
	step 11099:lm_loss: 4.5239, ppl: 92.1916, loss: 4.5519
	step 11100:lm_loss: 4.5240, ppl: 92.2021, loss: 4.5520
	step 11101:lm_loss: 4.5240, ppl: 92.2073, loss: 4.5521
	step 11102:lm_loss: 4.5240, ppl: 92.2004, loss: 4.5520
	step 11103:lm_loss: 4.5240, ppl: 92.2041, loss: 4.5520
	step 11104:lm_loss: 4.5241, ppl: 92.2117, loss: 4.5521
	step 11105:lm_loss: 4.5242, ppl: 92.2221, loss: 4.5522
	step 11106:lm_loss: 4.5243, ppl: 92.2324, loss: 4.5523
	step 11107:lm_loss: 4.5243, ppl: 92.2294, loss: 4.5523
	step 11108:lm_loss: 4.5244, ppl: 92.2373, loss: 4.5523
	step 11109:lm_loss: 4.5242, ppl: 92.2227, loss: 4.5522
	step 11110:lm_loss: 4.5241, ppl: 92.2163, loss: 4.5522
	step 11111:lm_loss: 4.5241, ppl: 92.2169, loss: 4.5522
	step 11112:lm_loss: 4.5242, ppl: 92.2181, loss: 4.5522
	step 11113:lm_loss: 4.5242, ppl: 92.2204, loss: 4.5522
	step 11114:lm_loss: 4.5242, ppl: 92.2231, loss: 4.5522
	step 11115:lm_loss: 4.5242, ppl: 92.2245, loss: 4.5522
	step 11116:lm_loss: 4.5241, ppl: 92.2149, loss: 4.5522
	step 11117:lm_loss: 4.5242, ppl: 92.2262, loss: 4.5522
	step 11118:lm_loss: 4.5242, ppl: 92.2248, loss: 4.5522
	step 11119:lm_loss: 4.5243, ppl: 92.2286, loss: 4.5523
	step 11120:lm_loss: 4.5243, ppl: 92.2283, loss: 4.5522
	step 11121:lm_loss: 4.5242, ppl: 92.2265, loss: 4.5522
	step 11122:lm_loss: 4.5243, ppl: 92.2304, loss: 4.5523
	step 11123:lm_loss: 4.5243, ppl: 92.2307, loss: 4.5523
	step 11124:lm_loss: 4.5243, ppl: 92.2298, loss: 4.5523
	step 11125:lm_loss: 4.5242, ppl: 92.2240, loss: 4.5522
	step 11126:lm_loss: 4.5242, ppl: 92.2210, loss: 4.5522
	step 11127:lm_loss: 4.5242, ppl: 92.2243, loss: 4.5522
	step 11128:lm_loss: 4.5242, ppl: 92.2213, loss: 4.5522
	step 11129:lm_loss: 4.5242, ppl: 92.2217, loss: 4.5522
	step 11130:lm_loss: 4.5241, ppl: 92.2171, loss: 4.5521
	step 11131:lm_loss: 4.5241, ppl: 92.2132, loss: 4.5520
	step 11132:lm_loss: 4.5243, ppl: 92.2308, loss: 4.5522
	step 11133:lm_loss: 4.5243, ppl: 92.2352, loss: 4.5523
	step 11134:lm_loss: 4.5244, ppl: 92.2399, loss: 4.5523
	step 11135:lm_loss: 4.5244, ppl: 92.2360, loss: 4.5523
	step 11136:lm_loss: 4.5244, ppl: 92.2395, loss: 4.5523
	step 11137:lm_loss: 4.5244, ppl: 92.2403, loss: 4.5523
	step 11138:lm_loss: 4.5245, ppl: 92.2499, loss: 4.5524
	step 11139:lm_loss: 4.5246, ppl: 92.2556, loss: 4.5525
	step 11140:lm_loss: 4.5245, ppl: 92.2517, loss: 4.5524
	step 11141:lm_loss: 4.5245, ppl: 92.2503, loss: 4.5524
	step 11142:lm_loss: 4.5246, ppl: 92.2584, loss: 4.5525
	step 11143:lm_loss: 4.5244, ppl: 92.2360, loss: 4.5524
	step 11144:lm_loss: 4.5243, ppl: 92.2325, loss: 4.5523
	step 11145:lm_loss: 4.5241, ppl: 92.2136, loss: 4.5522
	step 11146:lm_loss: 4.5240, ppl: 92.2069, loss: 4.5521
	step 11147:lm_loss: 4.5241, ppl: 92.2149, loss: 4.5522
	step 11148:lm_loss: 4.5241, ppl: 92.2171, loss: 4.5522
	step 11149:lm_loss: 4.5241, ppl: 92.2170, loss: 4.5522
	step 11150:lm_loss: 4.5242, ppl: 92.2187, loss: 4.5522
	step 11151:lm_loss: 4.5242, ppl: 92.2251, loss: 4.5522
	step 11152:lm_loss: 4.5243, ppl: 92.2290, loss: 4.5523
	step 11153:lm_loss: 4.5244, ppl: 92.2380, loss: 4.5524
	step 11154:lm_loss: 4.5245, ppl: 92.2510, loss: 4.5525
	step 11155:lm_loss: 4.5242, ppl: 92.2196, loss: 4.5523
	step 11156:lm_loss: 4.5241, ppl: 92.2169, loss: 4.5523
	step 11157:lm_loss: 4.5242, ppl: 92.2230, loss: 4.5524
	step 11158:lm_loss: 4.5242, ppl: 92.2258, loss: 4.5524
	step 11159:lm_loss: 4.5242, ppl: 92.2176, loss: 4.5523
	step 11160:lm_loss: 4.5243, ppl: 92.2297, loss: 4.5524
	step 11161:lm_loss: 4.5243, ppl: 92.2329, loss: 4.5525
	step 11162:lm_loss: 4.5243, ppl: 92.2343, loss: 4.5525
	step 11163:lm_loss: 4.5244, ppl: 92.2406, loss: 4.5526
	step 11164:lm_loss: 4.5244, ppl: 92.2444, loss: 4.5526
	step 11165:lm_loss: 4.5243, ppl: 92.2319, loss: 4.5525
	step 11166:lm_loss: 4.5244, ppl: 92.2390, loss: 4.5526
	step 11167:lm_loss: 4.5245, ppl: 92.2493, loss: 4.5527
	step 11168:lm_loss: 4.5244, ppl: 92.2451, loss: 4.5526
	step 11169:lm_loss: 4.5244, ppl: 92.2401, loss: 4.5526
	step 11170:lm_loss: 4.5244, ppl: 92.2392, loss: 4.5525
	step 11171:lm_loss: 4.5244, ppl: 92.2415, loss: 4.5526
	step 11172:lm_loss: 4.5244, ppl: 92.2451, loss: 4.5526
	step 11173:lm_loss: 4.5246, ppl: 92.2568, loss: 4.5528
	step 11174:lm_loss: 4.5246, ppl: 92.2580, loss: 4.5528
	step 11175:lm_loss: 4.5247, ppl: 92.2673, loss: 4.5529
	step 11176:lm_loss: 4.5246, ppl: 92.2595, loss: 4.5528
	step 11177:lm_loss: 4.5247, ppl: 92.2685, loss: 4.5529
	step 11178:lm_loss: 4.5246, ppl: 92.2629, loss: 4.5528
	step 11179:lm_loss: 4.5246, ppl: 92.2595, loss: 4.5528
	step 11180:lm_loss: 4.5246, ppl: 92.2618, loss: 4.5528
	step 11181:lm_loss: 4.5247, ppl: 92.2647, loss: 4.5528
	step 11182:lm_loss: 4.5247, ppl: 92.2651, loss: 4.5528
	step 11183:lm_loss: 4.5248, ppl: 92.2756, loss: 4.5530
	step 11184:lm_loss: 4.5248, ppl: 92.2770, loss: 4.5530
	step 11185:lm_loss: 4.5248, ppl: 92.2804, loss: 4.5530
	step 11186:lm_loss: 4.5248, ppl: 92.2793, loss: 4.5530
	step 11187:lm_loss: 4.5248, ppl: 92.2793, loss: 4.5530
	step 11188:lm_loss: 4.5248, ppl: 92.2765, loss: 4.5529
	step 11189:lm_loss: 4.5248, ppl: 92.2733, loss: 4.5529
	step 11190:lm_loss: 4.5248, ppl: 92.2786, loss: 4.5530
	step 11191:lm_loss: 4.5249, ppl: 92.2869, loss: 4.5531
	step 11192:lm_loss: 4.5249, ppl: 92.2869, loss: 4.5531
	step 11193:lm_loss: 4.5249, ppl: 92.2909, loss: 4.5531
	step 11194:lm_loss: 4.5250, ppl: 92.2934, loss: 4.5531
	step 11195:lm_loss: 4.5250, ppl: 92.2961, loss: 4.5532
	step 11196:lm_loss: 4.5249, ppl: 92.2878, loss: 4.5531
	step 11197:lm_loss: 4.5250, ppl: 92.2963, loss: 4.5532
	step 11198:lm_loss: 4.5250, ppl: 92.2914, loss: 4.5531
	step 11199:lm_loss: 4.5250, ppl: 92.2927, loss: 4.5532
	step 11200:lm_loss: 4.5250, ppl: 92.2931, loss: 4.5532
	step 11201:lm_loss: 4.5249, ppl: 92.2896, loss: 4.5531
	step 11202:lm_loss: 4.5248, ppl: 92.2793, loss: 4.5530
	step 11203:lm_loss: 4.5248, ppl: 92.2789, loss: 4.5530
	step 11204:lm_loss: 4.5249, ppl: 92.2880, loss: 4.5531
	step 11205:lm_loss: 4.5249, ppl: 92.2832, loss: 4.5530
	step 11206:lm_loss: 4.5248, ppl: 92.2811, loss: 4.5530
	step 11207:lm_loss: 4.5248, ppl: 92.2785, loss: 4.5530
	step 11208:lm_loss: 4.5249, ppl: 92.2847, loss: 4.5530
	step 11209:lm_loss: 4.5249, ppl: 92.2901, loss: 4.5531
	step 11210:lm_loss: 4.5249, ppl: 92.2868, loss: 4.5530
	step 11211:lm_loss: 4.5250, ppl: 92.2963, loss: 4.5532
	step 11212:lm_loss: 4.5250, ppl: 92.2931, loss: 4.5531
	step 11213:lm_loss: 4.5248, ppl: 92.2803, loss: 4.5530
	step 11214:lm_loss: 4.5249, ppl: 92.2893, loss: 4.5531
	step 11215:lm_loss: 4.5249, ppl: 92.2902, loss: 4.5531
	step 11216:lm_loss: 4.5249, ppl: 92.2909, loss: 4.5531
	step 11217:lm_loss: 4.5250, ppl: 92.2935, loss: 4.5531
	step 11218:lm_loss: 4.5251, ppl: 92.3010, loss: 4.5532
	step 11219:lm_loss: 4.5251, ppl: 92.3027, loss: 4.5532
	step 11220:lm_loss: 4.5251, ppl: 92.3085, loss: 4.5533
	step 11221:lm_loss: 4.5251, ppl: 92.3053, loss: 4.5532
	step 11222:lm_loss: 4.5247, ppl: 92.2700, loss: 4.5531
	step 11223:lm_loss: 4.5247, ppl: 92.2710, loss: 4.5531
	step 11224:lm_loss: 4.5248, ppl: 92.2793, loss: 4.5531
	step 11225:lm_loss: 4.5249, ppl: 92.2843, loss: 4.5532
	step 11226:lm_loss: 4.5249, ppl: 92.2846, loss: 4.5532
	step 11227:lm_loss: 4.5250, ppl: 92.2915, loss: 4.5533
	step 11228:lm_loss: 4.5250, ppl: 92.2916, loss: 4.5533
	step 11229:lm_loss: 4.5248, ppl: 92.2803, loss: 4.5531
	step 11230:lm_loss: 4.5249, ppl: 92.2826, loss: 4.5532
	step 11231:lm_loss: 4.5249, ppl: 92.2825, loss: 4.5532
	step 11232:lm_loss: 4.5248, ppl: 92.2816, loss: 4.5532
	step 11233:lm_loss: 4.5249, ppl: 92.2881, loss: 4.5532
	step 11234:lm_loss: 4.5250, ppl: 92.2945, loss: 4.5533
	step 11235:lm_loss: 4.5250, ppl: 92.2941, loss: 4.5533
	step 11236:lm_loss: 4.5250, ppl: 92.2982, loss: 4.5533
	step 11237:lm_loss: 4.5251, ppl: 92.3050, loss: 4.5535
	step 11238:lm_loss: 4.5251, ppl: 92.3086, loss: 4.5535
	step 11239:lm_loss: 4.5252, ppl: 92.3143, loss: 4.5535
	step 11240:lm_loss: 4.5252, ppl: 92.3105, loss: 4.5535
	step 11241:lm_loss: 4.5250, ppl: 92.3004, loss: 4.5534
	step 11242:lm_loss: 4.5251, ppl: 92.3021, loss: 4.5534
	step 11243:lm_loss: 4.5251, ppl: 92.3028, loss: 4.5534
	step 11244:lm_loss: 4.5251, ppl: 92.3034, loss: 4.5534
	step 11245:lm_loss: 4.5252, ppl: 92.3137, loss: 4.5535
	step 11246:lm_loss: 4.5252, ppl: 92.3135, loss: 4.5535
	step 11247:lm_loss: 4.5253, ppl: 92.3196, loss: 4.5536
	step 11248:lm_loss: 4.5253, ppl: 92.3214, loss: 4.5536
	step 11249:lm_loss: 4.5254, ppl: 92.3295, loss: 4.5537
	step 11250:lm_loss: 4.5253, ppl: 92.3263, loss: 4.5537
	step 11251:lm_loss: 4.5254, ppl: 92.3285, loss: 4.5537
	step 11252:lm_loss: 4.5255, ppl: 92.3376, loss: 4.5538
	step 11253:lm_loss: 4.5255, ppl: 92.3407, loss: 4.5538
	step 11254:lm_loss: 4.5255, ppl: 92.3441, loss: 4.5538
	step 11255:lm_loss: 4.5255, ppl: 92.3461, loss: 4.5539
	step 11256:lm_loss: 4.5256, ppl: 92.3526, loss: 4.5539
	step 11257:lm_loss: 4.5257, ppl: 92.3589, loss: 4.5540
	step 11258:lm_loss: 4.5257, ppl: 92.3636, loss: 4.5541
	step 11259:lm_loss: 4.5257, ppl: 92.3606, loss: 4.5540
	step 11260:lm_loss: 4.5257, ppl: 92.3617, loss: 4.5540
	step 11261:lm_loss: 4.5258, ppl: 92.3654, loss: 4.5541
	step 11262:lm_loss: 4.5258, ppl: 92.3664, loss: 4.5541
	step 11263:lm_loss: 4.5257, ppl: 92.3634, loss: 4.5541
	step 11264:lm_loss: 4.5256, ppl: 92.3552, loss: 4.5540
	step 11265:lm_loss: 4.5256, ppl: 92.3510, loss: 4.5539
	step 11266:lm_loss: 4.5256, ppl: 92.3534, loss: 4.5539
	step 11267:lm_loss: 4.5256, ppl: 92.3542, loss: 4.5539
	step 11268:lm_loss: 4.5258, ppl: 92.3653, loss: 4.5541
	step 11269:lm_loss: 4.5258, ppl: 92.3685, loss: 4.5541
	step 11270:lm_loss: 4.5259, ppl: 92.3749, loss: 4.5542
	step 11271:lm_loss: 4.5259, ppl: 92.3808, loss: 4.5542
	step 11272:lm_loss: 4.5259, ppl: 92.3830, loss: 4.5543
	step 11273:lm_loss: 4.5260, ppl: 92.3842, loss: 4.5543
	step 11274:lm_loss: 4.5259, ppl: 92.3820, loss: 4.5543
	step 11275:lm_loss: 4.5259, ppl: 92.3786, loss: 4.5542
	step 11276:lm_loss: 4.5259, ppl: 92.3813, loss: 4.5542
	step 11277:lm_loss: 4.5260, ppl: 92.3845, loss: 4.5543
	step 11278:lm_loss: 4.5260, ppl: 92.3861, loss: 4.5543
	step 11279:lm_loss: 4.5260, ppl: 92.3908, loss: 4.5543
	step 11280:lm_loss: 4.5261, ppl: 92.3984, loss: 4.5544
	step 11281:lm_loss: 4.5261, ppl: 92.3996, loss: 4.5544
	step 11282:lm_loss: 4.5261, ppl: 92.4007, loss: 4.5544
	step 11283:lm_loss: 4.5261, ppl: 92.4015, loss: 4.5544
	step 11284:lm_loss: 4.5261, ppl: 92.4000, loss: 4.5544
	step 11285:lm_loss: 4.5262, ppl: 92.4060, loss: 4.5545
	step 11286:lm_loss: 4.5262, ppl: 92.4050, loss: 4.5545
	step 11287:lm_loss: 4.5262, ppl: 92.4068, loss: 4.5545
	step 11288:lm_loss: 4.5261, ppl: 92.4016, loss: 4.5545
	step 11289:lm_loss: 4.5263, ppl: 92.4149, loss: 4.5546
	step 11290:lm_loss: 4.5263, ppl: 92.4170, loss: 4.5546
	step 11291:lm_loss: 4.5263, ppl: 92.4139, loss: 4.5546
	step 11292:lm_loss: 4.5262, ppl: 92.4113, loss: 4.5545
	step 11293:lm_loss: 4.5262, ppl: 92.4106, loss: 4.5545
	step 11294:lm_loss: 4.5262, ppl: 92.4025, loss: 4.5544
	step 11295:lm_loss: 4.5262, ppl: 92.4052, loss: 4.5545
	step 11296:lm_loss: 4.5262, ppl: 92.4024, loss: 4.5544
	step 11297:lm_loss: 4.5262, ppl: 92.4026, loss: 4.5544
	step 11298:lm_loss: 4.5261, ppl: 92.4000, loss: 4.5544
	step 11299:lm_loss: 4.5261, ppl: 92.3986, loss: 4.5544
	step 11300:lm_loss: 4.5262, ppl: 92.4059, loss: 4.5545
	step 11301:lm_loss: 4.5262, ppl: 92.4064, loss: 4.5545
	step 11302:lm_loss: 4.5262, ppl: 92.4058, loss: 4.5545
	step 11303:lm_loss: 4.5262, ppl: 92.4103, loss: 4.5545
	step 11304:lm_loss: 4.5262, ppl: 92.4077, loss: 4.5545
	step 11305:lm_loss: 4.5262, ppl: 92.4056, loss: 4.5544
	step 11306:lm_loss: 4.5262, ppl: 92.4047, loss: 4.5544
	step 11307:lm_loss: 4.5262, ppl: 92.4033, loss: 4.5544
	step 11308:lm_loss: 4.5262, ppl: 92.4023, loss: 4.5544
	step 11309:lm_loss: 4.5262, ppl: 92.4077, loss: 4.5545
	step 11310:lm_loss: 4.5262, ppl: 92.4085, loss: 4.5545
	step 11311:lm_loss: 4.5263, ppl: 92.4115, loss: 4.5545
	step 11312:lm_loss: 4.5261, ppl: 92.3973, loss: 4.5544
	step 11313:lm_loss: 4.5261, ppl: 92.3972, loss: 4.5544
	step 11314:lm_loss: 4.5260, ppl: 92.3840, loss: 4.5542
	step 11315:lm_loss: 4.5259, ppl: 92.3836, loss: 4.5542
	step 11316:lm_loss: 4.5259, ppl: 92.3799, loss: 4.5542
	step 11317:lm_loss: 4.5260, ppl: 92.3895, loss: 4.5543
	step 11318:lm_loss: 4.5260, ppl: 92.3894, loss: 4.5543
	step 11319:lm_loss: 4.5260, ppl: 92.3865, loss: 4.5542
	step 11320:lm_loss: 4.5260, ppl: 92.3873, loss: 4.5542
	step 11321:lm_loss: 4.5260, ppl: 92.3867, loss: 4.5542
	step 11322:lm_loss: 4.5260, ppl: 92.3848, loss: 4.5542
	step 11323:lm_loss: 4.5260, ppl: 92.3891, loss: 4.5543
	step 11324:lm_loss: 4.5260, ppl: 92.3921, loss: 4.5543
	step 11325:lm_loss: 4.5262, ppl: 92.4060, loss: 4.5543
	step 11326:lm_loss: 4.5263, ppl: 92.4164, loss: 4.5545
	step 11327:lm_loss: 4.5263, ppl: 92.4171, loss: 4.5545
	step 11328:lm_loss: 4.5263, ppl: 92.4146, loss: 4.5544
	step 11329:lm_loss: 4.5263, ppl: 92.4148, loss: 4.5544
	step 11330:lm_loss: 4.5264, ppl: 92.4222, loss: 4.5545
	step 11331:lm_loss: 4.5264, ppl: 92.4278, loss: 4.5546
	step 11332:lm_loss: 4.5265, ppl: 92.4304, loss: 4.5546
	step 11333:lm_loss: 4.5266, ppl: 92.4397, loss: 4.5548
	step 11334:lm_loss: 4.5266, ppl: 92.4412, loss: 4.5548
	step 11335:lm_loss: 4.5266, ppl: 92.4410, loss: 4.5548
	step 11336:lm_loss: 4.5266, ppl: 92.4445, loss: 4.5548
	step 11337:lm_loss: 4.5266, ppl: 92.4411, loss: 4.5548
	step 11338:lm_loss: 4.5267, ppl: 92.4534, loss: 4.5549
	step 11339:lm_loss: 4.5268, ppl: 92.4644, loss: 4.5550
	step 11340:lm_loss: 4.5269, ppl: 92.4695, loss: 4.5550
	step 11341:lm_loss: 4.5269, ppl: 92.4737, loss: 4.5550
	step 11342:lm_loss: 4.5270, ppl: 92.4782, loss: 4.5551
	step 11343:lm_loss: 4.5269, ppl: 92.4755, loss: 4.5551
	step 11344:lm_loss: 4.5269, ppl: 92.4735, loss: 4.5550
	step 11345:lm_loss: 4.5270, ppl: 92.4765, loss: 4.5551
	step 11346:lm_loss: 4.5270, ppl: 92.4804, loss: 4.5551
	step 11347:lm_loss: 4.5270, ppl: 92.4797, loss: 4.5551
	step 11348:lm_loss: 4.5269, ppl: 92.4738, loss: 4.5551
	step 11349:lm_loss: 4.5270, ppl: 92.4766, loss: 4.5551
	step 11350:lm_loss: 4.5270, ppl: 92.4844, loss: 4.5553
	step 11351:lm_loss: 4.5270, ppl: 92.4820, loss: 4.5552
	step 11352:lm_loss: 4.5270, ppl: 92.4840, loss: 4.5552
	step 11353:lm_loss: 4.5268, ppl: 92.4666, loss: 4.5551
	step 11354:lm_loss: 4.5269, ppl: 92.4688, loss: 4.5551
	step 11355:lm_loss: 4.5270, ppl: 92.4766, loss: 4.5552
	step 11356:lm_loss: 4.5269, ppl: 92.4751, loss: 4.5552
	step 11357:lm_loss: 4.5270, ppl: 92.4780, loss: 4.5552
	step 11358:lm_loss: 4.5270, ppl: 92.4783, loss: 4.5552
	step 11359:lm_loss: 4.5270, ppl: 92.4769, loss: 4.5552
	step 11360:lm_loss: 4.5270, ppl: 92.4773, loss: 4.5552
	step 11361:lm_loss: 4.5268, ppl: 92.4636, loss: 4.5551
	step 11362:lm_loss: 4.5268, ppl: 92.4636, loss: 4.5551
	step 11363:lm_loss: 4.5267, ppl: 92.4553, loss: 4.5550
	step 11364:lm_loss: 4.5266, ppl: 92.4453, loss: 4.5549
	step 11365:lm_loss: 4.5267, ppl: 92.4538, loss: 4.5550
	step 11366:lm_loss: 4.5268, ppl: 92.4579, loss: 4.5550
	step 11367:lm_loss: 4.5267, ppl: 92.4566, loss: 4.5550
	step 11368:lm_loss: 4.5267, ppl: 92.4569, loss: 4.5550
	step 11369:lm_loss: 4.5266, ppl: 92.4440, loss: 4.5549
	step 11370:lm_loss: 4.5267, ppl: 92.4500, loss: 4.5549
	step 11371:lm_loss: 4.5267, ppl: 92.4556, loss: 4.5550
	step 11372:lm_loss: 4.5268, ppl: 92.4595, loss: 4.5551
	step 11373:lm_loss: 4.5267, ppl: 92.4572, loss: 4.5550
	step 11374:lm_loss: 4.5268, ppl: 92.4626, loss: 4.5551
	step 11375:lm_loss: 4.5268, ppl: 92.4655, loss: 4.5551
	step 11376:lm_loss: 4.5268, ppl: 92.4644, loss: 4.5551
	step 11377:lm_loss: 4.5267, ppl: 92.4551, loss: 4.5550
	step 11378:lm_loss: 4.5267, ppl: 92.4574, loss: 4.5550
	step 11379:lm_loss: 4.5265, ppl: 92.4369, loss: 4.5549
	step 11380:lm_loss: 4.5266, ppl: 92.4416, loss: 4.5549
	step 11381:lm_loss: 4.5266, ppl: 92.4404, loss: 4.5549
	step 11382:lm_loss: 4.5267, ppl: 92.4523, loss: 4.5551
	step 11383:lm_loss: 4.5267, ppl: 92.4542, loss: 4.5551
	step 11384:lm_loss: 4.5267, ppl: 92.4509, loss: 4.5550
	step 11385:lm_loss: 4.5266, ppl: 92.4424, loss: 4.5549
	step 11386:lm_loss: 4.5266, ppl: 92.4477, loss: 4.5550
	step 11387:lm_loss: 4.5267, ppl: 92.4517, loss: 4.5550
	step 11388:lm_loss: 4.5267, ppl: 92.4498, loss: 4.5550
	step 11389:lm_loss: 4.5268, ppl: 92.4583, loss: 4.5551
	step 11390:lm_loss: 4.5267, ppl: 92.4535, loss: 4.5550
	step 11391:lm_loss: 4.5267, ppl: 92.4571, loss: 4.5550
	step 11392:lm_loss: 4.5268, ppl: 92.4584, loss: 4.5550
	step 11393:lm_loss: 4.5267, ppl: 92.4537, loss: 4.5550
	step 11394:lm_loss: 4.5267, ppl: 92.4513, loss: 4.5549
	step 11395:lm_loss: 4.5266, ppl: 92.4446, loss: 4.5549
	step 11396:lm_loss: 4.5266, ppl: 92.4480, loss: 4.5549
	step 11397:lm_loss: 4.5267, ppl: 92.4532, loss: 4.5550
	step 11398:lm_loss: 4.5267, ppl: 92.4546, loss: 4.5550
	step 11399:lm_loss: 4.5267, ppl: 92.4574, loss: 4.5550
	step 11400:lm_loss: 4.5267, ppl: 92.4569, loss: 4.5550
	step 11401:lm_loss: 4.5268, ppl: 92.4607, loss: 4.5550
	step 11402:lm_loss: 4.5267, ppl: 92.4561, loss: 4.5550
	step 11403:lm_loss: 4.5267, ppl: 92.4573, loss: 4.5550
	step 11404:lm_loss: 4.5268, ppl: 92.4613, loss: 4.5550
	step 11405:lm_loss: 4.5267, ppl: 92.4536, loss: 4.5550
	step 11406:lm_loss: 4.5267, ppl: 92.4537, loss: 4.5550
	step 11407:lm_loss: 4.5268, ppl: 92.4629, loss: 4.5551
	step 11408:lm_loss: 4.5268, ppl: 92.4621, loss: 4.5551
	step 11409:lm_loss: 4.5269, ppl: 92.4673, loss: 4.5551
	step 11410:lm_loss: 4.5269, ppl: 92.4689, loss: 4.5551
	step 11411:lm_loss: 4.5269, ppl: 92.4719, loss: 4.5552
	step 11412:lm_loss: 4.5269, ppl: 92.4727, loss: 4.5552
	step 11413:lm_loss: 4.5270, ppl: 92.4784, loss: 4.5552
	step 11414:lm_loss: 4.5270, ppl: 92.4834, loss: 4.5553
	step 11415:lm_loss: 4.5271, ppl: 92.4881, loss: 4.5553
	step 11416:lm_loss: 4.5270, ppl: 92.4849, loss: 4.5553
	step 11417:lm_loss: 4.5271, ppl: 92.4918, loss: 4.5554
	step 11418:lm_loss: 4.5271, ppl: 92.4924, loss: 4.5554
	step 11419:lm_loss: 4.5270, ppl: 92.4809, loss: 4.5553
	step 11420:lm_loss: 4.5270, ppl: 92.4794, loss: 4.5553
	step 11421:lm_loss: 4.5270, ppl: 92.4826, loss: 4.5553
	step 11422:lm_loss: 4.5270, ppl: 92.4766, loss: 4.5552
	step 11423:lm_loss: 4.5269, ppl: 92.4681, loss: 4.5551
	step 11424:lm_loss: 4.5268, ppl: 92.4647, loss: 4.5551
	step 11425:lm_loss: 4.5270, ppl: 92.4811, loss: 4.5553
	step 11426:lm_loss: 4.5270, ppl: 92.4826, loss: 4.5553
	step 11427:lm_loss: 4.5270, ppl: 92.4812, loss: 4.5553
	step 11428:lm_loss: 4.5269, ppl: 92.4678, loss: 4.5552
	step 11429:lm_loss: 4.5268, ppl: 92.4611, loss: 4.5551
	step 11430:lm_loss: 4.5269, ppl: 92.4691, loss: 4.5551
	step 11431:lm_loss: 4.5269, ppl: 92.4688, loss: 4.5551
	step 11432:lm_loss: 4.5268, ppl: 92.4663, loss: 4.5551
	step 11433:lm_loss: 4.5269, ppl: 92.4736, loss: 4.5551
	step 11434:lm_loss: 4.5270, ppl: 92.4825, loss: 4.5552
	step 11435:lm_loss: 4.5271, ppl: 92.4866, loss: 4.5553
	step 11436:lm_loss: 4.5271, ppl: 92.4877, loss: 4.5553
	step 11437:lm_loss: 4.5271, ppl: 92.4861, loss: 4.5553
	step 11438:lm_loss: 4.5271, ppl: 92.4931, loss: 4.5554
	step 11439:lm_loss: 4.5272, ppl: 92.4976, loss: 4.5554
	step 11440:lm_loss: 4.5272, ppl: 92.4970, loss: 4.5554
	step 11441:lm_loss: 4.5273, ppl: 92.5057, loss: 4.5555
	step 11442:lm_loss: 4.5272, ppl: 92.5033, loss: 4.5555
	step 11443:lm_loss: 4.5273, ppl: 92.5065, loss: 4.5555
	step 11444:lm_loss: 4.5272, ppl: 92.4949, loss: 4.5554
	step 11445:lm_loss: 4.5272, ppl: 92.5000, loss: 4.5555
	step 11446:lm_loss: 4.5273, ppl: 92.5071, loss: 4.5555
	step 11447:lm_loss: 4.5273, ppl: 92.5114, loss: 4.5555
	step 11448:lm_loss: 4.5273, ppl: 92.5083, loss: 4.5555
	step 11449:lm_loss: 4.5274, ppl: 92.5146, loss: 4.5555
	step 11450:lm_loss: 4.5273, ppl: 92.5083, loss: 4.5554
	step 11451:lm_loss: 4.5274, ppl: 92.5191, loss: 4.5555
	step 11452:lm_loss: 4.5274, ppl: 92.5138, loss: 4.5554
	step 11453:lm_loss: 4.5273, ppl: 92.5117, loss: 4.5554
	step 11454:lm_loss: 4.5273, ppl: 92.5122, loss: 4.5554
	step 11455:lm_loss: 4.5273, ppl: 92.5039, loss: 4.5553
	step 11456:lm_loss: 4.5272, ppl: 92.5002, loss: 4.5553
	step 11457:lm_loss: 4.5272, ppl: 92.4962, loss: 4.5552
	step 11458:lm_loss: 4.5271, ppl: 92.4864, loss: 4.5551
	step 11459:lm_loss: 4.5270, ppl: 92.4828, loss: 4.5551
	step 11460:lm_loss: 4.5270, ppl: 92.4811, loss: 4.5550
	step 11461:lm_loss: 4.5270, ppl: 92.4817, loss: 4.5550
	step 11462:lm_loss: 4.5270, ppl: 92.4826, loss: 4.5551
	step 11463:lm_loss: 4.5271, ppl: 92.4890, loss: 4.5551
	step 11464:lm_loss: 4.5271, ppl: 92.4880, loss: 4.5551
	step 11465:lm_loss: 4.5271, ppl: 92.4918, loss: 4.5551
	step 11466:lm_loss: 4.5271, ppl: 92.4890, loss: 4.5551
	step 11467:lm_loss: 4.5271, ppl: 92.4924, loss: 4.5551
	step 11468:lm_loss: 4.5271, ppl: 92.4918, loss: 4.5551
	step 11469:lm_loss: 4.5271, ppl: 92.4880, loss: 4.5551
	step 11470:lm_loss: 4.5271, ppl: 92.4902, loss: 4.5551
	step 11471:lm_loss: 4.5272, ppl: 92.4976, loss: 4.5552
	step 11472:lm_loss: 4.5273, ppl: 92.5051, loss: 4.5553
	step 11473:lm_loss: 4.5273, ppl: 92.5111, loss: 4.5554
	step 11474:lm_loss: 4.5273, ppl: 92.5078, loss: 4.5554
	step 11475:lm_loss: 4.5272, ppl: 92.4979, loss: 4.5552
	step 11476:lm_loss: 4.5272, ppl: 92.4978, loss: 4.5552
	step 11477:lm_loss: 4.5272, ppl: 92.4990, loss: 4.5552
	step 11478:lm_loss: 4.5272, ppl: 92.4969, loss: 4.5552
	step 11479:lm_loss: 4.5271, ppl: 92.4870, loss: 4.5551
	step 11480:lm_loss: 4.5271, ppl: 92.4903, loss: 4.5551
	step 11481:lm_loss: 4.5271, ppl: 92.4864, loss: 4.5550
	step 11482:lm_loss: 4.5271, ppl: 92.4863, loss: 4.5550
	step 11483:lm_loss: 4.5270, ppl: 92.4795, loss: 4.5550
	step 11484:lm_loss: 4.5269, ppl: 92.4755, loss: 4.5549
	step 11485:lm_loss: 4.5270, ppl: 92.4776, loss: 4.5549
	step 11486:lm_loss: 4.5270, ppl: 92.4767, loss: 4.5549
	step 11487:lm_loss: 4.5269, ppl: 92.4749, loss: 4.5549
	step 11488:lm_loss: 4.5269, ppl: 92.4722, loss: 4.5549
	step 11489:lm_loss: 4.5268, ppl: 92.4603, loss: 4.5548
	step 11490:lm_loss: 4.5268, ppl: 92.4626, loss: 4.5548
	step 11491:lm_loss: 4.5267, ppl: 92.4534, loss: 4.5547
	step 11492:lm_loss: 4.5266, ppl: 92.4471, loss: 4.5546
	step 11493:lm_loss: 4.5266, ppl: 92.4466, loss: 4.5546
	step 11494:lm_loss: 4.5265, ppl: 92.4382, loss: 4.5545
	step 11495:lm_loss: 4.5264, ppl: 92.4291, loss: 4.5545
	step 11496:lm_loss: 4.5264, ppl: 92.4295, loss: 4.5545
	step 11497:lm_loss: 4.5264, ppl: 92.4268, loss: 4.5544
	step 11498:lm_loss: 4.5264, ppl: 92.4267, loss: 4.5544
	step 11499:lm_loss: 4.5264, ppl: 92.4216, loss: 4.5544
	step 11500:lm_loss: 4.5265, ppl: 92.4368, loss: 4.5545
	step 11501:lm_loss: 4.5265, ppl: 92.4376, loss: 4.5545
	step 11502:lm_loss: 4.5265, ppl: 92.4307, loss: 4.5544
	step 11503:lm_loss: 4.5263, ppl: 92.4193, loss: 4.5544
	step 11504:lm_loss: 4.5263, ppl: 92.4147, loss: 4.5543
	step 11505:lm_loss: 4.5264, ppl: 92.4223, loss: 4.5544
	step 11506:lm_loss: 4.5264, ppl: 92.4218, loss: 4.5544
	step 11507:lm_loss: 4.5263, ppl: 92.4155, loss: 4.5543
	step 11508:lm_loss: 4.5264, ppl: 92.4269, loss: 4.5544
	step 11509:lm_loss: 4.5264, ppl: 92.4263, loss: 4.5544
	step 11510:lm_loss: 4.5264, ppl: 92.4212, loss: 4.5544
	step 11511:lm_loss: 4.5263, ppl: 92.4195, loss: 4.5544
	step 11512:lm_loss: 4.5264, ppl: 92.4232, loss: 4.5544
	step 11513:lm_loss: 4.5263, ppl: 92.4204, loss: 4.5544
	step 11514:lm_loss: 4.5264, ppl: 92.4249, loss: 4.5544
	step 11515:lm_loss: 4.5265, ppl: 92.4309, loss: 4.5545
	step 11516:lm_loss: 4.5263, ppl: 92.4166, loss: 4.5544
	step 11517:lm_loss: 4.5263, ppl: 92.4192, loss: 4.5544
	step 11518:lm_loss: 4.5263, ppl: 92.4144, loss: 4.5543
	step 11519:lm_loss: 4.5263, ppl: 92.4174, loss: 4.5544
	step 11520:lm_loss: 4.5263, ppl: 92.4119, loss: 4.5543
	step 11521:lm_loss: 4.5260, ppl: 92.3880, loss: 4.5542
	step 11522:lm_loss: 4.5260, ppl: 92.3840, loss: 4.5541
	step 11523:lm_loss: 4.5259, ppl: 92.3767, loss: 4.5540
	step 11524:lm_loss: 4.5259, ppl: 92.3794, loss: 4.5541
	step 11525:lm_loss: 4.5259, ppl: 92.3777, loss: 4.5540
	step 11526:lm_loss: 4.5259, ppl: 92.3798, loss: 4.5541
	step 11527:lm_loss: 4.5258, ppl: 92.3704, loss: 4.5540
	step 11528:lm_loss: 4.5258, ppl: 92.3710, loss: 4.5540
	step 11529:lm_loss: 4.5259, ppl: 92.3761, loss: 4.5540
	step 11530:lm_loss: 4.5259, ppl: 92.3757, loss: 4.5540
	step 11531:lm_loss: 4.5259, ppl: 92.3800, loss: 4.5541
	step 11532:lm_loss: 4.5259, ppl: 92.3794, loss: 4.5541
	step 11533:lm_loss: 4.5259, ppl: 92.3826, loss: 4.5541
	step 11534:lm_loss: 4.5259, ppl: 92.3778, loss: 4.5540
	step 11535:lm_loss: 4.5258, ppl: 92.3735, loss: 4.5540
	step 11536:lm_loss: 4.5259, ppl: 92.3780, loss: 4.5540
	step 11537:lm_loss: 4.5259, ppl: 92.3817, loss: 4.5541
	step 11538:lm_loss: 4.5259, ppl: 92.3796, loss: 4.5540
	step 11539:lm_loss: 4.5259, ppl: 92.3831, loss: 4.5541
	step 11540:lm_loss: 4.5260, ppl: 92.3907, loss: 4.5541
	step 11541:lm_loss: 4.5260, ppl: 92.3911, loss: 4.5541
	step 11542:lm_loss: 4.5260, ppl: 92.3885, loss: 4.5541
	step 11543:lm_loss: 4.5260, ppl: 92.3894, loss: 4.5541
	step 11544:lm_loss: 4.5260, ppl: 92.3899, loss: 4.5541
	step 11545:lm_loss: 4.5260, ppl: 92.3927, loss: 4.5541
	step 11546:lm_loss: 4.5261, ppl: 92.3961, loss: 4.5542
	step 11547:lm_loss: 4.5261, ppl: 92.3978, loss: 4.5542
	step 11548:lm_loss: 4.5262, ppl: 92.4044, loss: 4.5543
	step 11549:lm_loss: 4.5262, ppl: 92.4112, loss: 4.5544
	step 11550:lm_loss: 4.5263, ppl: 92.4123, loss: 4.5544
	step 11551:lm_loss: 4.5263, ppl: 92.4180, loss: 4.5544
	step 11552:lm_loss: 4.5263, ppl: 92.4145, loss: 4.5544
	step 11553:lm_loss: 4.5262, ppl: 92.4078, loss: 4.5543
	step 11554:lm_loss: 4.5263, ppl: 92.4135, loss: 4.5544
	step 11555:lm_loss: 4.5263, ppl: 92.4162, loss: 4.5544
	step 11556:lm_loss: 4.5262, ppl: 92.4058, loss: 4.5543
	step 11557:lm_loss: 4.5261, ppl: 92.4018, loss: 4.5542
	step 11558:lm_loss: 4.5260, ppl: 92.3865, loss: 4.5541
	step 11559:lm_loss: 4.5261, ppl: 92.3980, loss: 4.5543
	step 11560:lm_loss: 4.5262, ppl: 92.4048, loss: 4.5544
	step 11561:lm_loss: 4.5263, ppl: 92.4119, loss: 4.5544
	step 11562:lm_loss: 4.5263, ppl: 92.4176, loss: 4.5545
	step 11563:lm_loss: 4.5263, ppl: 92.4157, loss: 4.5545
	step 11564:lm_loss: 4.5262, ppl: 92.4063, loss: 4.5544
	step 11565:lm_loss: 4.5262, ppl: 92.4078, loss: 4.5544
	step 11566:lm_loss: 4.5262, ppl: 92.4043, loss: 4.5544
	step 11567:lm_loss: 4.5263, ppl: 92.4128, loss: 4.5545
	step 11568:lm_loss: 4.5262, ppl: 92.4106, loss: 4.5544
	step 11569:lm_loss: 4.5263, ppl: 92.4153, loss: 4.5545
	step 11570:lm_loss: 4.5263, ppl: 92.4172, loss: 4.5545
	step 11571:lm_loss: 4.5263, ppl: 92.4183, loss: 4.5545
	step 11572:lm_loss: 4.5264, ppl: 92.4242, loss: 4.5546
	step 11573:lm_loss: 4.5264, ppl: 92.4264, loss: 4.5546
	step 11574:lm_loss: 4.5264, ppl: 92.4295, loss: 4.5546
	step 11575:lm_loss: 4.5265, ppl: 92.4358, loss: 4.5547
	step 11576:lm_loss: 4.5265, ppl: 92.4311, loss: 4.5546
	step 11577:lm_loss: 4.5265, ppl: 92.4339, loss: 4.5546
	step 11578:lm_loss: 4.5266, ppl: 92.4410, loss: 4.5547
	step 11579:lm_loss: 4.5266, ppl: 92.4423, loss: 4.5547
	step 11580:lm_loss: 4.5264, ppl: 92.4279, loss: 4.5546
	step 11581:lm_loss: 4.5264, ppl: 92.4221, loss: 4.5546
	step 11582:lm_loss: 4.5264, ppl: 92.4229, loss: 4.5546
	step 11583:lm_loss: 4.5265, ppl: 92.4310, loss: 4.5547
	step 11584:lm_loss: 4.5264, ppl: 92.4282, loss: 4.5547
	step 11585:lm_loss: 4.5264, ppl: 92.4265, loss: 4.5547
	step 11586:lm_loss: 4.5264, ppl: 92.4256, loss: 4.5547
	step 11587:lm_loss: 4.5264, ppl: 92.4257, loss: 4.5547
	step 11588:lm_loss: 4.5265, ppl: 92.4332, loss: 4.5548
	step 11589:lm_loss: 4.5265, ppl: 92.4349, loss: 4.5548
	step 11590:lm_loss: 4.5265, ppl: 92.4368, loss: 4.5548
	step 11591:lm_loss: 4.5266, ppl: 92.4410, loss: 4.5548
	step 11592:lm_loss: 4.5266, ppl: 92.4412, loss: 4.5548
	step 11593:lm_loss: 4.5265, ppl: 92.4358, loss: 4.5548
	step 11594:lm_loss: 4.5265, ppl: 92.4353, loss: 4.5548
	step 11595:lm_loss: 4.5266, ppl: 92.4393, loss: 4.5548
	step 11596:lm_loss: 4.5265, ppl: 92.4359, loss: 4.5548
	step 11597:lm_loss: 4.5266, ppl: 92.4436, loss: 4.5549
	step 11598:lm_loss: 4.5266, ppl: 92.4401, loss: 4.5548
	step 11599:lm_loss: 4.5266, ppl: 92.4442, loss: 4.5548
	step 11600:lm_loss: 4.5266, ppl: 92.4405, loss: 4.5548
	step 11601:lm_loss: 4.5266, ppl: 92.4412, loss: 4.5548
	step 11602:lm_loss: 4.5266, ppl: 92.4410, loss: 4.5548
	step 11603:lm_loss: 4.5265, ppl: 92.4353, loss: 4.5547
	step 11604:lm_loss: 4.5265, ppl: 92.4369, loss: 4.5547
	step 11605:lm_loss: 4.5264, ppl: 92.4298, loss: 4.5547
	step 11606:lm_loss: 4.5265, ppl: 92.4315, loss: 4.5547
	step 11607:lm_loss: 4.5264, ppl: 92.4260, loss: 4.5547
	step 11608:lm_loss: 4.5264, ppl: 92.4285, loss: 4.5547
	step 11609:lm_loss: 4.5264, ppl: 92.4270, loss: 4.5547
	step 11610:lm_loss: 4.5264, ppl: 92.4290, loss: 4.5547
	step 11611:lm_loss: 4.5264, ppl: 92.4248, loss: 4.5546
	step 11612:lm_loss: 4.5265, ppl: 92.4307, loss: 4.5547
	step 11613:lm_loss: 4.5264, ppl: 92.4290, loss: 4.5547
	step 11614:lm_loss: 4.5265, ppl: 92.4371, loss: 4.5548
	step 11615:lm_loss: 4.5265, ppl: 92.4371, loss: 4.5548
	step 11616:lm_loss: 4.5266, ppl: 92.4429, loss: 4.5548
	step 11617:lm_loss: 4.5266, ppl: 92.4404, loss: 4.5548
	step 11618:lm_loss: 4.5265, ppl: 92.4377, loss: 4.5548
	step 11619:lm_loss: 4.5265, ppl: 92.4383, loss: 4.5548
	step 11620:lm_loss: 4.5265, ppl: 92.4371, loss: 4.5547
	step 11621:lm_loss: 4.5265, ppl: 92.4337, loss: 4.5547
	step 11622:lm_loss: 4.5264, ppl: 92.4222, loss: 4.5546
	step 11623:lm_loss: 4.5264, ppl: 92.4259, loss: 4.5546
	step 11624:lm_loss: 4.5264, ppl: 92.4244, loss: 4.5546
	step 11625:lm_loss: 4.5265, ppl: 92.4307, loss: 4.5547
	step 11626:lm_loss: 4.5263, ppl: 92.4140, loss: 4.5545
	step 11627:lm_loss: 4.5263, ppl: 92.4146, loss: 4.5545
	step 11628:lm_loss: 4.5263, ppl: 92.4136, loss: 4.5545
	step 11629:lm_loss: 4.5263, ppl: 92.4200, loss: 4.5546
	step 11630:lm_loss: 4.5264, ppl: 92.4242, loss: 4.5546
	step 11631:lm_loss: 4.5264, ppl: 92.4241, loss: 4.5546
	step 11632:lm_loss: 4.5264, ppl: 92.4222, loss: 4.5546
	step 11633:lm_loss: 4.5264, ppl: 92.4245, loss: 4.5546
	step 11634:lm_loss: 4.5264, ppl: 92.4291, loss: 4.5546
	step 11635:lm_loss: 4.5264, ppl: 92.4289, loss: 4.5546
	step 11636:lm_loss: 4.5265, ppl: 92.4336, loss: 4.5547
	step 11637:lm_loss: 4.5265, ppl: 92.4326, loss: 4.5547
	step 11638:lm_loss: 4.5265, ppl: 92.4310, loss: 4.5547
	step 11639:lm_loss: 4.5264, ppl: 92.4296, loss: 4.5546
	step 11640:lm_loss: 4.5265, ppl: 92.4305, loss: 4.5546
	step 11641:lm_loss: 4.5266, ppl: 92.4395, loss: 4.5548
	step 11642:lm_loss: 4.5265, ppl: 92.4342, loss: 4.5547
	step 11643:lm_loss: 4.5265, ppl: 92.4381, loss: 4.5547
	step 11644:lm_loss: 4.5265, ppl: 92.4389, loss: 4.5547
	step 11645:lm_loss: 4.5266, ppl: 92.4415, loss: 4.5548
	step 11646:lm_loss: 4.5265, ppl: 92.4360, loss: 4.5547
	step 11647:lm_loss: 4.5264, ppl: 92.4217, loss: 4.5546
	step 11648:lm_loss: 4.5264, ppl: 92.4233, loss: 4.5546
	step 11649:lm_loss: 4.5264, ppl: 92.4239, loss: 4.5546
	step 11650:lm_loss: 4.5264, ppl: 92.4256, loss: 4.5547
	step 11651:lm_loss: 4.5264, ppl: 92.4265, loss: 4.5547
	step 11652:lm_loss: 4.5265, ppl: 92.4347, loss: 4.5548
	step 11653:lm_loss: 4.5265, ppl: 92.4377, loss: 4.5548
	step 11654:lm_loss: 4.5266, ppl: 92.4420, loss: 4.5549
	step 11655:lm_loss: 4.5266, ppl: 92.4429, loss: 4.5549
	step 11656:lm_loss: 4.5266, ppl: 92.4474, loss: 4.5550
	step 11657:lm_loss: 4.5267, ppl: 92.4559, loss: 4.5550
	step 11658:lm_loss: 4.5268, ppl: 92.4579, loss: 4.5551
	step 11659:lm_loss: 4.5267, ppl: 92.4543, loss: 4.5550
	step 11660:lm_loss: 4.5268, ppl: 92.4624, loss: 4.5551
	step 11661:lm_loss: 4.5268, ppl: 92.4609, loss: 4.5551
	step 11662:lm_loss: 4.5268, ppl: 92.4657, loss: 4.5551
	step 11663:lm_loss: 4.5268, ppl: 92.4663, loss: 4.5551
	step 11664:lm_loss: 4.5269, ppl: 92.4705, loss: 4.5551
	step 11665:lm_loss: 4.5269, ppl: 92.4747, loss: 4.5552
	step 11666:lm_loss: 4.5270, ppl: 92.4791, loss: 4.5552
	step 11667:lm_loss: 4.5270, ppl: 92.4807, loss: 4.5552
	step 11668:lm_loss: 4.5269, ppl: 92.4728, loss: 4.5552
	step 11669:lm_loss: 4.5268, ppl: 92.4636, loss: 4.5551
	step 11670:lm_loss: 4.5269, ppl: 92.4671, loss: 4.5551
	step 11671:lm_loss: 4.5269, ppl: 92.4723, loss: 4.5552
	step 11672:lm_loss: 4.5269, ppl: 92.4699, loss: 4.5551
	step 11673:lm_loss: 4.5269, ppl: 92.4755, loss: 4.5552
	step 11674:lm_loss: 4.5270, ppl: 92.4792, loss: 4.5553
	step 11675:lm_loss: 4.5270, ppl: 92.4825, loss: 4.5553
	step 11676:lm_loss: 4.5271, ppl: 92.4923, loss: 4.5554
	step 11677:lm_loss: 4.5272, ppl: 92.4972, loss: 4.5554
	step 11678:lm_loss: 4.5272, ppl: 92.5006, loss: 4.5555
	step 11679:lm_loss: 4.5272, ppl: 92.5009, loss: 4.5555
	step 11680:lm_loss: 4.5272, ppl: 92.5034, loss: 4.5555
	step 11681:lm_loss: 4.5272, ppl: 92.4992, loss: 4.5554
	step 11682:lm_loss: 4.5272, ppl: 92.4981, loss: 4.5554
	step 11683:lm_loss: 4.5271, ppl: 92.4905, loss: 4.5554
	step 11684:lm_loss: 4.5271, ppl: 92.4897, loss: 4.5554
	step 11685:lm_loss: 4.5270, ppl: 92.4819, loss: 4.5553
	step 11686:lm_loss: 4.5270, ppl: 92.4810, loss: 4.5552
	step 11687:lm_loss: 4.5270, ppl: 92.4786, loss: 4.5552
	step 11688:lm_loss: 4.5270, ppl: 92.4837, loss: 4.5553
	step 11689:lm_loss: 4.5272, ppl: 92.4955, loss: 4.5554
	step 11690:lm_loss: 4.5272, ppl: 92.4986, loss: 4.5554
	step 11691:lm_loss: 4.5272, ppl: 92.4946, loss: 4.5554
	step 11692:lm_loss: 4.5271, ppl: 92.4932, loss: 4.5554
	step 11693:lm_loss: 4.5272, ppl: 92.4949, loss: 4.5554
	step 11694:lm_loss: 4.5272, ppl: 92.4963, loss: 4.5554
	step 11695:lm_loss: 4.5272, ppl: 92.4985, loss: 4.5554
	step 11696:lm_loss: 4.5272, ppl: 92.5010, loss: 4.5554
	step 11697:lm_loss: 4.5273, ppl: 92.5047, loss: 4.5555
	step 11698:lm_loss: 4.5273, ppl: 92.5091, loss: 4.5555
	step 11699:lm_loss: 4.5273, ppl: 92.5074, loss: 4.5555
	step 11700:lm_loss: 4.5273, ppl: 92.5107, loss: 4.5556
	step 11701:lm_loss: 4.5273, ppl: 92.5119, loss: 4.5556
	step 11702:lm_loss: 4.5273, ppl: 92.5099, loss: 4.5556
	step 11703:lm_loss: 4.5273, ppl: 92.5124, loss: 4.5556
	step 11704:lm_loss: 4.5274, ppl: 92.5169, loss: 4.5556
	step 11705:lm_loss: 4.5274, ppl: 92.5201, loss: 4.5556
	step 11706:lm_loss: 4.5275, ppl: 92.5228, loss: 4.5557
	step 11707:lm_loss: 4.5275, ppl: 92.5244, loss: 4.5557
	step 11708:lm_loss: 4.5275, ppl: 92.5279, loss: 4.5557
	step 11709:lm_loss: 4.5275, ppl: 92.5299, loss: 4.5557
	step 11710:lm_loss: 4.5276, ppl: 92.5350, loss: 4.5558
	step 11711:lm_loss: 4.5275, ppl: 92.5266, loss: 4.5557
	step 11712:lm_loss: 4.5275, ppl: 92.5238, loss: 4.5557
	step 11713:lm_loss: 4.5275, ppl: 92.5285, loss: 4.5557
	step 11714:lm_loss: 4.5276, ppl: 92.5317, loss: 4.5557
	step 11715:lm_loss: 4.5275, ppl: 92.5264, loss: 4.5557
	step 11716:lm_loss: 4.5276, ppl: 92.5324, loss: 4.5558
	step 11717:lm_loss: 4.5276, ppl: 92.5365, loss: 4.5558
	step 11718:lm_loss: 4.5276, ppl: 92.5382, loss: 4.5558
	step 11719:lm_loss: 4.5276, ppl: 92.5405, loss: 4.5559
	step 11720:lm_loss: 4.5277, ppl: 92.5488, loss: 4.5559
	step 11721:lm_loss: 4.5277, ppl: 92.5475, loss: 4.5559
	step 11722:lm_loss: 4.5276, ppl: 92.5335, loss: 4.5558
	step 11723:lm_loss: 4.5276, ppl: 92.5359, loss: 4.5559
	step 11724:lm_loss: 4.5276, ppl: 92.5337, loss: 4.5558
	step 11725:lm_loss: 4.5276, ppl: 92.5363, loss: 4.5558
	step 11726:lm_loss: 4.5276, ppl: 92.5381, loss: 4.5559
	step 11727:lm_loss: 4.5277, ppl: 92.5463, loss: 4.5560
	step 11728:lm_loss: 4.5277, ppl: 92.5497, loss: 4.5560
	step 11729:lm_loss: 4.5277, ppl: 92.5480, loss: 4.5560
	step 11730:lm_loss: 4.5277, ppl: 92.5424, loss: 4.5559
	step 11731:lm_loss: 4.5277, ppl: 92.5423, loss: 4.5559
	step 11732:lm_loss: 4.5276, ppl: 92.5408, loss: 4.5559
	step 11733:lm_loss: 4.5277, ppl: 92.5468, loss: 4.5560
	step 11734:lm_loss: 4.5277, ppl: 92.5428, loss: 4.5559
	step 11735:lm_loss: 4.5276, ppl: 92.5368, loss: 4.5558
	step 11736:lm_loss: 4.5276, ppl: 92.5397, loss: 4.5559
	step 11737:lm_loss: 4.5276, ppl: 92.5370, loss: 4.5559
	step 11738:lm_loss: 4.5276, ppl: 92.5362, loss: 4.5558
	step 11739:lm_loss: 4.5275, ppl: 92.5310, loss: 4.5558
	step 11740:lm_loss: 4.5276, ppl: 92.5318, loss: 4.5558
	step 11741:lm_loss: 4.5276, ppl: 92.5397, loss: 4.5559
	step 11742:lm_loss: 4.5276, ppl: 92.5343, loss: 4.5558
	step 11743:lm_loss: 4.5276, ppl: 92.5318, loss: 4.5558
	step 11744:lm_loss: 4.5276, ppl: 92.5326, loss: 4.5558
	step 11745:lm_loss: 4.5276, ppl: 92.5383, loss: 4.5559
	step 11746:lm_loss: 4.5277, ppl: 92.5500, loss: 4.5560
	step 11747:lm_loss: 4.5278, ppl: 92.5539, loss: 4.5561
	step 11748:lm_loss: 4.5277, ppl: 92.5490, loss: 4.5560
	step 11749:lm_loss: 4.5278, ppl: 92.5578, loss: 4.5561
	step 11750:lm_loss: 4.5278, ppl: 92.5534, loss: 4.5560
	step 11751:lm_loss: 4.5279, ppl: 92.5641, loss: 4.5561
	step 11752:lm_loss: 4.5280, ppl: 92.5691, loss: 4.5562
	step 11753:lm_loss: 4.5279, ppl: 92.5681, loss: 4.5562
	step 11754:lm_loss: 4.5280, ppl: 92.5689, loss: 4.5562
	step 11755:lm_loss: 4.5281, ppl: 92.5784, loss: 4.5563
	step 11756:lm_loss: 4.5280, ppl: 92.5695, loss: 4.5562
	step 11757:lm_loss: 4.5280, ppl: 92.5749, loss: 4.5562
	step 11758:lm_loss: 4.5281, ppl: 92.5809, loss: 4.5563
	step 11759:lm_loss: 4.5280, ppl: 92.5724, loss: 4.5562
	step 11760:lm_loss: 4.5280, ppl: 92.5732, loss: 4.5562
	step 11761:lm_loss: 4.5280, ppl: 92.5762, loss: 4.5562
	step 11762:lm_loss: 4.5280, ppl: 92.5712, loss: 4.5562
	step 11763:lm_loss: 4.5279, ppl: 92.5682, loss: 4.5561
	step 11764:lm_loss: 4.5279, ppl: 92.5685, loss: 4.5561
	step 11765:lm_loss: 4.5280, ppl: 92.5760, loss: 4.5562
	step 11766:lm_loss: 4.5281, ppl: 92.5791, loss: 4.5562
	step 11767:lm_loss: 4.5281, ppl: 92.5845, loss: 4.5563
	step 11768:lm_loss: 4.5280, ppl: 92.5727, loss: 4.5561
	step 11769:lm_loss: 4.5280, ppl: 92.5744, loss: 4.5561
	step 11770:lm_loss: 4.5280, ppl: 92.5765, loss: 4.5561
	step 11771:lm_loss: 4.5281, ppl: 92.5827, loss: 4.5562
	step 11772:lm_loss: 4.5282, ppl: 92.5876, loss: 4.5562
	step 11773:lm_loss: 4.5282, ppl: 92.5894, loss: 4.5562
	step 11774:lm_loss: 4.5282, ppl: 92.5924, loss: 4.5563
	step 11775:lm_loss: 4.5281, ppl: 92.5858, loss: 4.5562
	step 11776:lm_loss: 4.5282, ppl: 92.5880, loss: 4.5562
	step 11777:lm_loss: 4.5282, ppl: 92.5915, loss: 4.5563
	step 11778:lm_loss: 4.5283, ppl: 92.5979, loss: 4.5563
	step 11779:lm_loss: 4.5283, ppl: 92.5975, loss: 4.5563
	step 11780:lm_loss: 4.5283, ppl: 92.6011, loss: 4.5564
	step 11781:lm_loss: 4.5284, ppl: 92.6111, loss: 4.5565
	step 11782:lm_loss: 4.5284, ppl: 92.6083, loss: 4.5565
	step 11783:lm_loss: 4.5284, ppl: 92.6115, loss: 4.5565
	step 11784:lm_loss: 4.5285, ppl: 92.6182, loss: 4.5566
	step 11785:lm_loss: 4.5286, ppl: 92.6309, loss: 4.5567
	step 11786:lm_loss: 4.5286, ppl: 92.6267, loss: 4.5566
	step 11787:lm_loss: 4.5286, ppl: 92.6249, loss: 4.5566
	step 11788:lm_loss: 4.5285, ppl: 92.6232, loss: 4.5566
	step 11789:lm_loss: 4.5285, ppl: 92.6212, loss: 4.5566
	step 11790:lm_loss: 4.5285, ppl: 92.6225, loss: 4.5566
	step 11791:lm_loss: 4.5286, ppl: 92.6300, loss: 4.5567
	step 11792:lm_loss: 4.5286, ppl: 92.6264, loss: 4.5566
	step 11793:lm_loss: 4.5286, ppl: 92.6327, loss: 4.5567
	step 11794:lm_loss: 4.5287, ppl: 92.6379, loss: 4.5568
	step 11795:lm_loss: 4.5287, ppl: 92.6362, loss: 4.5567
	step 11796:lm_loss: 4.5287, ppl: 92.6336, loss: 4.5567
	step 11797:lm_loss: 4.5286, ppl: 92.6260, loss: 4.5566
	step 11798:lm_loss: 4.5286, ppl: 92.6254, loss: 4.5566
	step 11799:lm_loss: 4.5286, ppl: 92.6330, loss: 4.5567
	step 11800:lm_loss: 4.5286, ppl: 92.6295, loss: 4.5566
	step 11801:lm_loss: 4.5287, ppl: 92.6356, loss: 4.5567
	step 11802:lm_loss: 4.5287, ppl: 92.6378, loss: 4.5567
	step 11803:lm_loss: 4.5287, ppl: 92.6392, loss: 4.5568
	step 11804:lm_loss: 4.5287, ppl: 92.6421, loss: 4.5568
	step 11805:lm_loss: 4.5288, ppl: 92.6484, loss: 4.5568
	step 11806:lm_loss: 4.5289, ppl: 92.6544, loss: 4.5568
	step 11807:lm_loss: 4.5289, ppl: 92.6588, loss: 4.5569
	step 11808:lm_loss: 4.5289, ppl: 92.6604, loss: 4.5569
	step 11809:lm_loss: 4.5289, ppl: 92.6567, loss: 4.5569
	step 11810:lm_loss: 4.5289, ppl: 92.6554, loss: 4.5569
	step 11811:lm_loss: 4.5289, ppl: 92.6542, loss: 4.5568
	step 11812:lm_loss: 4.5288, ppl: 92.6481, loss: 4.5568
	step 11813:lm_loss: 4.5289, ppl: 92.6581, loss: 4.5569
	step 11814:lm_loss: 4.5289, ppl: 92.6607, loss: 4.5569
	step 11815:lm_loss: 4.5290, ppl: 92.6629, loss: 4.5569
	step 11816:lm_loss: 4.5290, ppl: 92.6621, loss: 4.5569
	step 11817:lm_loss: 4.5290, ppl: 92.6639, loss: 4.5569
	step 11818:lm_loss: 4.5290, ppl: 92.6633, loss: 4.5569
	step 11819:lm_loss: 4.5291, ppl: 92.6722, loss: 4.5570
	step 11820:lm_loss: 4.5291, ppl: 92.6761, loss: 4.5570
	step 11821:lm_loss: 4.5290, ppl: 92.6700, loss: 4.5570
	step 11822:lm_loss: 4.5291, ppl: 92.6788, loss: 4.5571
	step 11823:lm_loss: 4.5292, ppl: 92.6845, loss: 4.5572
	step 11824:lm_loss: 4.5292, ppl: 92.6885, loss: 4.5572
	step 11825:lm_loss: 4.5293, ppl: 92.6940, loss: 4.5573
	step 11826:lm_loss: 4.5292, ppl: 92.6835, loss: 4.5572
	step 11827:lm_loss: 4.5292, ppl: 92.6809, loss: 4.5572
	step 11828:lm_loss: 4.5292, ppl: 92.6855, loss: 4.5572
	step 11829:lm_loss: 4.5290, ppl: 92.6704, loss: 4.5571
	step 11830:lm_loss: 4.5290, ppl: 92.6689, loss: 4.5571
	step 11831:lm_loss: 4.5290, ppl: 92.6680, loss: 4.5571
	step 11832:lm_loss: 4.5289, ppl: 92.6565, loss: 4.5570
	step 11833:lm_loss: 4.5290, ppl: 92.6617, loss: 4.5570
	step 11834:lm_loss: 4.5290, ppl: 92.6621, loss: 4.5570
	step 11835:lm_loss: 4.5290, ppl: 92.6623, loss: 4.5570
	step 11836:lm_loss: 4.5288, ppl: 92.6505, loss: 4.5570
	step 11837:lm_loss: 4.5289, ppl: 92.6557, loss: 4.5570
	step 11838:lm_loss: 4.5289, ppl: 92.6596, loss: 4.5571
	step 11839:lm_loss: 4.5290, ppl: 92.6651, loss: 4.5571
	step 11840:lm_loss: 4.5290, ppl: 92.6652, loss: 4.5571
	step 11841:lm_loss: 4.5290, ppl: 92.6653, loss: 4.5571
	step 11842:lm_loss: 4.5290, ppl: 92.6653, loss: 4.5571
	step 11843:lm_loss: 4.5290, ppl: 92.6663, loss: 4.5571
	step 11844:lm_loss: 4.5291, ppl: 92.6733, loss: 4.5572
	step 11845:lm_loss: 4.5291, ppl: 92.6742, loss: 4.5572
	step 11846:lm_loss: 4.5292, ppl: 92.6801, loss: 4.5573
	step 11847:lm_loss: 4.5292, ppl: 92.6830, loss: 4.5573
	step 11848:lm_loss: 4.5292, ppl: 92.6864, loss: 4.5573
	step 11849:lm_loss: 4.5291, ppl: 92.6708, loss: 4.5572
	step 11850:lm_loss: 4.5291, ppl: 92.6718, loss: 4.5572
	step 11851:lm_loss: 4.5289, ppl: 92.6607, loss: 4.5571
	step 11852:lm_loss: 4.5290, ppl: 92.6635, loss: 4.5571
	step 11853:lm_loss: 4.5291, ppl: 92.6724, loss: 4.5572
	step 11854:lm_loss: 4.5290, ppl: 92.6668, loss: 4.5572
	step 11855:lm_loss: 4.5290, ppl: 92.6684, loss: 4.5572
	step 11856:lm_loss: 4.5290, ppl: 92.6696, loss: 4.5572
	step 11857:lm_loss: 4.5291, ppl: 92.6764, loss: 4.5573
	step 11858:lm_loss: 4.5290, ppl: 92.6696, loss: 4.5572
	step 11859:lm_loss: 4.5291, ppl: 92.6713, loss: 4.5572
	step 11860:lm_loss: 4.5290, ppl: 92.6684, loss: 4.5572
	step 11861:lm_loss: 4.5291, ppl: 92.6710, loss: 4.5572
	step 11862:lm_loss: 4.5292, ppl: 92.6805, loss: 4.5573
	step 11863:lm_loss: 4.5291, ppl: 92.6783, loss: 4.5572
	step 11864:lm_loss: 4.5291, ppl: 92.6795, loss: 4.5573
	step 11865:lm_loss: 4.5293, ppl: 92.6895, loss: 4.5573
	step 11866:lm_loss: 4.5293, ppl: 92.6907, loss: 4.5574
	step 11867:lm_loss: 4.5293, ppl: 92.6935, loss: 4.5574
	step 11868:lm_loss: 4.5293, ppl: 92.6906, loss: 4.5574
	step 11869:lm_loss: 4.5292, ppl: 92.6837, loss: 4.5573
	step 11870:lm_loss: 4.5293, ppl: 92.6920, loss: 4.5574
	step 11871:lm_loss: 4.5293, ppl: 92.6892, loss: 4.5573
	step 11872:lm_loss: 4.5293, ppl: 92.6956, loss: 4.5574
	step 11873:lm_loss: 4.5293, ppl: 92.6979, loss: 4.5574
	step 11874:lm_loss: 4.5293, ppl: 92.6924, loss: 4.5574
	step 11875:lm_loss: 4.5293, ppl: 92.6952, loss: 4.5574
	step 11876:lm_loss: 4.5294, ppl: 92.7008, loss: 4.5574
	step 11877:lm_loss: 4.5293, ppl: 92.6979, loss: 4.5574
	step 11878:lm_loss: 4.5294, ppl: 92.7004, loss: 4.5574
	step 11879:lm_loss: 4.5293, ppl: 92.6901, loss: 4.5573
	step 11880:lm_loss: 4.5292, ppl: 92.6887, loss: 4.5573
	step 11881:lm_loss: 4.5293, ppl: 92.6899, loss: 4.5573
	step 11882:lm_loss: 4.5293, ppl: 92.6973, loss: 4.5574
	step 11883:lm_loss: 4.5293, ppl: 92.6929, loss: 4.5573
	step 11884:lm_loss: 4.5292, ppl: 92.6883, loss: 4.5573
	step 11885:lm_loss: 4.5291, ppl: 92.6784, loss: 4.5572
	step 11886:lm_loss: 4.5292, ppl: 92.6800, loss: 4.5572
	step 11887:lm_loss: 4.5292, ppl: 92.6840, loss: 4.5572
	step 11888:lm_loss: 4.5292, ppl: 92.6817, loss: 4.5572
	step 11889:lm_loss: 4.5292, ppl: 92.6820, loss: 4.5572
	step 11890:lm_loss: 4.5290, ppl: 92.6656, loss: 4.5571
	step 11891:lm_loss: 4.5290, ppl: 92.6644, loss: 4.5571
	step 11892:lm_loss: 4.5290, ppl: 92.6683, loss: 4.5571
	step 11893:lm_loss: 4.5291, ppl: 92.6706, loss: 4.5571
	step 11894:lm_loss: 4.5290, ppl: 92.6668, loss: 4.5571
	step 11895:lm_loss: 4.5290, ppl: 92.6695, loss: 4.5571
	step 11896:lm_loss: 4.5291, ppl: 92.6722, loss: 4.5571
	step 11897:lm_loss: 4.5291, ppl: 92.6774, loss: 4.5572
	step 11898:lm_loss: 4.5291, ppl: 92.6784, loss: 4.5572
	step 11899:lm_loss: 4.5292, ppl: 92.6819, loss: 4.5572
	step 11900:lm_loss: 4.5292, ppl: 92.6844, loss: 4.5573
	step 11901:lm_loss: 4.5292, ppl: 92.6841, loss: 4.5573
	step 11902:lm_loss: 4.5292, ppl: 92.6827, loss: 4.5572
	step 11903:lm_loss: 4.5292, ppl: 92.6833, loss: 4.5572
	step 11904:lm_loss: 4.5292, ppl: 92.6845, loss: 4.5572
	step 11905:lm_loss: 4.5292, ppl: 92.6843, loss: 4.5572
	step 11906:lm_loss: 4.5292, ppl: 92.6826, loss: 4.5572
	step 11907:lm_loss: 4.5292, ppl: 92.6838, loss: 4.5572
	step 11908:lm_loss: 4.5291, ppl: 92.6740, loss: 4.5571
	step 11909:lm_loss: 4.5290, ppl: 92.6697, loss: 4.5571
	step 11910:lm_loss: 4.5290, ppl: 92.6700, loss: 4.5571
	step 11911:lm_loss: 4.5291, ppl: 92.6751, loss: 4.5571
	step 11912:lm_loss: 4.5292, ppl: 92.6851, loss: 4.5572
	step 11913:lm_loss: 4.5293, ppl: 92.6915, loss: 4.5573
	step 11914:lm_loss: 4.5293, ppl: 92.6959, loss: 4.5573
	step 11915:lm_loss: 4.5292, ppl: 92.6880, loss: 4.5572
	step 11916:lm_loss: 4.5293, ppl: 92.6913, loss: 4.5573
	step 11917:lm_loss: 4.5293, ppl: 92.6899, loss: 4.5572
	step 11918:lm_loss: 4.5292, ppl: 92.6876, loss: 4.5572
	step 11919:lm_loss: 4.5293, ppl: 92.6939, loss: 4.5573
	step 11920:lm_loss: 4.5294, ppl: 92.7007, loss: 4.5574
	step 11921:lm_loss: 4.5294, ppl: 92.6995, loss: 4.5573
	step 11922:lm_loss: 4.5294, ppl: 92.7052, loss: 4.5574
	step 11923:lm_loss: 4.5294, ppl: 92.7054, loss: 4.5574
	step 11924:lm_loss: 4.5295, ppl: 92.7150, loss: 4.5575
	step 11925:lm_loss: 4.5295, ppl: 92.7164, loss: 4.5576
	step 11926:lm_loss: 4.5294, ppl: 92.7075, loss: 4.5575
	step 11927:lm_loss: 4.5294, ppl: 92.7029, loss: 4.5574
	step 11928:lm_loss: 4.5294, ppl: 92.7023, loss: 4.5574
	step 11929:lm_loss: 4.5294, ppl: 92.7057, loss: 4.5575
	step 11930:lm_loss: 4.5294, ppl: 92.7060, loss: 4.5575
	step 11931:lm_loss: 4.5295, ppl: 92.7120, loss: 4.5575
	step 11932:lm_loss: 4.5295, ppl: 92.7158, loss: 4.5575
	step 11933:lm_loss: 4.5296, ppl: 92.7171, loss: 4.5576
	step 11934:lm_loss: 4.5296, ppl: 92.7169, loss: 4.5576
	step 11935:lm_loss: 4.5294, ppl: 92.7045, loss: 4.5574
	step 11936:lm_loss: 4.5292, ppl: 92.6873, loss: 4.5573
	step 11937:lm_loss: 4.5293, ppl: 92.6923, loss: 4.5574
	step 11938:lm_loss: 4.5293, ppl: 92.6948, loss: 4.5574
	step 11939:lm_loss: 4.5293, ppl: 92.6972, loss: 4.5574
	step 11940:lm_loss: 4.5293, ppl: 92.6971, loss: 4.5574
	step 11941:lm_loss: 4.5293, ppl: 92.6973, loss: 4.5574
	step 11942:lm_loss: 4.5294, ppl: 92.7014, loss: 4.5575
	step 11943:lm_loss: 4.5294, ppl: 92.7011, loss: 4.5575
	step 11944:lm_loss: 4.5293, ppl: 92.6978, loss: 4.5574
	step 11945:lm_loss: 4.5294, ppl: 92.6993, loss: 4.5574
	step 11946:lm_loss: 4.5293, ppl: 92.6963, loss: 4.5574
	step 11947:lm_loss: 4.5292, ppl: 92.6850, loss: 4.5573
	step 11948:lm_loss: 4.5291, ppl: 92.6767, loss: 4.5572
	step 11949:lm_loss: 4.5291, ppl: 92.6782, loss: 4.5572
	step 11950:lm_loss: 4.5291, ppl: 92.6776, loss: 4.5572
	step 11951:lm_loss: 4.5291, ppl: 92.6713, loss: 4.5571
	step 11952:lm_loss: 4.5290, ppl: 92.6660, loss: 4.5571
	step 11953:lm_loss: 4.5289, ppl: 92.6584, loss: 4.5570
	step 11954:lm_loss: 4.5289, ppl: 92.6534, loss: 4.5569
	step 11955:lm_loss: 4.5288, ppl: 92.6466, loss: 4.5569
	step 11956:lm_loss: 4.5289, ppl: 92.6530, loss: 4.5570
	step 11957:lm_loss: 4.5288, ppl: 92.6435, loss: 4.5569
	step 11958:lm_loss: 4.5287, ppl: 92.6381, loss: 4.5568
	step 11959:lm_loss: 4.5286, ppl: 92.6306, loss: 4.5567
	step 11960:lm_loss: 4.5287, ppl: 92.6358, loss: 4.5568
	step 11961:lm_loss: 4.5287, ppl: 92.6363, loss: 4.5568
	step 11962:lm_loss: 4.5289, ppl: 92.6573, loss: 4.5571
	step 11963:lm_loss: 4.5289, ppl: 92.6566, loss: 4.5571
	step 11964:lm_loss: 4.5289, ppl: 92.6612, loss: 4.5572
	step 11965:lm_loss: 4.5290, ppl: 92.6612, loss: 4.5572
	step 11966:lm_loss: 4.5289, ppl: 92.6596, loss: 4.5571
	step 11967:lm_loss: 4.5289, ppl: 92.6577, loss: 4.5571
	step 11968:lm_loss: 4.5289, ppl: 92.6582, loss: 4.5571
	step 11969:lm_loss: 4.5289, ppl: 92.6537, loss: 4.5571
	step 11970:lm_loss: 4.5289, ppl: 92.6550, loss: 4.5571
	step 11971:lm_loss: 4.5290, ppl: 92.6618, loss: 4.5572
	step 11972:lm_loss: 4.5290, ppl: 92.6672, loss: 4.5572
	step 11973:lm_loss: 4.5290, ppl: 92.6665, loss: 4.5572
	step 11974:lm_loss: 4.5290, ppl: 92.6658, loss: 4.5572
	step 11975:lm_loss: 4.5290, ppl: 92.6658, loss: 4.5572
	step 11976:lm_loss: 4.5290, ppl: 92.6661, loss: 4.5572
	step 11977:lm_loss: 4.5290, ppl: 92.6694, loss: 4.5572
	step 11978:lm_loss: 4.5291, ppl: 92.6708, loss: 4.5572
	step 11979:lm_loss: 4.5290, ppl: 92.6699, loss: 4.5572
	step 11980:lm_loss: 4.5289, ppl: 92.6568, loss: 4.5571
	step 11981:lm_loss: 4.5288, ppl: 92.6517, loss: 4.5570
	step 11982:lm_loss: 4.5288, ppl: 92.6496, loss: 4.5570
	step 11983:lm_loss: 4.5288, ppl: 92.6487, loss: 4.5570
	step 11984:lm_loss: 4.5288, ppl: 92.6451, loss: 4.5570
	step 11985:lm_loss: 4.5288, ppl: 92.6462, loss: 4.5570
	step 11986:lm_loss: 4.5288, ppl: 92.6495, loss: 4.5570
	step 11987:lm_loss: 4.5288, ppl: 92.6481, loss: 4.5570
	step 11988:lm_loss: 4.5289, ppl: 92.6544, loss: 4.5571
	step 11989:lm_loss: 4.5290, ppl: 92.6619, loss: 4.5572
	step 11990:lm_loss: 4.5290, ppl: 92.6665, loss: 4.5573
	step 11991:lm_loss: 4.5290, ppl: 92.6667, loss: 4.5573
	step 11992:lm_loss: 4.5291, ppl: 92.6748, loss: 4.5573
	step 11993:lm_loss: 4.5292, ppl: 92.6817, loss: 4.5574
	step 11994:lm_loss: 4.5291, ppl: 92.6790, loss: 4.5574
	step 11995:lm_loss: 4.5291, ppl: 92.6782, loss: 4.5574
	step 11996:lm_loss: 4.5291, ppl: 92.6750, loss: 4.5573
	step 11997:lm_loss: 4.5291, ppl: 92.6745, loss: 4.5573
	step 11998:lm_loss: 4.5291, ppl: 92.6736, loss: 4.5573
	step 11999:lm_loss: 4.5292, ppl: 92.6815, loss: 4.5574
	step 12000:lm_loss: 4.5292, ppl: 92.6802, loss: 4.5573
	step 12001:lm_loss: 4.5291, ppl: 92.6794, loss: 4.5573
	step 12002:lm_loss: 4.5292, ppl: 92.6887, loss: 4.5574
	step 12003:lm_loss: 4.5292, ppl: 92.6806, loss: 4.5574
	step 12004:lm_loss: 4.5292, ppl: 92.6812, loss: 4.5574
	step 12005:lm_loss: 4.5292, ppl: 92.6877, loss: 4.5574
	step 12006:lm_loss: 4.5293, ppl: 92.6982, loss: 4.5575
	step 12007:lm_loss: 4.5294, ppl: 92.7031, loss: 4.5576
	step 12008:lm_loss: 4.5294, ppl: 92.7007, loss: 4.5576
	step 12009:lm_loss: 4.5295, ppl: 92.7084, loss: 4.5576
	step 12010:lm_loss: 4.5294, ppl: 92.7000, loss: 4.5575
	step 12011:lm_loss: 4.5294, ppl: 92.7003, loss: 4.5575
	step 12012:lm_loss: 4.5293, ppl: 92.6949, loss: 4.5575
	step 12013:lm_loss: 4.5294, ppl: 92.7029, loss: 4.5575
	step 12014:lm_loss: 4.5294, ppl: 92.7023, loss: 4.5575
	step 12015:lm_loss: 4.5295, ppl: 92.7105, loss: 4.5576
	step 12016:lm_loss: 4.5295, ppl: 92.7135, loss: 4.5576
	step 12017:lm_loss: 4.5296, ppl: 92.7235, loss: 4.5578
	step 12018:lm_loss: 4.5296, ppl: 92.7237, loss: 4.5578
	step 12019:lm_loss: 4.5295, ppl: 92.7143, loss: 4.5577
	step 12020:lm_loss: 4.5295, ppl: 92.7124, loss: 4.5576
	step 12021:lm_loss: 4.5294, ppl: 92.7067, loss: 4.5576
	step 12022:lm_loss: 4.5295, ppl: 92.7095, loss: 4.5576
	step 12023:lm_loss: 4.5295, ppl: 92.7156, loss: 4.5577
	step 12024:lm_loss: 4.5294, ppl: 92.7020, loss: 4.5576
	step 12025:lm_loss: 4.5294, ppl: 92.7023, loss: 4.5576
	step 12026:lm_loss: 4.5294, ppl: 92.7000, loss: 4.5576
	step 12027:lm_loss: 4.5294, ppl: 92.7059, loss: 4.5576
	step 12028:lm_loss: 4.5294, ppl: 92.7060, loss: 4.5576
	step 12029:lm_loss: 4.5294, ppl: 92.7062, loss: 4.5576
	step 12030:lm_loss: 4.5295, ppl: 92.7101, loss: 4.5576
	step 12031:lm_loss: 4.5294, ppl: 92.7075, loss: 4.5576
	step 12032:lm_loss: 4.5294, ppl: 92.7058, loss: 4.5576
	step 12033:lm_loss: 4.5293, ppl: 92.6951, loss: 4.5575
	step 12034:lm_loss: 4.5293, ppl: 92.6966, loss: 4.5575
	step 12035:lm_loss: 4.5293, ppl: 92.6896, loss: 4.5574
	step 12036:lm_loss: 4.5293, ppl: 92.6934, loss: 4.5574
	step 12037:lm_loss: 4.5294, ppl: 92.7072, loss: 4.5576
	step 12038:lm_loss: 4.5295, ppl: 92.7121, loss: 4.5576
	step 12039:lm_loss: 4.5295, ppl: 92.7153, loss: 4.5577
	step 12040:lm_loss: 4.5296, ppl: 92.7177, loss: 4.5577
	step 12041:lm_loss: 4.5295, ppl: 92.7125, loss: 4.5576
	step 12042:lm_loss: 4.5295, ppl: 92.7161, loss: 4.5577
	step 12043:lm_loss: 4.5295, ppl: 92.7135, loss: 4.5576
	step 12044:lm_loss: 4.5295, ppl: 92.7137, loss: 4.5576
	step 12045:lm_loss: 4.5296, ppl: 92.7221, loss: 4.5577
	step 12046:lm_loss: 4.5295, ppl: 92.7105, loss: 4.5576
	step 12047:lm_loss: 4.5295, ppl: 92.7101, loss: 4.5576
	step 12048:lm_loss: 4.5295, ppl: 92.7082, loss: 4.5576
	step 12049:lm_loss: 4.5296, ppl: 92.7170, loss: 4.5576
	step 12050:lm_loss: 4.5296, ppl: 92.7182, loss: 4.5576
	step 12051:lm_loss: 4.5296, ppl: 92.7212, loss: 4.5577
	step 12052:lm_loss: 4.5295, ppl: 92.7156, loss: 4.5576
	step 12053:lm_loss: 4.5296, ppl: 92.7243, loss: 4.5578
	step 12054:lm_loss: 4.5296, ppl: 92.7259, loss: 4.5578
	step 12055:lm_loss: 4.5296, ppl: 92.7249, loss: 4.5578
	step 12056:lm_loss: 4.5297, ppl: 92.7262, loss: 4.5578
	step 12057:lm_loss: 4.5296, ppl: 92.7200, loss: 4.5577
	step 12058:lm_loss: 4.5295, ppl: 92.7151, loss: 4.5577
	step 12059:lm_loss: 4.5296, ppl: 92.7191, loss: 4.5577
	step 12060:lm_loss: 4.5295, ppl: 92.7104, loss: 4.5576
	step 12061:lm_loss: 4.5295, ppl: 92.7163, loss: 4.5577
	step 12062:lm_loss: 4.5295, ppl: 92.7121, loss: 4.5576
	step 12063:lm_loss: 4.5295, ppl: 92.7091, loss: 4.5576
	step 12064:lm_loss: 4.5294, ppl: 92.7069, loss: 4.5576
	step 12065:lm_loss: 4.5295, ppl: 92.7076, loss: 4.5576
	step 12066:lm_loss: 4.5294, ppl: 92.7070, loss: 4.5576
	step 12067:lm_loss: 4.5293, ppl: 92.6950, loss: 4.5575
	step 12068:lm_loss: 4.5293, ppl: 92.6949, loss: 4.5575
	step 12069:lm_loss: 4.5291, ppl: 92.6758, loss: 4.5573
	step 12070:lm_loss: 4.5290, ppl: 92.6683, loss: 4.5572
	step 12071:lm_loss: 4.5290, ppl: 92.6670, loss: 4.5572
	step 12072:lm_loss: 4.5291, ppl: 92.6710, loss: 4.5573
	step 12073:lm_loss: 4.5291, ppl: 92.6713, loss: 4.5573
	step 12074:lm_loss: 4.5290, ppl: 92.6680, loss: 4.5572
	step 12075:lm_loss: 4.5290, ppl: 92.6650, loss: 4.5572
	step 12076:lm_loss: 4.5289, ppl: 92.6601, loss: 4.5571
	step 12077:lm_loss: 4.5289, ppl: 92.6581, loss: 4.5571
	step 12078:lm_loss: 4.5289, ppl: 92.6596, loss: 4.5571
	step 12079:lm_loss: 4.5290, ppl: 92.6629, loss: 4.5571
	step 12080:lm_loss: 4.5290, ppl: 92.6616, loss: 4.5571
	step 12081:lm_loss: 4.5290, ppl: 92.6677, loss: 4.5572
	step 12082:lm_loss: 4.5291, ppl: 92.6709, loss: 4.5572
	step 12083:lm_loss: 4.5290, ppl: 92.6683, loss: 4.5572
	step 12084:lm_loss: 4.5288, ppl: 92.6445, loss: 4.5570
	step 12085:lm_loss: 4.5286, ppl: 92.6307, loss: 4.5569
	step 12086:lm_loss: 4.5285, ppl: 92.6183, loss: 4.5568
	step 12087:lm_loss: 4.5285, ppl: 92.6162, loss: 4.5567
	step 12088:lm_loss: 4.5285, ppl: 92.6217, loss: 4.5568
	step 12089:lm_loss: 4.5284, ppl: 92.6093, loss: 4.5566
	step 12090:lm_loss: 4.5284, ppl: 92.6129, loss: 4.5567
	step 12091:lm_loss: 4.5283, ppl: 92.6006, loss: 4.5566
	step 12092:lm_loss: 4.5283, ppl: 92.6024, loss: 4.5566
	step 12093:lm_loss: 4.5283, ppl: 92.6004, loss: 4.5566
	step 12094:lm_loss: 4.5284, ppl: 92.6069, loss: 4.5567
	step 12095:lm_loss: 4.5284, ppl: 92.6062, loss: 4.5566
	step 12096:lm_loss: 4.5284, ppl: 92.6097, loss: 4.5567
	step 12097:lm_loss: 4.5282, ppl: 92.5961, loss: 4.5566
	step 12098:lm_loss: 4.5283, ppl: 92.6008, loss: 4.5566
	step 12099:lm_loss: 4.5284, ppl: 92.6072, loss: 4.5567
	step 12100:lm_loss: 4.5283, ppl: 92.6003, loss: 4.5566
	step 12101:lm_loss: 4.5283, ppl: 92.6001, loss: 4.5566
	step 12102:lm_loss: 4.5282, ppl: 92.5937, loss: 4.5565
	step 12103:lm_loss: 4.5283, ppl: 92.5980, loss: 4.5566
	step 12104:lm_loss: 4.5282, ppl: 92.5913, loss: 4.5565
	step 12105:lm_loss: 4.5282, ppl: 92.5910, loss: 4.5565
	step 12106:lm_loss: 4.5281, ppl: 92.5830, loss: 4.5563
	step 12107:lm_loss: 4.5282, ppl: 92.5927, loss: 4.5564
	step 12108:lm_loss: 4.5282, ppl: 92.5951, loss: 4.5564
	step 12109:lm_loss: 4.5282, ppl: 92.5931, loss: 4.5564
	step 12110:lm_loss: 4.5282, ppl: 92.5935, loss: 4.5564
	step 12111:lm_loss: 4.5283, ppl: 92.6004, loss: 4.5564
	step 12112:lm_loss: 4.5283, ppl: 92.6018, loss: 4.5565
	step 12113:lm_loss: 4.5284, ppl: 92.6117, loss: 4.5566
	step 12114:lm_loss: 4.5285, ppl: 92.6153, loss: 4.5566
	step 12115:lm_loss: 4.5285, ppl: 92.6152, loss: 4.5566
	step 12116:lm_loss: 4.5285, ppl: 92.6154, loss: 4.5566
	step 12117:lm_loss: 4.5285, ppl: 92.6202, loss: 4.5567
	step 12118:lm_loss: 4.5284, ppl: 92.6129, loss: 4.5566
	step 12119:lm_loss: 4.5284, ppl: 92.6130, loss: 4.5566
	step 12120:lm_loss: 4.5285, ppl: 92.6181, loss: 4.5566
	step 12121:lm_loss: 4.5285, ppl: 92.6238, loss: 4.5567
	step 12122:lm_loss: 4.5285, ppl: 92.6156, loss: 4.5566
	step 12123:lm_loss: 4.5286, ppl: 92.6257, loss: 4.5567
	step 12124:lm_loss: 4.5286, ppl: 92.6287, loss: 4.5568
	step 12125:lm_loss: 4.5286, ppl: 92.6300, loss: 4.5568
	step 12126:lm_loss: 4.5285, ppl: 92.6233, loss: 4.5567
	step 12127:lm_loss: 4.5286, ppl: 92.6256, loss: 4.5568
	step 12128:lm_loss: 4.5286, ppl: 92.6268, loss: 4.5568
	step 12129:lm_loss: 4.5285, ppl: 92.6228, loss: 4.5567
	step 12130:lm_loss: 4.5286, ppl: 92.6323, loss: 4.5568
	step 12131:lm_loss: 4.5287, ppl: 92.6385, loss: 4.5569
	step 12132:lm_loss: 4.5287, ppl: 92.6371, loss: 4.5569
	step 12133:lm_loss: 4.5286, ppl: 92.6319, loss: 4.5568
	step 12134:lm_loss: 4.5284, ppl: 92.6105, loss: 4.5566
	step 12135:lm_loss: 4.5284, ppl: 92.6126, loss: 4.5567
	step 12136:lm_loss: 4.5283, ppl: 92.6042, loss: 4.5566
	step 12137:lm_loss: 4.5283, ppl: 92.6045, loss: 4.5566
	step 12138:lm_loss: 4.5283, ppl: 92.6052, loss: 4.5566
	step 12139:lm_loss: 4.5284, ppl: 92.6077, loss: 4.5566
	step 12140:lm_loss: 4.5283, ppl: 92.6027, loss: 4.5566
	step 12141:lm_loss: 4.5284, ppl: 92.6108, loss: 4.5567
	step 12142:lm_loss: 4.5284, ppl: 92.6068, loss: 4.5566
	step 12143:lm_loss: 4.5284, ppl: 92.6073, loss: 4.5566
	step 12144:lm_loss: 4.5284, ppl: 92.6061, loss: 4.5566
	step 12145:lm_loss: 4.5284, ppl: 92.6099, loss: 4.5566
	step 12146:lm_loss: 4.5284, ppl: 92.6126, loss: 4.5567
	step 12147:lm_loss: 4.5284, ppl: 92.6108, loss: 4.5566
	step 12148:lm_loss: 4.5284, ppl: 92.6129, loss: 4.5567
	step 12149:lm_loss: 4.5284, ppl: 92.6118, loss: 4.5566
	step 12150:lm_loss: 4.5284, ppl: 92.6083, loss: 4.5566
	step 12151:lm_loss: 4.5283, ppl: 92.5969, loss: 4.5565
	step 12152:lm_loss: 4.5283, ppl: 92.6027, loss: 4.5565
	step 12153:lm_loss: 4.5284, ppl: 92.6086, loss: 4.5566
	step 12154:lm_loss: 4.5285, ppl: 92.6155, loss: 4.5567
	step 12155:lm_loss: 4.5284, ppl: 92.6145, loss: 4.5566
	step 12156:lm_loss: 4.5285, ppl: 92.6171, loss: 4.5567
	step 12157:lm_loss: 4.5284, ppl: 92.6142, loss: 4.5566
	step 12158:lm_loss: 4.5284, ppl: 92.6119, loss: 4.5566
	step 12159:lm_loss: 4.5284, ppl: 92.6080, loss: 4.5566
	step 12160:lm_loss: 4.5284, ppl: 92.6080, loss: 4.5566
	step 12161:lm_loss: 4.5283, ppl: 92.6002, loss: 4.5565
	step 12162:lm_loss: 4.5284, ppl: 92.6091, loss: 4.5566
	step 12163:lm_loss: 4.5284, ppl: 92.6062, loss: 4.5566
	step 12164:lm_loss: 4.5283, ppl: 92.6053, loss: 4.5566
	step 12165:lm_loss: 4.5283, ppl: 92.6029, loss: 4.5565
	step 12166:lm_loss: 4.5283, ppl: 92.6018, loss: 4.5565
	step 12167:lm_loss: 4.5284, ppl: 92.6067, loss: 4.5566
	step 12168:lm_loss: 4.5284, ppl: 92.6137, loss: 4.5566
	step 12169:lm_loss: 4.5284, ppl: 92.6106, loss: 4.5566
	step 12170:lm_loss: 4.5283, ppl: 92.6016, loss: 4.5565
	step 12171:lm_loss: 4.5283, ppl: 92.6030, loss: 4.5565
	step 12172:lm_loss: 4.5281, ppl: 92.5783, loss: 4.5563
	step 12173:lm_loss: 4.5281, ppl: 92.5808, loss: 4.5564
	step 12174:lm_loss: 4.5281, ppl: 92.5839, loss: 4.5564
	step 12175:lm_loss: 4.5281, ppl: 92.5835, loss: 4.5564
	step 12176:lm_loss: 4.5281, ppl: 92.5824, loss: 4.5564
	step 12177:lm_loss: 4.5282, ppl: 92.5874, loss: 4.5564
	step 12178:lm_loss: 4.5281, ppl: 92.5838, loss: 4.5564
	step 12179:lm_loss: 4.5282, ppl: 92.5949, loss: 4.5565
	step 12180:lm_loss: 4.5281, ppl: 92.5820, loss: 4.5563
	step 12181:lm_loss: 4.5281, ppl: 92.5783, loss: 4.5563
	step 12182:lm_loss: 4.5280, ppl: 92.5722, loss: 4.5562
	step 12183:lm_loss: 4.5280, ppl: 92.5724, loss: 4.5562
	step 12184:lm_loss: 4.5279, ppl: 92.5655, loss: 4.5561
	step 12185:lm_loss: 4.5279, ppl: 92.5603, loss: 4.5560
	step 12186:lm_loss: 4.5279, ppl: 92.5611, loss: 4.5560
	step 12187:lm_loss: 4.5280, ppl: 92.5687, loss: 4.5561
	step 12188:lm_loss: 4.5278, ppl: 92.5572, loss: 4.5560
	step 12189:lm_loss: 4.5279, ppl: 92.5668, loss: 4.5561
	step 12190:lm_loss: 4.5279, ppl: 92.5643, loss: 4.5561
	step 12191:lm_loss: 4.5280, ppl: 92.5690, loss: 4.5561
	step 12192:lm_loss: 4.5280, ppl: 92.5748, loss: 4.5562
	step 12193:lm_loss: 4.5281, ppl: 92.5803, loss: 4.5563
	step 12194:lm_loss: 4.5281, ppl: 92.5828, loss: 4.5563
	step 12195:lm_loss: 4.5281, ppl: 92.5860, loss: 4.5563
	step 12196:lm_loss: 4.5281, ppl: 92.5836, loss: 4.5563
	step 12197:lm_loss: 4.5281, ppl: 92.5829, loss: 4.5563
	step 12198:lm_loss: 4.5281, ppl: 92.5815, loss: 4.5563
	step 12199:lm_loss: 4.5281, ppl: 92.5815, loss: 4.5563
	step 12200:lm_loss: 4.5281, ppl: 92.5821, loss: 4.5563
	step 12201:lm_loss: 4.5281, ppl: 92.5780, loss: 4.5562
	step 12202:lm_loss: 4.5280, ppl: 92.5754, loss: 4.5562
	step 12203:lm_loss: 4.5281, ppl: 92.5803, loss: 4.5563
	step 12204:lm_loss: 4.5281, ppl: 92.5853, loss: 4.5563
	step 12205:lm_loss: 4.5282, ppl: 92.5902, loss: 4.5564
	step 12206:lm_loss: 4.5282, ppl: 92.5963, loss: 4.5565
	step 12207:lm_loss: 4.5283, ppl: 92.6038, loss: 4.5565
	step 12208:lm_loss: 4.5284, ppl: 92.6070, loss: 4.5566
	step 12209:lm_loss: 4.5284, ppl: 92.6148, loss: 4.5566
	step 12210:lm_loss: 4.5284, ppl: 92.6121, loss: 4.5566
	step 12211:lm_loss: 4.5283, ppl: 92.6055, loss: 4.5565
	step 12212:lm_loss: 4.5283, ppl: 92.6045, loss: 4.5565
	step 12213:lm_loss: 4.5284, ppl: 92.6130, loss: 4.5566
	step 12214:lm_loss: 4.5285, ppl: 92.6164, loss: 4.5566
	step 12215:lm_loss: 4.5285, ppl: 92.6223, loss: 4.5567
	step 12216:lm_loss: 4.5284, ppl: 92.6143, loss: 4.5566
	step 12217:lm_loss: 4.5284, ppl: 92.6142, loss: 4.5566
	step 12218:lm_loss: 4.5284, ppl: 92.6092, loss: 4.5565
	step 12219:lm_loss: 4.5284, ppl: 92.6137, loss: 4.5565
	step 12220:lm_loss: 4.5285, ppl: 92.6197, loss: 4.5566
	step 12221:lm_loss: 4.5285, ppl: 92.6176, loss: 4.5566
	step 12222:lm_loss: 4.5284, ppl: 92.6147, loss: 4.5565
	step 12223:lm_loss: 4.5286, ppl: 92.6250, loss: 4.5567
	step 12224:lm_loss: 4.5286, ppl: 92.6282, loss: 4.5567
	step 12225:lm_loss: 4.5286, ppl: 92.6292, loss: 4.5567
	step 12226:lm_loss: 4.5286, ppl: 92.6317, loss: 4.5567
	step 12227:lm_loss: 4.5286, ppl: 92.6321, loss: 4.5567
	step 12228:lm_loss: 4.5287, ppl: 92.6360, loss: 4.5568
	step 12229:lm_loss: 4.5287, ppl: 92.6401, loss: 4.5568
	step 12230:lm_loss: 4.5288, ppl: 92.6438, loss: 4.5569
	step 12231:lm_loss: 4.5288, ppl: 92.6480, loss: 4.5569
	step 12232:lm_loss: 4.5288, ppl: 92.6454, loss: 4.5569
	step 12233:lm_loss: 4.5288, ppl: 92.6479, loss: 4.5569
	step 12234:lm_loss: 4.5288, ppl: 92.6472, loss: 4.5569
	step 12235:lm_loss: 4.5289, ppl: 92.6554, loss: 4.5570
	step 12236:lm_loss: 4.5289, ppl: 92.6575, loss: 4.5570
	step 12237:lm_loss: 4.5289, ppl: 92.6583, loss: 4.5570
	step 12238:lm_loss: 4.5290, ppl: 92.6623, loss: 4.5571
	step 12239:lm_loss: 4.5290, ppl: 92.6695, loss: 4.5571
	step 12240:lm_loss: 4.5290, ppl: 92.6652, loss: 4.5571
	step 12241:lm_loss: 4.5289, ppl: 92.6601, loss: 4.5570
	step 12242:lm_loss: 4.5289, ppl: 92.6589, loss: 4.5570
	step 12243:lm_loss: 4.5288, ppl: 92.6517, loss: 4.5569
	step 12244:lm_loss: 4.5288, ppl: 92.6474, loss: 4.5569
	step 12245:lm_loss: 4.5289, ppl: 92.6555, loss: 4.5570
	step 12246:lm_loss: 4.5289, ppl: 92.6570, loss: 4.5570
	step 12247:lm_loss: 4.5289, ppl: 92.6582, loss: 4.5570
	step 12248:lm_loss: 4.5290, ppl: 92.6612, loss: 4.5570
	step 12249:lm_loss: 4.5290, ppl: 92.6676, loss: 4.5571
	step 12250:lm_loss: 4.5290, ppl: 92.6664, loss: 4.5571
	step 12251:lm_loss: 4.5290, ppl: 92.6681, loss: 4.5571
	step 12252:lm_loss: 4.5291, ppl: 92.6708, loss: 4.5571
	step 12253:lm_loss: 4.5291, ppl: 92.6711, loss: 4.5571
	step 12254:lm_loss: 4.5291, ppl: 92.6718, loss: 4.5571
	step 12255:lm_loss: 4.5291, ppl: 92.6717, loss: 4.5571
	step 12256:lm_loss: 4.5291, ppl: 92.6775, loss: 4.5572
	step 12257:lm_loss: 4.5291, ppl: 92.6758, loss: 4.5572
	step 12258:lm_loss: 4.5291, ppl: 92.6774, loss: 4.5572
	step 12259:lm_loss: 4.5292, ppl: 92.6835, loss: 4.5573
	step 12260:lm_loss: 4.5291, ppl: 92.6787, loss: 4.5572
	step 12261:lm_loss: 4.5292, ppl: 92.6820, loss: 4.5573
	step 12262:lm_loss: 4.5292, ppl: 92.6811, loss: 4.5573
	step 12263:lm_loss: 4.5291, ppl: 92.6714, loss: 4.5572
	step 12264:lm_loss: 4.5290, ppl: 92.6702, loss: 4.5572
	step 12265:lm_loss: 4.5291, ppl: 92.6767, loss: 4.5573
	step 12266:lm_loss: 4.5292, ppl: 92.6830, loss: 4.5573
	step 12267:lm_loss: 4.5291, ppl: 92.6725, loss: 4.5572
	step 12268:lm_loss: 4.5290, ppl: 92.6687, loss: 4.5571
	step 12269:lm_loss: 4.5290, ppl: 92.6638, loss: 4.5571
	step 12270:lm_loss: 4.5289, ppl: 92.6602, loss: 4.5571
	step 12271:lm_loss: 4.5289, ppl: 92.6568, loss: 4.5570
	step 12272:lm_loss: 4.5290, ppl: 92.6660, loss: 4.5571
	step 12273:lm_loss: 4.5290, ppl: 92.6691, loss: 4.5571
	step 12274:lm_loss: 4.5290, ppl: 92.6685, loss: 4.5571
	step 12275:lm_loss: 4.5290, ppl: 92.6656, loss: 4.5571
	step 12276:lm_loss: 4.5290, ppl: 92.6663, loss: 4.5571
	step 12277:lm_loss: 4.5291, ppl: 92.6710, loss: 4.5571
	step 12278:lm_loss: 4.5291, ppl: 92.6757, loss: 4.5572
	step 12279:lm_loss: 4.5291, ppl: 92.6786, loss: 4.5572
	step 12280:lm_loss: 4.5292, ppl: 92.6807, loss: 4.5572
	step 12281:lm_loss: 4.5292, ppl: 92.6819, loss: 4.5572
	step 12282:lm_loss: 4.5291, ppl: 92.6790, loss: 4.5572
	step 12283:lm_loss: 4.5291, ppl: 92.6795, loss: 4.5572
	step 12284:lm_loss: 4.5291, ppl: 92.6783, loss: 4.5572
	step 12285:lm_loss: 4.5291, ppl: 92.6790, loss: 4.5572
	step 12286:lm_loss: 4.5291, ppl: 92.6789, loss: 4.5572
	step 12287:lm_loss: 4.5291, ppl: 92.6789, loss: 4.5572
	step 12288:lm_loss: 4.5292, ppl: 92.6861, loss: 4.5573
	step 12289:lm_loss: 4.5292, ppl: 92.6868, loss: 4.5573
	step 12290:lm_loss: 4.5294, ppl: 92.7020, loss: 4.5574
	step 12291:lm_loss: 4.5294, ppl: 92.7001, loss: 4.5574
	step 12292:lm_loss: 4.5294, ppl: 92.7064, loss: 4.5574
	step 12293:lm_loss: 4.5295, ppl: 92.7079, loss: 4.5574
	step 12294:lm_loss: 4.5294, ppl: 92.7039, loss: 4.5574
	step 12295:lm_loss: 4.5295, ppl: 92.7099, loss: 4.5574
	step 12296:lm_loss: 4.5295, ppl: 92.7081, loss: 4.5574
	step 12297:lm_loss: 4.5295, ppl: 92.7126, loss: 4.5574
	step 12298:lm_loss: 4.5296, ppl: 92.7230, loss: 4.5575
	step 12299:lm_loss: 4.5296, ppl: 92.7234, loss: 4.5575
	step 12300:lm_loss: 4.5297, ppl: 92.7282, loss: 4.5576
	step 12301:lm_loss: 4.5297, ppl: 92.7318, loss: 4.5576
	step 12302:lm_loss: 4.5297, ppl: 92.7294, loss: 4.5576
	step 12303:lm_loss: 4.5297, ppl: 92.7338, loss: 4.5576
	step 12304:lm_loss: 4.5297, ppl: 92.7268, loss: 4.5576
	step 12305:lm_loss: 4.5297, ppl: 92.7288, loss: 4.5576
	step 12306:lm_loss: 4.5297, ppl: 92.7318, loss: 4.5577
	step 12307:lm_loss: 4.5297, ppl: 92.7300, loss: 4.5576
	step 12308:lm_loss: 4.5297, ppl: 92.7335, loss: 4.5577
	step 12309:lm_loss: 4.5297, ppl: 92.7273, loss: 4.5576
	step 12310:lm_loss: 4.5296, ppl: 92.7228, loss: 4.5575
	step 12311:lm_loss: 4.5296, ppl: 92.7218, loss: 4.5575
	step 12312:lm_loss: 4.5296, ppl: 92.7208, loss: 4.5575
	step 12313:lm_loss: 4.5296, ppl: 92.7215, loss: 4.5575
	step 12314:lm_loss: 4.5297, ppl: 92.7293, loss: 4.5576
	step 12315:lm_loss: 4.5298, ppl: 92.7413, loss: 4.5577
	step 12316:lm_loss: 4.5299, ppl: 92.7449, loss: 4.5577
	step 12317:lm_loss: 4.5297, ppl: 92.7275, loss: 4.5576
	step 12318:lm_loss: 4.5297, ppl: 92.7310, loss: 4.5577
	step 12319:lm_loss: 4.5298, ppl: 92.7431, loss: 4.5578
	step 12320:lm_loss: 4.5298, ppl: 92.7380, loss: 4.5577
	step 12321:lm_loss: 4.5298, ppl: 92.7384, loss: 4.5577
	step 12322:lm_loss: 4.5298, ppl: 92.7424, loss: 4.5578
	step 12323:lm_loss: 4.5299, ppl: 92.7455, loss: 4.5578
	step 12324:lm_loss: 4.5296, ppl: 92.7257, loss: 4.5576
	step 12325:lm_loss: 4.5297, ppl: 92.7265, loss: 4.5576
	step 12326:lm_loss: 4.5297, ppl: 92.7303, loss: 4.5576
	step 12327:lm_loss: 4.5295, ppl: 92.7117, loss: 4.5575
	step 12328:lm_loss: 4.5296, ppl: 92.7235, loss: 4.5577
	step 12329:lm_loss: 4.5295, ppl: 92.7132, loss: 4.5576
	step 12330:lm_loss: 4.5295, ppl: 92.7138, loss: 4.5576
	step 12331:lm_loss: 4.5295, ppl: 92.7161, loss: 4.5576
	step 12332:lm_loss: 4.5295, ppl: 92.7113, loss: 4.5576
	step 12333:lm_loss: 4.5295, ppl: 92.7136, loss: 4.5576
	step 12334:lm_loss: 4.5295, ppl: 92.7136, loss: 4.5576
	step 12335:lm_loss: 4.5295, ppl: 92.7111, loss: 4.5575
	step 12336:lm_loss: 4.5295, ppl: 92.7168, loss: 4.5576
	step 12337:lm_loss: 4.5296, ppl: 92.7220, loss: 4.5576
	step 12338:lm_loss: 4.5297, ppl: 92.7335, loss: 4.5577
	step 12339:lm_loss: 4.5298, ppl: 92.7425, loss: 4.5578
	step 12340:lm_loss: 4.5299, ppl: 92.7469, loss: 4.5579
	step 12341:lm_loss: 4.5297, ppl: 92.7336, loss: 4.5577
	step 12342:lm_loss: 4.5297, ppl: 92.7338, loss: 4.5577
	step 12343:lm_loss: 4.5297, ppl: 92.7346, loss: 4.5577
	step 12344:lm_loss: 4.5298, ppl: 92.7382, loss: 4.5578
	step 12345:lm_loss: 4.5298, ppl: 92.7408, loss: 4.5578
	step 12346:lm_loss: 4.5298, ppl: 92.7409, loss: 4.5578
	step 12347:lm_loss: 4.5297, ppl: 92.7342, loss: 4.5577
	step 12348:lm_loss: 4.5296, ppl: 92.7254, loss: 4.5576
	step 12349:lm_loss: 4.5296, ppl: 92.7250, loss: 4.5576
	step 12350:lm_loss: 4.5296, ppl: 92.7209, loss: 4.5576
	step 12351:lm_loss: 4.5296, ppl: 92.7200, loss: 4.5576
	step 12352:lm_loss: 4.5295, ppl: 92.7108, loss: 4.5574
	step 12353:lm_loss: 4.5294, ppl: 92.7071, loss: 4.5574
	step 12354:lm_loss: 4.5296, ppl: 92.7180, loss: 4.5575
	step 12355:lm_loss: 4.5296, ppl: 92.7212, loss: 4.5575
	step 12356:lm_loss: 4.5297, ppl: 92.7299, loss: 4.5576
	step 12357:lm_loss: 4.5297, ppl: 92.7274, loss: 4.5575
	step 12358:lm_loss: 4.5297, ppl: 92.7275, loss: 4.5575
	step 12359:lm_loss: 4.5297, ppl: 92.7286, loss: 4.5576
	step 12360:lm_loss: 4.5297, ppl: 92.7313, loss: 4.5576
	step 12361:lm_loss: 4.5296, ppl: 92.7224, loss: 4.5575
	step 12362:lm_loss: 4.5296, ppl: 92.7175, loss: 4.5575
	step 12363:lm_loss: 4.5296, ppl: 92.7172, loss: 4.5575
	step 12364:lm_loss: 4.5296, ppl: 92.7191, loss: 4.5575
	step 12365:lm_loss: 4.5295, ppl: 92.7166, loss: 4.5575
	step 12366:lm_loss: 4.5297, ppl: 92.7275, loss: 4.5576
	step 12367:lm_loss: 4.5298, ppl: 92.7416, loss: 4.5578
	step 12368:lm_loss: 4.5298, ppl: 92.7441, loss: 4.5578
	step 12369:lm_loss: 4.5299, ppl: 92.7475, loss: 4.5579
	step 12370:lm_loss: 4.5299, ppl: 92.7476, loss: 4.5579
	step 12371:lm_loss: 4.5299, ppl: 92.7461, loss: 4.5578
	step 12372:lm_loss: 4.5299, ppl: 92.7498, loss: 4.5579
	step 12373:lm_loss: 4.5300, ppl: 92.7585, loss: 4.5579
	step 12374:lm_loss: 4.5300, ppl: 92.7601, loss: 4.5580
	step 12375:lm_loss: 4.5301, ppl: 92.7653, loss: 4.5580
	step 12376:lm_loss: 4.5301, ppl: 92.7675, loss: 4.5580
	step 12377:lm_loss: 4.5301, ppl: 92.7688, loss: 4.5580
	step 12378:lm_loss: 4.5302, ppl: 92.7746, loss: 4.5581
	step 12379:lm_loss: 4.5302, ppl: 92.7728, loss: 4.5581
	step 12380:lm_loss: 4.5302, ppl: 92.7761, loss: 4.5581
	step 12381:lm_loss: 4.5302, ppl: 92.7759, loss: 4.5581
	step 12382:lm_loss: 4.5302, ppl: 92.7760, loss: 4.5581
	step 12383:lm_loss: 4.5301, ppl: 92.7684, loss: 4.5580
	step 12384:lm_loss: 4.5301, ppl: 92.7696, loss: 4.5580
	step 12385:lm_loss: 4.5301, ppl: 92.7645, loss: 4.5579
	step 12386:lm_loss: 4.5301, ppl: 92.7644, loss: 4.5579
	step 12387:lm_loss: 4.5301, ppl: 92.7634, loss: 4.5579
	step 12388:lm_loss: 4.5300, ppl: 92.7608, loss: 4.5579
	step 12389:lm_loss: 4.5300, ppl: 92.7620, loss: 4.5579
	step 12390:lm_loss: 4.5299, ppl: 92.7498, loss: 4.5578
	step 12391:lm_loss: 4.5299, ppl: 92.7493, loss: 4.5578
	step 12392:lm_loss: 4.5298, ppl: 92.7431, loss: 4.5577
	step 12393:lm_loss: 4.5299, ppl: 92.7501, loss: 4.5578
	step 12394:lm_loss: 4.5300, ppl: 92.7545, loss: 4.5578
	step 12395:lm_loss: 4.5300, ppl: 92.7577, loss: 4.5578
	step 12396:lm_loss: 4.5300, ppl: 92.7551, loss: 4.5578
	step 12397:lm_loss: 4.5300, ppl: 92.7549, loss: 4.5578
	step 12398:lm_loss: 4.5300, ppl: 92.7560, loss: 4.5578
	step 12399:lm_loss: 4.5300, ppl: 92.7600, loss: 4.5578
	step 12400:lm_loss: 4.5300, ppl: 92.7566, loss: 4.5578
	step 12401:lm_loss: 4.5301, ppl: 92.7653, loss: 4.5579
	step 12402:lm_loss: 4.5300, ppl: 92.7627, loss: 4.5579
	step 12403:lm_loss: 4.5301, ppl: 92.7675, loss: 4.5579
	step 12404:lm_loss: 4.5301, ppl: 92.7699, loss: 4.5579
	step 12405:lm_loss: 4.5301, ppl: 92.7691, loss: 4.5579
	step 12406:lm_loss: 4.5302, ppl: 92.7725, loss: 4.5580
	step 12407:lm_loss: 4.5302, ppl: 92.7784, loss: 4.5580
	step 12408:lm_loss: 4.5303, ppl: 92.7822, loss: 4.5581
	step 12409:lm_loss: 4.5303, ppl: 92.7863, loss: 4.5581
	step 12410:lm_loss: 4.5302, ppl: 92.7802, loss: 4.5581
	step 12411:lm_loss: 4.5303, ppl: 92.7850, loss: 4.5581
	step 12412:lm_loss: 4.5302, ppl: 92.7808, loss: 4.5581
	step 12413:lm_loss: 4.5303, ppl: 92.7854, loss: 4.5581
	step 12414:lm_loss: 4.5303, ppl: 92.7864, loss: 4.5581
	step 12415:lm_loss: 4.5303, ppl: 92.7865, loss: 4.5581
	step 12416:lm_loss: 4.5302, ppl: 92.7782, loss: 4.5581
	step 12417:lm_loss: 4.5303, ppl: 92.7840, loss: 4.5581
	step 12418:lm_loss: 4.5303, ppl: 92.7896, loss: 4.5582
	step 12419:lm_loss: 4.5304, ppl: 92.7988, loss: 4.5583
	step 12420:lm_loss: 4.5304, ppl: 92.7942, loss: 4.5582
	step 12421:lm_loss: 4.5304, ppl: 92.7922, loss: 4.5582
	step 12422:lm_loss: 4.5304, ppl: 92.7964, loss: 4.5583
	step 12423:lm_loss: 4.5304, ppl: 92.7928, loss: 4.5582
	step 12424:lm_loss: 4.5304, ppl: 92.7939, loss: 4.5583
	step 12425:lm_loss: 4.5302, ppl: 92.7809, loss: 4.5582
	step 12426:lm_loss: 4.5300, ppl: 92.7614, loss: 4.5580
	step 12427:lm_loss: 4.5301, ppl: 92.7646, loss: 4.5580
	step 12428:lm_loss: 4.5301, ppl: 92.7633, loss: 4.5580
	step 12429:lm_loss: 4.5301, ppl: 92.7717, loss: 4.5581
	step 12430:lm_loss: 4.5301, ppl: 92.7696, loss: 4.5581
	step 12431:lm_loss: 4.5301, ppl: 92.7718, loss: 4.5581
	step 12432:lm_loss: 4.5302, ppl: 92.7725, loss: 4.5581
	step 12433:lm_loss: 4.5301, ppl: 92.7690, loss: 4.5581
	step 12434:lm_loss: 4.5299, ppl: 92.7530, loss: 4.5580
	step 12435:lm_loss: 4.5299, ppl: 92.7485, loss: 4.5579
	step 12436:lm_loss: 4.5299, ppl: 92.7473, loss: 4.5579
	step 12437:lm_loss: 4.5298, ppl: 92.7428, loss: 4.5578
	step 12438:lm_loss: 4.5298, ppl: 92.7387, loss: 4.5578
	step 12439:lm_loss: 4.5298, ppl: 92.7414, loss: 4.5578
	step 12440:lm_loss: 4.5298, ppl: 92.7412, loss: 4.5578
	step 12441:lm_loss: 4.5298, ppl: 92.7395, loss: 4.5578
	step 12442:lm_loss: 4.5297, ppl: 92.7324, loss: 4.5577
	step 12443:lm_loss: 4.5296, ppl: 92.7257, loss: 4.5577
	step 12444:lm_loss: 4.5296, ppl: 92.7251, loss: 4.5577
	step 12445:lm_loss: 4.5298, ppl: 92.7367, loss: 4.5578
	step 12446:lm_loss: 4.5298, ppl: 92.7401, loss: 4.5579
	step 12447:lm_loss: 4.5298, ppl: 92.7413, loss: 4.5579
	step 12448:lm_loss: 4.5298, ppl: 92.7376, loss: 4.5578
	step 12449:lm_loss: 4.5297, ppl: 92.7347, loss: 4.5578
	step 12450:lm_loss: 4.5298, ppl: 92.7356, loss: 4.5578
	step 12451:lm_loss: 4.5297, ppl: 92.7342, loss: 4.5578
	step 12452:lm_loss: 4.5298, ppl: 92.7398, loss: 4.5579
	step 12453:lm_loss: 4.5298, ppl: 92.7371, loss: 4.5578
	step 12454:lm_loss: 4.5298, ppl: 92.7424, loss: 4.5579
	step 12455:lm_loss: 4.5297, ppl: 92.7350, loss: 4.5578
	step 12456:lm_loss: 4.5297, ppl: 92.7344, loss: 4.5578
	step 12457:lm_loss: 4.5297, ppl: 92.7343, loss: 4.5578
	step 12458:lm_loss: 4.5297, ppl: 92.7262, loss: 4.5577
	step 12459:lm_loss: 4.5298, ppl: 92.7356, loss: 4.5579
	step 12460:lm_loss: 4.5298, ppl: 92.7386, loss: 4.5579
	step 12461:lm_loss: 4.5298, ppl: 92.7444, loss: 4.5579
	step 12462:lm_loss: 4.5299, ppl: 92.7450, loss: 4.5579
	step 12463:lm_loss: 4.5299, ppl: 92.7451, loss: 4.5579
	step 12464:lm_loss: 4.5299, ppl: 92.7448, loss: 4.5579
	step 12465:lm_loss: 4.5299, ppl: 92.7534, loss: 4.5581
	step 12466:lm_loss: 4.5299, ppl: 92.7528, loss: 4.5581
	step 12467:lm_loss: 4.5300, ppl: 92.7552, loss: 4.5581
	step 12468:lm_loss: 4.5300, ppl: 92.7605, loss: 4.5581
	step 12469:lm_loss: 4.5300, ppl: 92.7623, loss: 4.5581
	step 12470:lm_loss: 4.5301, ppl: 92.7709, loss: 4.5583
	step 12471:lm_loss: 4.5302, ppl: 92.7784, loss: 4.5584
	step 12472:lm_loss: 4.5302, ppl: 92.7744, loss: 4.5583
	step 12473:lm_loss: 4.5302, ppl: 92.7741, loss: 4.5583
	step 12474:lm_loss: 4.5301, ppl: 92.7723, loss: 4.5583
	step 12475:lm_loss: 4.5302, ppl: 92.7783, loss: 4.5584
	step 12476:lm_loss: 4.5302, ppl: 92.7809, loss: 4.5584
	step 12477:lm_loss: 4.5302, ppl: 92.7795, loss: 4.5584
	step 12478:lm_loss: 4.5302, ppl: 92.7769, loss: 4.5584
	step 12479:lm_loss: 4.5302, ppl: 92.7746, loss: 4.5583
	step 12480:lm_loss: 4.5302, ppl: 92.7788, loss: 4.5584
	step 12481:lm_loss: 4.5302, ppl: 92.7749, loss: 4.5583
	step 12482:lm_loss: 4.5299, ppl: 92.7501, loss: 4.5582
	step 12483:lm_loss: 4.5300, ppl: 92.7546, loss: 4.5583
	step 12484:lm_loss: 4.5300, ppl: 92.7554, loss: 4.5583
	step 12485:lm_loss: 4.5300, ppl: 92.7592, loss: 4.5583
	step 12486:lm_loss: 4.5300, ppl: 92.7629, loss: 4.5583
	step 12487:lm_loss: 4.5300, ppl: 92.7618, loss: 4.5583
	step 12488:lm_loss: 4.5301, ppl: 92.7717, loss: 4.5584
	step 12489:lm_loss: 4.5302, ppl: 92.7736, loss: 4.5584
	step 12490:lm_loss: 4.5303, ppl: 92.7821, loss: 4.5585
	step 12491:lm_loss: 4.5303, ppl: 92.7827, loss: 4.5585
	step 12492:lm_loss: 4.5302, ppl: 92.7775, loss: 4.5585
	step 12493:lm_loss: 4.5302, ppl: 92.7753, loss: 4.5584
	step 12494:lm_loss: 4.5301, ppl: 92.7725, loss: 4.5584
	step 12495:lm_loss: 4.5303, ppl: 92.7858, loss: 4.5586
	step 12496:lm_loss: 4.5303, ppl: 92.7903, loss: 4.5586
	step 12497:lm_loss: 4.5304, ppl: 92.7948, loss: 4.5587
	step 12498:lm_loss: 4.5304, ppl: 92.7939, loss: 4.5587
	step 12499:lm_loss: 4.5305, ppl: 92.8004, loss: 4.5588
	step 12500:lm_loss: 4.5304, ppl: 92.7963, loss: 4.5587
	step 12501:lm_loss: 4.5304, ppl: 92.7957, loss: 4.5587
	step 12502:lm_loss: 4.5305, ppl: 92.8009, loss: 4.5588
	step 12503:lm_loss: 4.5304, ppl: 92.7990, loss: 4.5588
	step 12504:lm_loss: 4.5305, ppl: 92.8009, loss: 4.5588
	step 12505:lm_loss: 4.5305, ppl: 92.8056, loss: 4.5588
	step 12506:lm_loss: 4.5306, ppl: 92.8130, loss: 4.5589
	step 12507:lm_loss: 4.5305, ppl: 92.8073, loss: 4.5589
	step 12508:lm_loss: 4.5305, ppl: 92.8079, loss: 4.5589
	step 12509:lm_loss: 4.5304, ppl: 92.7963, loss: 4.5588
	step 12510:lm_loss: 4.5304, ppl: 92.7936, loss: 4.5588
	step 12511:lm_loss: 4.5303, ppl: 92.7908, loss: 4.5587
	step 12512:lm_loss: 4.5302, ppl: 92.7814, loss: 4.5586
	step 12513:lm_loss: 4.5303, ppl: 92.7868, loss: 4.5587
	step 12514:lm_loss: 4.5305, ppl: 92.8063, loss: 4.5588
	step 12515:lm_loss: 4.5305, ppl: 92.8032, loss: 4.5588
	step 12516:lm_loss: 4.5304, ppl: 92.7936, loss: 4.5587
	step 12517:lm_loss: 4.5304, ppl: 92.7938, loss: 4.5587
	step 12518:lm_loss: 4.5303, ppl: 92.7846, loss: 4.5586
	step 12519:lm_loss: 4.5303, ppl: 92.7877, loss: 4.5586
	step 12520:lm_loss: 4.5303, ppl: 92.7868, loss: 4.5586
	step 12521:lm_loss: 4.5303, ppl: 92.7894, loss: 4.5586
	step 12522:lm_loss: 4.5303, ppl: 92.7892, loss: 4.5586
	step 12523:lm_loss: 4.5304, ppl: 92.7944, loss: 4.5587
	step 12524:lm_loss: 4.5303, ppl: 92.7910, loss: 4.5587
	step 12525:lm_loss: 4.5304, ppl: 92.7965, loss: 4.5587
	step 12526:lm_loss: 4.5300, ppl: 92.7618, loss: 4.5586
	step 12527:lm_loss: 4.5301, ppl: 92.7636, loss: 4.5586
	step 12528:lm_loss: 4.5301, ppl: 92.7687, loss: 4.5587
	step 12529:lm_loss: 4.5302, ppl: 92.7739, loss: 4.5587
	step 12530:lm_loss: 4.5302, ppl: 92.7740, loss: 4.5587
	step 12531:lm_loss: 4.5302, ppl: 92.7765, loss: 4.5588
	step 12532:lm_loss: 4.5302, ppl: 92.7806, loss: 4.5588
	step 12533:lm_loss: 4.5303, ppl: 92.7867, loss: 4.5589
	step 12534:lm_loss: 4.5303, ppl: 92.7844, loss: 4.5588
	step 12535:lm_loss: 4.5302, ppl: 92.7795, loss: 4.5588
	step 12536:lm_loss: 4.5303, ppl: 92.7824, loss: 4.5588
	step 12537:lm_loss: 4.5303, ppl: 92.7883, loss: 4.5588
	step 12538:lm_loss: 4.5302, ppl: 92.7806, loss: 4.5587
	step 12539:lm_loss: 4.5303, ppl: 92.7844, loss: 4.5588
	step 12540:lm_loss: 4.5303, ppl: 92.7828, loss: 4.5588
	step 12541:lm_loss: 4.5302, ppl: 92.7780, loss: 4.5587
	step 12542:lm_loss: 4.5300, ppl: 92.7626, loss: 4.5586
	step 12543:lm_loss: 4.5301, ppl: 92.7646, loss: 4.5586
	step 12544:lm_loss: 4.5301, ppl: 92.7684, loss: 4.5587
	step 12545:lm_loss: 4.5301, ppl: 92.7660, loss: 4.5586
	step 12546:lm_loss: 4.5302, ppl: 92.7728, loss: 4.5587
	step 12547:lm_loss: 4.5301, ppl: 92.7693, loss: 4.5587
	step 12548:lm_loss: 4.5300, ppl: 92.7578, loss: 4.5586
	step 12549:lm_loss: 4.5301, ppl: 92.7666, loss: 4.5587
	step 12550:lm_loss: 4.5301, ppl: 92.7681, loss: 4.5587
	step 12551:lm_loss: 4.5301, ppl: 92.7721, loss: 4.5587
	step 12552:lm_loss: 4.5301, ppl: 92.7705, loss: 4.5587
	step 12553:lm_loss: 4.5301, ppl: 92.7690, loss: 4.5587
	step 12554:lm_loss: 4.5301, ppl: 92.7719, loss: 4.5587
	step 12555:lm_loss: 4.5301, ppl: 92.7649, loss: 4.5586
	step 12556:lm_loss: 4.5301, ppl: 92.7650, loss: 4.5586
	step 12557:lm_loss: 4.5300, ppl: 92.7611, loss: 4.5586
	step 12558:lm_loss: 4.5301, ppl: 92.7670, loss: 4.5586
	step 12559:lm_loss: 4.5301, ppl: 92.7660, loss: 4.5586
	step 12560:lm_loss: 4.5301, ppl: 92.7724, loss: 4.5587
	step 12561:lm_loss: 4.5300, ppl: 92.7614, loss: 4.5585
	step 12562:lm_loss: 4.5301, ppl: 92.7648, loss: 4.5586
	step 12563:lm_loss: 4.5301, ppl: 92.7706, loss: 4.5586
	step 12564:lm_loss: 4.5301, ppl: 92.7663, loss: 4.5586
	step 12565:lm_loss: 4.5301, ppl: 92.7632, loss: 4.5586
	step 12566:lm_loss: 4.5301, ppl: 92.7700, loss: 4.5586
	step 12567:lm_loss: 4.5301, ppl: 92.7680, loss: 4.5586
	step 12568:lm_loss: 4.5301, ppl: 92.7681, loss: 4.5586
	step 12569:lm_loss: 4.5301, ppl: 92.7657, loss: 4.5586
	step 12570:lm_loss: 4.5300, ppl: 92.7598, loss: 4.5585
	step 12571:lm_loss: 4.5299, ppl: 92.7527, loss: 4.5585
	step 12572:lm_loss: 4.5300, ppl: 92.7632, loss: 4.5586
	step 12573:lm_loss: 4.5300, ppl: 92.7599, loss: 4.5585
	step 12574:lm_loss: 4.5300, ppl: 92.7599, loss: 4.5585
	step 12575:lm_loss: 4.5300, ppl: 92.7625, loss: 4.5586
	step 12576:lm_loss: 4.5300, ppl: 92.7630, loss: 4.5586
	step 12577:lm_loss: 4.5301, ppl: 92.7644, loss: 4.5586
	step 12578:lm_loss: 4.5300, ppl: 92.7626, loss: 4.5586
	step 12579:lm_loss: 4.5299, ppl: 92.7484, loss: 4.5585
	step 12580:lm_loss: 4.5299, ppl: 92.7497, loss: 4.5585
	step 12581:lm_loss: 4.5299, ppl: 92.7470, loss: 4.5585
	step 12582:lm_loss: 4.5298, ppl: 92.7384, loss: 4.5584
	step 12583:lm_loss: 4.5298, ppl: 92.7415, loss: 4.5584
	step 12584:lm_loss: 4.5297, ppl: 92.7341, loss: 4.5583
	step 12585:lm_loss: 4.5297, ppl: 92.7282, loss: 4.5583
	step 12586:lm_loss: 4.5296, ppl: 92.7237, loss: 4.5582
	step 12587:lm_loss: 4.5297, ppl: 92.7288, loss: 4.5583
	step 12588:lm_loss: 4.5297, ppl: 92.7273, loss: 4.5583
	step 12589:lm_loss: 4.5297, ppl: 92.7290, loss: 4.5583
	step 12590:lm_loss: 4.5297, ppl: 92.7303, loss: 4.5583
	step 12591:lm_loss: 4.5298, ppl: 92.7407, loss: 4.5584
	step 12592:lm_loss: 4.5298, ppl: 92.7401, loss: 4.5584
	step 12593:lm_loss: 4.5298, ppl: 92.7373, loss: 4.5583
	step 12594:lm_loss: 4.5298, ppl: 92.7409, loss: 4.5584
	step 12595:lm_loss: 4.5298, ppl: 92.7356, loss: 4.5583
	step 12596:lm_loss: 4.5297, ppl: 92.7349, loss: 4.5583
	step 12597:lm_loss: 4.5298, ppl: 92.7410, loss: 4.5584
	step 12598:lm_loss: 4.5298, ppl: 92.7389, loss: 4.5583
	step 12599:lm_loss: 4.5298, ppl: 92.7438, loss: 4.5584
	step 12600:lm_loss: 4.5299, ppl: 92.7465, loss: 4.5584
	step 12601:lm_loss: 4.5298, ppl: 92.7441, loss: 4.5584
	step 12602:lm_loss: 4.5299, ppl: 92.7497, loss: 4.5585
	step 12603:lm_loss: 4.5299, ppl: 92.7526, loss: 4.5585
	step 12604:lm_loss: 4.5300, ppl: 92.7581, loss: 4.5586
	step 12605:lm_loss: 4.5300, ppl: 92.7584, loss: 4.5586
	step 12606:lm_loss: 4.5300, ppl: 92.7600, loss: 4.5586
	step 12607:lm_loss: 4.5299, ppl: 92.7463, loss: 4.5585
	step 12608:lm_loss: 4.5299, ppl: 92.7481, loss: 4.5585
	step 12609:lm_loss: 4.5299, ppl: 92.7481, loss: 4.5585
	step 12610:lm_loss: 4.5299, ppl: 92.7492, loss: 4.5585
	step 12611:lm_loss: 4.5298, ppl: 92.7432, loss: 4.5585
	step 12612:lm_loss: 4.5297, ppl: 92.7352, loss: 4.5584
	step 12613:lm_loss: 4.5298, ppl: 92.7354, loss: 4.5584
	step 12614:lm_loss: 4.5296, ppl: 92.7229, loss: 4.5583
	step 12615:lm_loss: 4.5296, ppl: 92.7256, loss: 4.5583
	step 12616:lm_loss: 4.5297, ppl: 92.7276, loss: 4.5583
	step 12617:lm_loss: 4.5297, ppl: 92.7317, loss: 4.5584
	step 12618:lm_loss: 4.5297, ppl: 92.7294, loss: 4.5583
	step 12619:lm_loss: 4.5296, ppl: 92.7250, loss: 4.5583
	step 12620:lm_loss: 4.5296, ppl: 92.7247, loss: 4.5583
	step 12621:lm_loss: 4.5297, ppl: 92.7280, loss: 4.5583
	step 12622:lm_loss: 4.5297, ppl: 92.7267, loss: 4.5583
	step 12623:lm_loss: 4.5297, ppl: 92.7321, loss: 4.5583
	step 12624:lm_loss: 4.5297, ppl: 92.7340, loss: 4.5583
	step 12625:lm_loss: 4.5297, ppl: 92.7298, loss: 4.5583
	step 12626:lm_loss: 4.5297, ppl: 92.7282, loss: 4.5583
	step 12627:lm_loss: 4.5296, ppl: 92.7246, loss: 4.5582
	step 12628:lm_loss: 4.5296, ppl: 92.7185, loss: 4.5581
	step 12629:lm_loss: 4.5295, ppl: 92.7140, loss: 4.5581
	step 12630:lm_loss: 4.5295, ppl: 92.7101, loss: 4.5580
	step 12631:lm_loss: 4.5295, ppl: 92.7150, loss: 4.5581
	step 12632:lm_loss: 4.5295, ppl: 92.7143, loss: 4.5581
	step 12633:lm_loss: 4.5295, ppl: 92.7094, loss: 4.5581
	step 12634:lm_loss: 4.5294, ppl: 92.7075, loss: 4.5580
	step 12635:lm_loss: 4.5294, ppl: 92.7073, loss: 4.5580
	step 12636:lm_loss: 4.5295, ppl: 92.7118, loss: 4.5581
	step 12637:lm_loss: 4.5296, ppl: 92.7172, loss: 4.5582
	step 12638:lm_loss: 4.5295, ppl: 92.7139, loss: 4.5581
	step 12639:lm_loss: 4.5295, ppl: 92.7157, loss: 4.5582
	step 12640:lm_loss: 4.5295, ppl: 92.7139, loss: 4.5581
	step 12641:lm_loss: 4.5296, ppl: 92.7221, loss: 4.5582
	step 12642:lm_loss: 4.5296, ppl: 92.7214, loss: 4.5582
	step 12643:lm_loss: 4.5296, ppl: 92.7236, loss: 4.5582
	step 12644:lm_loss: 4.5296, ppl: 92.7228, loss: 4.5582
	step 12645:lm_loss: 4.5297, ppl: 92.7319, loss: 4.5583
	step 12646:lm_loss: 4.5298, ppl: 92.7375, loss: 4.5584
	step 12647:lm_loss: 4.5297, ppl: 92.7336, loss: 4.5584
	step 12648:lm_loss: 4.5297, ppl: 92.7303, loss: 4.5583
	step 12649:lm_loss: 4.5297, ppl: 92.7295, loss: 4.5583
	step 12650:lm_loss: 4.5297, ppl: 92.7314, loss: 4.5583
	step 12651:lm_loss: 4.5296, ppl: 92.7197, loss: 4.5582
	step 12652:lm_loss: 4.5296, ppl: 92.7211, loss: 4.5582
	step 12653:lm_loss: 4.5295, ppl: 92.7135, loss: 4.5582
	step 12654:lm_loss: 4.5296, ppl: 92.7173, loss: 4.5582
	step 12655:lm_loss: 4.5296, ppl: 92.7231, loss: 4.5583
	step 12656:lm_loss: 4.5295, ppl: 92.7099, loss: 4.5582
	step 12657:lm_loss: 4.5294, ppl: 92.7016, loss: 4.5581
	step 12658:lm_loss: 4.5293, ppl: 92.6978, loss: 4.5581
	step 12659:lm_loss: 4.5293, ppl: 92.6923, loss: 4.5580
	step 12660:lm_loss: 4.5293, ppl: 92.6913, loss: 4.5580
	step 12661:lm_loss: 4.5292, ppl: 92.6800, loss: 4.5578
	step 12662:lm_loss: 4.5292, ppl: 92.6848, loss: 4.5579
	step 12663:lm_loss: 4.5292, ppl: 92.6884, loss: 4.5579
	step 12664:lm_loss: 4.5294, ppl: 92.7013, loss: 4.5581
	step 12665:lm_loss: 4.5292, ppl: 92.6804, loss: 4.5580
	step 12666:lm_loss: 4.5291, ppl: 92.6797, loss: 4.5579
	step 12667:lm_loss: 4.5292, ppl: 92.6828, loss: 4.5580
	step 12668:lm_loss: 4.5292, ppl: 92.6819, loss: 4.5580
	step 12669:lm_loss: 4.5292, ppl: 92.6805, loss: 4.5580
	step 12670:lm_loss: 4.5292, ppl: 92.6820, loss: 4.5580
	step 12671:lm_loss: 4.5289, ppl: 92.6576, loss: 4.5577
	step 12672:lm_loss: 4.5288, ppl: 92.6519, loss: 4.5577
	step 12673:lm_loss: 4.5289, ppl: 92.6542, loss: 4.5577
	step 12674:lm_loss: 4.5290, ppl: 92.6640, loss: 4.5578
	step 12675:lm_loss: 4.5288, ppl: 92.6511, loss: 4.5577
	step 12676:lm_loss: 4.5288, ppl: 92.6480, loss: 4.5577
	step 12677:lm_loss: 4.5288, ppl: 92.6474, loss: 4.5577
	step 12678:lm_loss: 4.5287, ppl: 92.6354, loss: 4.5576
	step 12679:lm_loss: 4.5287, ppl: 92.6381, loss: 4.5576
	step 12680:lm_loss: 4.5286, ppl: 92.6286, loss: 4.5575
	step 12681:lm_loss: 4.5285, ppl: 92.6178, loss: 4.5575
	step 12682:lm_loss: 4.5284, ppl: 92.6130, loss: 4.5574
	step 12683:lm_loss: 4.5284, ppl: 92.6096, loss: 4.5573
	step 12684:lm_loss: 4.5284, ppl: 92.6077, loss: 4.5573
	step 12685:lm_loss: 4.5284, ppl: 92.6122, loss: 4.5574
	step 12686:lm_loss: 4.5284, ppl: 92.6120, loss: 4.5574
	step 12687:lm_loss: 4.5284, ppl: 92.6086, loss: 4.5573
	step 12688:lm_loss: 4.5283, ppl: 92.6034, loss: 4.5572
	step 12689:lm_loss: 4.5283, ppl: 92.5993, loss: 4.5572
	step 12690:lm_loss: 4.5284, ppl: 92.6067, loss: 4.5573
	step 12691:lm_loss: 4.5283, ppl: 92.6047, loss: 4.5573
	step 12692:lm_loss: 4.5284, ppl: 92.6108, loss: 4.5573
	step 12693:lm_loss: 4.5285, ppl: 92.6157, loss: 4.5574
	step 12694:lm_loss: 4.5284, ppl: 92.6068, loss: 4.5573
	step 12695:lm_loss: 4.5284, ppl: 92.6130, loss: 4.5574
	step 12696:lm_loss: 4.5284, ppl: 92.6114, loss: 4.5573
	step 12697:lm_loss: 4.5284, ppl: 92.6057, loss: 4.5572
	step 12698:lm_loss: 4.5284, ppl: 92.6081, loss: 4.5573
	step 12699:lm_loss: 4.5283, ppl: 92.6040, loss: 4.5572
	step 12700:lm_loss: 4.5283, ppl: 92.5998, loss: 4.5572
	step 12701:lm_loss: 4.5282, ppl: 92.5958, loss: 4.5572
	step 12702:lm_loss: 4.5283, ppl: 92.5970, loss: 4.5572
	step 12703:lm_loss: 4.5283, ppl: 92.5987, loss: 4.5572
	step 12704:lm_loss: 4.5283, ppl: 92.5999, loss: 4.5572
	step 12705:lm_loss: 4.5283, ppl: 92.5977, loss: 4.5572
	step 12706:lm_loss: 4.5282, ppl: 92.5956, loss: 4.5571
	step 12707:lm_loss: 4.5283, ppl: 92.6004, loss: 4.5572
	step 12708:lm_loss: 4.5282, ppl: 92.5939, loss: 4.5571
	step 12709:lm_loss: 4.5282, ppl: 92.5913, loss: 4.5571
	step 12710:lm_loss: 4.5282, ppl: 92.5899, loss: 4.5571
	step 12711:lm_loss: 4.5282, ppl: 92.5932, loss: 4.5571
	step 12712:lm_loss: 4.5282, ppl: 92.5876, loss: 4.5570
	step 12713:lm_loss: 4.5281, ppl: 92.5870, loss: 4.5570
	step 12714:lm_loss: 4.5282, ppl: 92.5949, loss: 4.5571
	step 12715:lm_loss: 4.5283, ppl: 92.5981, loss: 4.5572
	step 12716:lm_loss: 4.5283, ppl: 92.5980, loss: 4.5572
	step 12717:lm_loss: 4.5283, ppl: 92.6006, loss: 4.5572
	step 12718:lm_loss: 4.5283, ppl: 92.5992, loss: 4.5572
	step 12719:lm_loss: 4.5283, ppl: 92.6006, loss: 4.5572
	step 12720:lm_loss: 4.5283, ppl: 92.5991, loss: 4.5572
	step 12721:lm_loss: 4.5283, ppl: 92.5971, loss: 4.5571
	step 12722:lm_loss: 4.5282, ppl: 92.5892, loss: 4.5571
	step 12723:lm_loss: 4.5283, ppl: 92.5987, loss: 4.5571
	step 12724:lm_loss: 4.5282, ppl: 92.5940, loss: 4.5571
	step 12725:lm_loss: 4.5282, ppl: 92.5901, loss: 4.5570
	step 12726:lm_loss: 4.5281, ppl: 92.5837, loss: 4.5569
	step 12727:lm_loss: 4.5281, ppl: 92.5842, loss: 4.5569
	step 12728:lm_loss: 4.5281, ppl: 92.5843, loss: 4.5569
	step 12729:lm_loss: 4.5282, ppl: 92.5929, loss: 4.5570
	step 12730:lm_loss: 4.5282, ppl: 92.5874, loss: 4.5569
	step 12731:lm_loss: 4.5281, ppl: 92.5859, loss: 4.5569
	step 12732:lm_loss: 4.5281, ppl: 92.5864, loss: 4.5569
	step 12733:lm_loss: 4.5282, ppl: 92.5910, loss: 4.5570
	step 12734:lm_loss: 4.5282, ppl: 92.5880, loss: 4.5569
	step 12735:lm_loss: 4.5282, ppl: 92.5879, loss: 4.5569
	step 12736:lm_loss: 4.5281, ppl: 92.5857, loss: 4.5569
	step 12737:lm_loss: 4.5281, ppl: 92.5852, loss: 4.5569
	step 12738:lm_loss: 4.5282, ppl: 92.5874, loss: 4.5569
	step 12739:lm_loss: 4.5282, ppl: 92.5871, loss: 4.5569
	step 12740:lm_loss: 4.5281, ppl: 92.5856, loss: 4.5569
	step 12741:lm_loss: 4.5282, ppl: 92.5910, loss: 4.5569
	step 12742:lm_loss: 4.5282, ppl: 92.5903, loss: 4.5569
	step 12743:lm_loss: 4.5281, ppl: 92.5861, loss: 4.5569
	step 12744:lm_loss: 4.5281, ppl: 92.5839, loss: 4.5569
	step 12745:lm_loss: 4.5281, ppl: 92.5843, loss: 4.5569
	step 12746:lm_loss: 4.5281, ppl: 92.5809, loss: 4.5568
	step 12747:lm_loss: 4.5281, ppl: 92.5828, loss: 4.5568
	step 12748:lm_loss: 4.5282, ppl: 92.5885, loss: 4.5569
	step 12749:lm_loss: 4.5282, ppl: 92.5882, loss: 4.5569
	step 12750:lm_loss: 4.5282, ppl: 92.5872, loss: 4.5569
	step 12751:lm_loss: 4.5281, ppl: 92.5857, loss: 4.5569
	step 12752:lm_loss: 4.5282, ppl: 92.5878, loss: 4.5569
	step 12753:lm_loss: 4.5282, ppl: 92.5883, loss: 4.5569
	step 12754:lm_loss: 4.5282, ppl: 92.5892, loss: 4.5569
	step 12755:lm_loss: 4.5281, ppl: 92.5804, loss: 4.5568
	step 12756:lm_loss: 4.5281, ppl: 92.5797, loss: 4.5568
	step 12757:lm_loss: 4.5281, ppl: 92.5797, loss: 4.5568
	step 12758:lm_loss: 4.5280, ppl: 92.5709, loss: 4.5567
	step 12759:lm_loss: 4.5280, ppl: 92.5691, loss: 4.5567
	step 12760:lm_loss: 4.5279, ppl: 92.5653, loss: 4.5566
	step 12761:lm_loss: 4.5279, ppl: 92.5628, loss: 4.5566
	step 12762:lm_loss: 4.5280, ppl: 92.5743, loss: 4.5567
	step 12763:lm_loss: 4.5280, ppl: 92.5698, loss: 4.5566
	step 12764:lm_loss: 4.5279, ppl: 92.5640, loss: 4.5565
	step 12765:lm_loss: 4.5279, ppl: 92.5641, loss: 4.5565
	step 12766:lm_loss: 4.5279, ppl: 92.5646, loss: 4.5565
	step 12767:lm_loss: 4.5279, ppl: 92.5649, loss: 4.5565
	step 12768:lm_loss: 4.5280, ppl: 92.5709, loss: 4.5567
	step 12769:lm_loss: 4.5278, ppl: 92.5567, loss: 4.5565
	step 12770:lm_loss: 4.5278, ppl: 92.5584, loss: 4.5565
	step 12771:lm_loss: 4.5278, ppl: 92.5532, loss: 4.5565
	step 12772:lm_loss: 4.5278, ppl: 92.5554, loss: 4.5565
	step 12773:lm_loss: 4.5279, ppl: 92.5632, loss: 4.5566
	step 12774:lm_loss: 4.5279, ppl: 92.5643, loss: 4.5566
	step 12775:lm_loss: 4.5279, ppl: 92.5649, loss: 4.5566
	step 12776:lm_loss: 4.5279, ppl: 92.5683, loss: 4.5567
	step 12777:lm_loss: 4.5280, ppl: 92.5697, loss: 4.5567
	step 12778:lm_loss: 4.5279, ppl: 92.5667, loss: 4.5567
	step 12779:lm_loss: 4.5279, ppl: 92.5656, loss: 4.5566
	step 12780:lm_loss: 4.5280, ppl: 92.5745, loss: 4.5568
	step 12781:lm_loss: 4.5280, ppl: 92.5751, loss: 4.5568
	step 12782:lm_loss: 4.5280, ppl: 92.5753, loss: 4.5568
	step 12783:lm_loss: 4.5280, ppl: 92.5750, loss: 4.5568
	step 12784:lm_loss: 4.5280, ppl: 92.5718, loss: 4.5567
	step 12785:lm_loss: 4.5280, ppl: 92.5724, loss: 4.5567
	step 12786:lm_loss: 4.5280, ppl: 92.5724, loss: 4.5567
	step 12787:lm_loss: 4.5280, ppl: 92.5698, loss: 4.5567
	step 12788:lm_loss: 4.5280, ppl: 92.5709, loss: 4.5567
	step 12789:lm_loss: 4.5280, ppl: 92.5735, loss: 4.5567
	step 12790:lm_loss: 4.5280, ppl: 92.5708, loss: 4.5567
	step 12791:lm_loss: 4.5280, ppl: 92.5740, loss: 4.5567
	step 12792:lm_loss: 4.5281, ppl: 92.5810, loss: 4.5568
	step 12793:lm_loss: 4.5281, ppl: 92.5814, loss: 4.5568
	step 12794:lm_loss: 4.5281, ppl: 92.5860, loss: 4.5569
	step 12795:lm_loss: 4.5282, ppl: 92.5956, loss: 4.5570
	step 12796:lm_loss: 4.5282, ppl: 92.5908, loss: 4.5570
	step 12797:lm_loss: 4.5282, ppl: 92.5949, loss: 4.5570
	step 12798:lm_loss: 4.5283, ppl: 92.5973, loss: 4.5570
	step 12799:lm_loss: 4.5283, ppl: 92.6016, loss: 4.5571
	step 12800:lm_loss: 4.5283, ppl: 92.5970, loss: 4.5570
	step 12801:lm_loss: 4.5282, ppl: 92.5884, loss: 4.5569
	step 12802:lm_loss: 4.5282, ppl: 92.5915, loss: 4.5569
	step 12803:lm_loss: 4.5282, ppl: 92.5938, loss: 4.5569
	step 12804:lm_loss: 4.5282, ppl: 92.5929, loss: 4.5569
	step 12805:lm_loss: 4.5282, ppl: 92.5894, loss: 4.5569
	step 12806:lm_loss: 4.5282, ppl: 92.5924, loss: 4.5569
	step 12807:lm_loss: 4.5281, ppl: 92.5869, loss: 4.5568
	step 12808:lm_loss: 4.5281, ppl: 92.5847, loss: 4.5568
	step 12809:lm_loss: 4.5280, ppl: 92.5773, loss: 4.5567
	step 12810:lm_loss: 4.5281, ppl: 92.5793, loss: 4.5568
	step 12811:lm_loss: 4.5281, ppl: 92.5790, loss: 4.5567
	step 12812:lm_loss: 4.5281, ppl: 92.5784, loss: 4.5567
	step 12813:lm_loss: 4.5281, ppl: 92.5815, loss: 4.5568
	step 12814:lm_loss: 4.5281, ppl: 92.5790, loss: 4.5567
	step 12815:lm_loss: 4.5280, ppl: 92.5713, loss: 4.5567
	step 12816:lm_loss: 4.5280, ppl: 92.5733, loss: 4.5567
	step 12817:lm_loss: 4.5280, ppl: 92.5702, loss: 4.5566
	step 12818:lm_loss: 4.5280, ppl: 92.5738, loss: 4.5567
	step 12819:lm_loss: 4.5280, ppl: 92.5719, loss: 4.5566
	step 12820:lm_loss: 4.5280, ppl: 92.5729, loss: 4.5567
	step 12821:lm_loss: 4.5281, ppl: 92.5820, loss: 4.5567
	step 12822:lm_loss: 4.5280, ppl: 92.5749, loss: 4.5566
	step 12823:lm_loss: 4.5280, ppl: 92.5719, loss: 4.5566
	step 12824:lm_loss: 4.5280, ppl: 92.5775, loss: 4.5566
	step 12825:lm_loss: 4.5281, ppl: 92.5816, loss: 4.5567
	step 12826:lm_loss: 4.5281, ppl: 92.5785, loss: 4.5566
	step 12827:lm_loss: 4.5280, ppl: 92.5737, loss: 4.5566
	step 12828:lm_loss: 4.5279, ppl: 92.5686, loss: 4.5565
	step 12829:lm_loss: 4.5280, ppl: 92.5695, loss: 4.5565
	step 12830:lm_loss: 4.5280, ppl: 92.5715, loss: 4.5566
	step 12831:lm_loss: 4.5280, ppl: 92.5746, loss: 4.5566
	step 12832:lm_loss: 4.5280, ppl: 92.5738, loss: 4.5566
	step 12833:lm_loss: 4.5280, ppl: 92.5763, loss: 4.5566
	step 12834:lm_loss: 4.5279, ppl: 92.5648, loss: 4.5564
	step 12835:lm_loss: 4.5279, ppl: 92.5630, loss: 4.5564
	step 12836:lm_loss: 4.5279, ppl: 92.5596, loss: 4.5564
	step 12837:lm_loss: 4.5279, ppl: 92.5595, loss: 4.5564
	step 12838:lm_loss: 4.5279, ppl: 92.5640, loss: 4.5564
	step 12839:lm_loss: 4.5279, ppl: 92.5643, loss: 4.5564
	step 12840:lm_loss: 4.5279, ppl: 92.5636, loss: 4.5564
	step 12841:lm_loss: 4.5278, ppl: 92.5572, loss: 4.5563
	step 12842:lm_loss: 4.5278, ppl: 92.5589, loss: 4.5564
	step 12843:lm_loss: 4.5280, ppl: 92.5691, loss: 4.5565
	step 12844:lm_loss: 4.5280, ppl: 92.5699, loss: 4.5565
	step 12845:lm_loss: 4.5280, ppl: 92.5688, loss: 4.5565
	step 12846:lm_loss: 4.5279, ppl: 92.5682, loss: 4.5565
	step 12847:lm_loss: 4.5280, ppl: 92.5700, loss: 4.5565
	step 12848:lm_loss: 4.5279, ppl: 92.5682, loss: 4.5565
	step 12849:lm_loss: 4.5280, ppl: 92.5764, loss: 4.5566
	step 12850:lm_loss: 4.5281, ppl: 92.5803, loss: 4.5566
	step 12851:lm_loss: 4.5281, ppl: 92.5818, loss: 4.5567
	step 12852:lm_loss: 4.5281, ppl: 92.5813, loss: 4.5566
	step 12853:lm_loss: 4.5281, ppl: 92.5835, loss: 4.5567
	step 12854:lm_loss: 4.5281, ppl: 92.5825, loss: 4.5567
	step 12855:lm_loss: 4.5282, ppl: 92.5898, loss: 4.5568
	step 12856:lm_loss: 4.5282, ppl: 92.5907, loss: 4.5568
	step 12857:lm_loss: 4.5282, ppl: 92.5921, loss: 4.5568
	step 12858:lm_loss: 4.5282, ppl: 92.5925, loss: 4.5568
	step 12859:lm_loss: 4.5282, ppl: 92.5899, loss: 4.5567
	step 12860:lm_loss: 4.5284, ppl: 92.6076, loss: 4.5569
	step 12861:lm_loss: 4.5284, ppl: 92.6110, loss: 4.5569
	step 12862:lm_loss: 4.5284, ppl: 92.6078, loss: 4.5569
	step 12863:lm_loss: 4.5284, ppl: 92.6079, loss: 4.5569
	step 12864:lm_loss: 4.5284, ppl: 92.6074, loss: 4.5569
	step 12865:lm_loss: 4.5283, ppl: 92.6022, loss: 4.5568
	step 12866:lm_loss: 4.5283, ppl: 92.5977, loss: 4.5568
	step 12867:lm_loss: 4.5283, ppl: 92.6037, loss: 4.5569
	step 12868:lm_loss: 4.5284, ppl: 92.6112, loss: 4.5569
	step 12869:lm_loss: 4.5284, ppl: 92.6092, loss: 4.5569
	step 12870:lm_loss: 4.5284, ppl: 92.6064, loss: 4.5569
	step 12871:lm_loss: 4.5283, ppl: 92.6020, loss: 4.5568
	step 12872:lm_loss: 4.5283, ppl: 92.6035, loss: 4.5569
	step 12873:lm_loss: 4.5284, ppl: 92.6064, loss: 4.5569
	step 12874:lm_loss: 4.5283, ppl: 92.6045, loss: 4.5569
	step 12875:lm_loss: 4.5283, ppl: 92.6055, loss: 4.5569
	step 12876:lm_loss: 4.5284, ppl: 92.6060, loss: 4.5569
	step 12877:lm_loss: 4.5284, ppl: 92.6084, loss: 4.5569
	step 12878:lm_loss: 4.5283, ppl: 92.6048, loss: 4.5568
	step 12879:lm_loss: 4.5284, ppl: 92.6112, loss: 4.5569
	step 12880:lm_loss: 4.5284, ppl: 92.6089, loss: 4.5568
	step 12881:lm_loss: 4.5284, ppl: 92.6100, loss: 4.5569
	step 12882:lm_loss: 4.5284, ppl: 92.6066, loss: 4.5568
	step 12883:lm_loss: 4.5284, ppl: 92.6114, loss: 4.5569
	step 12884:lm_loss: 4.5285, ppl: 92.6150, loss: 4.5569
	step 12885:lm_loss: 4.5284, ppl: 92.6090, loss: 4.5568
	step 12886:lm_loss: 4.5283, ppl: 92.5979, loss: 4.5568
	step 12887:lm_loss: 4.5283, ppl: 92.6035, loss: 4.5568
	step 12888:lm_loss: 4.5283, ppl: 92.6033, loss: 4.5568
	step 12889:lm_loss: 4.5284, ppl: 92.6071, loss: 4.5568
	step 12890:lm_loss: 4.5284, ppl: 92.6058, loss: 4.5568
	step 12891:lm_loss: 4.5284, ppl: 92.6097, loss: 4.5569
	step 12892:lm_loss: 4.5284, ppl: 92.6128, loss: 4.5569
	step 12893:lm_loss: 4.5284, ppl: 92.6130, loss: 4.5569
	step 12894:lm_loss: 4.5284, ppl: 92.6131, loss: 4.5569
	step 12895:lm_loss: 4.5285, ppl: 92.6207, loss: 4.5570
	step 12896:lm_loss: 4.5285, ppl: 92.6188, loss: 4.5570
	step 12897:lm_loss: 4.5286, ppl: 92.6249, loss: 4.5570
	step 12898:lm_loss: 4.5285, ppl: 92.6226, loss: 4.5570
	step 12899:lm_loss: 4.5286, ppl: 92.6287, loss: 4.5570
	step 12900:lm_loss: 4.5286, ppl: 92.6311, loss: 4.5571
	step 12901:lm_loss: 4.5287, ppl: 92.6379, loss: 4.5571
	step 12902:lm_loss: 4.5286, ppl: 92.6305, loss: 4.5571
	step 12903:lm_loss: 4.5287, ppl: 92.6372, loss: 4.5571
	step 12904:lm_loss: 4.5287, ppl: 92.6383, loss: 4.5571
	step 12905:lm_loss: 4.5287, ppl: 92.6342, loss: 4.5571
	step 12906:lm_loss: 4.5287, ppl: 92.6367, loss: 4.5571
	step 12907:lm_loss: 4.5288, ppl: 92.6432, loss: 4.5572
	step 12908:lm_loss: 4.5287, ppl: 92.6338, loss: 4.5571
	step 12909:lm_loss: 4.5287, ppl: 92.6394, loss: 4.5572
	step 12910:lm_loss: 4.5287, ppl: 92.6392, loss: 4.5572
	step 12911:lm_loss: 4.5287, ppl: 92.6422, loss: 4.5572
	step 12912:lm_loss: 4.5287, ppl: 92.6424, loss: 4.5572
	step 12913:lm_loss: 4.5288, ppl: 92.6439, loss: 4.5572
	step 12914:lm_loss: 4.5288, ppl: 92.6456, loss: 4.5572
	step 12915:lm_loss: 4.5287, ppl: 92.6422, loss: 4.5572
	step 12916:lm_loss: 4.5288, ppl: 92.6497, loss: 4.5573
	step 12917:lm_loss: 4.5288, ppl: 92.6449, loss: 4.5572
	step 12918:lm_loss: 4.5288, ppl: 92.6469, loss: 4.5572
	step 12919:lm_loss: 4.5288, ppl: 92.6493, loss: 4.5573
	step 12920:lm_loss: 4.5288, ppl: 92.6512, loss: 4.5573
	step 12921:lm_loss: 4.5288, ppl: 92.6512, loss: 4.5573
	step 12922:lm_loss: 4.5288, ppl: 92.6481, loss: 4.5573
	step 12923:lm_loss: 4.5288, ppl: 92.6457, loss: 4.5572
	step 12924:lm_loss: 4.5288, ppl: 92.6450, loss: 4.5572
	step 12925:lm_loss: 4.5289, ppl: 92.6533, loss: 4.5573
	step 12926:lm_loss: 4.5289, ppl: 92.6533, loss: 4.5573
	step 12927:lm_loss: 4.5289, ppl: 92.6577, loss: 4.5573
	step 12928:lm_loss: 4.5289, ppl: 92.6584, loss: 4.5573
	step 12929:lm_loss: 4.5290, ppl: 92.6638, loss: 4.5574
	step 12930:lm_loss: 4.5290, ppl: 92.6631, loss: 4.5574
	step 12931:lm_loss: 4.5290, ppl: 92.6626, loss: 4.5573
	step 12932:lm_loss: 4.5290, ppl: 92.6639, loss: 4.5574
	step 12933:lm_loss: 4.5290, ppl: 92.6639, loss: 4.5574
	step 12934:lm_loss: 4.5290, ppl: 92.6622, loss: 4.5573
	step 12935:lm_loss: 4.5289, ppl: 92.6610, loss: 4.5573
	step 12936:lm_loss: 4.5290, ppl: 92.6614, loss: 4.5573
	step 12937:lm_loss: 4.5290, ppl: 92.6633, loss: 4.5574
	step 12938:lm_loss: 4.5289, ppl: 92.6607, loss: 4.5573
	step 12939:lm_loss: 4.5289, ppl: 92.6599, loss: 4.5573
	step 12940:lm_loss: 4.5289, ppl: 92.6576, loss: 4.5573
	step 12941:lm_loss: 4.5290, ppl: 92.6630, loss: 4.5573
	step 12942:lm_loss: 4.5289, ppl: 92.6567, loss: 4.5573
	step 12943:lm_loss: 4.5289, ppl: 92.6577, loss: 4.5573
	step 12944:lm_loss: 4.5289, ppl: 92.6528, loss: 4.5572
	step 12945:lm_loss: 4.5288, ppl: 92.6516, loss: 4.5572
	step 12946:lm_loss: 4.5290, ppl: 92.6628, loss: 4.5573
	step 12947:lm_loss: 4.5290, ppl: 92.6659, loss: 4.5574
	step 12948:lm_loss: 4.5289, ppl: 92.6610, loss: 4.5573
	step 12949:lm_loss: 4.5289, ppl: 92.6569, loss: 4.5573
	step 12950:lm_loss: 4.5290, ppl: 92.6658, loss: 4.5574
	step 12951:lm_loss: 4.5290, ppl: 92.6647, loss: 4.5574
	step 12952:lm_loss: 4.5290, ppl: 92.6653, loss: 4.5574
	step 12953:lm_loss: 4.5291, ppl: 92.6753, loss: 4.5575
	step 12954:lm_loss: 4.5291, ppl: 92.6734, loss: 4.5575
	step 12955:lm_loss: 4.5290, ppl: 92.6694, loss: 4.5574
	step 12956:lm_loss: 4.5291, ppl: 92.6718, loss: 4.5574
	step 12957:lm_loss: 4.5290, ppl: 92.6666, loss: 4.5574
	step 12958:lm_loss: 4.5289, ppl: 92.6601, loss: 4.5573
	step 12959:lm_loss: 4.5290, ppl: 92.6652, loss: 4.5573
	step 12960:lm_loss: 4.5290, ppl: 92.6618, loss: 4.5573
	step 12961:lm_loss: 4.5290, ppl: 92.6652, loss: 4.5573
	step 12962:lm_loss: 4.5290, ppl: 92.6662, loss: 4.5573
	step 12963:lm_loss: 4.5289, ppl: 92.6573, loss: 4.5573
	step 12964:lm_loss: 4.5289, ppl: 92.6554, loss: 4.5573
	step 12965:lm_loss: 4.5288, ppl: 92.6488, loss: 4.5572
	step 12966:lm_loss: 4.5288, ppl: 92.6496, loss: 4.5572
	step 12967:lm_loss: 4.5288, ppl: 92.6514, loss: 4.5572
	step 12968:lm_loss: 4.5289, ppl: 92.6586, loss: 4.5573
	step 12969:lm_loss: 4.5289, ppl: 92.6531, loss: 4.5572
	step 12970:lm_loss: 4.5288, ppl: 92.6515, loss: 4.5572
	step 12971:lm_loss: 4.5287, ppl: 92.6403, loss: 4.5571
	step 12972:lm_loss: 4.5287, ppl: 92.6399, loss: 4.5571
	step 12973:lm_loss: 4.5287, ppl: 92.6411, loss: 4.5571
	step 12974:lm_loss: 4.5288, ppl: 92.6448, loss: 4.5572
	step 12975:lm_loss: 4.5288, ppl: 92.6508, loss: 4.5572
	step 12976:lm_loss: 4.5289, ppl: 92.6538, loss: 4.5573
	step 12977:lm_loss: 4.5289, ppl: 92.6564, loss: 4.5573
	step 12978:lm_loss: 4.5289, ppl: 92.6575, loss: 4.5573
	step 12979:lm_loss: 4.5289, ppl: 92.6553, loss: 4.5573
	step 12980:lm_loss: 4.5289, ppl: 92.6527, loss: 4.5572
	step 12981:lm_loss: 4.5288, ppl: 92.6514, loss: 4.5572
	step 12982:lm_loss: 4.5289, ppl: 92.6539, loss: 4.5572
	step 12983:lm_loss: 4.5288, ppl: 92.6502, loss: 4.5572
	step 12984:lm_loss: 4.5288, ppl: 92.6450, loss: 4.5571
	step 12985:lm_loss: 4.5288, ppl: 92.6429, loss: 4.5571
	step 12986:lm_loss: 4.5284, ppl: 92.6085, loss: 4.5570
	step 12987:lm_loss: 4.5284, ppl: 92.6102, loss: 4.5570
	step 12988:lm_loss: 4.5284, ppl: 92.6061, loss: 4.5570
	step 12989:lm_loss: 4.5282, ppl: 92.5914, loss: 4.5569
	step 12990:lm_loss: 4.5283, ppl: 92.5976, loss: 4.5569
	step 12991:lm_loss: 4.5283, ppl: 92.5977, loss: 4.5569
	step 12992:lm_loss: 4.5282, ppl: 92.5960, loss: 4.5569
	step 12993:lm_loss: 4.5282, ppl: 92.5942, loss: 4.5569
	step 12994:lm_loss: 4.5282, ppl: 92.5912, loss: 4.5568
	step 12995:lm_loss: 4.5282, ppl: 92.5892, loss: 4.5568
	step 12996:lm_loss: 4.5282, ppl: 92.5939, loss: 4.5569
	step 12997:lm_loss: 4.5282, ppl: 92.5930, loss: 4.5569
	step 12998:lm_loss: 4.5283, ppl: 92.5968, loss: 4.5569
	step 12999:lm_loss: 4.5281, ppl: 92.5823, loss: 4.5568
	step 13000:lm_loss: 4.5281, ppl: 92.5817, loss: 4.5568
	step 13001:lm_loss: 4.5281, ppl: 92.5853, loss: 4.5568
	step 13002:lm_loss: 4.5282, ppl: 92.5877, loss: 4.5569
	step 13003:lm_loss: 4.5282, ppl: 92.5873, loss: 4.5568
	step 13004:lm_loss: 4.5281, ppl: 92.5851, loss: 4.5568
	step 13005:lm_loss: 4.5281, ppl: 92.5779, loss: 4.5568
	step 13006:lm_loss: 4.5282, ppl: 92.5897, loss: 4.5569
	step 13007:lm_loss: 4.5282, ppl: 92.5895, loss: 4.5569
	step 13008:lm_loss: 4.5283, ppl: 92.6017, loss: 4.5570
	step 13009:lm_loss: 4.5283, ppl: 92.6009, loss: 4.5570
	step 13010:lm_loss: 4.5283, ppl: 92.5972, loss: 4.5569
	step 13011:lm_loss: 4.5283, ppl: 92.5991, loss: 4.5570
	step 13012:lm_loss: 4.5283, ppl: 92.6011, loss: 4.5570
	step 13013:lm_loss: 4.5284, ppl: 92.6098, loss: 4.5570
	step 13014:lm_loss: 4.5284, ppl: 92.6058, loss: 4.5570
	step 13015:lm_loss: 4.5283, ppl: 92.6046, loss: 4.5570
	step 13016:lm_loss: 4.5283, ppl: 92.5996, loss: 4.5569
	step 13017:lm_loss: 4.5282, ppl: 92.5873, loss: 4.5568
	step 13018:lm_loss: 4.5282, ppl: 92.5922, loss: 4.5568
	step 13019:lm_loss: 4.5282, ppl: 92.5962, loss: 4.5569
	step 13020:lm_loss: 4.5282, ppl: 92.5953, loss: 4.5569
	step 13021:lm_loss: 4.5281, ppl: 92.5853, loss: 4.5568
	step 13022:lm_loss: 4.5282, ppl: 92.5896, loss: 4.5568
	step 13023:lm_loss: 4.5282, ppl: 92.5926, loss: 4.5569
	step 13024:lm_loss: 4.5282, ppl: 92.5913, loss: 4.5568
	step 13025:lm_loss: 4.5281, ppl: 92.5852, loss: 4.5568
	step 13026:lm_loss: 4.5281, ppl: 92.5840, loss: 4.5568
	step 13027:lm_loss: 4.5281, ppl: 92.5861, loss: 4.5568
	step 13028:lm_loss: 4.5283, ppl: 92.5982, loss: 4.5569
	step 13029:lm_loss: 4.5283, ppl: 92.5990, loss: 4.5569
	step 13030:lm_loss: 4.5282, ppl: 92.5941, loss: 4.5569
	step 13031:lm_loss: 4.5282, ppl: 92.5914, loss: 4.5569
	step 13032:lm_loss: 4.5282, ppl: 92.5894, loss: 4.5568
	step 13033:lm_loss: 4.5281, ppl: 92.5836, loss: 4.5568
	step 13034:lm_loss: 4.5281, ppl: 92.5853, loss: 4.5568
	step 13035:lm_loss: 4.5282, ppl: 92.5889, loss: 4.5568
	step 13036:lm_loss: 4.5281, ppl: 92.5779, loss: 4.5567
	step 13037:lm_loss: 4.5282, ppl: 92.5891, loss: 4.5569
	step 13038:lm_loss: 4.5281, ppl: 92.5846, loss: 4.5568
	step 13039:lm_loss: 4.5281, ppl: 92.5782, loss: 4.5568
	step 13040:lm_loss: 4.5281, ppl: 92.5797, loss: 4.5568
	step 13041:lm_loss: 4.5281, ppl: 92.5824, loss: 4.5568
	step 13042:lm_loss: 4.5281, ppl: 92.5790, loss: 4.5568
	step 13043:lm_loss: 4.5281, ppl: 92.5861, loss: 4.5569
	step 13044:lm_loss: 4.5282, ppl: 92.5917, loss: 4.5570
	step 13045:lm_loss: 4.5283, ppl: 92.6017, loss: 4.5570
	step 13046:lm_loss: 4.5283, ppl: 92.6000, loss: 4.5570
	step 13047:lm_loss: 4.5283, ppl: 92.6001, loss: 4.5570
	step 13048:lm_loss: 4.5283, ppl: 92.6035, loss: 4.5571
	step 13049:lm_loss: 4.5283, ppl: 92.6035, loss: 4.5571
	step 13050:lm_loss: 4.5283, ppl: 92.6028, loss: 4.5571
	step 13051:lm_loss: 4.5283, ppl: 92.6031, loss: 4.5571
	step 13052:lm_loss: 4.5284, ppl: 92.6063, loss: 4.5571
	step 13053:lm_loss: 4.5284, ppl: 92.6102, loss: 4.5571
	step 13054:lm_loss: 4.5282, ppl: 92.5963, loss: 4.5570
	step 13055:lm_loss: 4.5283, ppl: 92.6023, loss: 4.5571
	step 13056:lm_loss: 4.5283, ppl: 92.6009, loss: 4.5571
	step 13057:lm_loss: 4.5283, ppl: 92.5967, loss: 4.5571
	step 13058:lm_loss: 4.5281, ppl: 92.5845, loss: 4.5569
	step 13059:lm_loss: 4.5280, ppl: 92.5717, loss: 4.5568
	step 13060:lm_loss: 4.5279, ppl: 92.5672, loss: 4.5568
	step 13061:lm_loss: 4.5280, ppl: 92.5706, loss: 4.5568
	step 13062:lm_loss: 4.5281, ppl: 92.5787, loss: 4.5569
	step 13063:lm_loss: 4.5281, ppl: 92.5780, loss: 4.5569
	step 13064:lm_loss: 4.5280, ppl: 92.5776, loss: 4.5569
	step 13065:lm_loss: 4.5281, ppl: 92.5785, loss: 4.5569
	step 13066:lm_loss: 4.5280, ppl: 92.5765, loss: 4.5569
	step 13067:lm_loss: 4.5281, ppl: 92.5787, loss: 4.5569
	step 13068:lm_loss: 4.5280, ppl: 92.5773, loss: 4.5569
	step 13069:lm_loss: 4.5281, ppl: 92.5779, loss: 4.5569
	step 13070:lm_loss: 4.5280, ppl: 92.5762, loss: 4.5569
	step 13071:lm_loss: 4.5280, ppl: 92.5747, loss: 4.5568
	step 13072:lm_loss: 4.5280, ppl: 92.5715, loss: 4.5568
	step 13073:lm_loss: 4.5280, ppl: 92.5695, loss: 4.5568
	step 13074:lm_loss: 4.5280, ppl: 92.5711, loss: 4.5568
	step 13075:lm_loss: 4.5280, ppl: 92.5728, loss: 4.5568
	step 13076:lm_loss: 4.5279, ppl: 92.5679, loss: 4.5567
	step 13077:lm_loss: 4.5279, ppl: 92.5667, loss: 4.5567
	step 13078:lm_loss: 4.5280, ppl: 92.5753, loss: 4.5568
	step 13079:lm_loss: 4.5280, ppl: 92.5758, loss: 4.5568
	step 13080:lm_loss: 4.5280, ppl: 92.5771, loss: 4.5569
	step 13081:lm_loss: 4.5280, ppl: 92.5712, loss: 4.5568
	step 13082:lm_loss: 4.5280, ppl: 92.5754, loss: 4.5568
	step 13083:lm_loss: 4.5280, ppl: 92.5767, loss: 4.5568
	step 13084:lm_loss: 4.5281, ppl: 92.5847, loss: 4.5569
	step 13085:lm_loss: 4.5281, ppl: 92.5834, loss: 4.5569
	step 13086:lm_loss: 4.5280, ppl: 92.5746, loss: 4.5568
	step 13087:lm_loss: 4.5281, ppl: 92.5800, loss: 4.5569
	step 13088:lm_loss: 4.5281, ppl: 92.5804, loss: 4.5569
	step 13089:lm_loss: 4.5281, ppl: 92.5846, loss: 4.5569
	step 13090:lm_loss: 4.5281, ppl: 92.5870, loss: 4.5570
	step 13091:lm_loss: 4.5281, ppl: 92.5811, loss: 4.5569
	step 13092:lm_loss: 4.5281, ppl: 92.5865, loss: 4.5570
	step 13093:lm_loss: 4.5281, ppl: 92.5851, loss: 4.5569
	step 13094:lm_loss: 4.5282, ppl: 92.5893, loss: 4.5570
	step 13095:lm_loss: 4.5283, ppl: 92.5999, loss: 4.5571
	step 13096:lm_loss: 4.5284, ppl: 92.6067, loss: 4.5572
	step 13097:lm_loss: 4.5284, ppl: 92.6125, loss: 4.5573
	step 13098:lm_loss: 4.5284, ppl: 92.6126, loss: 4.5573
	step 13099:lm_loss: 4.5283, ppl: 92.6001, loss: 4.5572
	step 13100:lm_loss: 4.5283, ppl: 92.6036, loss: 4.5572
	step 13101:lm_loss: 4.5283, ppl: 92.6002, loss: 4.5572
	step 13102:lm_loss: 4.5282, ppl: 92.5920, loss: 4.5571
	step 13103:lm_loss: 4.5282, ppl: 92.5944, loss: 4.5571
	step 13104:lm_loss: 4.5282, ppl: 92.5914, loss: 4.5571
	step 13105:lm_loss: 4.5282, ppl: 92.5920, loss: 4.5571
	step 13106:lm_loss: 4.5282, ppl: 92.5878, loss: 4.5570
	step 13107:lm_loss: 4.5281, ppl: 92.5846, loss: 4.5570
	step 13108:lm_loss: 4.5282, ppl: 92.5882, loss: 4.5570
	step 13109:lm_loss: 4.5281, ppl: 92.5784, loss: 4.5569
	step 13110:lm_loss: 4.5279, ppl: 92.5596, loss: 4.5568
	step 13111:lm_loss: 4.5278, ppl: 92.5581, loss: 4.5568
	step 13112:lm_loss: 4.5279, ppl: 92.5618, loss: 4.5568
	step 13113:lm_loss: 4.5279, ppl: 92.5615, loss: 4.5568
	step 13114:lm_loss: 4.5278, ppl: 92.5591, loss: 4.5568
	step 13115:lm_loss: 4.5278, ppl: 92.5557, loss: 4.5567
	step 13116:lm_loss: 4.5278, ppl: 92.5538, loss: 4.5567
	step 13117:lm_loss: 4.5278, ppl: 92.5541, loss: 4.5567
	step 13118:lm_loss: 4.5278, ppl: 92.5530, loss: 4.5567
	step 13119:lm_loss: 4.5278, ppl: 92.5526, loss: 4.5567
	step 13120:lm_loss: 4.5278, ppl: 92.5540, loss: 4.5567
	step 13121:lm_loss: 4.5279, ppl: 92.5615, loss: 4.5568
	step 13122:lm_loss: 4.5279, ppl: 92.5611, loss: 4.5568
	step 13123:lm_loss: 4.5279, ppl: 92.5596, loss: 4.5567
	step 13124:lm_loss: 4.5279, ppl: 92.5600, loss: 4.5567
	step 13125:lm_loss: 4.5278, ppl: 92.5575, loss: 4.5567
	step 13126:lm_loss: 4.5279, ppl: 92.5649, loss: 4.5568
	step 13127:lm_loss: 4.5279, ppl: 92.5685, loss: 4.5568
	step 13128:lm_loss: 4.5279, ppl: 92.5676, loss: 4.5568
	step 13129:lm_loss: 4.5279, ppl: 92.5683, loss: 4.5568
	step 13130:lm_loss: 4.5280, ppl: 92.5719, loss: 4.5569
	step 13131:lm_loss: 4.5278, ppl: 92.5593, loss: 4.5568
	step 13132:lm_loss: 4.5277, ppl: 92.5411, loss: 4.5566
	step 13133:lm_loss: 4.5277, ppl: 92.5416, loss: 4.5566
	step 13134:lm_loss: 4.5277, ppl: 92.5411, loss: 4.5566
	step 13135:lm_loss: 4.5277, ppl: 92.5441, loss: 4.5567
	step 13136:lm_loss: 4.5277, ppl: 92.5438, loss: 4.5567
	step 13137:lm_loss: 4.5277, ppl: 92.5456, loss: 4.5567
	step 13138:lm_loss: 4.5277, ppl: 92.5480, loss: 4.5567
	step 13139:lm_loss: 4.5277, ppl: 92.5485, loss: 4.5567
	step 13140:lm_loss: 4.5277, ppl: 92.5444, loss: 4.5566
	step 13141:lm_loss: 4.5278, ppl: 92.5522, loss: 4.5568
	step 13142:lm_loss: 4.5278, ppl: 92.5530, loss: 4.5568
	step 13143:lm_loss: 4.5278, ppl: 92.5551, loss: 4.5568
	step 13144:lm_loss: 4.5278, ppl: 92.5519, loss: 4.5568
	step 13145:lm_loss: 4.5279, ppl: 92.5599, loss: 4.5568
	step 13146:lm_loss: 4.5279, ppl: 92.5633, loss: 4.5568
	step 13147:lm_loss: 4.5279, ppl: 92.5620, loss: 4.5568
	step 13148:lm_loss: 4.5278, ppl: 92.5563, loss: 4.5567
	step 13149:lm_loss: 4.5279, ppl: 92.5601, loss: 4.5568
	step 13150:lm_loss: 4.5279, ppl: 92.5615, loss: 4.5568
	step 13151:lm_loss: 4.5278, ppl: 92.5585, loss: 4.5568
	step 13152:lm_loss: 4.5279, ppl: 92.5617, loss: 4.5568
	step 13153:lm_loss: 4.5278, ppl: 92.5588, loss: 4.5568
	step 13154:lm_loss: 4.5278, ppl: 92.5576, loss: 4.5568
	step 13155:lm_loss: 4.5278, ppl: 92.5544, loss: 4.5567
	step 13156:lm_loss: 4.5278, ppl: 92.5564, loss: 4.5567
	step 13157:lm_loss: 4.5278, ppl: 92.5563, loss: 4.5567
	step 13158:lm_loss: 4.5279, ppl: 92.5609, loss: 4.5568
	step 13159:lm_loss: 4.5279, ppl: 92.5594, loss: 4.5568
	step 13160:lm_loss: 4.5279, ppl: 92.5648, loss: 4.5568
	step 13161:lm_loss: 4.5280, ppl: 92.5702, loss: 4.5569
	step 13162:lm_loss: 4.5279, ppl: 92.5654, loss: 4.5568
	step 13163:lm_loss: 4.5280, ppl: 92.5712, loss: 4.5569
	step 13164:lm_loss: 4.5281, ppl: 92.5779, loss: 4.5570
	step 13165:lm_loss: 4.5281, ppl: 92.5831, loss: 4.5570
	step 13166:lm_loss: 4.5282, ppl: 92.5879, loss: 4.5571
	step 13167:lm_loss: 4.5281, ppl: 92.5870, loss: 4.5571
	step 13168:lm_loss: 4.5281, ppl: 92.5836, loss: 4.5571
	step 13169:lm_loss: 4.5281, ppl: 92.5858, loss: 4.5571
	step 13170:lm_loss: 4.5281, ppl: 92.5845, loss: 4.5571
	step 13171:lm_loss: 4.5281, ppl: 92.5829, loss: 4.5570
	step 13172:lm_loss: 4.5281, ppl: 92.5864, loss: 4.5571
	step 13173:lm_loss: 4.5282, ppl: 92.5897, loss: 4.5571
	step 13174:lm_loss: 4.5282, ppl: 92.5919, loss: 4.5571
	step 13175:lm_loss: 4.5282, ppl: 92.5897, loss: 4.5571
	step 13176:lm_loss: 4.5282, ppl: 92.5920, loss: 4.5571
	step 13177:lm_loss: 4.5281, ppl: 92.5832, loss: 4.5570
	step 13178:lm_loss: 4.5281, ppl: 92.5809, loss: 4.5570
	step 13179:lm_loss: 4.5281, ppl: 92.5848, loss: 4.5570
	step 13180:lm_loss: 4.5281, ppl: 92.5837, loss: 4.5570
	step 13181:lm_loss: 4.5281, ppl: 92.5810, loss: 4.5570
	step 13182:lm_loss: 4.5281, ppl: 92.5830, loss: 4.5570
	step 13183:lm_loss: 4.5281, ppl: 92.5795, loss: 4.5570
	step 13184:lm_loss: 4.5281, ppl: 92.5812, loss: 4.5570
	step 13185:lm_loss: 4.5280, ppl: 92.5771, loss: 4.5569
	step 13186:lm_loss: 4.5281, ppl: 92.5788, loss: 4.5570
	step 13187:lm_loss: 4.5280, ppl: 92.5720, loss: 4.5569
	step 13188:lm_loss: 4.5280, ppl: 92.5691, loss: 4.5568
	step 13189:lm_loss: 4.5281, ppl: 92.5789, loss: 4.5569
	step 13190:lm_loss: 4.5281, ppl: 92.5786, loss: 4.5569
	step 13191:lm_loss: 4.5279, ppl: 92.5677, loss: 4.5568
	step 13192:lm_loss: 4.5278, ppl: 92.5579, loss: 4.5567
	step 13193:lm_loss: 4.5279, ppl: 92.5600, loss: 4.5567
	step 13194:lm_loss: 4.5279, ppl: 92.5614, loss: 4.5567
	step 13195:lm_loss: 4.5278, ppl: 92.5586, loss: 4.5567
	step 13196:lm_loss: 4.5279, ppl: 92.5631, loss: 4.5567
	step 13197:lm_loss: 4.5279, ppl: 92.5629, loss: 4.5567
	step 13198:lm_loss: 4.5279, ppl: 92.5617, loss: 4.5567
	step 13199:lm_loss: 4.5279, ppl: 92.5628, loss: 4.5567
	step 13200:lm_loss: 4.5278, ppl: 92.5566, loss: 4.5566
	step 13201:lm_loss: 4.5278, ppl: 92.5523, loss: 4.5566
	step 13202:lm_loss: 4.5278, ppl: 92.5542, loss: 4.5566
	step 13203:lm_loss: 4.5277, ppl: 92.5483, loss: 4.5565
	step 13204:lm_loss: 4.5278, ppl: 92.5505, loss: 4.5565
	step 13205:lm_loss: 4.5277, ppl: 92.5495, loss: 4.5565
	step 13206:lm_loss: 4.5278, ppl: 92.5537, loss: 4.5566
	step 13207:lm_loss: 4.5278, ppl: 92.5572, loss: 4.5566
	step 13208:lm_loss: 4.5279, ppl: 92.5629, loss: 4.5566
	step 13209:lm_loss: 4.5279, ppl: 92.5674, loss: 4.5567
	step 13210:lm_loss: 4.5279, ppl: 92.5647, loss: 4.5567
	step 13211:lm_loss: 4.5280, ppl: 92.5715, loss: 4.5567
	step 13212:lm_loss: 4.5280, ppl: 92.5701, loss: 4.5567
	step 13213:lm_loss: 4.5280, ppl: 92.5715, loss: 4.5567
	step 13214:lm_loss: 4.5280, ppl: 92.5710, loss: 4.5567
	step 13215:lm_loss: 4.5278, ppl: 92.5588, loss: 4.5566
	step 13216:lm_loss: 4.5278, ppl: 92.5592, loss: 4.5566
	step 13217:lm_loss: 4.5279, ppl: 92.5631, loss: 4.5566
	step 13218:lm_loss: 4.5279, ppl: 92.5680, loss: 4.5567
	step 13219:lm_loss: 4.5279, ppl: 92.5660, loss: 4.5567
	step 13220:lm_loss: 4.5279, ppl: 92.5650, loss: 4.5566
	step 13221:lm_loss: 4.5279, ppl: 92.5649, loss: 4.5566
	step 13222:lm_loss: 4.5279, ppl: 92.5612, loss: 4.5566
	step 13223:lm_loss: 4.5279, ppl: 92.5630, loss: 4.5566
	step 13224:lm_loss: 4.5280, ppl: 92.5727, loss: 4.5567
	step 13225:lm_loss: 4.5280, ppl: 92.5744, loss: 4.5567
	step 13226:lm_loss: 4.5280, ppl: 92.5754, loss: 4.5567
	step 13227:lm_loss: 4.5281, ppl: 92.5812, loss: 4.5568
	step 13228:lm_loss: 4.5280, ppl: 92.5759, loss: 4.5567
	step 13229:lm_loss: 4.5281, ppl: 92.5789, loss: 4.5568
	step 13230:lm_loss: 4.5281, ppl: 92.5819, loss: 4.5568
	step 13231:lm_loss: 4.5281, ppl: 92.5789, loss: 4.5568
	step 13232:lm_loss: 4.5280, ppl: 92.5773, loss: 4.5567
	step 13233:lm_loss: 4.5280, ppl: 92.5754, loss: 4.5567
	step 13234:lm_loss: 4.5280, ppl: 92.5708, loss: 4.5566
	step 13235:lm_loss: 4.5279, ppl: 92.5666, loss: 4.5566
	step 13236:lm_loss: 4.5279, ppl: 92.5605, loss: 4.5565
	step 13237:lm_loss: 4.5278, ppl: 92.5587, loss: 4.5565
	step 13238:lm_loss: 4.5279, ppl: 92.5620, loss: 4.5565
	step 13239:lm_loss: 4.5279, ppl: 92.5601, loss: 4.5565
	step 13240:lm_loss: 4.5279, ppl: 92.5642, loss: 4.5566
	step 13241:lm_loss: 4.5280, ppl: 92.5695, loss: 4.5566
	step 13242:lm_loss: 4.5279, ppl: 92.5647, loss: 4.5566
	step 13243:lm_loss: 4.5279, ppl: 92.5684, loss: 4.5566
	step 13244:lm_loss: 4.5280, ppl: 92.5704, loss: 4.5566
	step 13245:lm_loss: 4.5280, ppl: 92.5731, loss: 4.5566
	step 13246:lm_loss: 4.5279, ppl: 92.5684, loss: 4.5566
	step 13247:lm_loss: 4.5280, ppl: 92.5736, loss: 4.5567
	step 13248:lm_loss: 4.5281, ppl: 92.5796, loss: 4.5567
	step 13249:lm_loss: 4.5281, ppl: 92.5852, loss: 4.5568
	step 13250:lm_loss: 4.5281, ppl: 92.5864, loss: 4.5568
	step 13251:lm_loss: 4.5282, ppl: 92.5916, loss: 4.5569
	step 13252:lm_loss: 4.5282, ppl: 92.5903, loss: 4.5568
	step 13253:lm_loss: 4.5282, ppl: 92.5960, loss: 4.5569
	step 13254:lm_loss: 4.5282, ppl: 92.5897, loss: 4.5568
	step 13255:lm_loss: 4.5282, ppl: 92.5908, loss: 4.5568
	step 13256:lm_loss: 4.5282, ppl: 92.5941, loss: 4.5569
	step 13257:lm_loss: 4.5282, ppl: 92.5960, loss: 4.5569
	step 13258:lm_loss: 4.5283, ppl: 92.6047, loss: 4.5570
	step 13259:lm_loss: 4.5284, ppl: 92.6080, loss: 4.5570
	step 13260:lm_loss: 4.5284, ppl: 92.6099, loss: 4.5571
	step 13261:lm_loss: 4.5284, ppl: 92.6128, loss: 4.5571
	step 13262:lm_loss: 4.5284, ppl: 92.6143, loss: 4.5571
	step 13263:lm_loss: 4.5285, ppl: 92.6173, loss: 4.5571
	step 13264:lm_loss: 4.5285, ppl: 92.6198, loss: 4.5572
	step 13265:lm_loss: 4.5285, ppl: 92.6169, loss: 4.5571
	step 13266:lm_loss: 4.5285, ppl: 92.6179, loss: 4.5572
	step 13267:lm_loss: 4.5284, ppl: 92.6102, loss: 4.5570
	step 13268:lm_loss: 4.5285, ppl: 92.6151, loss: 4.5571
	step 13269:lm_loss: 4.5284, ppl: 92.6123, loss: 4.5571
	step 13270:lm_loss: 4.5284, ppl: 92.6122, loss: 4.5571
	step 13271:lm_loss: 4.5285, ppl: 92.6161, loss: 4.5571
	step 13272:lm_loss: 4.5285, ppl: 92.6196, loss: 4.5571
	step 13273:lm_loss: 4.5286, ppl: 92.6253, loss: 4.5572
	step 13274:lm_loss: 4.5286, ppl: 92.6260, loss: 4.5572
	step 13275:lm_loss: 4.5286, ppl: 92.6282, loss: 4.5572
	step 13276:lm_loss: 4.5287, ppl: 92.6350, loss: 4.5573
	step 13277:lm_loss: 4.5287, ppl: 92.6387, loss: 4.5574
	step 13278:lm_loss: 4.5287, ppl: 92.6345, loss: 4.5573
	step 13279:lm_loss: 4.5287, ppl: 92.6346, loss: 4.5573
	step 13280:lm_loss: 4.5287, ppl: 92.6371, loss: 4.5573
	step 13281:lm_loss: 4.5286, ppl: 92.6266, loss: 4.5573
	step 13282:lm_loss: 4.5286, ppl: 92.6319, loss: 4.5573
	step 13283:lm_loss: 4.5287, ppl: 92.6344, loss: 4.5573
	step 13284:lm_loss: 4.5287, ppl: 92.6348, loss: 4.5573
	step 13285:lm_loss: 4.5286, ppl: 92.6331, loss: 4.5573
	step 13286:lm_loss: 4.5287, ppl: 92.6336, loss: 4.5573
	step 13287:lm_loss: 4.5287, ppl: 92.6392, loss: 4.5574
	step 13288:lm_loss: 4.5288, ppl: 92.6472, loss: 4.5574
	step 13289:lm_loss: 4.5288, ppl: 92.6461, loss: 4.5574
	step 13290:lm_loss: 4.5289, ppl: 92.6565, loss: 4.5575
	step 13291:lm_loss: 4.5288, ppl: 92.6494, loss: 4.5575
	step 13292:lm_loss: 4.5288, ppl: 92.6464, loss: 4.5574
	step 13293:lm_loss: 4.5288, ppl: 92.6448, loss: 4.5574
	step 13294:lm_loss: 4.5287, ppl: 92.6413, loss: 4.5574
	step 13295:lm_loss: 4.5287, ppl: 92.6415, loss: 4.5574
	step 13296:lm_loss: 4.5288, ppl: 92.6464, loss: 4.5574
	step 13297:lm_loss: 4.5288, ppl: 92.6481, loss: 4.5574
	step 13298:lm_loss: 4.5289, ppl: 92.6571, loss: 4.5575
	step 13299:lm_loss: 4.5290, ppl: 92.6659, loss: 4.5577
	step 13300:lm_loss: 4.5291, ppl: 92.6763, loss: 4.5577
	step 13301:lm_loss: 4.5291, ppl: 92.6735, loss: 4.5577
	step 13302:lm_loss: 4.5291, ppl: 92.6782, loss: 4.5578
	step 13303:lm_loss: 4.5291, ppl: 92.6710, loss: 4.5577
	step 13304:lm_loss: 4.5290, ppl: 92.6680, loss: 4.5576
	step 13305:lm_loss: 4.5290, ppl: 92.6702, loss: 4.5577
	step 13306:lm_loss: 4.5290, ppl: 92.6682, loss: 4.5576
	step 13307:lm_loss: 4.5291, ppl: 92.6710, loss: 4.5577
	step 13308:lm_loss: 4.5291, ppl: 92.6771, loss: 4.5577
	step 13309:lm_loss: 4.5291, ppl: 92.6754, loss: 4.5577
	step 13310:lm_loss: 4.5290, ppl: 92.6696, loss: 4.5577
	step 13311:lm_loss: 4.5290, ppl: 92.6671, loss: 4.5577
	step 13312:lm_loss: 4.5290, ppl: 92.6665, loss: 4.5577
	step 13313:lm_loss: 4.5289, ppl: 92.6584, loss: 4.5576
	step 13314:lm_loss: 4.5289, ppl: 92.6575, loss: 4.5576
	step 13315:lm_loss: 4.5289, ppl: 92.6590, loss: 4.5576
	step 13316:lm_loss: 4.5289, ppl: 92.6608, loss: 4.5576
	step 13317:lm_loss: 4.5290, ppl: 92.6656, loss: 4.5577
	step 13318:lm_loss: 4.5290, ppl: 92.6651, loss: 4.5577
	step 13319:lm_loss: 4.5289, ppl: 92.6564, loss: 4.5576
	step 13320:lm_loss: 4.5290, ppl: 92.6632, loss: 4.5577
	step 13321:lm_loss: 4.5289, ppl: 92.6559, loss: 4.5576
	step 13322:lm_loss: 4.5289, ppl: 92.6548, loss: 4.5576
	step 13323:lm_loss: 4.5289, ppl: 92.6568, loss: 4.5576
	step 13324:lm_loss: 4.5289, ppl: 92.6524, loss: 4.5576
	step 13325:lm_loss: 4.5288, ppl: 92.6485, loss: 4.5575
	step 13326:lm_loss: 4.5288, ppl: 92.6482, loss: 4.5575
	step 13327:lm_loss: 4.5288, ppl: 92.6491, loss: 4.5575
	step 13328:lm_loss: 4.5288, ppl: 92.6484, loss: 4.5575
	step 13329:lm_loss: 4.5288, ppl: 92.6493, loss: 4.5575
	step 13330:lm_loss: 4.5288, ppl: 92.6502, loss: 4.5576
	step 13331:lm_loss: 4.5289, ppl: 92.6551, loss: 4.5576
	step 13332:lm_loss: 4.5289, ppl: 92.6593, loss: 4.5577
	step 13333:lm_loss: 4.5290, ppl: 92.6613, loss: 4.5577
	step 13334:lm_loss: 4.5290, ppl: 92.6625, loss: 4.5577
	step 13335:lm_loss: 4.5290, ppl: 92.6671, loss: 4.5578
	step 13336:lm_loss: 4.5291, ppl: 92.6779, loss: 4.5578
	step 13337:lm_loss: 4.5291, ppl: 92.6757, loss: 4.5578
	step 13338:lm_loss: 4.5291, ppl: 92.6733, loss: 4.5578
	step 13339:lm_loss: 4.5290, ppl: 92.6634, loss: 4.5577
	step 13340:lm_loss: 4.5289, ppl: 92.6596, loss: 4.5576
	step 13341:lm_loss: 4.5289, ppl: 92.6577, loss: 4.5576
	step 13342:lm_loss: 4.5288, ppl: 92.6507, loss: 4.5575
	step 13343:lm_loss: 4.5288, ppl: 92.6515, loss: 4.5575
	step 13344:lm_loss: 4.5289, ppl: 92.6549, loss: 4.5575
	step 13345:lm_loss: 4.5289, ppl: 92.6538, loss: 4.5575
	step 13346:lm_loss: 4.5288, ppl: 92.6505, loss: 4.5575
	step 13347:lm_loss: 4.5288, ppl: 92.6492, loss: 4.5575
	step 13348:lm_loss: 4.5288, ppl: 92.6502, loss: 4.5575
	step 13349:lm_loss: 4.5288, ppl: 92.6489, loss: 4.5575
	step 13350:lm_loss: 4.5289, ppl: 92.6538, loss: 4.5575
	step 13351:lm_loss: 4.5288, ppl: 92.6494, loss: 4.5574
	step 13352:lm_loss: 4.5288, ppl: 92.6452, loss: 4.5574
	step 13353:lm_loss: 4.5288, ppl: 92.6447, loss: 4.5574
	step 13354:lm_loss: 4.5287, ppl: 92.6404, loss: 4.5573
	step 13355:lm_loss: 4.5287, ppl: 92.6352, loss: 4.5573
	step 13356:lm_loss: 4.5287, ppl: 92.6348, loss: 4.5573
	step 13357:lm_loss: 4.5287, ppl: 92.6393, loss: 4.5573
	step 13358:lm_loss: 4.5287, ppl: 92.6343, loss: 4.5573
	step 13359:lm_loss: 4.5287, ppl: 92.6394, loss: 4.5573
	step 13360:lm_loss: 4.5288, ppl: 92.6435, loss: 4.5574
	step 13361:lm_loss: 4.5288, ppl: 92.6508, loss: 4.5575
	step 13362:lm_loss: 4.5288, ppl: 92.6487, loss: 4.5574
	step 13363:lm_loss: 4.5289, ppl: 92.6559, loss: 4.5575
	step 13364:lm_loss: 4.5289, ppl: 92.6589, loss: 4.5576
	step 13365:lm_loss: 4.5290, ppl: 92.6620, loss: 4.5576
	step 13366:lm_loss: 4.5290, ppl: 92.6702, loss: 4.5577
	step 13367:lm_loss: 4.5290, ppl: 92.6689, loss: 4.5577
	step 13368:lm_loss: 4.5290, ppl: 92.6689, loss: 4.5577
	step 13369:lm_loss: 4.5290, ppl: 92.6661, loss: 4.5576
	step 13370:lm_loss: 4.5290, ppl: 92.6640, loss: 4.5576
	step 13371:lm_loss: 4.5291, ppl: 92.6720, loss: 4.5577
	step 13372:lm_loss: 4.5290, ppl: 92.6679, loss: 4.5576
	step 13373:lm_loss: 4.5289, ppl: 92.6600, loss: 4.5575
	step 13374:lm_loss: 4.5290, ppl: 92.6625, loss: 4.5576
	step 13375:lm_loss: 4.5290, ppl: 92.6686, loss: 4.5576
	step 13376:lm_loss: 4.5290, ppl: 92.6686, loss: 4.5576
	step 13377:lm_loss: 4.5289, ppl: 92.6604, loss: 4.5575
	step 13378:lm_loss: 4.5290, ppl: 92.6642, loss: 4.5576
	step 13379:lm_loss: 4.5290, ppl: 92.6621, loss: 4.5575
	step 13380:lm_loss: 4.5289, ppl: 92.6601, loss: 4.5575
	step 13381:lm_loss: 4.5290, ppl: 92.6625, loss: 4.5575
	step 13382:lm_loss: 4.5290, ppl: 92.6669, loss: 4.5576
	step 13383:lm_loss: 4.5290, ppl: 92.6675, loss: 4.5576
	step 13384:lm_loss: 4.5291, ppl: 92.6719, loss: 4.5576
	step 13385:lm_loss: 4.5290, ppl: 92.6703, loss: 4.5576
	step 13386:lm_loss: 4.5291, ppl: 92.6718, loss: 4.5576
	step 13387:lm_loss: 4.5291, ppl: 92.6789, loss: 4.5577
	step 13388:lm_loss: 4.5290, ppl: 92.6644, loss: 4.5576
	step 13389:lm_loss: 4.5290, ppl: 92.6633, loss: 4.5576
	step 13390:lm_loss: 4.5289, ppl: 92.6595, loss: 4.5575
	step 13391:lm_loss: 4.5289, ppl: 92.6536, loss: 4.5575
	step 13392:lm_loss: 4.5288, ppl: 92.6473, loss: 4.5574
	step 13393:lm_loss: 4.5288, ppl: 92.6436, loss: 4.5573
	step 13394:lm_loss: 4.5287, ppl: 92.6356, loss: 4.5573
	step 13395:lm_loss: 4.5286, ppl: 92.6333, loss: 4.5573
	step 13396:lm_loss: 4.5287, ppl: 92.6354, loss: 4.5573
	step 13397:lm_loss: 4.5287, ppl: 92.6357, loss: 4.5573
	step 13398:lm_loss: 4.5288, ppl: 92.6437, loss: 4.5574
	step 13399:lm_loss: 4.5287, ppl: 92.6409, loss: 4.5574
	step 13400:lm_loss: 4.5288, ppl: 92.6506, loss: 4.5575
	step 13401:lm_loss: 4.5288, ppl: 92.6427, loss: 4.5573
	step 13402:lm_loss: 4.5287, ppl: 92.6364, loss: 4.5573
	step 13403:lm_loss: 4.5287, ppl: 92.6405, loss: 4.5574
	step 13404:lm_loss: 4.5287, ppl: 92.6410, loss: 4.5574
	step 13405:lm_loss: 4.5287, ppl: 92.6374, loss: 4.5573
	step 13406:lm_loss: 4.5287, ppl: 92.6360, loss: 4.5573
	step 13407:lm_loss: 4.5286, ppl: 92.6307, loss: 4.5572
	step 13408:lm_loss: 4.5286, ppl: 92.6300, loss: 4.5572
	step 13409:lm_loss: 4.5286, ppl: 92.6328, loss: 4.5573
	step 13410:lm_loss: 4.5287, ppl: 92.6338, loss: 4.5573
	step 13411:lm_loss: 4.5286, ppl: 92.6291, loss: 4.5572
	step 13412:lm_loss: 4.5287, ppl: 92.6378, loss: 4.5573
	step 13413:lm_loss: 4.5287, ppl: 92.6355, loss: 4.5573
	step 13414:lm_loss: 4.5288, ppl: 92.6441, loss: 4.5573
	step 13415:lm_loss: 4.5288, ppl: 92.6480, loss: 4.5574
	step 13416:lm_loss: 4.5288, ppl: 92.6505, loss: 4.5574
	step 13417:lm_loss: 4.5289, ppl: 92.6578, loss: 4.5575
	step 13418:lm_loss: 4.5289, ppl: 92.6569, loss: 4.5575
	step 13419:lm_loss: 4.5289, ppl: 92.6594, loss: 4.5575
	step 13420:lm_loss: 4.5289, ppl: 92.6574, loss: 4.5575
	step 13421:lm_loss: 4.5289, ppl: 92.6596, loss: 4.5575
	step 13422:lm_loss: 4.5290, ppl: 92.6687, loss: 4.5576
	step 13423:lm_loss: 4.5290, ppl: 92.6690, loss: 4.5576
	step 13424:lm_loss: 4.5290, ppl: 92.6693, loss: 4.5576
	step 13425:lm_loss: 4.5291, ppl: 92.6762, loss: 4.5576
	step 13426:lm_loss: 4.5291, ppl: 92.6787, loss: 4.5577
	step 13427:lm_loss: 4.5291, ppl: 92.6766, loss: 4.5576
	step 13428:lm_loss: 4.5291, ppl: 92.6742, loss: 4.5576
	step 13429:lm_loss: 4.5292, ppl: 92.6835, loss: 4.5577
	step 13430:lm_loss: 4.5292, ppl: 92.6859, loss: 4.5577
	step 13431:lm_loss: 4.5292, ppl: 92.6853, loss: 4.5577
	step 13432:lm_loss: 4.5291, ppl: 92.6782, loss: 4.5576
	step 13433:lm_loss: 4.5292, ppl: 92.6819, loss: 4.5577
	step 13434:lm_loss: 4.5292, ppl: 92.6830, loss: 4.5577
	step 13435:lm_loss: 4.5291, ppl: 92.6713, loss: 4.5576
	step 13436:lm_loss: 4.5290, ppl: 92.6660, loss: 4.5576
	step 13437:lm_loss: 4.5292, ppl: 92.6819, loss: 4.5577
	step 13438:lm_loss: 4.5291, ppl: 92.6788, loss: 4.5576
	step 13439:lm_loss: 4.5291, ppl: 92.6752, loss: 4.5576
	step 13440:lm_loss: 4.5291, ppl: 92.6796, loss: 4.5576
	step 13441:lm_loss: 4.5291, ppl: 92.6785, loss: 4.5576
	step 13442:lm_loss: 4.5290, ppl: 92.6672, loss: 4.5575
	step 13443:lm_loss: 4.5289, ppl: 92.6579, loss: 4.5574
	step 13444:lm_loss: 4.5288, ppl: 92.6517, loss: 4.5573
	step 13445:lm_loss: 4.5289, ppl: 92.6533, loss: 4.5573
	step 13446:lm_loss: 4.5289, ppl: 92.6549, loss: 4.5573
	step 13447:lm_loss: 4.5289, ppl: 92.6576, loss: 4.5574
	step 13448:lm_loss: 4.5289, ppl: 92.6567, loss: 4.5573
	step 13449:lm_loss: 4.5289, ppl: 92.6531, loss: 4.5573
	step 13450:lm_loss: 4.5287, ppl: 92.6397, loss: 4.5572
	step 13451:lm_loss: 4.5287, ppl: 92.6346, loss: 4.5571
	step 13452:lm_loss: 4.5286, ppl: 92.6321, loss: 4.5571
	step 13453:lm_loss: 4.5287, ppl: 92.6364, loss: 4.5572
	step 13454:lm_loss: 4.5287, ppl: 92.6349, loss: 4.5572
	step 13455:lm_loss: 4.5287, ppl: 92.6423, loss: 4.5572
	step 13456:lm_loss: 4.5287, ppl: 92.6382, loss: 4.5572
	step 13457:lm_loss: 4.5287, ppl: 92.6349, loss: 4.5571
	step 13458:lm_loss: 4.5287, ppl: 92.6403, loss: 4.5572
	step 13459:lm_loss: 4.5286, ppl: 92.6299, loss: 4.5571
	step 13460:lm_loss: 4.5286, ppl: 92.6277, loss: 4.5570
	step 13461:lm_loss: 4.5287, ppl: 92.6337, loss: 4.5571
	step 13462:lm_loss: 4.5286, ppl: 92.6330, loss: 4.5571
	step 13463:lm_loss: 4.5287, ppl: 92.6341, loss: 4.5571
	step 13464:lm_loss: 4.5287, ppl: 92.6406, loss: 4.5571
	step 13465:lm_loss: 4.5287, ppl: 92.6401, loss: 4.5571
	step 13466:lm_loss: 4.5287, ppl: 92.6423, loss: 4.5572
	step 13467:lm_loss: 4.5287, ppl: 92.6380, loss: 4.5571
	step 13468:lm_loss: 4.5288, ppl: 92.6451, loss: 4.5572
	step 13469:lm_loss: 4.5287, ppl: 92.6416, loss: 4.5572
	step 13470:lm_loss: 4.5287, ppl: 92.6384, loss: 4.5571
	step 13471:lm_loss: 4.5287, ppl: 92.6418, loss: 4.5572
	step 13472:lm_loss: 4.5287, ppl: 92.6349, loss: 4.5571
	step 13473:lm_loss: 4.5286, ppl: 92.6293, loss: 4.5570
	step 13474:lm_loss: 4.5286, ppl: 92.6300, loss: 4.5570
	step 13475:lm_loss: 4.5286, ppl: 92.6306, loss: 4.5570
	step 13476:lm_loss: 4.5287, ppl: 92.6360, loss: 4.5571
	step 13477:lm_loss: 4.5287, ppl: 92.6357, loss: 4.5571
	step 13478:lm_loss: 4.5287, ppl: 92.6385, loss: 4.5571
	step 13479:lm_loss: 4.5288, ppl: 92.6445, loss: 4.5572
	step 13480:lm_loss: 4.5288, ppl: 92.6440, loss: 4.5572
	step 13481:lm_loss: 4.5288, ppl: 92.6494, loss: 4.5572
	step 13482:lm_loss: 4.5289, ppl: 92.6568, loss: 4.5573
	step 13483:lm_loss: 4.5289, ppl: 92.6604, loss: 4.5574
	step 13484:lm_loss: 4.5290, ppl: 92.6665, loss: 4.5574
	step 13485:lm_loss: 4.5290, ppl: 92.6657, loss: 4.5574
	step 13486:lm_loss: 4.5290, ppl: 92.6652, loss: 4.5574
	step 13487:lm_loss: 4.5290, ppl: 92.6700, loss: 4.5574
	step 13488:lm_loss: 4.5292, ppl: 92.6813, loss: 4.5575
	step 13489:lm_loss: 4.5291, ppl: 92.6780, loss: 4.5575
	step 13490:lm_loss: 4.5291, ppl: 92.6740, loss: 4.5574
	step 13491:lm_loss: 4.5291, ppl: 92.6782, loss: 4.5575
	step 13492:lm_loss: 4.5291, ppl: 92.6752, loss: 4.5574
	step 13493:lm_loss: 4.5291, ppl: 92.6776, loss: 4.5574
	step 13494:lm_loss: 4.5292, ppl: 92.6817, loss: 4.5575
	step 13495:lm_loss: 4.5292, ppl: 92.6860, loss: 4.5575
	step 13496:lm_loss: 4.5291, ppl: 92.6734, loss: 4.5574
	step 13497:lm_loss: 4.5291, ppl: 92.6731, loss: 4.5574
	step 13498:lm_loss: 4.5290, ppl: 92.6693, loss: 4.5573
	step 13499:lm_loss: 4.5291, ppl: 92.6745, loss: 4.5574
	step 13500:lm_loss: 4.5291, ppl: 92.6732, loss: 4.5574
	step 13501:lm_loss: 4.5290, ppl: 92.6699, loss: 4.5573
	step 13502:lm_loss: 4.5291, ppl: 92.6727, loss: 4.5574
	step 13503:lm_loss: 4.5291, ppl: 92.6754, loss: 4.5574
	step 13504:lm_loss: 4.5291, ppl: 92.6783, loss: 4.5574
	step 13505:lm_loss: 4.5290, ppl: 92.6671, loss: 4.5573
	step 13506:lm_loss: 4.5290, ppl: 92.6667, loss: 4.5573
	step 13507:lm_loss: 4.5289, ppl: 92.6611, loss: 4.5573
	step 13508:lm_loss: 4.5290, ppl: 92.6619, loss: 4.5573
	step 13509:lm_loss: 4.5289, ppl: 92.6584, loss: 4.5572
	step 13510:lm_loss: 4.5289, ppl: 92.6589, loss: 4.5572
	step 13511:lm_loss: 4.5290, ppl: 92.6676, loss: 4.5573
	step 13512:lm_loss: 4.5290, ppl: 92.6671, loss: 4.5573
	step 13513:lm_loss: 4.5290, ppl: 92.6663, loss: 4.5573
	step 13514:lm_loss: 4.5290, ppl: 92.6684, loss: 4.5573
	step 13515:lm_loss: 4.5291, ppl: 92.6705, loss: 4.5573
	step 13516:lm_loss: 4.5290, ppl: 92.6677, loss: 4.5573
	step 13517:lm_loss: 4.5291, ppl: 92.6706, loss: 4.5573
	step 13518:lm_loss: 4.5290, ppl: 92.6704, loss: 4.5573
	step 13519:lm_loss: 4.5290, ppl: 92.6700, loss: 4.5573
	step 13520:lm_loss: 4.5291, ppl: 92.6762, loss: 4.5574
	step 13521:lm_loss: 4.5288, ppl: 92.6501, loss: 4.5572
	step 13522:lm_loss: 4.5289, ppl: 92.6525, loss: 4.5573
	step 13523:lm_loss: 4.5289, ppl: 92.6527, loss: 4.5573
	step 13524:lm_loss: 4.5289, ppl: 92.6532, loss: 4.5573
	step 13525:lm_loss: 4.5288, ppl: 92.6519, loss: 4.5573
	step 13526:lm_loss: 4.5289, ppl: 92.6531, loss: 4.5573
	step 13527:lm_loss: 4.5289, ppl: 92.6527, loss: 4.5573
	step 13528:lm_loss: 4.5288, ppl: 92.6519, loss: 4.5573
	step 13529:lm_loss: 4.5288, ppl: 92.6494, loss: 4.5572
	step 13530:lm_loss: 4.5288, ppl: 92.6515, loss: 4.5572
	step 13531:lm_loss: 4.5288, ppl: 92.6480, loss: 4.5572
	step 13532:lm_loss: 4.5288, ppl: 92.6438, loss: 4.5572
	step 13533:lm_loss: 4.5287, ppl: 92.6424, loss: 4.5572
	step 13534:lm_loss: 4.5288, ppl: 92.6447, loss: 4.5572
	step 13535:lm_loss: 4.5288, ppl: 92.6455, loss: 4.5572
	step 13536:lm_loss: 4.5288, ppl: 92.6447, loss: 4.5572
	step 13537:lm_loss: 4.5287, ppl: 92.6384, loss: 4.5571
	step 13538:lm_loss: 4.5288, ppl: 92.6445, loss: 4.5571
	step 13539:lm_loss: 4.5288, ppl: 92.6461, loss: 4.5571
	step 13540:lm_loss: 4.5287, ppl: 92.6421, loss: 4.5571
	step 13541:lm_loss: 4.5287, ppl: 92.6421, loss: 4.5571
	step 13542:lm_loss: 4.5287, ppl: 92.6415, loss: 4.5571
	step 13543:lm_loss: 4.5287, ppl: 92.6416, loss: 4.5571
	step 13544:lm_loss: 4.5287, ppl: 92.6368, loss: 4.5570
	step 13545:lm_loss: 4.5287, ppl: 92.6338, loss: 4.5570
	step 13546:lm_loss: 4.5287, ppl: 92.6411, loss: 4.5571
	step 13547:lm_loss: 4.5288, ppl: 92.6474, loss: 4.5572
	step 13548:lm_loss: 4.5288, ppl: 92.6504, loss: 4.5572
	step 13549:lm_loss: 4.5289, ppl: 92.6529, loss: 4.5573
	step 13550:lm_loss: 4.5289, ppl: 92.6591, loss: 4.5573
	step 13551:lm_loss: 4.5289, ppl: 92.6607, loss: 4.5574
	step 13552:lm_loss: 4.5290, ppl: 92.6698, loss: 4.5575
	step 13553:lm_loss: 4.5290, ppl: 92.6646, loss: 4.5574
	step 13554:lm_loss: 4.5290, ppl: 92.6648, loss: 4.5574
	step 13555:lm_loss: 4.5291, ppl: 92.6713, loss: 4.5575
	step 13556:lm_loss: 4.5289, ppl: 92.6571, loss: 4.5574
	step 13557:lm_loss: 4.5289, ppl: 92.6555, loss: 4.5574
	step 13558:lm_loss: 4.5288, ppl: 92.6503, loss: 4.5573
	step 13559:lm_loss: 4.5289, ppl: 92.6542, loss: 4.5574
	step 13560:lm_loss: 4.5289, ppl: 92.6560, loss: 4.5574
	step 13561:lm_loss: 4.5289, ppl: 92.6610, loss: 4.5575
	step 13562:lm_loss: 4.5290, ppl: 92.6635, loss: 4.5575
	step 13563:lm_loss: 4.5289, ppl: 92.6541, loss: 4.5574
	step 13564:lm_loss: 4.5289, ppl: 92.6533, loss: 4.5574
	step 13565:lm_loss: 4.5288, ppl: 92.6479, loss: 4.5573
	step 13566:lm_loss: 4.5288, ppl: 92.6437, loss: 4.5573
	step 13567:lm_loss: 4.5288, ppl: 92.6481, loss: 4.5573
	step 13568:lm_loss: 4.5287, ppl: 92.6367, loss: 4.5572
	step 13569:lm_loss: 4.5287, ppl: 92.6355, loss: 4.5572
	step 13570:lm_loss: 4.5287, ppl: 92.6376, loss: 4.5572
	step 13571:lm_loss: 4.5286, ppl: 92.6298, loss: 4.5571
	step 13572:lm_loss: 4.5285, ppl: 92.6220, loss: 4.5571
	step 13573:lm_loss: 4.5285, ppl: 92.6229, loss: 4.5571
	step 13574:lm_loss: 4.5285, ppl: 92.6237, loss: 4.5571
	step 13575:lm_loss: 4.5285, ppl: 92.6224, loss: 4.5571
	step 13576:lm_loss: 4.5285, ppl: 92.6237, loss: 4.5571
	step 13577:lm_loss: 4.5285, ppl: 92.6230, loss: 4.5571
	step 13578:lm_loss: 4.5285, ppl: 92.6225, loss: 4.5571
	step 13579:lm_loss: 4.5285, ppl: 92.6220, loss: 4.5571
	step 13580:lm_loss: 4.5285, ppl: 92.6153, loss: 4.5570
	step 13581:lm_loss: 4.5284, ppl: 92.6115, loss: 4.5569
	step 13582:lm_loss: 4.5284, ppl: 92.6119, loss: 4.5569
	step 13583:lm_loss: 4.5285, ppl: 92.6219, loss: 4.5570
	step 13584:lm_loss: 4.5285, ppl: 92.6226, loss: 4.5570
	step 13585:lm_loss: 4.5286, ppl: 92.6298, loss: 4.5570
	step 13586:lm_loss: 4.5287, ppl: 92.6342, loss: 4.5571
	step 13587:lm_loss: 4.5286, ppl: 92.6317, loss: 4.5570
	step 13588:lm_loss: 4.5285, ppl: 92.6169, loss: 4.5569
	step 13589:lm_loss: 4.5285, ppl: 92.6176, loss: 4.5569
	step 13590:lm_loss: 4.5284, ppl: 92.6149, loss: 4.5569
	step 13591:lm_loss: 4.5284, ppl: 92.6119, loss: 4.5568
	step 13592:lm_loss: 4.5284, ppl: 92.6132, loss: 4.5569
	step 13593:lm_loss: 4.5284, ppl: 92.6136, loss: 4.5569
	step 13594:lm_loss: 4.5283, ppl: 92.6005, loss: 4.5567
	step 13595:lm_loss: 4.5283, ppl: 92.6016, loss: 4.5567
	step 13596:lm_loss: 4.5283, ppl: 92.6013, loss: 4.5567
	step 13597:lm_loss: 4.5282, ppl: 92.5928, loss: 4.5567
	step 13598:lm_loss: 4.5283, ppl: 92.6014, loss: 4.5567
	step 13599:lm_loss: 4.5282, ppl: 92.5938, loss: 4.5567
	step 13600:lm_loss: 4.5282, ppl: 92.5903, loss: 4.5566
	step 13601:lm_loss: 4.5282, ppl: 92.5918, loss: 4.5566
	step 13602:lm_loss: 4.5283, ppl: 92.6020, loss: 4.5567
	step 13603:lm_loss: 4.5283, ppl: 92.6003, loss: 4.5567
	step 13604:lm_loss: 4.5283, ppl: 92.6053, loss: 4.5568
	step 13605:lm_loss: 4.5284, ppl: 92.6138, loss: 4.5568
	step 13606:lm_loss: 4.5285, ppl: 92.6199, loss: 4.5569
	step 13607:lm_loss: 4.5285, ppl: 92.6153, loss: 4.5568
	step 13608:lm_loss: 4.5284, ppl: 92.6131, loss: 4.5568
	step 13609:lm_loss: 4.5285, ppl: 92.6157, loss: 4.5568
	step 13610:lm_loss: 4.5285, ppl: 92.6216, loss: 4.5569
	step 13611:lm_loss: 4.5285, ppl: 92.6222, loss: 4.5569
	step 13612:lm_loss: 4.5285, ppl: 92.6234, loss: 4.5569
	step 13613:lm_loss: 4.5285, ppl: 92.6150, loss: 4.5568
	step 13614:lm_loss: 4.5284, ppl: 92.6140, loss: 4.5568
	step 13615:lm_loss: 4.5285, ppl: 92.6214, loss: 4.5569
	step 13616:lm_loss: 4.5285, ppl: 92.6194, loss: 4.5569
	step 13617:lm_loss: 4.5285, ppl: 92.6189, loss: 4.5569
	step 13618:lm_loss: 4.5286, ppl: 92.6262, loss: 4.5570
	step 13619:lm_loss: 4.5285, ppl: 92.6232, loss: 4.5569
	step 13620:lm_loss: 4.5286, ppl: 92.6269, loss: 4.5570
	step 13621:lm_loss: 4.5286, ppl: 92.6275, loss: 4.5570
	step 13622:lm_loss: 4.5286, ppl: 92.6330, loss: 4.5570
	step 13623:lm_loss: 4.5285, ppl: 92.6177, loss: 4.5569
	step 13624:lm_loss: 4.5283, ppl: 92.6018, loss: 4.5568
	step 13625:lm_loss: 4.5284, ppl: 92.6124, loss: 4.5569
	step 13626:lm_loss: 4.5284, ppl: 92.6114, loss: 4.5569
	step 13627:lm_loss: 4.5284, ppl: 92.6132, loss: 4.5569
	step 13628:lm_loss: 4.5285, ppl: 92.6177, loss: 4.5569
	step 13629:lm_loss: 4.5285, ppl: 92.6191, loss: 4.5569
	step 13630:lm_loss: 4.5285, ppl: 92.6201, loss: 4.5570
	step 13631:lm_loss: 4.5285, ppl: 92.6234, loss: 4.5570
	step 13632:lm_loss: 4.5285, ppl: 92.6231, loss: 4.5570
	step 13633:lm_loss: 4.5283, ppl: 92.5986, loss: 4.5569
	step 13634:lm_loss: 4.5283, ppl: 92.6001, loss: 4.5569
	step 13635:lm_loss: 4.5283, ppl: 92.5995, loss: 4.5569
	step 13636:lm_loss: 4.5282, ppl: 92.5961, loss: 4.5569
	step 13637:lm_loss: 4.5283, ppl: 92.6032, loss: 4.5569
	step 13638:lm_loss: 4.5283, ppl: 92.6032, loss: 4.5569
	step 13639:lm_loss: 4.5283, ppl: 92.6051, loss: 4.5569
	step 13640:lm_loss: 4.5283, ppl: 92.6025, loss: 4.5569
	step 13641:lm_loss: 4.5283, ppl: 92.6035, loss: 4.5569
	step 13642:lm_loss: 4.5283, ppl: 92.6012, loss: 4.5569
	step 13643:lm_loss: 4.5282, ppl: 92.5953, loss: 4.5569
	step 13644:lm_loss: 4.5281, ppl: 92.5862, loss: 4.5567
	step 13645:lm_loss: 4.5281, ppl: 92.5787, loss: 4.5567
	step 13646:lm_loss: 4.5281, ppl: 92.5815, loss: 4.5567
	step 13647:lm_loss: 4.5280, ppl: 92.5762, loss: 4.5566
	step 13648:lm_loss: 4.5280, ppl: 92.5755, loss: 4.5566
	step 13649:lm_loss: 4.5279, ppl: 92.5683, loss: 4.5565
	step 13650:lm_loss: 4.5280, ppl: 92.5768, loss: 4.5566
	step 13651:lm_loss: 4.5281, ppl: 92.5828, loss: 4.5567
	step 13652:lm_loss: 4.5281, ppl: 92.5841, loss: 4.5567
	step 13653:lm_loss: 4.5282, ppl: 92.5898, loss: 4.5568
	step 13654:lm_loss: 4.5282, ppl: 92.5909, loss: 4.5568
	step 13655:lm_loss: 4.5281, ppl: 92.5844, loss: 4.5567
	step 13656:lm_loss: 4.5281, ppl: 92.5836, loss: 4.5567
	step 13657:lm_loss: 4.5282, ppl: 92.5882, loss: 4.5568
	step 13658:lm_loss: 4.5282, ppl: 92.5887, loss: 4.5568
	step 13659:lm_loss: 4.5282, ppl: 92.5919, loss: 4.5568
	step 13660:lm_loss: 4.5281, ppl: 92.5836, loss: 4.5567
	step 13661:lm_loss: 4.5281, ppl: 92.5840, loss: 4.5567
	step 13662:lm_loss: 4.5281, ppl: 92.5832, loss: 4.5567
	step 13663:lm_loss: 4.5281, ppl: 92.5831, loss: 4.5567
	step 13664:lm_loss: 4.5281, ppl: 92.5828, loss: 4.5567
	step 13665:lm_loss: 4.5281, ppl: 92.5828, loss: 4.5567
	step 13666:lm_loss: 4.5281, ppl: 92.5863, loss: 4.5568
	step 13667:lm_loss: 4.5281, ppl: 92.5852, loss: 4.5567
	step 13668:lm_loss: 4.5282, ppl: 92.5883, loss: 4.5568
	step 13669:lm_loss: 4.5282, ppl: 92.5875, loss: 4.5568
	step 13670:lm_loss: 4.5281, ppl: 92.5794, loss: 4.5567
	step 13671:lm_loss: 4.5280, ppl: 92.5691, loss: 4.5566
	step 13672:lm_loss: 4.5279, ppl: 92.5609, loss: 4.5565
	step 13673:lm_loss: 4.5279, ppl: 92.5609, loss: 4.5565
	step 13674:lm_loss: 4.5277, ppl: 92.5471, loss: 4.5564
	step 13675:lm_loss: 4.5277, ppl: 92.5417, loss: 4.5563
	step 13676:lm_loss: 4.5276, ppl: 92.5374, loss: 4.5563
	step 13677:lm_loss: 4.5276, ppl: 92.5344, loss: 4.5562
	step 13678:lm_loss: 4.5275, ppl: 92.5250, loss: 4.5562
	step 13679:lm_loss: 4.5275, ppl: 92.5259, loss: 4.5562
	step 13680:lm_loss: 4.5274, ppl: 92.5203, loss: 4.5561
	step 13681:lm_loss: 4.5275, ppl: 92.5226, loss: 4.5561
	step 13682:lm_loss: 4.5275, ppl: 92.5233, loss: 4.5562
	step 13683:lm_loss: 4.5275, ppl: 92.5316, loss: 4.5562
	step 13684:lm_loss: 4.5275, ppl: 92.5305, loss: 4.5562
	step 13685:lm_loss: 4.5275, ppl: 92.5294, loss: 4.5562
	step 13686:lm_loss: 4.5275, ppl: 92.5307, loss: 4.5562
	step 13687:lm_loss: 4.5276, ppl: 92.5339, loss: 4.5563
	step 13688:lm_loss: 4.5276, ppl: 92.5358, loss: 4.5563
	step 13689:lm_loss: 4.5275, ppl: 92.5294, loss: 4.5562
	step 13690:lm_loss: 4.5275, ppl: 92.5244, loss: 4.5562
	step 13691:lm_loss: 4.5275, ppl: 92.5257, loss: 4.5562
	step 13692:lm_loss: 4.5275, ppl: 92.5293, loss: 4.5562
	step 13693:lm_loss: 4.5275, ppl: 92.5289, loss: 4.5562
	step 13694:lm_loss: 4.5276, ppl: 92.5388, loss: 4.5563
	step 13695:lm_loss: 4.5277, ppl: 92.5448, loss: 4.5563
	step 13696:lm_loss: 4.5276, ppl: 92.5369, loss: 4.5562
	step 13697:lm_loss: 4.5276, ppl: 92.5323, loss: 4.5562
	step 13698:lm_loss: 4.5277, ppl: 92.5415, loss: 4.5562
	step 13699:lm_loss: 4.5277, ppl: 92.5420, loss: 4.5563
	step 13700:lm_loss: 4.5277, ppl: 92.5410, loss: 4.5562
	step 13701:lm_loss: 4.5276, ppl: 92.5385, loss: 4.5562
	step 13702:lm_loss: 4.5276, ppl: 92.5394, loss: 4.5562
	step 13703:lm_loss: 4.5276, ppl: 92.5386, loss: 4.5562
	step 13704:lm_loss: 4.5277, ppl: 92.5443, loss: 4.5563
	step 13705:lm_loss: 4.5277, ppl: 92.5414, loss: 4.5562
	step 13706:lm_loss: 4.5276, ppl: 92.5356, loss: 4.5561
	step 13707:lm_loss: 4.5276, ppl: 92.5405, loss: 4.5562
	step 13708:lm_loss: 4.5277, ppl: 92.5439, loss: 4.5562
	step 13709:lm_loss: 4.5277, ppl: 92.5441, loss: 4.5562
	step 13710:lm_loss: 4.5276, ppl: 92.5408, loss: 4.5562
	step 13711:lm_loss: 4.5277, ppl: 92.5411, loss: 4.5562
	step 13712:lm_loss: 4.5277, ppl: 92.5434, loss: 4.5562
	step 13713:lm_loss: 4.5276, ppl: 92.5370, loss: 4.5562
	step 13714:lm_loss: 4.5276, ppl: 92.5397, loss: 4.5562
	step 13715:lm_loss: 4.5276, ppl: 92.5406, loss: 4.5562
	step 13716:lm_loss: 4.5276, ppl: 92.5366, loss: 4.5562
	step 13717:lm_loss: 4.5275, ppl: 92.5307, loss: 4.5561
	step 13718:lm_loss: 4.5275, ppl: 92.5281, loss: 4.5560
	step 13719:lm_loss: 4.5275, ppl: 92.5312, loss: 4.5561
	step 13720:lm_loss: 4.5275, ppl: 92.5277, loss: 4.5560
	step 13721:lm_loss: 4.5274, ppl: 92.5185, loss: 4.5560
	step 13722:lm_loss: 4.5274, ppl: 92.5142, loss: 4.5559
	step 13723:lm_loss: 4.5273, ppl: 92.5071, loss: 4.5558
	step 13724:lm_loss: 4.5274, ppl: 92.5137, loss: 4.5559
	step 13725:lm_loss: 4.5273, ppl: 92.5072, loss: 4.5558
	step 13726:lm_loss: 4.5273, ppl: 92.5062, loss: 4.5558
	step 13727:lm_loss: 4.5273, ppl: 92.5067, loss: 4.5558
	step 13728:lm_loss: 4.5273, ppl: 92.5057, loss: 4.5558
	step 13729:lm_loss: 4.5273, ppl: 92.5075, loss: 4.5558
	step 13730:lm_loss: 4.5272, ppl: 92.5038, loss: 4.5558
	step 13731:lm_loss: 4.5272, ppl: 92.5021, loss: 4.5558
	step 13732:lm_loss: 4.5273, ppl: 92.5039, loss: 4.5558
	step 13733:lm_loss: 4.5272, ppl: 92.4989, loss: 4.5557
	step 13734:lm_loss: 4.5273, ppl: 92.5062, loss: 4.5558
	step 13735:lm_loss: 4.5273, ppl: 92.5103, loss: 4.5558
	step 13736:lm_loss: 4.5272, ppl: 92.5028, loss: 4.5558
	step 13737:lm_loss: 4.5273, ppl: 92.5104, loss: 4.5559
	step 13738:lm_loss: 4.5272, ppl: 92.5033, loss: 4.5558
	step 13739:lm_loss: 4.5273, ppl: 92.5130, loss: 4.5558
	step 13740:lm_loss: 4.5274, ppl: 92.5147, loss: 4.5559
	step 13741:lm_loss: 4.5274, ppl: 92.5222, loss: 4.5559
	step 13742:lm_loss: 4.5273, ppl: 92.5130, loss: 4.5558
	step 13743:lm_loss: 4.5273, ppl: 92.5099, loss: 4.5558
	step 13744:lm_loss: 4.5273, ppl: 92.5069, loss: 4.5558
	step 13745:lm_loss: 4.5273, ppl: 92.5055, loss: 4.5557
	step 13746:lm_loss: 4.5273, ppl: 92.5082, loss: 4.5558
	step 13747:lm_loss: 4.5273, ppl: 92.5068, loss: 4.5557
	step 13748:lm_loss: 4.5273, ppl: 92.5107, loss: 4.5558
	step 13749:lm_loss: 4.5273, ppl: 92.5047, loss: 4.5557
	step 13750:lm_loss: 4.5273, ppl: 92.5128, loss: 4.5558
	step 13751:lm_loss: 4.5273, ppl: 92.5106, loss: 4.5558
	step 13752:lm_loss: 4.5273, ppl: 92.5088, loss: 4.5558
	step 13753:lm_loss: 4.5273, ppl: 92.5081, loss: 4.5557
	step 13754:lm_loss: 4.5272, ppl: 92.5030, loss: 4.5557
	step 13755:lm_loss: 4.5272, ppl: 92.5015, loss: 4.5557
	step 13756:lm_loss: 4.5272, ppl: 92.5014, loss: 4.5557
	step 13757:lm_loss: 4.5272, ppl: 92.5025, loss: 4.5557
	step 13758:lm_loss: 4.5272, ppl: 92.4968, loss: 4.5556
	step 13759:lm_loss: 4.5272, ppl: 92.4958, loss: 4.5556
	step 13760:lm_loss: 4.5272, ppl: 92.4960, loss: 4.5556
	step 13761:lm_loss: 4.5272, ppl: 92.4998, loss: 4.5556
	step 13762:lm_loss: 4.5273, ppl: 92.5043, loss: 4.5557
	step 13763:lm_loss: 4.5272, ppl: 92.5030, loss: 4.5557
	step 13764:lm_loss: 4.5272, ppl: 92.5034, loss: 4.5557
	step 13765:lm_loss: 4.5272, ppl: 92.4995, loss: 4.5556
	step 13766:lm_loss: 4.5273, ppl: 92.5074, loss: 4.5557
	step 13767:lm_loss: 4.5273, ppl: 92.5058, loss: 4.5557
	step 13768:lm_loss: 4.5273, ppl: 92.5096, loss: 4.5557
	step 13769:lm_loss: 4.5273, ppl: 92.5111, loss: 4.5557
	step 13770:lm_loss: 4.5273, ppl: 92.5118, loss: 4.5557
	step 13771:lm_loss: 4.5273, ppl: 92.5087, loss: 4.5557
	step 13772:lm_loss: 4.5272, ppl: 92.5037, loss: 4.5556
	step 13773:lm_loss: 4.5272, ppl: 92.4955, loss: 4.5556
	step 13774:lm_loss: 4.5272, ppl: 92.4987, loss: 4.5556
	step 13775:lm_loss: 4.5272, ppl: 92.4998, loss: 4.5556
	step 13776:lm_loss: 4.5272, ppl: 92.5038, loss: 4.5556
	step 13777:lm_loss: 4.5273, ppl: 92.5096, loss: 4.5557
	step 13778:lm_loss: 4.5273, ppl: 92.5050, loss: 4.5556
	step 13779:lm_loss: 4.5272, ppl: 92.5033, loss: 4.5556
	step 13780:lm_loss: 4.5273, ppl: 92.5067, loss: 4.5557
	step 13781:lm_loss: 4.5272, ppl: 92.5013, loss: 4.5556
	step 13782:lm_loss: 4.5272, ppl: 92.5000, loss: 4.5556
	step 13783:lm_loss: 4.5272, ppl: 92.4976, loss: 4.5556
	step 13784:lm_loss: 4.5272, ppl: 92.4991, loss: 4.5556
	step 13785:lm_loss: 4.5271, ppl: 92.4903, loss: 4.5555
	step 13786:lm_loss: 4.5271, ppl: 92.4918, loss: 4.5555
	step 13787:lm_loss: 4.5272, ppl: 92.5004, loss: 4.5555
	step 13788:lm_loss: 4.5273, ppl: 92.5049, loss: 4.5556
	step 13789:lm_loss: 4.5273, ppl: 92.5118, loss: 4.5557
	step 13790:lm_loss: 4.5274, ppl: 92.5146, loss: 4.5557
	step 13791:lm_loss: 4.5273, ppl: 92.5083, loss: 4.5556
	step 13792:lm_loss: 4.5273, ppl: 92.5085, loss: 4.5556
	step 13793:lm_loss: 4.5273, ppl: 92.5072, loss: 4.5556
	step 13794:lm_loss: 4.5273, ppl: 92.5065, loss: 4.5556
	step 13795:lm_loss: 4.5273, ppl: 92.5073, loss: 4.5556
	step 13796:lm_loss: 4.5273, ppl: 92.5057, loss: 4.5556
	step 13797:lm_loss: 4.5273, ppl: 92.5078, loss: 4.5556
	step 13798:lm_loss: 4.5273, ppl: 92.5128, loss: 4.5556
	step 13799:lm_loss: 4.5273, ppl: 92.5109, loss: 4.5556
	step 13800:lm_loss: 4.5274, ppl: 92.5178, loss: 4.5557
	step 13801:lm_loss: 4.5273, ppl: 92.5110, loss: 4.5557
	step 13802:lm_loss: 4.5274, ppl: 92.5157, loss: 4.5557
	step 13803:lm_loss: 4.5274, ppl: 92.5151, loss: 4.5557
	step 13804:lm_loss: 4.5274, ppl: 92.5173, loss: 4.5557
	step 13805:lm_loss: 4.5275, ppl: 92.5232, loss: 4.5558
	step 13806:lm_loss: 4.5274, ppl: 92.5203, loss: 4.5557
	step 13807:lm_loss: 4.5274, ppl: 92.5202, loss: 4.5557
	step 13808:lm_loss: 4.5275, ppl: 92.5262, loss: 4.5558
	step 13809:lm_loss: 4.5274, ppl: 92.5199, loss: 4.5557
	step 13810:lm_loss: 4.5274, ppl: 92.5215, loss: 4.5558
	step 13811:lm_loss: 4.5275, ppl: 92.5270, loss: 4.5558
	step 13812:lm_loss: 4.5275, ppl: 92.5239, loss: 4.5558
	step 13813:lm_loss: 4.5274, ppl: 92.5140, loss: 4.5556
	step 13814:lm_loss: 4.5273, ppl: 92.5099, loss: 4.5556
	step 13815:lm_loss: 4.5273, ppl: 92.5126, loss: 4.5556
	step 13816:lm_loss: 4.5274, ppl: 92.5183, loss: 4.5557
	step 13817:lm_loss: 4.5274, ppl: 92.5217, loss: 4.5557
	step 13818:lm_loss: 4.5274, ppl: 92.5175, loss: 4.5556
	step 13819:lm_loss: 4.5275, ppl: 92.5257, loss: 4.5558
	step 13820:lm_loss: 4.5274, ppl: 92.5212, loss: 4.5557
	step 13821:lm_loss: 4.5274, ppl: 92.5181, loss: 4.5557
	step 13822:lm_loss: 4.5274, ppl: 92.5221, loss: 4.5557
	step 13823:lm_loss: 4.5275, ppl: 92.5240, loss: 4.5557
	step 13824:lm_loss: 4.5276, ppl: 92.5333, loss: 4.5558
	step 13825:lm_loss: 4.5276, ppl: 92.5357, loss: 4.5559
	step 13826:lm_loss: 4.5276, ppl: 92.5387, loss: 4.5559
	step 13827:lm_loss: 4.5276, ppl: 92.5393, loss: 4.5559
	step 13828:lm_loss: 4.5276, ppl: 92.5336, loss: 4.5558
	step 13829:lm_loss: 4.5276, ppl: 92.5354, loss: 4.5559
	step 13830:lm_loss: 4.5276, ppl: 92.5334, loss: 4.5558
	step 13831:lm_loss: 4.5276, ppl: 92.5344, loss: 4.5559
	step 13832:lm_loss: 4.5276, ppl: 92.5334, loss: 4.5558
	step 13833:lm_loss: 4.5275, ppl: 92.5312, loss: 4.5558
	step 13834:lm_loss: 4.5276, ppl: 92.5320, loss: 4.5558
	step 13835:lm_loss: 4.5276, ppl: 92.5332, loss: 4.5558
	step 13836:lm_loss: 4.5276, ppl: 92.5383, loss: 4.5559
	step 13837:lm_loss: 4.5276, ppl: 92.5374, loss: 4.5559
	step 13838:lm_loss: 4.5276, ppl: 92.5339, loss: 4.5558
	step 13839:lm_loss: 4.5276, ppl: 92.5318, loss: 4.5558
	step 13840:lm_loss: 4.5275, ppl: 92.5303, loss: 4.5558
	step 13841:lm_loss: 4.5276, ppl: 92.5319, loss: 4.5558
	step 13842:lm_loss: 4.5275, ppl: 92.5269, loss: 4.5557
	step 13843:lm_loss: 4.5275, ppl: 92.5277, loss: 4.5557
	step 13844:lm_loss: 4.5275, ppl: 92.5246, loss: 4.5557
	step 13845:lm_loss: 4.5274, ppl: 92.5197, loss: 4.5557
	step 13846:lm_loss: 4.5274, ppl: 92.5180, loss: 4.5557
	step 13847:lm_loss: 4.5273, ppl: 92.5120, loss: 4.5556
	step 13848:lm_loss: 4.5274, ppl: 92.5143, loss: 4.5556
	step 13849:lm_loss: 4.5274, ppl: 92.5199, loss: 4.5557
	step 13850:lm_loss: 4.5275, ppl: 92.5232, loss: 4.5557
	step 13851:lm_loss: 4.5274, ppl: 92.5219, loss: 4.5557
	step 13852:lm_loss: 4.5275, ppl: 92.5231, loss: 4.5557
	step 13853:lm_loss: 4.5273, ppl: 92.5124, loss: 4.5556
	step 13854:lm_loss: 4.5273, ppl: 92.5072, loss: 4.5555
	step 13855:lm_loss: 4.5273, ppl: 92.5045, loss: 4.5555
	step 13856:lm_loss: 4.5272, ppl: 92.5036, loss: 4.5555
	step 13857:lm_loss: 4.5273, ppl: 92.5078, loss: 4.5556
	step 13858:lm_loss: 4.5273, ppl: 92.5097, loss: 4.5556
	step 13859:lm_loss: 4.5273, ppl: 92.5097, loss: 4.5556
	step 13860:lm_loss: 4.5273, ppl: 92.5091, loss: 4.5556
	step 13861:lm_loss: 4.5273, ppl: 92.5117, loss: 4.5556
	step 13862:lm_loss: 4.5274, ppl: 92.5147, loss: 4.5556
	step 13863:lm_loss: 4.5274, ppl: 92.5146, loss: 4.5556
	step 13864:lm_loss: 4.5274, ppl: 92.5133, loss: 4.5556
	step 13865:lm_loss: 4.5274, ppl: 92.5193, loss: 4.5557
	step 13866:lm_loss: 4.5274, ppl: 92.5155, loss: 4.5556
	step 13867:lm_loss: 4.5273, ppl: 92.5049, loss: 4.5555
	step 13868:lm_loss: 4.5272, ppl: 92.5025, loss: 4.5555
	step 13869:lm_loss: 4.5272, ppl: 92.5025, loss: 4.5555
	step 13870:lm_loss: 4.5272, ppl: 92.5001, loss: 4.5555
	step 13871:lm_loss: 4.5272, ppl: 92.5032, loss: 4.5555
	step 13872:lm_loss: 4.5273, ppl: 92.5061, loss: 4.5555
	step 13873:lm_loss: 4.5273, ppl: 92.5054, loss: 4.5555
	step 13874:lm_loss: 4.5273, ppl: 92.5046, loss: 4.5555
	step 13875:lm_loss: 4.5273, ppl: 92.5067, loss: 4.5555
	step 13876:lm_loss: 4.5273, ppl: 92.5095, loss: 4.5555
	step 13877:lm_loss: 4.5273, ppl: 92.5099, loss: 4.5555
	step 13878:lm_loss: 4.5273, ppl: 92.5109, loss: 4.5555
	step 13879:lm_loss: 4.5273, ppl: 92.5079, loss: 4.5555
	step 13880:lm_loss: 4.5273, ppl: 92.5076, loss: 4.5555
	step 13881:lm_loss: 4.5274, ppl: 92.5155, loss: 4.5555
	step 13882:lm_loss: 4.5274, ppl: 92.5164, loss: 4.5556
	step 13883:lm_loss: 4.5274, ppl: 92.5212, loss: 4.5556
	step 13884:lm_loss: 4.5275, ppl: 92.5280, loss: 4.5557
	step 13885:lm_loss: 4.5276, ppl: 92.5320, loss: 4.5557
	step 13886:lm_loss: 4.5275, ppl: 92.5271, loss: 4.5557
	step 13887:lm_loss: 4.5275, ppl: 92.5244, loss: 4.5557
	step 13888:lm_loss: 4.5274, ppl: 92.5199, loss: 4.5556
	step 13889:lm_loss: 4.5274, ppl: 92.5183, loss: 4.5556
	step 13890:lm_loss: 4.5274, ppl: 92.5164, loss: 4.5555
	step 13891:lm_loss: 4.5274, ppl: 92.5184, loss: 4.5556
	step 13892:lm_loss: 4.5275, ppl: 92.5228, loss: 4.5556
	step 13893:lm_loss: 4.5275, ppl: 92.5229, loss: 4.5556
	step 13894:lm_loss: 4.5274, ppl: 92.5195, loss: 4.5556
	step 13895:lm_loss: 4.5274, ppl: 92.5164, loss: 4.5555
	step 13896:lm_loss: 4.5274, ppl: 92.5140, loss: 4.5555
	step 13897:lm_loss: 4.5274, ppl: 92.5148, loss: 4.5555
	step 13898:lm_loss: 4.5274, ppl: 92.5182, loss: 4.5555
	step 13899:lm_loss: 4.5274, ppl: 92.5179, loss: 4.5555
	step 13900:lm_loss: 4.5275, ppl: 92.5230, loss: 4.5556
	step 13901:lm_loss: 4.5275, ppl: 92.5237, loss: 4.5556
	step 13902:lm_loss: 4.5275, ppl: 92.5227, loss: 4.5556
	step 13903:lm_loss: 4.5274, ppl: 92.5191, loss: 4.5555
	step 13904:lm_loss: 4.5275, ppl: 92.5241, loss: 4.5556
	step 13905:lm_loss: 4.5274, ppl: 92.5185, loss: 4.5555
	step 13906:lm_loss: 4.5274, ppl: 92.5203, loss: 4.5555
	step 13907:lm_loss: 4.5274, ppl: 92.5210, loss: 4.5555
	step 13908:lm_loss: 4.5275, ppl: 92.5231, loss: 4.5555
	step 13909:lm_loss: 4.5275, ppl: 92.5232, loss: 4.5555
	step 13910:lm_loss: 4.5274, ppl: 92.5215, loss: 4.5555
	step 13911:lm_loss: 4.5274, ppl: 92.5182, loss: 4.5555
	step 13912:lm_loss: 4.5274, ppl: 92.5145, loss: 4.5555
	step 13913:lm_loss: 4.5274, ppl: 92.5186, loss: 4.5555
	step 13914:lm_loss: 4.5275, ppl: 92.5253, loss: 4.5556
	step 13915:lm_loss: 4.5275, ppl: 92.5233, loss: 4.5556
	step 13916:lm_loss: 4.5276, ppl: 92.5379, loss: 4.5557
	step 13917:lm_loss: 4.5276, ppl: 92.5402, loss: 4.5557
	step 13918:lm_loss: 4.5277, ppl: 92.5435, loss: 4.5557
	step 13919:lm_loss: 4.5277, ppl: 92.5444, loss: 4.5558
	step 13920:lm_loss: 4.5277, ppl: 92.5482, loss: 4.5558
	step 13921:lm_loss: 4.5277, ppl: 92.5477, loss: 4.5558
	step 13922:lm_loss: 4.5277, ppl: 92.5412, loss: 4.5557
	step 13923:lm_loss: 4.5275, ppl: 92.5273, loss: 4.5556
	step 13924:lm_loss: 4.5276, ppl: 92.5364, loss: 4.5557
	step 13925:lm_loss: 4.5276, ppl: 92.5394, loss: 4.5557
	step 13926:lm_loss: 4.5277, ppl: 92.5421, loss: 4.5557
	step 13927:lm_loss: 4.5276, ppl: 92.5364, loss: 4.5556
	step 13928:lm_loss: 4.5276, ppl: 92.5378, loss: 4.5557
	step 13929:lm_loss: 4.5276, ppl: 92.5371, loss: 4.5557
	step 13930:lm_loss: 4.5276, ppl: 92.5395, loss: 4.5557
	step 13931:lm_loss: 4.5276, ppl: 92.5400, loss: 4.5557
	step 13932:lm_loss: 4.5276, ppl: 92.5394, loss: 4.5557
	step 13933:lm_loss: 4.5276, ppl: 92.5365, loss: 4.5556
	step 13934:lm_loss: 4.5276, ppl: 92.5364, loss: 4.5556
	step 13935:lm_loss: 4.5276, ppl: 92.5337, loss: 4.5556
	step 13936:lm_loss: 4.5276, ppl: 92.5364, loss: 4.5556
	step 13937:lm_loss: 4.5276, ppl: 92.5407, loss: 4.5557
	step 13938:lm_loss: 4.5275, ppl: 92.5273, loss: 4.5556
	step 13939:lm_loss: 4.5276, ppl: 92.5329, loss: 4.5557
	step 13940:lm_loss: 4.5276, ppl: 92.5363, loss: 4.5557
	step 13941:lm_loss: 4.5276, ppl: 92.5395, loss: 4.5557
	step 13942:lm_loss: 4.5276, ppl: 92.5399, loss: 4.5557
	step 13943:lm_loss: 4.5277, ppl: 92.5452, loss: 4.5558
	step 13944:lm_loss: 4.5276, ppl: 92.5398, loss: 4.5558
	step 13945:lm_loss: 4.5277, ppl: 92.5420, loss: 4.5558
	step 13946:lm_loss: 4.5274, ppl: 92.5184, loss: 4.5557
	step 13947:lm_loss: 4.5274, ppl: 92.5143, loss: 4.5556
	step 13948:lm_loss: 4.5274, ppl: 92.5223, loss: 4.5557
	step 13949:lm_loss: 4.5275, ppl: 92.5295, loss: 4.5558
	step 13950:lm_loss: 4.5275, ppl: 92.5308, loss: 4.5558
	step 13951:lm_loss: 4.5275, ppl: 92.5312, loss: 4.5558
	step 13952:lm_loss: 4.5275, ppl: 92.5284, loss: 4.5558
	step 13953:lm_loss: 4.5275, ppl: 92.5302, loss: 4.5558
	step 13954:lm_loss: 4.5276, ppl: 92.5343, loss: 4.5558
	step 13955:lm_loss: 4.5276, ppl: 92.5384, loss: 4.5559
	step 13956:lm_loss: 4.5276, ppl: 92.5339, loss: 4.5558
	step 13957:lm_loss: 4.5276, ppl: 92.5397, loss: 4.5559
	step 13958:lm_loss: 4.5275, ppl: 92.5260, loss: 4.5558
	step 13959:lm_loss: 4.5275, ppl: 92.5258, loss: 4.5558
	step 13960:lm_loss: 4.5274, ppl: 92.5171, loss: 4.5557
	step 13961:lm_loss: 4.5274, ppl: 92.5154, loss: 4.5557
	step 13962:lm_loss: 4.5274, ppl: 92.5173, loss: 4.5557
	step 13963:lm_loss: 4.5273, ppl: 92.5127, loss: 4.5556
	step 13964:lm_loss: 4.5274, ppl: 92.5199, loss: 4.5557
	step 13965:lm_loss: 4.5274, ppl: 92.5219, loss: 4.5558
	step 13966:lm_loss: 4.5275, ppl: 92.5231, loss: 4.5558
	step 13967:lm_loss: 4.5274, ppl: 92.5216, loss: 4.5558
	step 13968:lm_loss: 4.5275, ppl: 92.5299, loss: 4.5558
	step 13969:lm_loss: 4.5275, ppl: 92.5297, loss: 4.5558
	step 13970:lm_loss: 4.5276, ppl: 92.5331, loss: 4.5558
	step 13971:lm_loss: 4.5275, ppl: 92.5244, loss: 4.5557
	step 13972:lm_loss: 4.5275, ppl: 92.5277, loss: 4.5558
	step 13973:lm_loss: 4.5276, ppl: 92.5331, loss: 4.5558
	step 13974:lm_loss: 4.5276, ppl: 92.5329, loss: 4.5558
	step 13975:lm_loss: 4.5275, ppl: 92.5283, loss: 4.5558
	step 13976:lm_loss: 4.5275, ppl: 92.5284, loss: 4.5558
	step 13977:lm_loss: 4.5276, ppl: 92.5401, loss: 4.5559
	step 13978:lm_loss: 4.5276, ppl: 92.5407, loss: 4.5559
	step 13979:lm_loss: 4.5276, ppl: 92.5355, loss: 4.5558
	step 13980:lm_loss: 4.5276, ppl: 92.5363, loss: 4.5558
	step 13981:lm_loss: 4.5276, ppl: 92.5354, loss: 4.5558
	step 13982:lm_loss: 4.5275, ppl: 92.5305, loss: 4.5558
	step 13983:lm_loss: 4.5275, ppl: 92.5314, loss: 4.5558
	step 13984:lm_loss: 4.5275, ppl: 92.5276, loss: 4.5557
	step 13985:lm_loss: 4.5275, ppl: 92.5288, loss: 4.5557
	step 13986:lm_loss: 4.5275, ppl: 92.5248, loss: 4.5557
	step 13987:lm_loss: 4.5275, ppl: 92.5274, loss: 4.5557
	step 13988:lm_loss: 4.5274, ppl: 92.5173, loss: 4.5556
	step 13989:lm_loss: 4.5274, ppl: 92.5177, loss: 4.5556
	step 13990:lm_loss: 4.5274, ppl: 92.5168, loss: 4.5556
	step 13991:lm_loss: 4.5273, ppl: 92.5121, loss: 4.5555
	step 13992:lm_loss: 4.5272, ppl: 92.5029, loss: 4.5555
	step 13993:lm_loss: 4.5272, ppl: 92.5019, loss: 4.5554
	step 13994:lm_loss: 4.5273, ppl: 92.5071, loss: 4.5555
	step 13995:lm_loss: 4.5273, ppl: 92.5101, loss: 4.5555
	step 13996:lm_loss: 4.5274, ppl: 92.5163, loss: 4.5556
	step 13997:lm_loss: 4.5273, ppl: 92.5130, loss: 4.5555
	step 13998:lm_loss: 4.5273, ppl: 92.5057, loss: 4.5554
	step 13999:lm_loss: 4.5273, ppl: 92.5112, loss: 4.5555
	step 14000:lm_loss: 4.5274, ppl: 92.5135, loss: 4.5555
	step 14001:lm_loss: 4.5274, ppl: 92.5168, loss: 4.5555
	step 14002:lm_loss: 4.5274, ppl: 92.5211, loss: 4.5556
	step 14003:lm_loss: 4.5276, ppl: 92.5347, loss: 4.5557
	step 14004:lm_loss: 4.5275, ppl: 92.5306, loss: 4.5556
	step 14005:lm_loss: 4.5275, ppl: 92.5295, loss: 4.5556
	step 14006:lm_loss: 4.5275, ppl: 92.5294, loss: 4.5556
	step 14007:lm_loss: 4.5276, ppl: 92.5327, loss: 4.5556
	step 14008:lm_loss: 4.5276, ppl: 92.5372, loss: 4.5557
	step 14009:lm_loss: 4.5276, ppl: 92.5372, loss: 4.5557
	step 14010:lm_loss: 4.5276, ppl: 92.5345, loss: 4.5557
	step 14011:lm_loss: 4.5276, ppl: 92.5335, loss: 4.5557
	step 14012:lm_loss: 4.5277, ppl: 92.5420, loss: 4.5557
	step 14013:lm_loss: 4.5275, ppl: 92.5279, loss: 4.5556
	step 14014:lm_loss: 4.5275, ppl: 92.5251, loss: 4.5555
	step 14015:lm_loss: 4.5275, ppl: 92.5239, loss: 4.5555
	step 14016:lm_loss: 4.5274, ppl: 92.5220, loss: 4.5555
	step 14017:lm_loss: 4.5275, ppl: 92.5228, loss: 4.5555
	step 14018:lm_loss: 4.5275, ppl: 92.5282, loss: 4.5556
	step 14019:lm_loss: 4.5274, ppl: 92.5212, loss: 4.5555
	step 14020:lm_loss: 4.5275, ppl: 92.5226, loss: 4.5555
	step 14021:lm_loss: 4.5275, ppl: 92.5243, loss: 4.5555
	step 14022:lm_loss: 4.5274, ppl: 92.5202, loss: 4.5555
	step 14023:lm_loss: 4.5275, ppl: 92.5261, loss: 4.5555
	step 14024:lm_loss: 4.5275, ppl: 92.5280, loss: 4.5555
	step 14025:lm_loss: 4.5274, ppl: 92.5216, loss: 4.5555
	step 14026:lm_loss: 4.5275, ppl: 92.5246, loss: 4.5555
	step 14027:lm_loss: 4.5273, ppl: 92.5069, loss: 4.5554
	step 14028:lm_loss: 4.5274, ppl: 92.5133, loss: 4.5554
	step 14029:lm_loss: 4.5273, ppl: 92.5111, loss: 4.5554
	step 14030:lm_loss: 4.5274, ppl: 92.5173, loss: 4.5554
	step 14031:lm_loss: 4.5274, ppl: 92.5197, loss: 4.5555
	step 14032:lm_loss: 4.5274, ppl: 92.5156, loss: 4.5554
	step 14033:lm_loss: 4.5272, ppl: 92.5025, loss: 4.5553
	step 14034:lm_loss: 4.5272, ppl: 92.5021, loss: 4.5553
	step 14035:lm_loss: 4.5273, ppl: 92.5077, loss: 4.5553
	step 14036:lm_loss: 4.5273, ppl: 92.5096, loss: 4.5553
	step 14037:lm_loss: 4.5273, ppl: 92.5047, loss: 4.5553
	step 14038:lm_loss: 4.5272, ppl: 92.5025, loss: 4.5552
	step 14039:lm_loss: 4.5273, ppl: 92.5058, loss: 4.5553
	step 14040:lm_loss: 4.5272, ppl: 92.5000, loss: 4.5552
	step 14041:lm_loss: 4.5273, ppl: 92.5056, loss: 4.5553
	step 14042:lm_loss: 4.5273, ppl: 92.5045, loss: 4.5553
	step 14043:lm_loss: 4.5272, ppl: 92.5007, loss: 4.5552
	step 14044:lm_loss: 4.5272, ppl: 92.4999, loss: 4.5552
	step 14045:lm_loss: 4.5273, ppl: 92.5056, loss: 4.5553
	step 14046:lm_loss: 4.5273, ppl: 92.5060, loss: 4.5553
	step 14047:lm_loss: 4.5272, ppl: 92.4963, loss: 4.5552
	step 14048:lm_loss: 4.5270, ppl: 92.4813, loss: 4.5551
	step 14049:lm_loss: 4.5270, ppl: 92.4800, loss: 4.5551
	step 14050:lm_loss: 4.5270, ppl: 92.4770, loss: 4.5551
	step 14051:lm_loss: 4.5269, ppl: 92.4716, loss: 4.5550
	step 14052:lm_loss: 4.5270, ppl: 92.4786, loss: 4.5550
	step 14053:lm_loss: 4.5269, ppl: 92.4724, loss: 4.5550
	step 14054:lm_loss: 4.5269, ppl: 92.4682, loss: 4.5550
	step 14055:lm_loss: 4.5269, ppl: 92.4709, loss: 4.5550
	step 14056:lm_loss: 4.5269, ppl: 92.4735, loss: 4.5550
	step 14057:lm_loss: 4.5269, ppl: 92.4725, loss: 4.5550
	step 14058:lm_loss: 4.5269, ppl: 92.4741, loss: 4.5550
	step 14059:lm_loss: 4.5270, ppl: 92.4764, loss: 4.5550
	step 14060:lm_loss: 4.5269, ppl: 92.4702, loss: 4.5549
	step 14061:lm_loss: 4.5269, ppl: 92.4754, loss: 4.5550
	step 14062:lm_loss: 4.5269, ppl: 92.4740, loss: 4.5550
	step 14063:lm_loss: 4.5269, ppl: 92.4751, loss: 4.5550
	step 14064:lm_loss: 4.5268, ppl: 92.4634, loss: 4.5549
	step 14065:lm_loss: 4.5268, ppl: 92.4635, loss: 4.5549
	step 14066:lm_loss: 4.5267, ppl: 92.4570, loss: 4.5548
	step 14067:lm_loss: 4.5267, ppl: 92.4566, loss: 4.5548
	step 14068:lm_loss: 4.5268, ppl: 92.4618, loss: 4.5548
	step 14069:lm_loss: 4.5268, ppl: 92.4632, loss: 4.5549
	step 14070:lm_loss: 4.5268, ppl: 92.4658, loss: 4.5549
	step 14071:lm_loss: 4.5269, ppl: 92.4681, loss: 4.5549
	step 14072:lm_loss: 4.5269, ppl: 92.4705, loss: 4.5549
	step 14073:lm_loss: 4.5269, ppl: 92.4713, loss: 4.5549
	step 14074:lm_loss: 4.5269, ppl: 92.4680, loss: 4.5549
	step 14075:lm_loss: 4.5268, ppl: 92.4651, loss: 4.5549
	step 14076:lm_loss: 4.5268, ppl: 92.4648, loss: 4.5549
	step 14077:lm_loss: 4.5268, ppl: 92.4658, loss: 4.5549
	step 14078:lm_loss: 4.5268, ppl: 92.4632, loss: 4.5548
	step 14079:lm_loss: 4.5268, ppl: 92.4615, loss: 4.5548
	step 14080:lm_loss: 4.5267, ppl: 92.4557, loss: 4.5547
	step 14081:lm_loss: 4.5267, ppl: 92.4519, loss: 4.5547
	step 14082:lm_loss: 4.5268, ppl: 92.4597, loss: 4.5548
	step 14083:lm_loss: 4.5267, ppl: 92.4546, loss: 4.5547
	step 14084:lm_loss: 4.5268, ppl: 92.4600, loss: 4.5547
	step 14085:lm_loss: 4.5268, ppl: 92.4586, loss: 4.5547
	step 14086:lm_loss: 4.5268, ppl: 92.4641, loss: 4.5548
	step 14087:lm_loss: 4.5268, ppl: 92.4642, loss: 4.5548
	step 14088:lm_loss: 4.5268, ppl: 92.4602, loss: 4.5547
	step 14089:lm_loss: 4.5267, ppl: 92.4571, loss: 4.5547
	step 14090:lm_loss: 4.5268, ppl: 92.4600, loss: 4.5547
	step 14091:lm_loss: 4.5267, ppl: 92.4532, loss: 4.5546
	step 14092:lm_loss: 4.5268, ppl: 92.4614, loss: 4.5547
	step 14093:lm_loss: 4.5268, ppl: 92.4610, loss: 4.5547
	step 14094:lm_loss: 4.5269, ppl: 92.4701, loss: 4.5549
	step 14095:lm_loss: 4.5269, ppl: 92.4703, loss: 4.5549
	step 14096:lm_loss: 4.5268, ppl: 92.4666, loss: 4.5548
	step 14097:lm_loss: 4.5270, ppl: 92.4762, loss: 4.5549
	step 14098:lm_loss: 4.5269, ppl: 92.4748, loss: 4.5549
	step 14099:lm_loss: 4.5270, ppl: 92.4833, loss: 4.5549
	step 14100:lm_loss: 4.5270, ppl: 92.4808, loss: 4.5549
	step 14101:lm_loss: 4.5270, ppl: 92.4795, loss: 4.5549
	step 14102:lm_loss: 4.5269, ppl: 92.4759, loss: 4.5549
	step 14103:lm_loss: 4.5270, ppl: 92.4829, loss: 4.5549
	step 14104:lm_loss: 4.5270, ppl: 92.4820, loss: 4.5549
	step 14105:lm_loss: 4.5270, ppl: 92.4844, loss: 4.5549
	step 14106:lm_loss: 4.5271, ppl: 92.4903, loss: 4.5550
	step 14107:lm_loss: 4.5272, ppl: 92.4998, loss: 4.5551
	step 14108:lm_loss: 4.5272, ppl: 92.4992, loss: 4.5551
	step 14109:lm_loss: 4.5272, ppl: 92.4995, loss: 4.5551
	step 14110:lm_loss: 4.5273, ppl: 92.5065, loss: 4.5551
	step 14111:lm_loss: 4.5272, ppl: 92.5032, loss: 4.5551
	step 14112:lm_loss: 4.5272, ppl: 92.5008, loss: 4.5551
	step 14113:lm_loss: 4.5272, ppl: 92.5033, loss: 4.5551
	step 14114:lm_loss: 4.5272, ppl: 92.5000, loss: 4.5550
	step 14115:lm_loss: 4.5272, ppl: 92.4967, loss: 4.5550
	step 14116:lm_loss: 4.5272, ppl: 92.4991, loss: 4.5550
	step 14117:lm_loss: 4.5272, ppl: 92.4955, loss: 4.5550
	step 14118:lm_loss: 4.5271, ppl: 92.4901, loss: 4.5549
	step 14119:lm_loss: 4.5272, ppl: 92.4951, loss: 4.5550
	step 14120:lm_loss: 4.5271, ppl: 92.4914, loss: 4.5549
	step 14121:lm_loss: 4.5271, ppl: 92.4891, loss: 4.5549
	step 14122:lm_loss: 4.5271, ppl: 92.4944, loss: 4.5549
	step 14123:lm_loss: 4.5271, ppl: 92.4913, loss: 4.5549
	step 14124:lm_loss: 4.5271, ppl: 92.4942, loss: 4.5549
	step 14125:lm_loss: 4.5271, ppl: 92.4928, loss: 4.5549
	step 14126:lm_loss: 4.5271, ppl: 92.4931, loss: 4.5549
	step 14127:lm_loss: 4.5272, ppl: 92.4962, loss: 4.5549
	step 14128:lm_loss: 4.5271, ppl: 92.4905, loss: 4.5549
	step 14129:lm_loss: 4.5271, ppl: 92.4914, loss: 4.5549
	step 14130:lm_loss: 4.5271, ppl: 92.4874, loss: 4.5548
	step 14131:lm_loss: 4.5271, ppl: 92.4893, loss: 4.5548
	step 14132:lm_loss: 4.5270, ppl: 92.4818, loss: 4.5547
	step 14133:lm_loss: 4.5270, ppl: 92.4789, loss: 4.5547
	step 14134:lm_loss: 4.5270, ppl: 92.4801, loss: 4.5547
	step 14135:lm_loss: 4.5270, ppl: 92.4845, loss: 4.5548
	step 14136:lm_loss: 4.5270, ppl: 92.4786, loss: 4.5547
	step 14137:lm_loss: 4.5269, ppl: 92.4758, loss: 4.5546
	step 14138:lm_loss: 4.5269, ppl: 92.4703, loss: 4.5546
	step 14139:lm_loss: 4.5269, ppl: 92.4691, loss: 4.5546
	step 14140:lm_loss: 4.5269, ppl: 92.4699, loss: 4.5546
	step 14141:lm_loss: 4.5269, ppl: 92.4697, loss: 4.5546
	step 14142:lm_loss: 4.5269, ppl: 92.4717, loss: 4.5546
	step 14143:lm_loss: 4.5269, ppl: 92.4760, loss: 4.5546
	step 14144:lm_loss: 4.5268, ppl: 92.4635, loss: 4.5545
	step 14145:lm_loss: 4.5268, ppl: 92.4659, loss: 4.5546
	step 14146:lm_loss: 4.5269, ppl: 92.4692, loss: 4.5546
	step 14147:lm_loss: 4.5269, ppl: 92.4701, loss: 4.5546
	step 14148:lm_loss: 4.5269, ppl: 92.4728, loss: 4.5546
	step 14149:lm_loss: 4.5268, ppl: 92.4656, loss: 4.5546
	step 14150:lm_loss: 4.5268, ppl: 92.4631, loss: 4.5545
	step 14151:lm_loss: 4.5268, ppl: 92.4660, loss: 4.5546
	step 14152:lm_loss: 4.5269, ppl: 92.4675, loss: 4.5546
	step 14153:lm_loss: 4.5268, ppl: 92.4618, loss: 4.5545
	step 14154:lm_loss: 4.5268, ppl: 92.4659, loss: 4.5545
	step 14155:lm_loss: 4.5268, ppl: 92.4639, loss: 4.5545
	step 14156:lm_loss: 4.5268, ppl: 92.4605, loss: 4.5545
	step 14157:lm_loss: 4.5268, ppl: 92.4590, loss: 4.5545
	step 14158:lm_loss: 4.5267, ppl: 92.4574, loss: 4.5545
	step 14159:lm_loss: 4.5268, ppl: 92.4657, loss: 4.5545
	step 14160:lm_loss: 4.5268, ppl: 92.4665, loss: 4.5545
	step 14161:lm_loss: 4.5268, ppl: 92.4661, loss: 4.5545
	step 14162:lm_loss: 4.5269, ppl: 92.4760, loss: 4.5546
	step 14163:lm_loss: 4.5269, ppl: 92.4747, loss: 4.5546
	step 14164:lm_loss: 4.5269, ppl: 92.4750, loss: 4.5546
	step 14165:lm_loss: 4.5269, ppl: 92.4754, loss: 4.5546
	step 14166:lm_loss: 4.5270, ppl: 92.4775, loss: 4.5547
	step 14167:lm_loss: 4.5270, ppl: 92.4824, loss: 4.5547
	step 14168:lm_loss: 4.5270, ppl: 92.4851, loss: 4.5548
	step 14169:lm_loss: 4.5271, ppl: 92.4873, loss: 4.5548
	step 14170:lm_loss: 4.5271, ppl: 92.4943, loss: 4.5548
	step 14171:lm_loss: 4.5272, ppl: 92.4951, loss: 4.5548
	step 14172:lm_loss: 4.5272, ppl: 92.4957, loss: 4.5549
	step 14173:lm_loss: 4.5270, ppl: 92.4836, loss: 4.5548
	step 14174:lm_loss: 4.5271, ppl: 92.4857, loss: 4.5548
	step 14175:lm_loss: 4.5269, ppl: 92.4758, loss: 4.5547
	step 14176:lm_loss: 4.5270, ppl: 92.4795, loss: 4.5547
	step 14177:lm_loss: 4.5269, ppl: 92.4736, loss: 4.5546
	step 14178:lm_loss: 4.5269, ppl: 92.4725, loss: 4.5546
	step 14179:lm_loss: 4.5269, ppl: 92.4727, loss: 4.5546
	step 14180:lm_loss: 4.5269, ppl: 92.4675, loss: 4.5546
	step 14181:lm_loss: 4.5269, ppl: 92.4692, loss: 4.5546
	step 14182:lm_loss: 4.5269, ppl: 92.4711, loss: 4.5546
	step 14183:lm_loss: 4.5269, ppl: 92.4693, loss: 4.5546
	step 14184:lm_loss: 4.5268, ppl: 92.4614, loss: 4.5546
	step 14185:lm_loss: 4.5268, ppl: 92.4618, loss: 4.5546
	step 14186:lm_loss: 4.5268, ppl: 92.4649, loss: 4.5546
	step 14187:lm_loss: 4.5268, ppl: 92.4581, loss: 4.5545
	step 14188:lm_loss: 4.5268, ppl: 92.4593, loss: 4.5545
	step 14189:lm_loss: 4.5268, ppl: 92.4593, loss: 4.5545
	step 14190:lm_loss: 4.5268, ppl: 92.4600, loss: 4.5545
	step 14191:lm_loss: 4.5269, ppl: 92.4687, loss: 4.5546
	step 14192:lm_loss: 4.5268, ppl: 92.4632, loss: 4.5545
	step 14193:lm_loss: 4.5268, ppl: 92.4616, loss: 4.5545
	step 14194:lm_loss: 4.5268, ppl: 92.4636, loss: 4.5545
	step 14195:lm_loss: 4.5268, ppl: 92.4579, loss: 4.5545
	step 14196:lm_loss: 4.5267, ppl: 92.4567, loss: 4.5545
	step 14197:lm_loss: 4.5268, ppl: 92.4577, loss: 4.5545
	step 14198:lm_loss: 4.5267, ppl: 92.4529, loss: 4.5544
	step 14199:lm_loss: 4.5267, ppl: 92.4492, loss: 4.5544
	step 14200:lm_loss: 4.5267, ppl: 92.4517, loss: 4.5544
	step 14201:lm_loss: 4.5266, ppl: 92.4482, loss: 4.5544
	step 14202:lm_loss: 4.5267, ppl: 92.4504, loss: 4.5544
	step 14203:lm_loss: 4.5266, ppl: 92.4450, loss: 4.5544
	step 14204:lm_loss: 4.5266, ppl: 92.4470, loss: 4.5544
	step 14205:lm_loss: 4.5267, ppl: 92.4491, loss: 4.5544
	step 14206:lm_loss: 4.5266, ppl: 92.4456, loss: 4.5544
	step 14207:lm_loss: 4.5266, ppl: 92.4432, loss: 4.5543
	step 14208:lm_loss: 4.5266, ppl: 92.4472, loss: 4.5544
	step 14209:lm_loss: 4.5267, ppl: 92.4513, loss: 4.5544
	step 14210:lm_loss: 4.5268, ppl: 92.4630, loss: 4.5545
	step 14211:lm_loss: 4.5268, ppl: 92.4650, loss: 4.5545
	step 14212:lm_loss: 4.5268, ppl: 92.4590, loss: 4.5544
	step 14213:lm_loss: 4.5268, ppl: 92.4629, loss: 4.5545
	step 14214:lm_loss: 4.5268, ppl: 92.4622, loss: 4.5545
	step 14215:lm_loss: 4.5268, ppl: 92.4656, loss: 4.5545
	step 14216:lm_loss: 4.5268, ppl: 92.4635, loss: 4.5545
	step 14217:lm_loss: 4.5268, ppl: 92.4617, loss: 4.5545
	step 14218:lm_loss: 4.5267, ppl: 92.4538, loss: 4.5544
	step 14219:lm_loss: 4.5268, ppl: 92.4610, loss: 4.5544
	step 14220:lm_loss: 4.5268, ppl: 92.4648, loss: 4.5545
	step 14221:lm_loss: 4.5268, ppl: 92.4651, loss: 4.5545
	step 14222:lm_loss: 4.5269, ppl: 92.4716, loss: 4.5545
	step 14223:lm_loss: 4.5269, ppl: 92.4693, loss: 4.5545
	step 14224:lm_loss: 4.5269, ppl: 92.4727, loss: 4.5546
	step 14225:lm_loss: 4.5269, ppl: 92.4703, loss: 4.5545
	step 14226:lm_loss: 4.5269, ppl: 92.4714, loss: 4.5545
	step 14227:lm_loss: 4.5268, ppl: 92.4640, loss: 4.5545
	step 14228:lm_loss: 4.5266, ppl: 92.4456, loss: 4.5543
	step 14229:lm_loss: 4.5266, ppl: 92.4472, loss: 4.5543
	step 14230:lm_loss: 4.5267, ppl: 92.4494, loss: 4.5544
	step 14231:lm_loss: 4.5267, ppl: 92.4523, loss: 4.5544
	step 14232:lm_loss: 4.5266, ppl: 92.4473, loss: 4.5543
	step 14233:lm_loss: 4.5267, ppl: 92.4504, loss: 4.5544
	step 14234:lm_loss: 4.5267, ppl: 92.4500, loss: 4.5544
	step 14235:lm_loss: 4.5267, ppl: 92.4539, loss: 4.5544
	step 14236:lm_loss: 4.5268, ppl: 92.4610, loss: 4.5544
	step 14237:lm_loss: 4.5267, ppl: 92.4526, loss: 4.5543
	step 14238:lm_loss: 4.5267, ppl: 92.4567, loss: 4.5544
	step 14239:lm_loss: 4.5267, ppl: 92.4545, loss: 4.5544
	step 14240:lm_loss: 4.5267, ppl: 92.4569, loss: 4.5544
	step 14241:lm_loss: 4.5267, ppl: 92.4497, loss: 4.5543
	step 14242:lm_loss: 4.5266, ppl: 92.4429, loss: 4.5542
	step 14243:lm_loss: 4.5267, ppl: 92.4505, loss: 4.5543
	step 14244:lm_loss: 4.5267, ppl: 92.4492, loss: 4.5543
	step 14245:lm_loss: 4.5267, ppl: 92.4505, loss: 4.5543
	step 14246:lm_loss: 4.5267, ppl: 92.4521, loss: 4.5543
	step 14247:lm_loss: 4.5267, ppl: 92.4515, loss: 4.5543
	step 14248:lm_loss: 4.5266, ppl: 92.4401, loss: 4.5542
	step 14249:lm_loss: 4.5265, ppl: 92.4362, loss: 4.5542
	step 14250:lm_loss: 4.5266, ppl: 92.4441, loss: 4.5542
	step 14251:lm_loss: 4.5266, ppl: 92.4431, loss: 4.5542
	step 14252:lm_loss: 4.5267, ppl: 92.4515, loss: 4.5543
	step 14253:lm_loss: 4.5267, ppl: 92.4498, loss: 4.5543
	step 14254:lm_loss: 4.5267, ppl: 92.4505, loss: 4.5543
	step 14255:lm_loss: 4.5266, ppl: 92.4418, loss: 4.5542
	step 14256:lm_loss: 4.5265, ppl: 92.4380, loss: 4.5542
	step 14257:lm_loss: 4.5265, ppl: 92.4352, loss: 4.5541
	step 14258:lm_loss: 4.5266, ppl: 92.4397, loss: 4.5541
	step 14259:lm_loss: 4.5265, ppl: 92.4307, loss: 4.5541
	step 14260:lm_loss: 4.5264, ppl: 92.4269, loss: 4.5540
	step 14261:lm_loss: 4.5265, ppl: 92.4309, loss: 4.5541
	step 14262:lm_loss: 4.5265, ppl: 92.4307, loss: 4.5541
	step 14263:lm_loss: 4.5265, ppl: 92.4343, loss: 4.5541
	step 14264:lm_loss: 4.5265, ppl: 92.4372, loss: 4.5541
	step 14265:lm_loss: 4.5265, ppl: 92.4363, loss: 4.5541
	step 14266:lm_loss: 4.5266, ppl: 92.4435, loss: 4.5542
	step 14267:lm_loss: 4.5267, ppl: 92.4512, loss: 4.5542
	step 14268:lm_loss: 4.5267, ppl: 92.4546, loss: 4.5543
	step 14269:lm_loss: 4.5268, ppl: 92.4586, loss: 4.5543
	step 14270:lm_loss: 4.5268, ppl: 92.4665, loss: 4.5544
	step 14271:lm_loss: 4.5268, ppl: 92.4621, loss: 4.5543
	step 14272:lm_loss: 4.5268, ppl: 92.4616, loss: 4.5543
	step 14273:lm_loss: 4.5269, ppl: 92.4672, loss: 4.5543
	step 14274:lm_loss: 4.5269, ppl: 92.4673, loss: 4.5543
	step 14275:lm_loss: 4.5268, ppl: 92.4642, loss: 4.5543
	step 14276:lm_loss: 4.5268, ppl: 92.4648, loss: 4.5543
	step 14277:lm_loss: 4.5269, ppl: 92.4674, loss: 4.5543
	step 14278:lm_loss: 4.5269, ppl: 92.4669, loss: 4.5543
	step 14279:lm_loss: 4.5268, ppl: 92.4630, loss: 4.5543
	step 14280:lm_loss: 4.5268, ppl: 92.4587, loss: 4.5542
	step 14281:lm_loss: 4.5267, ppl: 92.4538, loss: 4.5542
	step 14282:lm_loss: 4.5267, ppl: 92.4516, loss: 4.5542
	step 14283:lm_loss: 4.5267, ppl: 92.4541, loss: 4.5542
	step 14284:lm_loss: 4.5267, ppl: 92.4561, loss: 4.5542
	step 14285:lm_loss: 4.5267, ppl: 92.4519, loss: 4.5542
	step 14286:lm_loss: 4.5267, ppl: 92.4518, loss: 4.5541
	step 14287:lm_loss: 4.5267, ppl: 92.4496, loss: 4.5541
	step 14288:lm_loss: 4.5266, ppl: 92.4460, loss: 4.5541
	step 14289:lm_loss: 4.5266, ppl: 92.4434, loss: 4.5541
	step 14290:lm_loss: 4.5266, ppl: 92.4410, loss: 4.5540
	step 14291:lm_loss: 4.5266, ppl: 92.4396, loss: 4.5540
	step 14292:lm_loss: 4.5265, ppl: 92.4380, loss: 4.5540
	step 14293:lm_loss: 4.5266, ppl: 92.4410, loss: 4.5540
	step 14294:lm_loss: 4.5266, ppl: 92.4396, loss: 4.5540
	step 14295:lm_loss: 4.5266, ppl: 92.4397, loss: 4.5540
	step 14296:lm_loss: 4.5266, ppl: 92.4413, loss: 4.5540
	step 14297:lm_loss: 4.5266, ppl: 92.4433, loss: 4.5540
	step 14298:lm_loss: 4.5266, ppl: 92.4427, loss: 4.5540
	step 14299:lm_loss: 4.5265, ppl: 92.4390, loss: 4.5540
	step 14300:lm_loss: 4.5266, ppl: 92.4464, loss: 4.5541
	step 14301:lm_loss: 4.5266, ppl: 92.4450, loss: 4.5541
	step 14302:lm_loss: 4.5265, ppl: 92.4348, loss: 4.5540
	step 14303:lm_loss: 4.5265, ppl: 92.4379, loss: 4.5540
	step 14304:lm_loss: 4.5266, ppl: 92.4420, loss: 4.5541
	step 14305:lm_loss: 4.5266, ppl: 92.4436, loss: 4.5541
	step 14306:lm_loss: 4.5265, ppl: 92.4383, loss: 4.5540
	step 14307:lm_loss: 4.5266, ppl: 92.4413, loss: 4.5541
	step 14308:lm_loss: 4.5266, ppl: 92.4453, loss: 4.5541
	step 14309:lm_loss: 4.5266, ppl: 92.4433, loss: 4.5541
	step 14310:lm_loss: 4.5266, ppl: 92.4403, loss: 4.5541
	step 14311:lm_loss: 4.5265, ppl: 92.4389, loss: 4.5540
	step 14312:lm_loss: 4.5266, ppl: 92.4403, loss: 4.5541
	step 14313:lm_loss: 4.5265, ppl: 92.4332, loss: 4.5540
	step 14314:lm_loss: 4.5264, ppl: 92.4289, loss: 4.5539
	step 14315:lm_loss: 4.5265, ppl: 92.4361, loss: 4.5540
	step 14316:lm_loss: 4.5265, ppl: 92.4378, loss: 4.5541
	step 14317:lm_loss: 4.5265, ppl: 92.4356, loss: 4.5540
	step 14318:lm_loss: 4.5266, ppl: 92.4444, loss: 4.5541
	step 14319:lm_loss: 4.5266, ppl: 92.4452, loss: 4.5541
	step 14320:lm_loss: 4.5266, ppl: 92.4444, loss: 4.5541
	step 14321:lm_loss: 4.5266, ppl: 92.4452, loss: 4.5541
	step 14322:lm_loss: 4.5267, ppl: 92.4498, loss: 4.5542
	step 14323:lm_loss: 4.5268, ppl: 92.4582, loss: 4.5542
	step 14324:lm_loss: 4.5268, ppl: 92.4665, loss: 4.5543
	step 14325:lm_loss: 4.5269, ppl: 92.4670, loss: 4.5543
	step 14326:lm_loss: 4.5268, ppl: 92.4627, loss: 4.5543
	step 14327:lm_loss: 4.5268, ppl: 92.4659, loss: 4.5543
	step 14328:lm_loss: 4.5268, ppl: 92.4648, loss: 4.5543
	step 14329:lm_loss: 4.5268, ppl: 92.4658, loss: 4.5543
	step 14330:lm_loss: 4.5268, ppl: 92.4617, loss: 4.5543
	step 14331:lm_loss: 4.5269, ppl: 92.4727, loss: 4.5544
	step 14332:lm_loss: 4.5269, ppl: 92.4745, loss: 4.5544
	step 14333:lm_loss: 4.5269, ppl: 92.4754, loss: 4.5544
	step 14334:lm_loss: 4.5270, ppl: 92.4775, loss: 4.5544
	step 14335:lm_loss: 4.5269, ppl: 92.4738, loss: 4.5544
	step 14336:lm_loss: 4.5269, ppl: 92.4686, loss: 4.5543
	step 14337:lm_loss: 4.5268, ppl: 92.4662, loss: 4.5543
	step 14338:lm_loss: 4.5268, ppl: 92.4645, loss: 4.5543
	step 14339:lm_loss: 4.5269, ppl: 92.4687, loss: 4.5543
	step 14340:lm_loss: 4.5268, ppl: 92.4665, loss: 4.5543
	step 14341:lm_loss: 4.5268, ppl: 92.4656, loss: 4.5543
	step 14342:lm_loss: 4.5269, ppl: 92.4675, loss: 4.5543
	step 14343:lm_loss: 4.5269, ppl: 92.4684, loss: 4.5543
	step 14344:lm_loss: 4.5268, ppl: 92.4614, loss: 4.5542
	step 14345:lm_loss: 4.5267, ppl: 92.4534, loss: 4.5542
	step 14346:lm_loss: 4.5268, ppl: 92.4601, loss: 4.5542
	step 14347:lm_loss: 4.5268, ppl: 92.4624, loss: 4.5543
	step 14348:lm_loss: 4.5268, ppl: 92.4623, loss: 4.5543
	step 14349:lm_loss: 4.5268, ppl: 92.4613, loss: 4.5543
	step 14350:lm_loss: 4.5268, ppl: 92.4656, loss: 4.5543
	step 14351:lm_loss: 4.5269, ppl: 92.4709, loss: 4.5544
	step 14352:lm_loss: 4.5268, ppl: 92.4636, loss: 4.5543
	step 14353:lm_loss: 4.5268, ppl: 92.4658, loss: 4.5543
	step 14354:lm_loss: 4.5268, ppl: 92.4649, loss: 4.5543
	step 14355:lm_loss: 4.5268, ppl: 92.4584, loss: 4.5542
	step 14356:lm_loss: 4.5266, ppl: 92.4473, loss: 4.5541
	step 14357:lm_loss: 4.5267, ppl: 92.4557, loss: 4.5542
	step 14358:lm_loss: 4.5267, ppl: 92.4526, loss: 4.5541
	step 14359:lm_loss: 4.5267, ppl: 92.4563, loss: 4.5542
	step 14360:lm_loss: 4.5267, ppl: 92.4565, loss: 4.5542
	step 14361:lm_loss: 4.5268, ppl: 92.4603, loss: 4.5542
	step 14362:lm_loss: 4.5268, ppl: 92.4622, loss: 4.5542
	step 14363:lm_loss: 4.5268, ppl: 92.4652, loss: 4.5543
	step 14364:lm_loss: 4.5268, ppl: 92.4666, loss: 4.5543
	step 14365:lm_loss: 4.5268, ppl: 92.4582, loss: 4.5542
	step 14366:lm_loss: 4.5268, ppl: 92.4591, loss: 4.5542
	step 14367:lm_loss: 4.5266, ppl: 92.4476, loss: 4.5541
	step 14368:lm_loss: 4.5267, ppl: 92.4542, loss: 4.5542
	step 14369:lm_loss: 4.5266, ppl: 92.4477, loss: 4.5541
	step 14370:lm_loss: 4.5267, ppl: 92.4507, loss: 4.5541
	step 14371:lm_loss: 4.5268, ppl: 92.4607, loss: 4.5543
	step 14372:lm_loss: 4.5268, ppl: 92.4631, loss: 4.5543
	step 14373:lm_loss: 4.5268, ppl: 92.4617, loss: 4.5543
	step 14374:lm_loss: 4.5267, ppl: 92.4550, loss: 4.5542
	step 14375:lm_loss: 4.5266, ppl: 92.4479, loss: 4.5541
	step 14376:lm_loss: 4.5267, ppl: 92.4574, loss: 4.5542
	step 14377:lm_loss: 4.5266, ppl: 92.4472, loss: 4.5541
	step 14378:lm_loss: 4.5267, ppl: 92.4489, loss: 4.5542
	step 14379:lm_loss: 4.5266, ppl: 92.4454, loss: 4.5541
	step 14380:lm_loss: 4.5266, ppl: 92.4446, loss: 4.5541
	step 14381:lm_loss: 4.5266, ppl: 92.4447, loss: 4.5541
	step 14382:lm_loss: 4.5266, ppl: 92.4433, loss: 4.5541
	step 14383:lm_loss: 4.5265, ppl: 92.4366, loss: 4.5540
	step 14384:lm_loss: 4.5265, ppl: 92.4355, loss: 4.5540
	step 14385:lm_loss: 4.5265, ppl: 92.4306, loss: 4.5539
	step 14386:lm_loss: 4.5265, ppl: 92.4327, loss: 4.5540
	step 14387:lm_loss: 4.5265, ppl: 92.4307, loss: 4.5539
	step 14388:lm_loss: 4.5263, ppl: 92.4197, loss: 4.5538
	step 14389:lm_loss: 4.5263, ppl: 92.4194, loss: 4.5538
	step 14390:lm_loss: 4.5263, ppl: 92.4202, loss: 4.5538
	step 14391:lm_loss: 4.5264, ppl: 92.4286, loss: 4.5539
	step 14392:lm_loss: 4.5265, ppl: 92.4323, loss: 4.5540
	step 14393:lm_loss: 4.5265, ppl: 92.4307, loss: 4.5539
	step 14394:lm_loss: 4.5265, ppl: 92.4301, loss: 4.5539
	step 14395:lm_loss: 4.5265, ppl: 92.4334, loss: 4.5540
	step 14396:lm_loss: 4.5264, ppl: 92.4290, loss: 4.5539
	step 14397:lm_loss: 4.5264, ppl: 92.4298, loss: 4.5539
	step 14398:lm_loss: 4.5265, ppl: 92.4349, loss: 4.5539
	step 14399:lm_loss: 4.5265, ppl: 92.4373, loss: 4.5540
	step 14400:lm_loss: 4.5265, ppl: 92.4383, loss: 4.5540
	step 14401:lm_loss: 4.5266, ppl: 92.4427, loss: 4.5540
	step 14402:lm_loss: 4.5266, ppl: 92.4402, loss: 4.5540
	step 14403:lm_loss: 4.5265, ppl: 92.4389, loss: 4.5540
	step 14404:lm_loss: 4.5265, ppl: 92.4375, loss: 4.5540
	step 14405:lm_loss: 4.5265, ppl: 92.4321, loss: 4.5539
	step 14406:lm_loss: 4.5265, ppl: 92.4324, loss: 4.5539
	step 14407:lm_loss: 4.5265, ppl: 92.4342, loss: 4.5539
	step 14408:lm_loss: 4.5265, ppl: 92.4336, loss: 4.5539
	step 14409:lm_loss: 4.5265, ppl: 92.4316, loss: 4.5539
	step 14410:lm_loss: 4.5265, ppl: 92.4316, loss: 4.5539
	step 14411:lm_loss: 4.5265, ppl: 92.4314, loss: 4.5539
	step 14412:lm_loss: 4.5264, ppl: 92.4288, loss: 4.5539
	step 14413:lm_loss: 4.5264, ppl: 92.4286, loss: 4.5538
	step 14414:lm_loss: 4.5264, ppl: 92.4284, loss: 4.5538
	step 14415:lm_loss: 4.5265, ppl: 92.4302, loss: 4.5539
	step 14416:lm_loss: 4.5265, ppl: 92.4358, loss: 4.5539
	step 14417:lm_loss: 4.5266, ppl: 92.4423, loss: 4.5540
	step 14418:lm_loss: 4.5266, ppl: 92.4476, loss: 4.5540
	step 14419:lm_loss: 4.5266, ppl: 92.4464, loss: 4.5540
	step 14420:lm_loss: 4.5266, ppl: 92.4477, loss: 4.5540
	step 14421:lm_loss: 4.5266, ppl: 92.4471, loss: 4.5540
	step 14422:lm_loss: 4.5266, ppl: 92.4473, loss: 4.5540
	step 14423:lm_loss: 4.5266, ppl: 92.4436, loss: 4.5539
	step 14424:lm_loss: 4.5266, ppl: 92.4438, loss: 4.5539
	step 14425:lm_loss: 4.5266, ppl: 92.4470, loss: 4.5540
	step 14426:lm_loss: 4.5267, ppl: 92.4525, loss: 4.5540
	step 14427:lm_loss: 4.5268, ppl: 92.4598, loss: 4.5541
	step 14428:lm_loss: 4.5268, ppl: 92.4621, loss: 4.5541
	step 14429:lm_loss: 4.5266, ppl: 92.4462, loss: 4.5540
	step 14430:lm_loss: 4.5267, ppl: 92.4535, loss: 4.5541
	step 14431:lm_loss: 4.5268, ppl: 92.4579, loss: 4.5541
	step 14432:lm_loss: 4.5268, ppl: 92.4595, loss: 4.5542
	step 14433:lm_loss: 4.5267, ppl: 92.4538, loss: 4.5541
	step 14434:lm_loss: 4.5267, ppl: 92.4516, loss: 4.5541
	step 14435:lm_loss: 4.5267, ppl: 92.4501, loss: 4.5540
	step 14436:lm_loss: 4.5267, ppl: 92.4522, loss: 4.5541
	step 14437:lm_loss: 4.5267, ppl: 92.4514, loss: 4.5541
	step 14438:lm_loss: 4.5267, ppl: 92.4559, loss: 4.5541
	step 14439:lm_loss: 4.5267, ppl: 92.4504, loss: 4.5540
	step 14440:lm_loss: 4.5267, ppl: 92.4539, loss: 4.5541
	step 14441:lm_loss: 4.5268, ppl: 92.4625, loss: 4.5542
	step 14442:lm_loss: 4.5268, ppl: 92.4652, loss: 4.5542
	step 14443:lm_loss: 4.5268, ppl: 92.4662, loss: 4.5542
	step 14444:lm_loss: 4.5269, ppl: 92.4683, loss: 4.5542
	step 14445:lm_loss: 4.5268, ppl: 92.4600, loss: 4.5542
	step 14446:lm_loss: 4.5268, ppl: 92.4652, loss: 4.5542
	step 14447:lm_loss: 4.5268, ppl: 92.4664, loss: 4.5542
	step 14448:lm_loss: 4.5268, ppl: 92.4650, loss: 4.5542
	step 14449:lm_loss: 4.5268, ppl: 92.4637, loss: 4.5542
	step 14450:lm_loss: 4.5267, ppl: 92.4538, loss: 4.5541
	step 14451:lm_loss: 4.5267, ppl: 92.4550, loss: 4.5541
	step 14452:lm_loss: 4.5266, ppl: 92.4435, loss: 4.5540
	step 14453:lm_loss: 4.5266, ppl: 92.4463, loss: 4.5540
	step 14454:lm_loss: 4.5266, ppl: 92.4403, loss: 4.5540
	step 14455:lm_loss: 4.5266, ppl: 92.4393, loss: 4.5539
	step 14456:lm_loss: 4.5265, ppl: 92.4373, loss: 4.5539
	step 14457:lm_loss: 4.5265, ppl: 92.4327, loss: 4.5539
	step 14458:lm_loss: 4.5265, ppl: 92.4307, loss: 4.5538
	step 14459:lm_loss: 4.5264, ppl: 92.4271, loss: 4.5538
	step 14460:lm_loss: 4.5264, ppl: 92.4253, loss: 4.5537
	step 14461:lm_loss: 4.5263, ppl: 92.4150, loss: 4.5536
	step 14462:lm_loss: 4.5263, ppl: 92.4162, loss: 4.5536
	step 14463:lm_loss: 4.5263, ppl: 92.4140, loss: 4.5536
	step 14464:lm_loss: 4.5262, ppl: 92.4108, loss: 4.5536
	step 14465:lm_loss: 4.5263, ppl: 92.4118, loss: 4.5536
	step 14466:lm_loss: 4.5263, ppl: 92.4118, loss: 4.5536
	step 14467:lm_loss: 4.5263, ppl: 92.4174, loss: 4.5536
	step 14468:lm_loss: 4.5262, ppl: 92.4041, loss: 4.5535
	step 14469:lm_loss: 4.5262, ppl: 92.4089, loss: 4.5535
	step 14470:lm_loss: 4.5262, ppl: 92.4099, loss: 4.5535
	step 14471:lm_loss: 4.5263, ppl: 92.4137, loss: 4.5536
	step 14472:lm_loss: 4.5263, ppl: 92.4130, loss: 4.5536
	step 14473:lm_loss: 4.5263, ppl: 92.4193, loss: 4.5536
	step 14474:lm_loss: 4.5263, ppl: 92.4119, loss: 4.5535
	step 14475:lm_loss: 4.5261, ppl: 92.3975, loss: 4.5534
	step 14476:lm_loss: 4.5260, ppl: 92.3922, loss: 4.5534
	step 14477:lm_loss: 4.5261, ppl: 92.3951, loss: 4.5534
	step 14478:lm_loss: 4.5261, ppl: 92.4003, loss: 4.5535
	step 14479:lm_loss: 4.5260, ppl: 92.3895, loss: 4.5534
	step 14480:lm_loss: 4.5260, ppl: 92.3915, loss: 4.5534
	step 14481:lm_loss: 4.5261, ppl: 92.3978, loss: 4.5535
	step 14482:lm_loss: 4.5261, ppl: 92.3962, loss: 4.5534
	step 14483:lm_loss: 4.5262, ppl: 92.4041, loss: 4.5535
	step 14484:lm_loss: 4.5262, ppl: 92.4030, loss: 4.5535
	step 14485:lm_loss: 4.5262, ppl: 92.4026, loss: 4.5535
	step 14486:lm_loss: 4.5262, ppl: 92.4075, loss: 4.5536
	step 14487:lm_loss: 4.5262, ppl: 92.4100, loss: 4.5536
	step 14488:lm_loss: 4.5262, ppl: 92.4044, loss: 4.5535
	step 14489:lm_loss: 4.5262, ppl: 92.4040, loss: 4.5535
	step 14490:lm_loss: 4.5261, ppl: 92.4003, loss: 4.5535
	step 14491:lm_loss: 4.5261, ppl: 92.3968, loss: 4.5534
	step 14492:lm_loss: 4.5261, ppl: 92.3945, loss: 4.5534
	step 14493:lm_loss: 4.5261, ppl: 92.3930, loss: 4.5534
	step 14494:lm_loss: 4.5261, ppl: 92.3953, loss: 4.5534
	step 14495:lm_loss: 4.5261, ppl: 92.3975, loss: 4.5534
	step 14496:lm_loss: 4.5260, ppl: 92.3912, loss: 4.5533
	step 14497:lm_loss: 4.5260, ppl: 92.3897, loss: 4.5533
	step 14498:lm_loss: 4.5260, ppl: 92.3912, loss: 4.5533
	step 14499:lm_loss: 4.5260, ppl: 92.3848, loss: 4.5532
	step 14500:lm_loss: 4.5259, ppl: 92.3815, loss: 4.5532
	step 14501:lm_loss: 4.5260, ppl: 92.3849, loss: 4.5532
	step 14502:lm_loss: 4.5260, ppl: 92.3897, loss: 4.5533
	step 14503:lm_loss: 4.5260, ppl: 92.3890, loss: 4.5533
	step 14504:lm_loss: 4.5261, ppl: 92.3972, loss: 4.5534
	step 14505:lm_loss: 4.5261, ppl: 92.4010, loss: 4.5534
	step 14506:lm_loss: 4.5262, ppl: 92.4045, loss: 4.5534
	step 14507:lm_loss: 4.5261, ppl: 92.3998, loss: 4.5534
	step 14508:lm_loss: 4.5260, ppl: 92.3898, loss: 4.5533
	step 14509:lm_loss: 4.5261, ppl: 92.3952, loss: 4.5534
	step 14510:lm_loss: 4.5261, ppl: 92.3949, loss: 4.5534
	step 14511:lm_loss: 4.5261, ppl: 92.3949, loss: 4.5534
	step 14512:lm_loss: 4.5261, ppl: 92.3991, loss: 4.5534
	step 14513:lm_loss: 4.5262, ppl: 92.4023, loss: 4.5535
	step 14514:lm_loss: 4.5261, ppl: 92.4001, loss: 4.5534
	step 14515:lm_loss: 4.5262, ppl: 92.4034, loss: 4.5534
	step 14516:lm_loss: 4.5262, ppl: 92.4030, loss: 4.5534
	step 14517:lm_loss: 4.5261, ppl: 92.4019, loss: 4.5534
	step 14518:lm_loss: 4.5262, ppl: 92.4023, loss: 4.5534
	step 14519:lm_loss: 4.5262, ppl: 92.4061, loss: 4.5535
	step 14520:lm_loss: 4.5262, ppl: 92.4031, loss: 4.5534
	step 14521:lm_loss: 4.5261, ppl: 92.4013, loss: 4.5534
	step 14522:lm_loss: 4.5261, ppl: 92.3997, loss: 4.5534
	step 14523:lm_loss: 4.5261, ppl: 92.3997, loss: 4.5534
	step 14524:lm_loss: 4.5262, ppl: 92.4033, loss: 4.5534
	step 14525:lm_loss: 4.5261, ppl: 92.3991, loss: 4.5533
	step 14526:lm_loss: 4.5261, ppl: 92.3999, loss: 4.5533
	step 14527:lm_loss: 4.5261, ppl: 92.3980, loss: 4.5533
	step 14528:lm_loss: 4.5261, ppl: 92.3995, loss: 4.5533
	step 14529:lm_loss: 4.5261, ppl: 92.3990, loss: 4.5533
	step 14530:lm_loss: 4.5261, ppl: 92.3984, loss: 4.5533
	step 14531:lm_loss: 4.5261, ppl: 92.4010, loss: 4.5534
	step 14532:lm_loss: 4.5261, ppl: 92.3984, loss: 4.5533
	step 14533:lm_loss: 4.5262, ppl: 92.4033, loss: 4.5534
	step 14534:lm_loss: 4.5261, ppl: 92.3987, loss: 4.5533
	step 14535:lm_loss: 4.5261, ppl: 92.3933, loss: 4.5533
	step 14536:lm_loss: 4.5261, ppl: 92.3930, loss: 4.5533
	step 14537:lm_loss: 4.5260, ppl: 92.3907, loss: 4.5533
	step 14538:lm_loss: 4.5261, ppl: 92.3969, loss: 4.5533
	step 14539:lm_loss: 4.5262, ppl: 92.4053, loss: 4.5534
	step 14540:lm_loss: 4.5262, ppl: 92.4100, loss: 4.5534
	step 14541:lm_loss: 4.5262, ppl: 92.4087, loss: 4.5534
	step 14542:lm_loss: 4.5262, ppl: 92.4065, loss: 4.5534
	step 14543:lm_loss: 4.5261, ppl: 92.4019, loss: 4.5534
	step 14544:lm_loss: 4.5260, ppl: 92.3891, loss: 4.5532
	step 14545:lm_loss: 4.5260, ppl: 92.3874, loss: 4.5532
	step 14546:lm_loss: 4.5260, ppl: 92.3898, loss: 4.5532
	step 14547:lm_loss: 4.5260, ppl: 92.3885, loss: 4.5532
	step 14548:lm_loss: 4.5260, ppl: 92.3875, loss: 4.5531
	step 14549:lm_loss: 4.5260, ppl: 92.3886, loss: 4.5532
	step 14550:lm_loss: 4.5260, ppl: 92.3850, loss: 4.5531
	step 14551:lm_loss: 4.5260, ppl: 92.3853, loss: 4.5531
	step 14552:lm_loss: 4.5260, ppl: 92.3900, loss: 4.5532
	step 14553:lm_loss: 4.5258, ppl: 92.3744, loss: 4.5531
	step 14554:lm_loss: 4.5259, ppl: 92.3783, loss: 4.5531
	step 14555:lm_loss: 4.5259, ppl: 92.3746, loss: 4.5530
	step 14556:lm_loss: 4.5259, ppl: 92.3827, loss: 4.5531
	step 14557:lm_loss: 4.5260, ppl: 92.3869, loss: 4.5532
	step 14558:lm_loss: 4.5260, ppl: 92.3869, loss: 4.5532
	step 14559:lm_loss: 4.5260, ppl: 92.3900, loss: 4.5532
	step 14560:lm_loss: 4.5259, ppl: 92.3820, loss: 4.5531
	step 14561:lm_loss: 4.5259, ppl: 92.3834, loss: 4.5531
	step 14562:lm_loss: 4.5260, ppl: 92.3923, loss: 4.5532
	step 14563:lm_loss: 4.5260, ppl: 92.3910, loss: 4.5532
	step 14564:lm_loss: 4.5260, ppl: 92.3907, loss: 4.5532
	step 14565:lm_loss: 4.5260, ppl: 92.3893, loss: 4.5532
	step 14566:lm_loss: 4.5260, ppl: 92.3917, loss: 4.5532
	step 14567:lm_loss: 4.5259, ppl: 92.3823, loss: 4.5531
	step 14568:lm_loss: 4.5259, ppl: 92.3823, loss: 4.5531
	step 14569:lm_loss: 4.5259, ppl: 92.3803, loss: 4.5531
	step 14570:lm_loss: 4.5260, ppl: 92.3839, loss: 4.5531
	step 14571:lm_loss: 4.5260, ppl: 92.3907, loss: 4.5532
	step 14572:lm_loss: 4.5260, ppl: 92.3873, loss: 4.5532
	step 14573:lm_loss: 4.5260, ppl: 92.3864, loss: 4.5532
	step 14574:lm_loss: 4.5260, ppl: 92.3878, loss: 4.5532
	step 14575:lm_loss: 4.5260, ppl: 92.3875, loss: 4.5532
	step 14576:lm_loss: 4.5261, ppl: 92.3946, loss: 4.5533
	step 14577:lm_loss: 4.5261, ppl: 92.3982, loss: 4.5533
	step 14578:lm_loss: 4.5261, ppl: 92.3977, loss: 4.5533
	step 14579:lm_loss: 4.5261, ppl: 92.4007, loss: 4.5533
	step 14580:lm_loss: 4.5262, ppl: 92.4092, loss: 4.5534
	step 14581:lm_loss: 4.5262, ppl: 92.4110, loss: 4.5535
	step 14582:lm_loss: 4.5262, ppl: 92.4103, loss: 4.5534
	step 14583:lm_loss: 4.5263, ppl: 92.4150, loss: 4.5535
	step 14584:lm_loss: 4.5263, ppl: 92.4195, loss: 4.5535
	step 14585:lm_loss: 4.5264, ppl: 92.4222, loss: 4.5535
	step 14586:lm_loss: 4.5263, ppl: 92.4170, loss: 4.5535
	step 14587:lm_loss: 4.5263, ppl: 92.4200, loss: 4.5535
	step 14588:lm_loss: 4.5264, ppl: 92.4208, loss: 4.5535
	step 14589:lm_loss: 4.5263, ppl: 92.4144, loss: 4.5534
	step 14590:lm_loss: 4.5262, ppl: 92.4103, loss: 4.5534
	step 14591:lm_loss: 4.5262, ppl: 92.4079, loss: 4.5534
	step 14592:lm_loss: 4.5262, ppl: 92.4060, loss: 4.5534
	step 14593:lm_loss: 4.5261, ppl: 92.3953, loss: 4.5532
	step 14594:lm_loss: 4.5260, ppl: 92.3894, loss: 4.5532
	step 14595:lm_loss: 4.5260, ppl: 92.3904, loss: 4.5532
	step 14596:lm_loss: 4.5260, ppl: 92.3874, loss: 4.5531
	step 14597:lm_loss: 4.5260, ppl: 92.3850, loss: 4.5531
	step 14598:lm_loss: 4.5260, ppl: 92.3863, loss: 4.5531
	step 14599:lm_loss: 4.5259, ppl: 92.3805, loss: 4.5531
	step 14600:lm_loss: 4.5260, ppl: 92.3869, loss: 4.5531
	step 14601:lm_loss: 4.5260, ppl: 92.3852, loss: 4.5531
	step 14602:lm_loss: 4.5260, ppl: 92.3892, loss: 4.5532
	step 14603:lm_loss: 4.5260, ppl: 92.3920, loss: 4.5532
	step 14604:lm_loss: 4.5260, ppl: 92.3916, loss: 4.5532
	step 14605:lm_loss: 4.5260, ppl: 92.3906, loss: 4.5532
	step 14606:lm_loss: 4.5261, ppl: 92.3946, loss: 4.5532
	step 14607:lm_loss: 4.5261, ppl: 92.3950, loss: 4.5532
	step 14608:lm_loss: 4.5261, ppl: 92.3931, loss: 4.5532
	step 14609:lm_loss: 4.5261, ppl: 92.3947, loss: 4.5532
	step 14610:lm_loss: 4.5260, ppl: 92.3914, loss: 4.5532
	step 14611:lm_loss: 4.5261, ppl: 92.3933, loss: 4.5532
	step 14612:lm_loss: 4.5260, ppl: 92.3924, loss: 4.5532
	step 14613:lm_loss: 4.5261, ppl: 92.4003, loss: 4.5533
	step 14614:lm_loss: 4.5261, ppl: 92.3947, loss: 4.5532
	step 14615:lm_loss: 4.5262, ppl: 92.4058, loss: 4.5532
	step 14616:lm_loss: 4.5262, ppl: 92.4072, loss: 4.5533
	step 14617:lm_loss: 4.5261, ppl: 92.4020, loss: 4.5532
	step 14618:lm_loss: 4.5261, ppl: 92.4015, loss: 4.5532
	step 14619:lm_loss: 4.5261, ppl: 92.3997, loss: 4.5531
	step 14620:lm_loss: 4.5262, ppl: 92.4032, loss: 4.5532
	step 14621:lm_loss: 4.5261, ppl: 92.3991, loss: 4.5531
	step 14622:lm_loss: 4.5261, ppl: 92.4004, loss: 4.5531
	step 14623:lm_loss: 4.5259, ppl: 92.3791, loss: 4.5530
	step 14624:lm_loss: 4.5259, ppl: 92.3799, loss: 4.5530
	step 14625:lm_loss: 4.5259, ppl: 92.3810, loss: 4.5530
	step 14626:lm_loss: 4.5260, ppl: 92.3904, loss: 4.5531
	step 14627:lm_loss: 4.5260, ppl: 92.3905, loss: 4.5531
	step 14628:lm_loss: 4.5261, ppl: 92.3979, loss: 4.5531
	step 14629:lm_loss: 4.5261, ppl: 92.3983, loss: 4.5531
	step 14630:lm_loss: 4.5261, ppl: 92.3991, loss: 4.5531
	step 14631:lm_loss: 4.5261, ppl: 92.3948, loss: 4.5531
	step 14632:lm_loss: 4.5261, ppl: 92.3939, loss: 4.5530
	step 14633:lm_loss: 4.5261, ppl: 92.3985, loss: 4.5531
	step 14634:lm_loss: 4.5261, ppl: 92.3975, loss: 4.5531
	step 14635:lm_loss: 4.5262, ppl: 92.4064, loss: 4.5532
	step 14636:lm_loss: 4.5262, ppl: 92.4045, loss: 4.5531
	step 14637:lm_loss: 4.5262, ppl: 92.4053, loss: 4.5531
	step 14638:lm_loss: 4.5262, ppl: 92.4037, loss: 4.5531
	step 14639:lm_loss: 4.5261, ppl: 92.4015, loss: 4.5531
	step 14640:lm_loss: 4.5261, ppl: 92.3961, loss: 4.5531
	step 14641:lm_loss: 4.5261, ppl: 92.3939, loss: 4.5530
	step 14642:lm_loss: 4.5261, ppl: 92.3990, loss: 4.5531
	step 14643:lm_loss: 4.5260, ppl: 92.3927, loss: 4.5531
	step 14644:lm_loss: 4.5261, ppl: 92.3969, loss: 4.5531
	step 14645:lm_loss: 4.5261, ppl: 92.3964, loss: 4.5531
	step 14646:lm_loss: 4.5260, ppl: 92.3902, loss: 4.5530
	step 14647:lm_loss: 4.5261, ppl: 92.3971, loss: 4.5531
	step 14648:lm_loss: 4.5261, ppl: 92.3949, loss: 4.5531
	step 14649:lm_loss: 4.5261, ppl: 92.3948, loss: 4.5531
	step 14650:lm_loss: 4.5260, ppl: 92.3921, loss: 4.5530
	step 14651:lm_loss: 4.5261, ppl: 92.3940, loss: 4.5531
	step 14652:lm_loss: 4.5261, ppl: 92.3938, loss: 4.5531
	step 14653:lm_loss: 4.5259, ppl: 92.3805, loss: 4.5529
	step 14654:lm_loss: 4.5260, ppl: 92.3889, loss: 4.5530
	step 14655:lm_loss: 4.5261, ppl: 92.3955, loss: 4.5531
	step 14656:lm_loss: 4.5260, ppl: 92.3924, loss: 4.5530
	step 14657:lm_loss: 4.5261, ppl: 92.3930, loss: 4.5530
	step 14658:lm_loss: 4.5261, ppl: 92.3950, loss: 4.5530
	step 14659:lm_loss: 4.5261, ppl: 92.3973, loss: 4.5531
	step 14660:lm_loss: 4.5262, ppl: 92.4063, loss: 4.5532
	step 14661:lm_loss: 4.5262, ppl: 92.4061, loss: 4.5532
	step 14662:lm_loss: 4.5262, ppl: 92.4062, loss: 4.5532
	step 14663:lm_loss: 4.5262, ppl: 92.4038, loss: 4.5532
	step 14664:lm_loss: 4.5261, ppl: 92.4011, loss: 4.5531
	step 14665:lm_loss: 4.5261, ppl: 92.3997, loss: 4.5531
	step 14666:lm_loss: 4.5261, ppl: 92.3950, loss: 4.5531
	step 14667:lm_loss: 4.5261, ppl: 92.3950, loss: 4.5531
	step 14668:lm_loss: 4.5261, ppl: 92.3935, loss: 4.5531
	step 14669:lm_loss: 4.5260, ppl: 92.3926, loss: 4.5530
	step 14670:lm_loss: 4.5260, ppl: 92.3927, loss: 4.5530
	step 14671:lm_loss: 4.5260, ppl: 92.3863, loss: 4.5529
	step 14672:lm_loss: 4.5260, ppl: 92.3889, loss: 4.5530
	step 14673:lm_loss: 4.5260, ppl: 92.3906, loss: 4.5530
	step 14674:lm_loss: 4.5261, ppl: 92.3983, loss: 4.5530
	step 14675:lm_loss: 4.5261, ppl: 92.3968, loss: 4.5530
	step 14676:lm_loss: 4.5261, ppl: 92.3969, loss: 4.5530
	step 14677:lm_loss: 4.5261, ppl: 92.3989, loss: 4.5530
	step 14678:lm_loss: 4.5261, ppl: 92.3986, loss: 4.5530
	step 14679:lm_loss: 4.5262, ppl: 92.4024, loss: 4.5531
	step 14680:lm_loss: 4.5262, ppl: 92.4113, loss: 4.5532
	step 14681:lm_loss: 4.5262, ppl: 92.4046, loss: 4.5531
	step 14682:lm_loss: 4.5261, ppl: 92.3955, loss: 4.5530
	step 14683:lm_loss: 4.5260, ppl: 92.3874, loss: 4.5530
	step 14684:lm_loss: 4.5260, ppl: 92.3900, loss: 4.5530
	step 14685:lm_loss: 4.5260, ppl: 92.3906, loss: 4.5530
	step 14686:lm_loss: 4.5261, ppl: 92.3956, loss: 4.5531
	step 14687:lm_loss: 4.5261, ppl: 92.3994, loss: 4.5531
	step 14688:lm_loss: 4.5262, ppl: 92.4039, loss: 4.5532
	step 14689:lm_loss: 4.5261, ppl: 92.3960, loss: 4.5531
	step 14690:lm_loss: 4.5260, ppl: 92.3847, loss: 4.5530
	step 14691:lm_loss: 4.5260, ppl: 92.3860, loss: 4.5530
	step 14692:lm_loss: 4.5260, ppl: 92.3902, loss: 4.5531
	step 14693:lm_loss: 4.5259, ppl: 92.3820, loss: 4.5530
	step 14694:lm_loss: 4.5260, ppl: 92.3871, loss: 4.5530
	step 14695:lm_loss: 4.5260, ppl: 92.3873, loss: 4.5530
	step 14696:lm_loss: 4.5260, ppl: 92.3866, loss: 4.5530
	step 14697:lm_loss: 4.5259, ppl: 92.3797, loss: 4.5530
	step 14698:lm_loss: 4.5259, ppl: 92.3757, loss: 4.5529
	step 14699:lm_loss: 4.5259, ppl: 92.3788, loss: 4.5530
	step 14700:lm_loss: 4.5259, ppl: 92.3762, loss: 4.5529
	step 14701:lm_loss: 4.5259, ppl: 92.3759, loss: 4.5529
	step 14702:lm_loss: 4.5259, ppl: 92.3820, loss: 4.5530
	step 14703:lm_loss: 4.5259, ppl: 92.3784, loss: 4.5530
	step 14704:lm_loss: 4.5259, ppl: 92.3806, loss: 4.5530
	step 14705:lm_loss: 4.5259, ppl: 92.3802, loss: 4.5530
	step 14706:lm_loss: 4.5259, ppl: 92.3799, loss: 4.5530
	step 14707:lm_loss: 4.5259, ppl: 92.3751, loss: 4.5529
	step 14708:lm_loss: 4.5258, ppl: 92.3676, loss: 4.5528
	step 14709:lm_loss: 4.5258, ppl: 92.3709, loss: 4.5529
	step 14710:lm_loss: 4.5259, ppl: 92.3746, loss: 4.5529
	step 14711:lm_loss: 4.5258, ppl: 92.3710, loss: 4.5529
	step 14712:lm_loss: 4.5259, ppl: 92.3750, loss: 4.5529
	step 14713:lm_loss: 4.5258, ppl: 92.3732, loss: 4.5529
	step 14714:lm_loss: 4.5258, ppl: 92.3680, loss: 4.5528
	step 14715:lm_loss: 4.5258, ppl: 92.3703, loss: 4.5529
	step 14716:lm_loss: 4.5258, ppl: 92.3725, loss: 4.5529
	step 14717:lm_loss: 4.5259, ppl: 92.3775, loss: 4.5529
	step 14718:lm_loss: 4.5259, ppl: 92.3788, loss: 4.5529
	step 14719:lm_loss: 4.5259, ppl: 92.3748, loss: 4.5529
	step 14720:lm_loss: 4.5260, ppl: 92.3886, loss: 4.5530
	step 14721:lm_loss: 4.5260, ppl: 92.3916, loss: 4.5530
	step 14722:lm_loss: 4.5260, ppl: 92.3915, loss: 4.5530
	step 14723:lm_loss: 4.5260, ppl: 92.3868, loss: 4.5530
	step 14724:lm_loss: 4.5260, ppl: 92.3848, loss: 4.5529
	step 14725:lm_loss: 4.5259, ppl: 92.3834, loss: 4.5529
	step 14726:lm_loss: 4.5259, ppl: 92.3828, loss: 4.5529
	step 14727:lm_loss: 4.5260, ppl: 92.3864, loss: 4.5529
	step 14728:lm_loss: 4.5260, ppl: 92.3870, loss: 4.5529
	step 14729:lm_loss: 4.5260, ppl: 92.3917, loss: 4.5530
	step 14730:lm_loss: 4.5261, ppl: 92.3931, loss: 4.5530
	step 14731:lm_loss: 4.5261, ppl: 92.3973, loss: 4.5531
	step 14732:lm_loss: 4.5261, ppl: 92.3994, loss: 4.5531
	step 14733:lm_loss: 4.5262, ppl: 92.4047, loss: 4.5531
	step 14734:lm_loss: 4.5261, ppl: 92.3999, loss: 4.5531
	step 14735:lm_loss: 4.5261, ppl: 92.4005, loss: 4.5531
	step 14736:lm_loss: 4.5261, ppl: 92.3975, loss: 4.5530
	step 14737:lm_loss: 4.5262, ppl: 92.4049, loss: 4.5531
	step 14738:lm_loss: 4.5262, ppl: 92.4045, loss: 4.5531
	step 14739:lm_loss: 4.5262, ppl: 92.4103, loss: 4.5532
	step 14740:lm_loss: 4.5262, ppl: 92.4046, loss: 4.5531
	step 14741:lm_loss: 4.5261, ppl: 92.3989, loss: 4.5530
	step 14742:lm_loss: 4.5262, ppl: 92.4032, loss: 4.5531
	step 14743:lm_loss: 4.5262, ppl: 92.4083, loss: 4.5531
	step 14744:lm_loss: 4.5262, ppl: 92.4092, loss: 4.5531
	step 14745:lm_loss: 4.5262, ppl: 92.4114, loss: 4.5532
	step 14746:lm_loss: 4.5262, ppl: 92.4111, loss: 4.5532
	step 14747:lm_loss: 4.5262, ppl: 92.4081, loss: 4.5531
	step 14748:lm_loss: 4.5262, ppl: 92.4061, loss: 4.5531
	step 14749:lm_loss: 4.5262, ppl: 92.4057, loss: 4.5531
	step 14750:lm_loss: 4.5262, ppl: 92.4091, loss: 4.5531
	step 14751:lm_loss: 4.5262, ppl: 92.4087, loss: 4.5531
	step 14752:lm_loss: 4.5262, ppl: 92.4103, loss: 4.5531
	step 14753:lm_loss: 4.5262, ppl: 92.4095, loss: 4.5531
	step 14754:lm_loss: 4.5262, ppl: 92.4086, loss: 4.5531
	step 14755:lm_loss: 4.5262, ppl: 92.4024, loss: 4.5531
	step 14756:lm_loss: 4.5261, ppl: 92.4005, loss: 4.5531
	step 14757:lm_loss: 4.5262, ppl: 92.4063, loss: 4.5531
	step 14758:lm_loss: 4.5261, ppl: 92.4020, loss: 4.5531
	step 14759:lm_loss: 4.5261, ppl: 92.3994, loss: 4.5530
	step 14760:lm_loss: 4.5261, ppl: 92.3980, loss: 4.5530
	step 14761:lm_loss: 4.5262, ppl: 92.4044, loss: 4.5531
	step 14762:lm_loss: 4.5262, ppl: 92.4049, loss: 4.5531
	step 14763:lm_loss: 4.5262, ppl: 92.4027, loss: 4.5530
	step 14764:lm_loss: 4.5261, ppl: 92.4018, loss: 4.5530
	step 14765:lm_loss: 4.5261, ppl: 92.3998, loss: 4.5530
	step 14766:lm_loss: 4.5261, ppl: 92.3949, loss: 4.5529
	step 14767:lm_loss: 4.5261, ppl: 92.3941, loss: 4.5529
	step 14768:lm_loss: 4.5260, ppl: 92.3904, loss: 4.5529
	step 14769:lm_loss: 4.5260, ppl: 92.3925, loss: 4.5529
	step 14770:lm_loss: 4.5260, ppl: 92.3859, loss: 4.5528
	step 14771:lm_loss: 4.5259, ppl: 92.3792, loss: 4.5528
	step 14772:lm_loss: 4.5258, ppl: 92.3715, loss: 4.5527
	step 14773:lm_loss: 4.5258, ppl: 92.3740, loss: 4.5527
	step 14774:lm_loss: 4.5259, ppl: 92.3749, loss: 4.5527
	step 14775:lm_loss: 4.5259, ppl: 92.3775, loss: 4.5528
	step 14776:lm_loss: 4.5259, ppl: 92.3761, loss: 4.5527
	step 14777:lm_loss: 4.5259, ppl: 92.3754, loss: 4.5527
	step 14778:lm_loss: 4.5258, ppl: 92.3680, loss: 4.5526
	step 14779:lm_loss: 4.5258, ppl: 92.3719, loss: 4.5527
	step 14780:lm_loss: 4.5258, ppl: 92.3680, loss: 4.5526
	step 14781:lm_loss: 4.5258, ppl: 92.3738, loss: 4.5527
	step 14782:lm_loss: 4.5258, ppl: 92.3668, loss: 4.5527
	step 14783:lm_loss: 4.5257, ppl: 92.3610, loss: 4.5526
	step 14784:lm_loss: 4.5258, ppl: 92.3653, loss: 4.5526
	step 14785:lm_loss: 4.5257, ppl: 92.3592, loss: 4.5526
	step 14786:lm_loss: 4.5257, ppl: 92.3603, loss: 4.5526
	step 14787:lm_loss: 4.5257, ppl: 92.3614, loss: 4.5526
	step 14788:lm_loss: 4.5257, ppl: 92.3572, loss: 4.5525
	step 14789:lm_loss: 4.5256, ppl: 92.3548, loss: 4.5525
	step 14790:lm_loss: 4.5256, ppl: 92.3549, loss: 4.5525
	step 14791:lm_loss: 4.5256, ppl: 92.3539, loss: 4.5525
	step 14792:lm_loss: 4.5257, ppl: 92.3560, loss: 4.5525
	step 14793:lm_loss: 4.5257, ppl: 92.3579, loss: 4.5525
	step 14794:lm_loss: 4.5256, ppl: 92.3526, loss: 4.5525
	step 14795:lm_loss: 4.5257, ppl: 92.3578, loss: 4.5525
	step 14796:lm_loss: 4.5257, ppl: 92.3588, loss: 4.5525
	step 14797:lm_loss: 4.5257, ppl: 92.3613, loss: 4.5526
	step 14798:lm_loss: 4.5256, ppl: 92.3485, loss: 4.5524
	step 14799:lm_loss: 4.5255, ppl: 92.3464, loss: 4.5524
	step 14800:lm_loss: 4.5255, ppl: 92.3464, loss: 4.5524
	step 14801:lm_loss: 4.5256, ppl: 92.3477, loss: 4.5524
	step 14802:lm_loss: 4.5255, ppl: 92.3413, loss: 4.5523
	step 14803:lm_loss: 4.5255, ppl: 92.3435, loss: 4.5523
	step 14804:lm_loss: 4.5255, ppl: 92.3410, loss: 4.5523
	step 14805:lm_loss: 4.5256, ppl: 92.3474, loss: 4.5524
	step 14806:lm_loss: 4.5255, ppl: 92.3450, loss: 4.5523
	step 14807:lm_loss: 4.5256, ppl: 92.3512, loss: 4.5524
	step 14808:lm_loss: 4.5257, ppl: 92.3574, loss: 4.5524
	step 14809:lm_loss: 4.5257, ppl: 92.3590, loss: 4.5524
	step 14810:lm_loss: 4.5257, ppl: 92.3608, loss: 4.5525
	step 14811:lm_loss: 4.5256, ppl: 92.3539, loss: 4.5524
	step 14812:lm_loss: 4.5256, ppl: 92.3473, loss: 4.5523
	step 14813:lm_loss: 4.5256, ppl: 92.3537, loss: 4.5524
	step 14814:lm_loss: 4.5257, ppl: 92.3562, loss: 4.5524
	step 14815:lm_loss: 4.5257, ppl: 92.3627, loss: 4.5526
	step 14816:lm_loss: 4.5257, ppl: 92.3631, loss: 4.5526
	step 14817:lm_loss: 4.5258, ppl: 92.3723, loss: 4.5527
	step 14818:lm_loss: 4.5257, ppl: 92.3650, loss: 4.5526
	step 14819:lm_loss: 4.5257, ppl: 92.3622, loss: 4.5525
	step 14820:lm_loss: 4.5257, ppl: 92.3581, loss: 4.5525
	step 14821:lm_loss: 4.5257, ppl: 92.3608, loss: 4.5525
	step 14822:lm_loss: 4.5257, ppl: 92.3610, loss: 4.5525
	step 14823:lm_loss: 4.5257, ppl: 92.3637, loss: 4.5525
	step 14824:lm_loss: 4.5258, ppl: 92.3679, loss: 4.5526
	step 14825:lm_loss: 4.5258, ppl: 92.3738, loss: 4.5526
	step 14826:lm_loss: 4.5259, ppl: 92.3756, loss: 4.5526
	step 14827:lm_loss: 4.5259, ppl: 92.3833, loss: 4.5527
	step 14828:lm_loss: 4.5260, ppl: 92.3841, loss: 4.5527
	step 14829:lm_loss: 4.5260, ppl: 92.3890, loss: 4.5528
	step 14830:lm_loss: 4.5260, ppl: 92.3881, loss: 4.5528
	step 14831:lm_loss: 4.5260, ppl: 92.3893, loss: 4.5528
	step 14832:lm_loss: 4.5260, ppl: 92.3885, loss: 4.5528
	step 14833:lm_loss: 4.5260, ppl: 92.3890, loss: 4.5528
	step 14834:lm_loss: 4.5260, ppl: 92.3852, loss: 4.5528
	step 14835:lm_loss: 4.5260, ppl: 92.3876, loss: 4.5528
	step 14836:lm_loss: 4.5260, ppl: 92.3900, loss: 4.5528
	step 14837:lm_loss: 4.5261, ppl: 92.3994, loss: 4.5530
	step 14838:lm_loss: 4.5262, ppl: 92.4064, loss: 4.5530
	step 14839:lm_loss: 4.5262, ppl: 92.4109, loss: 4.5531
	step 14840:lm_loss: 4.5263, ppl: 92.4158, loss: 4.5531
	step 14841:lm_loss: 4.5263, ppl: 92.4138, loss: 4.5531
	step 14842:lm_loss: 4.5261, ppl: 92.4016, loss: 4.5530
	step 14843:lm_loss: 4.5261, ppl: 92.3939, loss: 4.5529
	step 14844:lm_loss: 4.5261, ppl: 92.3973, loss: 4.5530
	step 14845:lm_loss: 4.5261, ppl: 92.3958, loss: 4.5530
	step 14846:lm_loss: 4.5261, ppl: 92.4012, loss: 4.5531
	step 14847:lm_loss: 4.5261, ppl: 92.4016, loss: 4.5531
	step 14848:lm_loss: 4.5262, ppl: 92.4050, loss: 4.5531
	step 14849:lm_loss: 4.5262, ppl: 92.4074, loss: 4.5531
	step 14850:lm_loss: 4.5262, ppl: 92.4087, loss: 4.5531
	step 14851:lm_loss: 4.5262, ppl: 92.4061, loss: 4.5531
	step 14852:lm_loss: 4.5262, ppl: 92.4110, loss: 4.5531
	step 14853:lm_loss: 4.5262, ppl: 92.4113, loss: 4.5531
	step 14854:lm_loss: 4.5262, ppl: 92.4077, loss: 4.5531
	step 14855:lm_loss: 4.5262, ppl: 92.4077, loss: 4.5531
	step 14856:lm_loss: 4.5263, ppl: 92.4132, loss: 4.5532
	step 14857:lm_loss: 4.5263, ppl: 92.4114, loss: 4.5531
	step 14858:lm_loss: 4.5262, ppl: 92.4110, loss: 4.5531
	step 14859:lm_loss: 4.5263, ppl: 92.4171, loss: 4.5532
	step 14860:lm_loss: 4.5263, ppl: 92.4138, loss: 4.5531
	step 14861:lm_loss: 4.5263, ppl: 92.4142, loss: 4.5531
	step 14862:lm_loss: 4.5263, ppl: 92.4183, loss: 4.5532
	step 14863:lm_loss: 4.5264, ppl: 92.4225, loss: 4.5532
	step 14864:lm_loss: 4.5263, ppl: 92.4206, loss: 4.5532
	step 14865:lm_loss: 4.5263, ppl: 92.4193, loss: 4.5532
	step 14866:lm_loss: 4.5263, ppl: 92.4144, loss: 4.5532
	step 14867:lm_loss: 4.5263, ppl: 92.4199, loss: 4.5532
	step 14868:lm_loss: 4.5264, ppl: 92.4230, loss: 4.5532
	step 14869:lm_loss: 4.5264, ppl: 92.4262, loss: 4.5533
	step 14870:lm_loss: 4.5264, ppl: 92.4221, loss: 4.5532
	step 14871:lm_loss: 4.5264, ppl: 92.4219, loss: 4.5532
	step 14872:lm_loss: 4.5264, ppl: 92.4231, loss: 4.5532
	step 14873:lm_loss: 4.5264, ppl: 92.4274, loss: 4.5533
	step 14874:lm_loss: 4.5264, ppl: 92.4288, loss: 4.5533
	step 14875:lm_loss: 4.5265, ppl: 92.4304, loss: 4.5533
	step 14876:lm_loss: 4.5266, ppl: 92.4433, loss: 4.5535
	step 14877:lm_loss: 4.5267, ppl: 92.4499, loss: 4.5535
	step 14878:lm_loss: 4.5267, ppl: 92.4562, loss: 4.5536
	step 14879:lm_loss: 4.5267, ppl: 92.4566, loss: 4.5536
	step 14880:lm_loss: 4.5267, ppl: 92.4564, loss: 4.5536
	step 14881:lm_loss: 4.5267, ppl: 92.4566, loss: 4.5536
	step 14882:lm_loss: 4.5268, ppl: 92.4619, loss: 4.5536
	step 14883:lm_loss: 4.5268, ppl: 92.4595, loss: 4.5536
	step 14884:lm_loss: 4.5267, ppl: 92.4558, loss: 4.5535
	step 14885:lm_loss: 4.5268, ppl: 92.4606, loss: 4.5536
	step 14886:lm_loss: 4.5268, ppl: 92.4635, loss: 4.5536
	step 14887:lm_loss: 4.5268, ppl: 92.4661, loss: 4.5536
	step 14888:lm_loss: 4.5268, ppl: 92.4631, loss: 4.5536
	step 14889:lm_loss: 4.5268, ppl: 92.4645, loss: 4.5536
	step 14890:lm_loss: 4.5269, ppl: 92.4694, loss: 4.5537
	step 14891:lm_loss: 4.5270, ppl: 92.4769, loss: 4.5538
	step 14892:lm_loss: 4.5270, ppl: 92.4794, loss: 4.5538
	step 14893:lm_loss: 4.5270, ppl: 92.4805, loss: 4.5538
	step 14894:lm_loss: 4.5270, ppl: 92.4768, loss: 4.5538
	step 14895:lm_loss: 4.5270, ppl: 92.4778, loss: 4.5538
	step 14896:lm_loss: 4.5269, ppl: 92.4729, loss: 4.5537
	step 14897:lm_loss: 4.5269, ppl: 92.4746, loss: 4.5537
	step 14898:lm_loss: 4.5269, ppl: 92.4760, loss: 4.5538
	step 14899:lm_loss: 4.5270, ppl: 92.4796, loss: 4.5538
	step 14900:lm_loss: 4.5269, ppl: 92.4728, loss: 4.5537
	step 14901:lm_loss: 4.5270, ppl: 92.4792, loss: 4.5538
	step 14902:lm_loss: 4.5270, ppl: 92.4779, loss: 4.5537
	step 14903:lm_loss: 4.5270, ppl: 92.4778, loss: 4.5537
	step 14904:lm_loss: 4.5270, ppl: 92.4811, loss: 4.5538
	step 14905:lm_loss: 4.5271, ppl: 92.4880, loss: 4.5538
	step 14906:lm_loss: 4.5271, ppl: 92.4890, loss: 4.5538
	step 14907:lm_loss: 4.5271, ppl: 92.4859, loss: 4.5538
	step 14908:lm_loss: 4.5271, ppl: 92.4906, loss: 4.5538
	step 14909:lm_loss: 4.5271, ppl: 92.4889, loss: 4.5538
	step 14910:lm_loss: 4.5270, ppl: 92.4841, loss: 4.5538
	step 14911:lm_loss: 4.5269, ppl: 92.4759, loss: 4.5537
	step 14912:lm_loss: 4.5269, ppl: 92.4693, loss: 4.5537
	step 14913:lm_loss: 4.5268, ppl: 92.4618, loss: 4.5536
	step 14914:lm_loss: 4.5268, ppl: 92.4611, loss: 4.5536
	step 14915:lm_loss: 4.5268, ppl: 92.4627, loss: 4.5536
	step 14916:lm_loss: 4.5267, ppl: 92.4550, loss: 4.5536
	step 14917:lm_loss: 4.5267, ppl: 92.4530, loss: 4.5535
	step 14918:lm_loss: 4.5266, ppl: 92.4455, loss: 4.5534
	step 14919:lm_loss: 4.5267, ppl: 92.4553, loss: 4.5536
	step 14920:lm_loss: 4.5267, ppl: 92.4514, loss: 4.5536
	step 14921:lm_loss: 4.5267, ppl: 92.4507, loss: 4.5535
	step 14922:lm_loss: 4.5267, ppl: 92.4539, loss: 4.5536
	step 14923:lm_loss: 4.5267, ppl: 92.4570, loss: 4.5536
	step 14924:lm_loss: 4.5268, ppl: 92.4581, loss: 4.5536
	step 14925:lm_loss: 4.5267, ppl: 92.4562, loss: 4.5536
	step 14926:lm_loss: 4.5268, ppl: 92.4612, loss: 4.5537
	step 14927:lm_loss: 4.5268, ppl: 92.4598, loss: 4.5537
	step 14928:lm_loss: 4.5268, ppl: 92.4646, loss: 4.5537
	step 14929:lm_loss: 4.5269, ppl: 92.4704, loss: 4.5537
	step 14930:lm_loss: 4.5269, ppl: 92.4711, loss: 4.5537
	step 14931:lm_loss: 4.5269, ppl: 92.4732, loss: 4.5538
	step 14932:lm_loss: 4.5269, ppl: 92.4703, loss: 4.5537
	step 14933:lm_loss: 4.5270, ppl: 92.4782, loss: 4.5538
	step 14934:lm_loss: 4.5270, ppl: 92.4828, loss: 4.5539
	step 14935:lm_loss: 4.5270, ppl: 92.4846, loss: 4.5539
	step 14936:lm_loss: 4.5270, ppl: 92.4847, loss: 4.5539
	step 14937:lm_loss: 4.5271, ppl: 92.4872, loss: 4.5539
	step 14938:lm_loss: 4.5271, ppl: 92.4870, loss: 4.5539
	step 14939:lm_loss: 4.5270, ppl: 92.4850, loss: 4.5539
	step 14940:lm_loss: 4.5271, ppl: 92.4863, loss: 4.5539
	step 14941:lm_loss: 4.5270, ppl: 92.4794, loss: 4.5538
	step 14942:lm_loss: 4.5270, ppl: 92.4774, loss: 4.5538
	step 14943:lm_loss: 4.5270, ppl: 92.4780, loss: 4.5538
	step 14944:lm_loss: 4.5269, ppl: 92.4700, loss: 4.5537
	step 14945:lm_loss: 4.5268, ppl: 92.4611, loss: 4.5536
	step 14946:lm_loss: 4.5268, ppl: 92.4598, loss: 4.5536
	step 14947:lm_loss: 4.5268, ppl: 92.4635, loss: 4.5537
	step 14948:lm_loss: 4.5269, ppl: 92.4681, loss: 4.5537
	step 14949:lm_loss: 4.5269, ppl: 92.4679, loss: 4.5537
	step 14950:lm_loss: 4.5268, ppl: 92.4580, loss: 4.5536
	step 14951:lm_loss: 4.5268, ppl: 92.4615, loss: 4.5536
	step 14952:lm_loss: 4.5269, ppl: 92.4679, loss: 4.5537
	step 14953:lm_loss: 4.5268, ppl: 92.4665, loss: 4.5536
	step 14954:lm_loss: 4.5269, ppl: 92.4713, loss: 4.5537
	step 14955:lm_loss: 4.5269, ppl: 92.4739, loss: 4.5537
	step 14956:lm_loss: 4.5270, ppl: 92.4781, loss: 4.5537
	step 14957:lm_loss: 4.5271, ppl: 92.4860, loss: 4.5538
	step 14958:lm_loss: 4.5271, ppl: 92.4889, loss: 4.5539
	step 14959:lm_loss: 4.5271, ppl: 92.4896, loss: 4.5539
	step 14960:lm_loss: 4.5270, ppl: 92.4838, loss: 4.5538
	step 14961:lm_loss: 4.5270, ppl: 92.4786, loss: 4.5538
	step 14962:lm_loss: 4.5269, ppl: 92.4758, loss: 4.5537
	step 14963:lm_loss: 4.5270, ppl: 92.4763, loss: 4.5538
	step 14964:lm_loss: 4.5270, ppl: 92.4767, loss: 4.5538
	step 14965:lm_loss: 4.5270, ppl: 92.4763, loss: 4.5537
	step 14966:lm_loss: 4.5269, ppl: 92.4757, loss: 4.5537
	step 14967:lm_loss: 4.5269, ppl: 92.4723, loss: 4.5537
	step 14968:lm_loss: 4.5269, ppl: 92.4740, loss: 4.5537
	step 14969:lm_loss: 4.5270, ppl: 92.4762, loss: 4.5537
	step 14970:lm_loss: 4.5269, ppl: 92.4749, loss: 4.5537
	step 14971:lm_loss: 4.5270, ppl: 92.4772, loss: 4.5538
	step 14972:lm_loss: 4.5270, ppl: 92.4787, loss: 4.5538
	step 14973:lm_loss: 4.5270, ppl: 92.4795, loss: 4.5538
	step 14974:lm_loss: 4.5270, ppl: 92.4819, loss: 4.5538
	step 14975:lm_loss: 4.5270, ppl: 92.4786, loss: 4.5538
	step 14976:lm_loss: 4.5270, ppl: 92.4785, loss: 4.5538
	step 14977:lm_loss: 4.5269, ppl: 92.4675, loss: 4.5537
	step 14978:lm_loss: 4.5268, ppl: 92.4627, loss: 4.5537
	step 14979:lm_loss: 4.5268, ppl: 92.4612, loss: 4.5536
	step 14980:lm_loss: 4.5268, ppl: 92.4580, loss: 4.5536
	step 14981:lm_loss: 4.5267, ppl: 92.4526, loss: 4.5535
	step 14982:lm_loss: 4.5267, ppl: 92.4531, loss: 4.5535
	step 14983:lm_loss: 4.5268, ppl: 92.4591, loss: 4.5536
	step 14984:lm_loss: 4.5267, ppl: 92.4557, loss: 4.5535
	step 14985:lm_loss: 4.5267, ppl: 92.4550, loss: 4.5535
	step 14986:lm_loss: 4.5268, ppl: 92.4588, loss: 4.5536
	step 14987:lm_loss: 4.5268, ppl: 92.4620, loss: 4.5536
	step 14988:lm_loss: 4.5268, ppl: 92.4630, loss: 4.5536
	step 14989:lm_loss: 4.5268, ppl: 92.4665, loss: 4.5536
	step 14990:lm_loss: 4.5268, ppl: 92.4667, loss: 4.5536
	step 14991:lm_loss: 4.5268, ppl: 92.4654, loss: 4.5536
	step 14992:lm_loss: 4.5268, ppl: 92.4662, loss: 4.5536
	step 14993:lm_loss: 4.5268, ppl: 92.4644, loss: 4.5536
	step 14994:lm_loss: 4.5269, ppl: 92.4671, loss: 4.5536
	step 14995:lm_loss: 4.5268, ppl: 92.4649, loss: 4.5536
	step 14996:lm_loss: 4.5268, ppl: 92.4650, loss: 4.5536
	step 14997:lm_loss: 4.5267, ppl: 92.4549, loss: 4.5535
	step 14998:lm_loss: 4.5267, ppl: 92.4574, loss: 4.5536
	step 14999:lm_loss: 4.5266, ppl: 92.4465, loss: 4.5535
	step 15000:lm_loss: 4.5266, ppl: 92.4445, loss: 4.5534
	step 15001:lm_loss: 4.5266, ppl: 92.4417, loss: 4.5534
	step 15002:lm_loss: 4.5267, ppl: 92.4499, loss: 4.5535
	step 15003:lm_loss: 4.5266, ppl: 92.4482, loss: 4.5535
	step 15004:lm_loss: 4.5266, ppl: 92.4472, loss: 4.5535
	step 15005:lm_loss: 4.5267, ppl: 92.4538, loss: 4.5535
	step 15006:lm_loss: 4.5267, ppl: 92.4537, loss: 4.5535
	step 15007:lm_loss: 4.5267, ppl: 92.4502, loss: 4.5535
	step 15008:lm_loss: 4.5267, ppl: 92.4548, loss: 4.5535
	step 15009:lm_loss: 4.5267, ppl: 92.4530, loss: 4.5535
	step 15010:lm_loss: 4.5267, ppl: 92.4514, loss: 4.5535
	step 15011:lm_loss: 4.5267, ppl: 92.4560, loss: 4.5535
	step 15012:lm_loss: 4.5267, ppl: 92.4556, loss: 4.5535
	step 15013:lm_loss: 4.5268, ppl: 92.4587, loss: 4.5535
	step 15014:lm_loss: 4.5268, ppl: 92.4631, loss: 4.5536
	step 15015:lm_loss: 4.5268, ppl: 92.4667, loss: 4.5536
	step 15016:lm_loss: 4.5268, ppl: 92.4662, loss: 4.5536
	step 15017:lm_loss: 4.5269, ppl: 92.4701, loss: 4.5536
	step 15018:lm_loss: 4.5269, ppl: 92.4734, loss: 4.5537
	step 15019:lm_loss: 4.5269, ppl: 92.4702, loss: 4.5536
	step 15020:lm_loss: 4.5269, ppl: 92.4690, loss: 4.5536
	step 15021:lm_loss: 4.5269, ppl: 92.4722, loss: 4.5537
	step 15022:lm_loss: 4.5269, ppl: 92.4715, loss: 4.5536
	step 15023:lm_loss: 4.5269, ppl: 92.4738, loss: 4.5537
	step 15024:lm_loss: 4.5269, ppl: 92.4760, loss: 4.5537
	step 15025:lm_loss: 4.5269, ppl: 92.4723, loss: 4.5536
	step 15026:lm_loss: 4.5269, ppl: 92.4724, loss: 4.5536
	step 15027:lm_loss: 4.5268, ppl: 92.4599, loss: 4.5535
	step 15028:lm_loss: 4.5268, ppl: 92.4618, loss: 4.5536
	step 15029:lm_loss: 4.5268, ppl: 92.4599, loss: 4.5535
	step 15030:lm_loss: 4.5268, ppl: 92.4638, loss: 4.5536
	step 15031:lm_loss: 4.5269, ppl: 92.4675, loss: 4.5536
	step 15032:lm_loss: 4.5269, ppl: 92.4735, loss: 4.5537
	step 15033:lm_loss: 4.5270, ppl: 92.4846, loss: 4.5538
	step 15034:lm_loss: 4.5271, ppl: 92.4891, loss: 4.5538
	step 15035:lm_loss: 4.5271, ppl: 92.4868, loss: 4.5538
	step 15036:lm_loss: 4.5271, ppl: 92.4879, loss: 4.5538
	step 15037:lm_loss: 4.5270, ppl: 92.4800, loss: 4.5537
	step 15038:lm_loss: 4.5270, ppl: 92.4847, loss: 4.5537
	step 15039:lm_loss: 4.5270, ppl: 92.4853, loss: 4.5538
	step 15040:lm_loss: 4.5269, ppl: 92.4738, loss: 4.5536
	step 15041:lm_loss: 4.5269, ppl: 92.4736, loss: 4.5536
	step 15042:lm_loss: 4.5269, ppl: 92.4744, loss: 4.5536
	step 15043:lm_loss: 4.5270, ppl: 92.4798, loss: 4.5536
	step 15044:lm_loss: 4.5270, ppl: 92.4798, loss: 4.5536
	step 15045:lm_loss: 4.5269, ppl: 92.4689, loss: 4.5535
	step 15046:lm_loss: 4.5270, ppl: 92.4764, loss: 4.5537
	step 15047:lm_loss: 4.5269, ppl: 92.4733, loss: 4.5536
	step 15048:lm_loss: 4.5269, ppl: 92.4760, loss: 4.5537
	step 15049:lm_loss: 4.5270, ppl: 92.4803, loss: 4.5537
	step 15050:lm_loss: 4.5270, ppl: 92.4806, loss: 4.5537
	step 15051:lm_loss: 4.5269, ppl: 92.4746, loss: 4.5537
	step 15052:lm_loss: 4.5269, ppl: 92.4755, loss: 4.5537
	step 15053:lm_loss: 4.5269, ppl: 92.4757, loss: 4.5537
	step 15054:lm_loss: 4.5269, ppl: 92.4757, loss: 4.5537
	step 15055:lm_loss: 4.5268, ppl: 92.4642, loss: 4.5536
	step 15056:lm_loss: 4.5270, ppl: 92.4761, loss: 4.5536
	step 15057:lm_loss: 4.5270, ppl: 92.4814, loss: 4.5537
	step 15058:lm_loss: 4.5270, ppl: 92.4840, loss: 4.5537
	step 15059:lm_loss: 4.5270, ppl: 92.4767, loss: 4.5536
	step 15060:lm_loss: 4.5270, ppl: 92.4794, loss: 4.5536
	step 15061:lm_loss: 4.5270, ppl: 92.4845, loss: 4.5537
	step 15062:lm_loss: 4.5271, ppl: 92.4869, loss: 4.5537
	step 15063:lm_loss: 4.5271, ppl: 92.4855, loss: 4.5537
	step 15064:lm_loss: 4.5271, ppl: 92.4885, loss: 4.5537
	step 15065:lm_loss: 4.5271, ppl: 92.4904, loss: 4.5538
	step 15066:lm_loss: 4.5270, ppl: 92.4798, loss: 4.5537
	step 15067:lm_loss: 4.5270, ppl: 92.4801, loss: 4.5537
	step 15068:lm_loss: 4.5269, ppl: 92.4760, loss: 4.5537
	step 15069:lm_loss: 4.5270, ppl: 92.4837, loss: 4.5537
	step 15070:lm_loss: 4.5270, ppl: 92.4817, loss: 4.5537
	step 15071:lm_loss: 4.5270, ppl: 92.4838, loss: 4.5537
	step 15072:lm_loss: 4.5271, ppl: 92.4886, loss: 4.5538
	step 15073:lm_loss: 4.5270, ppl: 92.4823, loss: 4.5537
	step 15074:lm_loss: 4.5270, ppl: 92.4793, loss: 4.5536
	step 15075:lm_loss: 4.5270, ppl: 92.4843, loss: 4.5537
	step 15076:lm_loss: 4.5271, ppl: 92.4897, loss: 4.5537
	step 15077:lm_loss: 4.5271, ppl: 92.4868, loss: 4.5537
	step 15078:lm_loss: 4.5271, ppl: 92.4916, loss: 4.5537
	step 15079:lm_loss: 4.5271, ppl: 92.4920, loss: 4.5537
	step 15080:lm_loss: 4.5272, ppl: 92.4946, loss: 4.5538
	step 15081:lm_loss: 4.5272, ppl: 92.4952, loss: 4.5538
	step 15082:lm_loss: 4.5272, ppl: 92.4957, loss: 4.5538
	step 15083:lm_loss: 4.5272, ppl: 92.4966, loss: 4.5538
	step 15084:lm_loss: 4.5272, ppl: 92.4950, loss: 4.5538
	step 15085:lm_loss: 4.5271, ppl: 92.4909, loss: 4.5537
	step 15086:lm_loss: 4.5271, ppl: 92.4860, loss: 4.5537
	step 15087:lm_loss: 4.5271, ppl: 92.4883, loss: 4.5537
	step 15088:lm_loss: 4.5271, ppl: 92.4904, loss: 4.5537
	step 15089:lm_loss: 4.5271, ppl: 92.4915, loss: 4.5538
	step 15090:lm_loss: 4.5271, ppl: 92.4923, loss: 4.5538
	step 15091:lm_loss: 4.5272, ppl: 92.4954, loss: 4.5538
	step 15092:lm_loss: 4.5271, ppl: 92.4919, loss: 4.5537
	step 15093:lm_loss: 4.5271, ppl: 92.4916, loss: 4.5537
	step 15094:lm_loss: 4.5272, ppl: 92.4962, loss: 4.5538
	step 15095:lm_loss: 4.5272, ppl: 92.5005, loss: 4.5538
	step 15096:lm_loss: 4.5271, ppl: 92.4929, loss: 4.5538
	step 15097:lm_loss: 4.5271, ppl: 92.4940, loss: 4.5538
	step 15098:lm_loss: 4.5271, ppl: 92.4881, loss: 4.5538
	step 15099:lm_loss: 4.5271, ppl: 92.4876, loss: 4.5538
	step 15100:lm_loss: 4.5271, ppl: 92.4892, loss: 4.5538
	step 15101:lm_loss: 4.5271, ppl: 92.4903, loss: 4.5538
	step 15102:lm_loss: 4.5270, ppl: 92.4816, loss: 4.5537
	step 15103:lm_loss: 4.5270, ppl: 92.4810, loss: 4.5537
	step 15104:lm_loss: 4.5270, ppl: 92.4777, loss: 4.5536
	step 15105:lm_loss: 4.5268, ppl: 92.4621, loss: 4.5535
	step 15106:lm_loss: 4.5268, ppl: 92.4617, loss: 4.5535
	step 15107:lm_loss: 4.5268, ppl: 92.4602, loss: 4.5535
	step 15108:lm_loss: 4.5267, ppl: 92.4569, loss: 4.5535
	step 15109:lm_loss: 4.5267, ppl: 92.4549, loss: 4.5535
	step 15110:lm_loss: 4.5268, ppl: 92.4586, loss: 4.5535
	step 15111:lm_loss: 4.5268, ppl: 92.4660, loss: 4.5536
	step 15112:lm_loss: 4.5268, ppl: 92.4649, loss: 4.5536
	step 15113:lm_loss: 4.5268, ppl: 92.4616, loss: 4.5536
	step 15114:lm_loss: 4.5268, ppl: 92.4615, loss: 4.5536
	step 15115:lm_loss: 4.5268, ppl: 92.4640, loss: 4.5536
	step 15116:lm_loss: 4.5268, ppl: 92.4618, loss: 4.5536
	step 15117:lm_loss: 4.5268, ppl: 92.4666, loss: 4.5536
	step 15118:lm_loss: 4.5269, ppl: 92.4719, loss: 4.5537
	step 15119:lm_loss: 4.5269, ppl: 92.4702, loss: 4.5537
	step 15120:lm_loss: 4.5270, ppl: 92.4768, loss: 4.5537
	step 15121:lm_loss: 4.5270, ppl: 92.4804, loss: 4.5538
	step 15122:lm_loss: 4.5270, ppl: 92.4783, loss: 4.5537
	step 15123:lm_loss: 4.5270, ppl: 92.4764, loss: 4.5537
	step 15124:lm_loss: 4.5269, ppl: 92.4751, loss: 4.5537
	step 15125:lm_loss: 4.5269, ppl: 92.4718, loss: 4.5537
	step 15126:lm_loss: 4.5269, ppl: 92.4700, loss: 4.5536
	step 15127:lm_loss: 4.5269, ppl: 92.4735, loss: 4.5537
	step 15128:lm_loss: 4.5270, ppl: 92.4765, loss: 4.5537
	step 15129:lm_loss: 4.5269, ppl: 92.4734, loss: 4.5536
	step 15130:lm_loss: 4.5270, ppl: 92.4786, loss: 4.5537
	step 15131:lm_loss: 4.5270, ppl: 92.4787, loss: 4.5537
	step 15132:lm_loss: 4.5269, ppl: 92.4758, loss: 4.5537
	step 15133:lm_loss: 4.5270, ppl: 92.4799, loss: 4.5537
	step 15134:lm_loss: 4.5270, ppl: 92.4805, loss: 4.5537
	step 15135:lm_loss: 4.5270, ppl: 92.4841, loss: 4.5538
	step 15136:lm_loss: 4.5271, ppl: 92.4918, loss: 4.5538
	step 15137:lm_loss: 4.5271, ppl: 92.4894, loss: 4.5538
	step 15138:lm_loss: 4.5271, ppl: 92.4904, loss: 4.5538
	step 15139:lm_loss: 4.5271, ppl: 92.4932, loss: 4.5538
	step 15140:lm_loss: 4.5271, ppl: 92.4910, loss: 4.5538
	step 15141:lm_loss: 4.5271, ppl: 92.4858, loss: 4.5538
	step 15142:lm_loss: 4.5270, ppl: 92.4846, loss: 4.5538
	step 15143:lm_loss: 4.5270, ppl: 92.4812, loss: 4.5537
	step 15144:lm_loss: 4.5270, ppl: 92.4774, loss: 4.5536
	step 15145:lm_loss: 4.5270, ppl: 92.4778, loss: 4.5536
	step 15146:lm_loss: 4.5269, ppl: 92.4736, loss: 4.5536
	step 15147:lm_loss: 4.5269, ppl: 92.4741, loss: 4.5536
	step 15148:lm_loss: 4.5270, ppl: 92.4769, loss: 4.5536
	step 15149:lm_loss: 4.5270, ppl: 92.4827, loss: 4.5537
	step 15150:lm_loss: 4.5271, ppl: 92.4863, loss: 4.5537
	step 15151:lm_loss: 4.5271, ppl: 92.4936, loss: 4.5538
	step 15152:lm_loss: 4.5271, ppl: 92.4912, loss: 4.5538
	step 15153:lm_loss: 4.5271, ppl: 92.4917, loss: 4.5538
	step 15154:lm_loss: 4.5272, ppl: 92.4946, loss: 4.5538
	step 15155:lm_loss: 4.5272, ppl: 92.4953, loss: 4.5539
	step 15156:lm_loss: 4.5271, ppl: 92.4917, loss: 4.5538
	step 15157:lm_loss: 4.5271, ppl: 92.4867, loss: 4.5538
	step 15158:lm_loss: 4.5271, ppl: 92.4866, loss: 4.5538
	step 15159:lm_loss: 4.5271, ppl: 92.4900, loss: 4.5538
	step 15160:lm_loss: 4.5271, ppl: 92.4917, loss: 4.5538
	step 15161:lm_loss: 4.5272, ppl: 92.4999, loss: 4.5539
	step 15162:lm_loss: 4.5273, ppl: 92.5040, loss: 4.5539
	step 15163:lm_loss: 4.5273, ppl: 92.5045, loss: 4.5539
	step 15164:lm_loss: 4.5272, ppl: 92.4981, loss: 4.5538
	step 15165:lm_loss: 4.5272, ppl: 92.4979, loss: 4.5538
	step 15166:lm_loss: 4.5273, ppl: 92.5042, loss: 4.5539
	step 15167:lm_loss: 4.5273, ppl: 92.5064, loss: 4.5539
	step 15168:lm_loss: 4.5272, ppl: 92.5027, loss: 4.5538
	step 15169:lm_loss: 4.5272, ppl: 92.5022, loss: 4.5538
	step 15170:lm_loss: 4.5272, ppl: 92.5025, loss: 4.5538
	step 15171:lm_loss: 4.5272, ppl: 92.4974, loss: 4.5538
	step 15172:lm_loss: 4.5272, ppl: 92.4995, loss: 4.5538
	step 15173:lm_loss: 4.5273, ppl: 92.5073, loss: 4.5538
	step 15174:lm_loss: 4.5273, ppl: 92.5049, loss: 4.5538
	step 15175:lm_loss: 4.5273, ppl: 92.5057, loss: 4.5538
	step 15176:lm_loss: 4.5273, ppl: 92.5099, loss: 4.5538
	step 15177:lm_loss: 4.5273, ppl: 92.5085, loss: 4.5538
	step 15178:lm_loss: 4.5272, ppl: 92.5015, loss: 4.5538
	step 15179:lm_loss: 4.5273, ppl: 92.5083, loss: 4.5539
	step 15180:lm_loss: 4.5273, ppl: 92.5053, loss: 4.5538
	step 15181:lm_loss: 4.5273, ppl: 92.5077, loss: 4.5538
	step 15182:lm_loss: 4.5273, ppl: 92.5075, loss: 4.5538
	step 15183:lm_loss: 4.5273, ppl: 92.5052, loss: 4.5538
	step 15184:lm_loss: 4.5273, ppl: 92.5112, loss: 4.5539
	step 15185:lm_loss: 4.5273, ppl: 92.5077, loss: 4.5538
	step 15186:lm_loss: 4.5273, ppl: 92.5075, loss: 4.5538
	step 15187:lm_loss: 4.5272, ppl: 92.5023, loss: 4.5538
	step 15188:lm_loss: 4.5272, ppl: 92.4983, loss: 4.5537
	step 15189:lm_loss: 4.5271, ppl: 92.4945, loss: 4.5537
	step 15190:lm_loss: 4.5271, ppl: 92.4929, loss: 4.5536
	step 15191:lm_loss: 4.5272, ppl: 92.4956, loss: 4.5537
	step 15192:lm_loss: 4.5273, ppl: 92.5040, loss: 4.5537
	step 15193:lm_loss: 4.5272, ppl: 92.5005, loss: 4.5537
	step 15194:lm_loss: 4.5272, ppl: 92.4989, loss: 4.5537
	step 15195:lm_loss: 4.5272, ppl: 92.5004, loss: 4.5537
	step 15196:lm_loss: 4.5272, ppl: 92.5007, loss: 4.5537
	step 15197:lm_loss: 4.5273, ppl: 92.5047, loss: 4.5537
	step 15198:lm_loss: 4.5273, ppl: 92.5093, loss: 4.5538
	step 15199:lm_loss: 4.5273, ppl: 92.5087, loss: 4.5538
	step 15200:lm_loss: 4.5274, ppl: 92.5159, loss: 4.5538
	step 15201:lm_loss: 4.5274, ppl: 92.5147, loss: 4.5538
	step 15202:lm_loss: 4.5273, ppl: 92.5122, loss: 4.5538
	step 15203:lm_loss: 4.5272, ppl: 92.5008, loss: 4.5537
	step 15204:lm_loss: 4.5272, ppl: 92.5019, loss: 4.5537
	step 15205:lm_loss: 4.5273, ppl: 92.5061, loss: 4.5538
	step 15206:lm_loss: 4.5273, ppl: 92.5064, loss: 4.5538
	step 15207:lm_loss: 4.5272, ppl: 92.5028, loss: 4.5537
	step 15208:lm_loss: 4.5272, ppl: 92.5001, loss: 4.5537
	step 15209:lm_loss: 4.5272, ppl: 92.4976, loss: 4.5537
	step 15210:lm_loss: 4.5273, ppl: 92.5050, loss: 4.5538
	step 15211:lm_loss: 4.5273, ppl: 92.5040, loss: 4.5538
	step 15212:lm_loss: 4.5272, ppl: 92.4985, loss: 4.5537
	step 15213:lm_loss: 4.5272, ppl: 92.4966, loss: 4.5537
	step 15214:lm_loss: 4.5272, ppl: 92.4970, loss: 4.5537
	step 15215:lm_loss: 4.5272, ppl: 92.4952, loss: 4.5537
	step 15216:lm_loss: 4.5271, ppl: 92.4939, loss: 4.5537
	step 15217:lm_loss: 4.5271, ppl: 92.4897, loss: 4.5536
	step 15218:lm_loss: 4.5270, ppl: 92.4844, loss: 4.5535
	step 15219:lm_loss: 4.5270, ppl: 92.4833, loss: 4.5535
	step 15220:lm_loss: 4.5271, ppl: 92.4909, loss: 4.5536
	step 15221:lm_loss: 4.5271, ppl: 92.4878, loss: 4.5535
	step 15222:lm_loss: 4.5271, ppl: 92.4887, loss: 4.5536
	step 15223:lm_loss: 4.5271, ppl: 92.4888, loss: 4.5536
	step 15224:lm_loss: 4.5270, ppl: 92.4841, loss: 4.5535
	step 15225:lm_loss: 4.5271, ppl: 92.4902, loss: 4.5535
	step 15226:lm_loss: 4.5270, ppl: 92.4850, loss: 4.5535
	step 15227:lm_loss: 4.5271, ppl: 92.4882, loss: 4.5535
	step 15228:lm_loss: 4.5271, ppl: 92.4895, loss: 4.5535
	step 15229:lm_loss: 4.5271, ppl: 92.4899, loss: 4.5535
	step 15230:lm_loss: 4.5271, ppl: 92.4899, loss: 4.5535
	step 15231:lm_loss: 4.5270, ppl: 92.4812, loss: 4.5534
	step 15232:lm_loss: 4.5270, ppl: 92.4795, loss: 4.5534
	step 15233:lm_loss: 4.5269, ppl: 92.4755, loss: 4.5533
	step 15234:lm_loss: 4.5269, ppl: 92.4718, loss: 4.5533
	step 15235:lm_loss: 4.5269, ppl: 92.4706, loss: 4.5533
	step 15236:lm_loss: 4.5269, ppl: 92.4695, loss: 4.5533
	step 15237:lm_loss: 4.5269, ppl: 92.4673, loss: 4.5532
	step 15238:lm_loss: 4.5268, ppl: 92.4646, loss: 4.5532
	step 15239:lm_loss: 4.5268, ppl: 92.4641, loss: 4.5532
	step 15240:lm_loss: 4.5269, ppl: 92.4726, loss: 4.5533
	step 15241:lm_loss: 4.5269, ppl: 92.4755, loss: 4.5533
	step 15242:lm_loss: 4.5270, ppl: 92.4811, loss: 4.5534
	step 15243:lm_loss: 4.5270, ppl: 92.4810, loss: 4.5534
	step 15244:lm_loss: 4.5269, ppl: 92.4733, loss: 4.5533
	step 15245:lm_loss: 4.5269, ppl: 92.4737, loss: 4.5533
	step 15246:lm_loss: 4.5269, ppl: 92.4726, loss: 4.5533
	step 15247:lm_loss: 4.5269, ppl: 92.4722, loss: 4.5533
	step 15248:lm_loss: 4.5269, ppl: 92.4706, loss: 4.5533
	step 15249:lm_loss: 4.5269, ppl: 92.4739, loss: 4.5533
	step 15250:lm_loss: 4.5269, ppl: 92.4738, loss: 4.5533
	step 15251:lm_loss: 4.5269, ppl: 92.4704, loss: 4.5533
	step 15252:lm_loss: 4.5269, ppl: 92.4714, loss: 4.5533
	step 15253:lm_loss: 4.5269, ppl: 92.4715, loss: 4.5533
	step 15254:lm_loss: 4.5269, ppl: 92.4729, loss: 4.5533
	step 15255:lm_loss: 4.5269, ppl: 92.4705, loss: 4.5533
	step 15256:lm_loss: 4.5269, ppl: 92.4701, loss: 4.5533
	step 15257:lm_loss: 4.5269, ppl: 92.4713, loss: 4.5533
	step 15258:lm_loss: 4.5269, ppl: 92.4747, loss: 4.5533
	step 15259:lm_loss: 4.5268, ppl: 92.4648, loss: 4.5532
	step 15260:lm_loss: 4.5269, ppl: 92.4687, loss: 4.5533
	step 15261:lm_loss: 4.5269, ppl: 92.4688, loss: 4.5533
	step 15262:lm_loss: 4.5269, ppl: 92.4712, loss: 4.5533
	step 15263:lm_loss: 4.5269, ppl: 92.4709, loss: 4.5533
	step 15264:lm_loss: 4.5269, ppl: 92.4701, loss: 4.5533
	step 15265:lm_loss: 4.5269, ppl: 92.4718, loss: 4.5533
	step 15266:lm_loss: 4.5269, ppl: 92.4714, loss: 4.5533
	step 15267:lm_loss: 4.5269, ppl: 92.4761, loss: 4.5534
	step 15268:lm_loss: 4.5270, ppl: 92.4795, loss: 4.5534
	step 15269:lm_loss: 4.5271, ppl: 92.4858, loss: 4.5534
	step 15270:lm_loss: 4.5271, ppl: 92.4855, loss: 4.5534
	step 15271:lm_loss: 4.5270, ppl: 92.4828, loss: 4.5534
	step 15272:lm_loss: 4.5271, ppl: 92.4876, loss: 4.5534
	step 15273:lm_loss: 4.5270, ppl: 92.4817, loss: 4.5534
	step 15274:lm_loss: 4.5270, ppl: 92.4794, loss: 4.5533
	step 15275:lm_loss: 4.5271, ppl: 92.4887, loss: 4.5534
	step 15276:lm_loss: 4.5270, ppl: 92.4839, loss: 4.5534
	step 15277:lm_loss: 4.5270, ppl: 92.4809, loss: 4.5533
	step 15278:lm_loss: 4.5271, ppl: 92.4872, loss: 4.5534
	step 15279:lm_loss: 4.5271, ppl: 92.4884, loss: 4.5534
	step 15280:lm_loss: 4.5271, ppl: 92.4857, loss: 4.5534
	step 15281:lm_loss: 4.5270, ppl: 92.4822, loss: 4.5533
	step 15282:lm_loss: 4.5270, ppl: 92.4794, loss: 4.5533
	step 15283:lm_loss: 4.5270, ppl: 92.4803, loss: 4.5533
	step 15284:lm_loss: 4.5270, ppl: 92.4814, loss: 4.5533
	step 15285:lm_loss: 4.5270, ppl: 92.4778, loss: 4.5533
	step 15286:lm_loss: 4.5271, ppl: 92.4906, loss: 4.5534
	step 15287:lm_loss: 4.5271, ppl: 92.4924, loss: 4.5534
	step 15288:lm_loss: 4.5271, ppl: 92.4937, loss: 4.5534
	step 15289:lm_loss: 4.5272, ppl: 92.4991, loss: 4.5534
	step 15290:lm_loss: 4.5272, ppl: 92.5030, loss: 4.5535
	step 15291:lm_loss: 4.5272, ppl: 92.5027, loss: 4.5535
	step 15292:lm_loss: 4.5272, ppl: 92.4983, loss: 4.5534
	step 15293:lm_loss: 4.5271, ppl: 92.4906, loss: 4.5533
	step 15294:lm_loss: 4.5272, ppl: 92.4998, loss: 4.5534
	step 15295:lm_loss: 4.5272, ppl: 92.5016, loss: 4.5534
	step 15296:lm_loss: 4.5272, ppl: 92.5001, loss: 4.5534
	step 15297:lm_loss: 4.5272, ppl: 92.4973, loss: 4.5534
	step 15298:lm_loss: 4.5272, ppl: 92.4980, loss: 4.5534
	step 15299:lm_loss: 4.5273, ppl: 92.5085, loss: 4.5535
	step 15300:lm_loss: 4.5273, ppl: 92.5071, loss: 4.5535
	step 15301:lm_loss: 4.5273, ppl: 92.5088, loss: 4.5535
	step 15302:lm_loss: 4.5273, ppl: 92.5067, loss: 4.5535
	step 15303:lm_loss: 4.5273, ppl: 92.5067, loss: 4.5535
	step 15304:lm_loss: 4.5272, ppl: 92.5024, loss: 4.5534
	step 15305:lm_loss: 4.5272, ppl: 92.4978, loss: 4.5533
	step 15306:lm_loss: 4.5272, ppl: 92.5005, loss: 4.5534
	step 15307:lm_loss: 4.5272, ppl: 92.4983, loss: 4.5533
	step 15308:lm_loss: 4.5273, ppl: 92.5061, loss: 4.5534
	step 15309:lm_loss: 4.5273, ppl: 92.5053, loss: 4.5534
	step 15310:lm_loss: 4.5272, ppl: 92.4999, loss: 4.5533
	step 15311:lm_loss: 4.5272, ppl: 92.4957, loss: 4.5533
	step 15312:lm_loss: 4.5272, ppl: 92.4954, loss: 4.5533
	step 15313:lm_loss: 4.5272, ppl: 92.4974, loss: 4.5533
	step 15314:lm_loss: 4.5272, ppl: 92.4993, loss: 4.5534
	step 15315:lm_loss: 4.5271, ppl: 92.4940, loss: 4.5533
	step 15316:lm_loss: 4.5272, ppl: 92.4975, loss: 4.5533
	step 15317:lm_loss: 4.5272, ppl: 92.4967, loss: 4.5533
	step 15318:lm_loss: 4.5271, ppl: 92.4938, loss: 4.5533
	step 15319:lm_loss: 4.5272, ppl: 92.4951, loss: 4.5533
	step 15320:lm_loss: 4.5272, ppl: 92.4958, loss: 4.5533
	step 15321:lm_loss: 4.5272, ppl: 92.5010, loss: 4.5534
	step 15322:lm_loss: 4.5272, ppl: 92.4962, loss: 4.5533
	step 15323:lm_loss: 4.5271, ppl: 92.4856, loss: 4.5532
	step 15324:lm_loss: 4.5270, ppl: 92.4841, loss: 4.5531
	step 15325:lm_loss: 4.5271, ppl: 92.4936, loss: 4.5532
	step 15326:lm_loss: 4.5272, ppl: 92.4954, loss: 4.5532
	step 15327:lm_loss: 4.5272, ppl: 92.4950, loss: 4.5532
	step 15328:lm_loss: 4.5271, ppl: 92.4894, loss: 4.5532
	step 15329:lm_loss: 4.5270, ppl: 92.4840, loss: 4.5531
	step 15330:lm_loss: 4.5271, ppl: 92.4859, loss: 4.5531
	step 15331:lm_loss: 4.5270, ppl: 92.4805, loss: 4.5530
	step 15332:lm_loss: 4.5270, ppl: 92.4809, loss: 4.5530
	step 15333:lm_loss: 4.5270, ppl: 92.4794, loss: 4.5530
	step 15334:lm_loss: 4.5269, ppl: 92.4760, loss: 4.5530
	step 15335:lm_loss: 4.5270, ppl: 92.4761, loss: 4.5530
	step 15336:lm_loss: 4.5270, ppl: 92.4806, loss: 4.5531
	step 15337:lm_loss: 4.5270, ppl: 92.4826, loss: 4.5531
	step 15338:lm_loss: 4.5270, ppl: 92.4799, loss: 4.5530
	step 15339:lm_loss: 4.5269, ppl: 92.4732, loss: 4.5530
	step 15340:lm_loss: 4.5269, ppl: 92.4727, loss: 4.5530
	step 15341:lm_loss: 4.5269, ppl: 92.4700, loss: 4.5530
	step 15342:lm_loss: 4.5269, ppl: 92.4729, loss: 4.5530
	step 15343:lm_loss: 4.5270, ppl: 92.4787, loss: 4.5531
	step 15344:lm_loss: 4.5270, ppl: 92.4813, loss: 4.5531
	step 15345:lm_loss: 4.5270, ppl: 92.4841, loss: 4.5531
	step 15346:lm_loss: 4.5271, ppl: 92.4876, loss: 4.5532
	step 15347:lm_loss: 4.5271, ppl: 92.4860, loss: 4.5532
	step 15348:lm_loss: 4.5271, ppl: 92.4859, loss: 4.5532
	step 15349:lm_loss: 4.5270, ppl: 92.4805, loss: 4.5531
	step 15350:lm_loss: 4.5271, ppl: 92.4855, loss: 4.5532
	step 15351:lm_loss: 4.5270, ppl: 92.4819, loss: 4.5531
	step 15352:lm_loss: 4.5271, ppl: 92.4867, loss: 4.5532
	step 15353:lm_loss: 4.5271, ppl: 92.4903, loss: 4.5532
	step 15354:lm_loss: 4.5271, ppl: 92.4926, loss: 4.5533
	step 15355:lm_loss: 4.5271, ppl: 92.4900, loss: 4.5532
	step 15356:lm_loss: 4.5271, ppl: 92.4933, loss: 4.5533
	step 15357:lm_loss: 4.5271, ppl: 92.4937, loss: 4.5533
	step 15358:lm_loss: 4.5271, ppl: 92.4901, loss: 4.5532
	step 15359:lm_loss: 4.5271, ppl: 92.4881, loss: 4.5532
	step 15360:lm_loss: 4.5271, ppl: 92.4890, loss: 4.5532
	step 15361:lm_loss: 4.5271, ppl: 92.4919, loss: 4.5532
	step 15362:lm_loss: 4.5271, ppl: 92.4910, loss: 4.5532
	step 15363:lm_loss: 4.5272, ppl: 92.4967, loss: 4.5533
	step 15364:lm_loss: 4.5272, ppl: 92.4952, loss: 4.5533
	step 15365:lm_loss: 4.5272, ppl: 92.5019, loss: 4.5534
	step 15366:lm_loss: 4.5272, ppl: 92.5023, loss: 4.5534
	step 15367:lm_loss: 4.5273, ppl: 92.5076, loss: 4.5534
	step 15368:lm_loss: 4.5272, ppl: 92.5035, loss: 4.5534
	step 15369:lm_loss: 4.5273, ppl: 92.5068, loss: 4.5534
	step 15370:lm_loss: 4.5273, ppl: 92.5111, loss: 4.5534
	step 15371:lm_loss: 4.5273, ppl: 92.5095, loss: 4.5534
	step 15372:lm_loss: 4.5272, ppl: 92.5025, loss: 4.5533
	step 15373:lm_loss: 4.5272, ppl: 92.4983, loss: 4.5533
	step 15374:lm_loss: 4.5272, ppl: 92.5028, loss: 4.5534
	step 15375:lm_loss: 4.5273, ppl: 92.5068, loss: 4.5534
	step 15376:lm_loss: 4.5273, ppl: 92.5076, loss: 4.5534
	step 15377:lm_loss: 4.5273, ppl: 92.5083, loss: 4.5534
	step 15378:lm_loss: 4.5273, ppl: 92.5125, loss: 4.5535
	step 15379:lm_loss: 4.5273, ppl: 92.5118, loss: 4.5534
	step 15380:lm_loss: 4.5273, ppl: 92.5100, loss: 4.5534
	step 15381:lm_loss: 4.5273, ppl: 92.5129, loss: 4.5535
	step 15382:lm_loss: 4.5273, ppl: 92.5116, loss: 4.5534
	step 15383:lm_loss: 4.5273, ppl: 92.5068, loss: 4.5534
	step 15384:lm_loss: 4.5272, ppl: 92.5025, loss: 4.5533
	step 15385:lm_loss: 4.5273, ppl: 92.5064, loss: 4.5534
	step 15386:lm_loss: 4.5271, ppl: 92.4941, loss: 4.5532
	step 15387:lm_loss: 4.5272, ppl: 92.4964, loss: 4.5533
	step 15388:lm_loss: 4.5272, ppl: 92.4952, loss: 4.5533
	step 15389:lm_loss: 4.5272, ppl: 92.5003, loss: 4.5533
	step 15390:lm_loss: 4.5273, ppl: 92.5044, loss: 4.5534
	step 15391:lm_loss: 4.5272, ppl: 92.4978, loss: 4.5533
	step 15392:lm_loss: 4.5272, ppl: 92.4974, loss: 4.5533
	step 15393:lm_loss: 4.5272, ppl: 92.5005, loss: 4.5533
	step 15394:lm_loss: 4.5272, ppl: 92.5033, loss: 4.5533
	step 15395:lm_loss: 4.5272, ppl: 92.5035, loss: 4.5533
	step 15396:lm_loss: 4.5273, ppl: 92.5104, loss: 4.5534
	step 15397:lm_loss: 4.5273, ppl: 92.5120, loss: 4.5534
	step 15398:lm_loss: 4.5274, ppl: 92.5133, loss: 4.5534
	step 15399:lm_loss: 4.5274, ppl: 92.5170, loss: 4.5535
	step 15400:lm_loss: 4.5274, ppl: 92.5187, loss: 4.5535
	step 15401:lm_loss: 4.5274, ppl: 92.5185, loss: 4.5535
	step 15402:lm_loss: 4.5274, ppl: 92.5190, loss: 4.5535
	step 15403:lm_loss: 4.5275, ppl: 92.5226, loss: 4.5535
	step 15404:lm_loss: 4.5275, ppl: 92.5252, loss: 4.5536
	step 15405:lm_loss: 4.5275, ppl: 92.5256, loss: 4.5536
	step 15406:lm_loss: 4.5275, ppl: 92.5276, loss: 4.5536
	step 15407:lm_loss: 4.5274, ppl: 92.5209, loss: 4.5535
	step 15408:lm_loss: 4.5275, ppl: 92.5234, loss: 4.5535
	step 15409:lm_loss: 4.5274, ppl: 92.5217, loss: 4.5535
	step 15410:lm_loss: 4.5274, ppl: 92.5193, loss: 4.5535
	step 15411:lm_loss: 4.5274, ppl: 92.5170, loss: 4.5534
	step 15412:lm_loss: 4.5274, ppl: 92.5198, loss: 4.5535
	step 15413:lm_loss: 4.5275, ppl: 92.5232, loss: 4.5535
	step 15414:lm_loss: 4.5275, ppl: 92.5254, loss: 4.5535
	step 15415:lm_loss: 4.5275, ppl: 92.5234, loss: 4.5535
	step 15416:lm_loss: 4.5275, ppl: 92.5291, loss: 4.5536
	step 15417:lm_loss: 4.5275, ppl: 92.5306, loss: 4.5536
	step 15418:lm_loss: 4.5276, ppl: 92.5346, loss: 4.5537
	step 15419:lm_loss: 4.5276, ppl: 92.5387, loss: 4.5537
	step 15420:lm_loss: 4.5276, ppl: 92.5388, loss: 4.5537
	step 15421:lm_loss: 4.5276, ppl: 92.5383, loss: 4.5537
	step 15422:lm_loss: 4.5277, ppl: 92.5410, loss: 4.5537
	step 15423:lm_loss: 4.5277, ppl: 92.5453, loss: 4.5538
	step 15424:lm_loss: 4.5277, ppl: 92.5486, loss: 4.5538
	step 15425:lm_loss: 4.5278, ppl: 92.5557, loss: 4.5539
	step 15426:lm_loss: 4.5278, ppl: 92.5518, loss: 4.5539
	step 15427:lm_loss: 4.5278, ppl: 92.5505, loss: 4.5539
	step 15428:lm_loss: 4.5278, ppl: 92.5547, loss: 4.5539
	step 15429:lm_loss: 4.5278, ppl: 92.5593, loss: 4.5540
	step 15430:lm_loss: 4.5278, ppl: 92.5541, loss: 4.5539
	step 15431:lm_loss: 4.5278, ppl: 92.5546, loss: 4.5539
	step 15432:lm_loss: 4.5279, ppl: 92.5604, loss: 4.5540
	step 15433:lm_loss: 4.5279, ppl: 92.5666, loss: 4.5541
	step 15434:lm_loss: 4.5279, ppl: 92.5670, loss: 4.5541
	step 15435:lm_loss: 4.5279, ppl: 92.5629, loss: 4.5540
	step 15436:lm_loss: 4.5279, ppl: 92.5596, loss: 4.5540
	step 15437:lm_loss: 4.5279, ppl: 92.5617, loss: 4.5540
	step 15438:lm_loss: 4.5279, ppl: 92.5611, loss: 4.5540
	step 15439:lm_loss: 4.5278, ppl: 92.5570, loss: 4.5540
	step 15440:lm_loss: 4.5279, ppl: 92.5603, loss: 4.5540
	step 15441:lm_loss: 4.5279, ppl: 92.5612, loss: 4.5540
	step 15442:lm_loss: 4.5277, ppl: 92.5481, loss: 4.5539
	step 15443:lm_loss: 4.5277, ppl: 92.5464, loss: 4.5539
	step 15444:lm_loss: 4.5278, ppl: 92.5501, loss: 4.5540
	step 15445:lm_loss: 4.5277, ppl: 92.5454, loss: 4.5539
	step 15446:lm_loss: 4.5277, ppl: 92.5483, loss: 4.5539
	step 15447:lm_loss: 4.5277, ppl: 92.5478, loss: 4.5539
	step 15448:lm_loss: 4.5277, ppl: 92.5494, loss: 4.5540
	step 15449:lm_loss: 4.5278, ppl: 92.5523, loss: 4.5540
	step 15450:lm_loss: 4.5278, ppl: 92.5549, loss: 4.5540
	step 15451:lm_loss: 4.5278, ppl: 92.5568, loss: 4.5540
	step 15452:lm_loss: 4.5278, ppl: 92.5501, loss: 4.5540
	step 15453:lm_loss: 4.5278, ppl: 92.5518, loss: 4.5540
	step 15454:lm_loss: 4.5277, ppl: 92.5495, loss: 4.5540
	step 15455:lm_loss: 4.5278, ppl: 92.5548, loss: 4.5540
	step 15456:lm_loss: 4.5278, ppl: 92.5556, loss: 4.5540
	step 15457:lm_loss: 4.5278, ppl: 92.5528, loss: 4.5540
	step 15458:lm_loss: 4.5278, ppl: 92.5587, loss: 4.5540
	step 15459:lm_loss: 4.5278, ppl: 92.5565, loss: 4.5540
	step 15460:lm_loss: 4.5278, ppl: 92.5526, loss: 4.5540
	step 15461:lm_loss: 4.5278, ppl: 92.5515, loss: 4.5540
	step 15462:lm_loss: 4.5278, ppl: 92.5511, loss: 4.5540
	step 15463:lm_loss: 4.5278, ppl: 92.5548, loss: 4.5540
	step 15464:lm_loss: 4.5278, ppl: 92.5573, loss: 4.5540
	step 15465:lm_loss: 4.5278, ppl: 92.5556, loss: 4.5540
	step 15466:lm_loss: 4.5278, ppl: 92.5571, loss: 4.5540
	step 15467:lm_loss: 4.5278, ppl: 92.5591, loss: 4.5541
	step 15468:lm_loss: 4.5279, ppl: 92.5601, loss: 4.5541
	step 15469:lm_loss: 4.5278, ppl: 92.5577, loss: 4.5541
	step 15470:lm_loss: 4.5279, ppl: 92.5611, loss: 4.5541
	step 15471:lm_loss: 4.5279, ppl: 92.5650, loss: 4.5541
	step 15472:lm_loss: 4.5279, ppl: 92.5649, loss: 4.5541
	step 15473:lm_loss: 4.5280, ppl: 92.5691, loss: 4.5542
	step 15474:lm_loss: 4.5280, ppl: 92.5690, loss: 4.5542
	step 15475:lm_loss: 4.5280, ppl: 92.5768, loss: 4.5543
	step 15476:lm_loss: 4.5280, ppl: 92.5777, loss: 4.5543
	step 15477:lm_loss: 4.5281, ppl: 92.5851, loss: 4.5544
	step 15478:lm_loss: 4.5281, ppl: 92.5846, loss: 4.5544
	step 15479:lm_loss: 4.5282, ppl: 92.5897, loss: 4.5544
	step 15480:lm_loss: 4.5282, ppl: 92.5907, loss: 4.5544
	step 15481:lm_loss: 4.5282, ppl: 92.5952, loss: 4.5545
	step 15482:lm_loss: 4.5283, ppl: 92.5998, loss: 4.5545
	step 15483:lm_loss: 4.5283, ppl: 92.5994, loss: 4.5545
{
  "is_distributed": false,
  "save_path": "./output",
  "train_file": "data/train.txt",
  "valid_file": "data/valid.txt",
  "start_step": 0,
  "num_epochs": 20,
  "log_steps": 1,
  "validation_steps": 1000,
  "save_steps": 1000,
  "Model": {
    "model": "Plato",
    "config_path": "./config/12L.json",
    "init_checkpoint": "",
    "init_pretraining_params": "12L",
    "learning_rate": 1e-05,
    "warmup_steps": 1000,
    "weight_decay": 0.01,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": true,
    "amp_loss_scaling": 12800,
    "max_seq_len": 512,
    "weight_sharing": true,
    "mem_efficient": false,
    "use_bow": true,
    "use_entropy": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": null,
    "topk": 10,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "DialogGeneration",
    "do_generation": false,
    "is_cn": false,
    "nsp_inference_model_path": null,
    "nsp_attention_style": "bidirectional",
    "ranking_score": "decode_score"
  },
  "Reader": {
    "max_src_len": 384,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": true,
    "batch_size": 8192,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  }
}
	step 15484:lm_loss: 4.5283, ppl: 92.6002, loss: 4.5545
Traceback (most recent call last):
  File "./train.py", line 175, in <module>
    train(args)
  File "./train.py", line 78, in train
    model = models.create_model(args, place)
  File "/home/aistudio/ldk/Knover/models/__init__.py", line 49, in create_model
    return MODEL_REGISTRY[args.model](args, place)
  File "/home/aistudio/ldk/Knover/models/plato.py", line 49, in __init__
    super(Plato, self).__init__(args, place)
  File "/home/aistudio/ldk/Knover/models/unified_transformer.py", line 93, in __init__
    super(UnifiedTransformer, self).__init__(args, place)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 74, in __init__
    self._build_programs()
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 121, in _build_programs
    outputs = self.forward(inputs)
  File "/home/aistudio/ldk/Knover/models/plato.py", line 172, in forward
    name=self.latent_emb_name, initializer=self.param_initializer))
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/tensor.py", line 142, in create_parameter
    numpy.int64), 'create_parameter')
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/data_feeder.py", line 97, in check_type
    (input_name, op_name, expected_type, type(input), extra_message))
TypeError: The type of 'item of shape' in create_parameter must be (<class 'int'>, <class 'numpy.uint8'>, <class 'numpy.int8'>, <class 'numpy.int16'>, <class 'numpy.int32'>, <class 'numpy.int64'>), but received <class 'NoneType'>. 
	step 15485:lm_loss: 4.5283, ppl: 92.6024, loss: 4.5545
	step 15486:lm_loss: 4.5283, ppl: 92.6019, loss: 4.5545
	step 15487:lm_loss: 4.5283, ppl: 92.6014, loss: 4.5545
	step 15488:lm_loss: 4.5283, ppl: 92.6004, loss: 4.5545
	step 15489:lm_loss: 4.5283, ppl: 92.5980, loss: 4.5545
	step 15490:lm_loss: 4.5282, ppl: 92.5900, loss: 4.5544
	step 15491:lm_loss: 4.5282, ppl: 92.5874, loss: 4.5544
	step 15492:lm_loss: 4.5281, ppl: 92.5841, loss: 4.5543
	step 15493:lm_loss: 4.5281, ppl: 92.5818, loss: 4.5543
	step 15494:lm_loss: 4.5281, ppl: 92.5866, loss: 4.5544
	step 15495:lm_loss: 4.5282, ppl: 92.5929, loss: 4.5544
	step 15496:lm_loss: 4.5283, ppl: 92.6000, loss: 4.5545
	step 15497:lm_loss: 4.5283, ppl: 92.5999, loss: 4.5545
	step 15498:lm_loss: 4.5283, ppl: 92.5972, loss: 4.5545
	step 15499:lm_loss: 4.5283, ppl: 92.5984, loss: 4.5545
	step 15500:lm_loss: 4.5283, ppl: 92.6055, loss: 4.5545
	step 15501:lm_loss: 4.5283, ppl: 92.6052, loss: 4.5545
	step 15502:lm_loss: 4.5284, ppl: 92.6110, loss: 4.5546
	step 15503:lm_loss: 4.5284, ppl: 92.6114, loss: 4.5546
	step 15504:lm_loss: 4.5284, ppl: 92.6105, loss: 4.5546
	step 15505:lm_loss: 4.5284, ppl: 92.6097, loss: 4.5546
	step 15506:lm_loss: 4.5284, ppl: 92.6130, loss: 4.5546
	step 15507:lm_loss: 4.5289, ppl: 92.6583, loss: 4.5548
	step 15508:lm_loss: 4.5290, ppl: 92.6614, loss: 4.5548
	step 15509:lm_loss: 4.5289, ppl: 92.6593, loss: 4.5548
	step 15510:lm_loss: 4.5289, ppl: 92.6593, loss: 4.5548
	step 15511:lm_loss: 4.5289, ppl: 92.6591, loss: 4.5548
	step 15512:lm_loss: 4.5289, ppl: 92.6579, loss: 4.5548
	step 15513:lm_loss: 4.5289, ppl: 92.6574, loss: 4.5547
	step 15514:lm_loss: 4.5289, ppl: 92.6521, loss: 4.5546
	step 15515:lm_loss: 4.5289, ppl: 92.6570, loss: 4.5547
Traceback (most recent call last):
  File "./train.py", line 175, in <module>
    train(args)
  File "./train.py", line 113, in train
    evaluate(task, model, valid_generator, args, dev_count, gpu_id, step)
  File "./train.py", line 131, in evaluate
    part_outputs = task.eval_step(model, data)
  File "/home/aistudio/ldk/Knover/tasks/task_base.py", line 37, in eval_step
    outputs = model.eval_step(inputs)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 288, in eval_step
    self.eval_fetch_dict)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 266, in _execute
    fetch_vars = self.exe.run(program, feed, fetch_list, **kwargs)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1066, in run
    return_merged=return_merged)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1154, in _run_impl
    use_program_cache=use_program_cache)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1224, in _run_program
    fetch_var_name=fetch_var_name)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 580, in _add_feed_fetch_ops
    tmp_program = program.clone()
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/framework.py", line 4151, in clone
    p._copy_param_info_from(self)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/framework.py", line 4581, in _copy_param_info_from
    self.global_block()._copy_param_info_from(other.global_block())
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/framework.py", line 2800, in _copy_param_info_from
    self.vars[new_p.name] = new_p
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/framework.py", line 1318, in name
    return cpt.to_text(self.desc.name())
KeyboardInterrupt
{
  "is_distributed": false,
  "save_path": "./output",
  "train_file": "data/train.txt",
  "valid_file": "data/valid.txt",
  "start_step": 0,
  "num_epochs": 20,
  "log_steps": 1,
  "validation_steps": 1000,
  "save_steps": 1000,
  "Model": {
    "model": "Plato",
    "config_path": "./config/12L.json",
    "init_checkpoint": "",
    "init_pretraining_params": "12L",
    "learning_rate": 1e-05,
    "warmup_steps": 1000,
    "weight_decay": 0.01,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": true,
    "amp_loss_scaling": 12800,
    "max_seq_len": 512,
    "weight_sharing": true,
    "mem_efficient": false,
    "use_bow": true,
    "use_entropy": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": null,
    "topk": 10,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "DialogGeneration",
    "do_generation": false,
    "is_cn": false,
    "nsp_inference_model_path": null,
    "nsp_attention_style": "bidirectional",
    "ranking_score": "decode_score"
  },
  "Reader": {
    "max_src_len": 384,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": true,
    "batch_size": 8192,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  }
}
Traceback (most recent call last):
  File "./train.py", line 175, in <module>
    train(args)
  File "./train.py", line 78, in train
    model = models.create_model(args, place)
  File "/home/aistudio/ldk/Knover/models/__init__.py", line 49, in create_model
    return MODEL_REGISTRY[args.model](args, place)
  File "/home/aistudio/ldk/Knover/models/plato.py", line 49, in __init__
    super(Plato, self).__init__(args, place)
  File "/home/aistudio/ldk/Knover/models/unified_transformer.py", line 93, in __init__
    super(UnifiedTransformer, self).__init__(args, place)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 74, in __init__
    self._build_programs()
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 121, in _build_programs
    outputs = self.forward(inputs)
  File "/home/aistudio/ldk/Knover/models/plato.py", line 172, in forward
    name=self.latent_emb_name, initializer=self.param_initializer))
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/tensor.py", line 142, in create_parameter
    numpy.int64), 'create_parameter')
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/data_feeder.py", line 97, in check_type
    (input_name, op_name, expected_type, type(input), extra_message))
TypeError: The type of 'item of shape' in create_parameter must be (<class 'int'>, <class 'numpy.uint8'>, <class 'numpy.int8'>, <class 'numpy.int16'>, <class 'numpy.int32'>, <class 'numpy.int64'>), but received <class 'NoneType'>. 
{
  "is_distributed": false,
  "save_path": "./output",
  "train_file": "data/train.txt",
  "valid_file": "data/valid.txt",
  "start_step": 0,
  "num_epochs": 20,
  "log_steps": 1,
  "validation_steps": 1000,
  "save_steps": 1000,
  "Model": {
    "model": "Plato",
    "config_path": "./config/12L.json",
    "init_checkpoint": "",
    "init_pretraining_params": "12L",
    "learning_rate": 1e-05,
    "warmup_steps": 1000,
    "weight_decay": 0.01,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": true,
    "amp_loss_scaling": 12800,
    "max_seq_len": 512,
    "weight_sharing": true,
    "mem_efficient": false,
    "use_bow": true,
    "use_entropy": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": null,
    "topk": 10,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "DialogGeneration",
    "do_generation": false,
    "is_cn": false,
    "nsp_inference_model_path": null,
    "nsp_attention_style": "bidirectional",
    "ranking_score": "decode_score"
  },
  "Reader": {
    "max_src_len": 384,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": true,
    "batch_size": 8192,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  }
}
Traceback (most recent call last):
  File "./train.py", line 175, in <module>
    train(args)
  File "./train.py", line 78, in train
    model = models.create_model(args, place)
  File "/home/aistudio/ldk/Knover/models/__init__.py", line 49, in create_model
    return MODEL_REGISTRY[args.model](args, place)
  File "/home/aistudio/ldk/Knover/models/plato.py", line 49, in __init__
    super(Plato, self).__init__(args, place)
  File "/home/aistudio/ldk/Knover/models/unified_transformer.py", line 93, in __init__
    super(UnifiedTransformer, self).__init__(args, place)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 74, in __init__
    self._build_programs()
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 121, in _build_programs
    outputs = self.forward(inputs)
  File "/home/aistudio/ldk/Knover/models/plato.py", line 172, in forward
    name=self.latent_emb_name, initializer=self.param_initializer))
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/tensor.py", line 142, in create_parameter
    numpy.int64), 'create_parameter')
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/data_feeder.py", line 97, in check_type
    (input_name, op_name, expected_type, type(input), extra_message))
TypeError: The type of 'item of shape' in create_parameter must be (<class 'int'>, <class 'numpy.uint8'>, <class 'numpy.int8'>, <class 'numpy.int16'>, <class 'numpy.int32'>, <class 'numpy.int64'>), but received <class 'NoneType'>. 
{
  "is_distributed": false,
  "save_path": "./output",
  "train_file": "data/train.txt",
  "valid_file": "data/valid.txt",
  "start_step": 0,
  "num_epochs": 20,
  "log_steps": 1,
  "validation_steps": 1000,
  "save_steps": 1000,
  "Model": {
    "model": "UnifiedTransformer",
    "config_path": "./config/12L.json",
    "init_checkpoint": "",
    "init_pretraining_params": "12L",
    "learning_rate": 1e-05,
    "warmup_steps": 1000,
    "weight_decay": 0.01,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": true,
    "amp_loss_scaling": 12800,
    "max_seq_len": 512,
    "weight_sharing": true,
    "mem_efficient": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": null,
    "topk": 10,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "DialogGeneration",
    "do_generation": false,
    "is_cn": false,
    "nsp_inference_model_path": null,
    "nsp_attention_style": "bidirectional",
    "ranking_score": "decode_score"
  },
  "Reader": {
    "max_src_len": 384,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": true,
    "batch_size": 8192,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  }
}
W1023 15:56:43.730015  9132 device_context.cc:252] Please NOTE: device: 0, CUDA Capability: 70, Driver API Version: 11.0, Runtime API Version: 9.0
W1023 15:56:43.734509  9132 device_context.cc:260] device: 0, cuDNN Version: 7.6.
Load pretraining parameters from 12L.
[train][1] progress: 1/1 step: 1, time: 3.080, speed: 0.325 steps/s
	current lr: 0.0000000
	lm_loss: 3.6167, ppl: 37.2152, loss: 3.6167
[train][1] progress: 1/1 step: 2, time: 0.464, speed: 2.156 steps/s
	current lr: 0.0000000
	lm_loss: 3.7217, ppl: 41.3351, loss: 3.7217
[train][1] progress: 1/1 step: 3, time: 0.461, speed: 2.167 steps/s
	current lr: 0.0000000
	lm_loss: 3.6913, ppl: 40.0958, loss: 3.6913
[train][1] progress: 1/1 step: 4, time: 0.460, speed: 2.174 steps/s
	current lr: 0.0000000
	lm_loss: 3.6062, ppl: 36.8242, loss: 3.6062
[train][1] progress: 1/1 step: 5, time: 0.469, speed: 2.130 steps/s
	current lr: 0.0000001
	lm_loss: 3.6761, ppl: 39.4919, loss: 3.6761
[train][1] progress: 1/1 step: 6, time: 0.473, speed: 2.116 steps/s
	current lr: 0.0000001
	lm_loss: 3.6217, ppl: 37.3999, loss: 3.6217
[train][1] progress: 1/1 step: 7, time: 0.462, speed: 2.163 steps/s
	current lr: 0.0000001
	lm_loss: 3.6784, ppl: 39.5829, loss: 3.6784
[train][1] progress: 1/1 step: 8, time: 0.469, speed: 2.130 steps/s
	current lr: 0.0000001
	lm_loss: 3.6987, ppl: 40.3949, loss: 3.6987
[train][1] progress: 1/1 step: 9, time: 0.468, speed: 2.137 steps/s
	current lr: 0.0000001
	lm_loss: 3.6122, ppl: 37.0482, loss: 3.6122
[train][1] progress: 1/1 step: 10, time: 0.462, speed: 2.163 steps/s
	current lr: 0.0000001
	lm_loss: 3.5881, ppl: 36.1644, loss: 3.5881
[train][1] progress: 1/1 step: 11, time: 0.464, speed: 2.157 steps/s
	current lr: 0.0000001
	lm_loss: 3.5828, ppl: 35.9749, loss: 3.5828
[train][1] progress: 1/1 step: 12, time: 0.461, speed: 2.169 steps/s
	current lr: 0.0000001
	lm_loss: 3.6028, ppl: 36.7020, loss: 3.6028
[train][1] progress: 1/1 step: 13, time: 0.466, speed: 2.146 steps/s
	current lr: 0.0000001
	lm_loss: 3.2962, ppl: 27.0102, loss: 3.2962
[train][1] progress: 1/1 step: 14, time: 0.466, speed: 2.146 steps/s
	current lr: 0.0000001
	lm_loss: 3.5913, ppl: 36.2798, loss: 3.5913
[train][1] progress: 1/1 step: 15, time: 0.464, speed: 2.153 steps/s
	current lr: 0.0000002
	lm_loss: 3.5132, ppl: 33.5554, loss: 3.5132
[train][1] progress: 1/1 step: 16, time: 0.462, speed: 2.163 steps/s
	current lr: 0.0000002
	lm_loss: 3.4586, ppl: 31.7732, loss: 3.4586
[train][1] progress: 1/1 step: 17, time: 0.464, speed: 2.153 steps/s
	current lr: 0.0000002
	lm_loss: 3.5805, ppl: 35.8904, loss: 3.5805
[train][1] progress: 1/1 step: 18, time: 0.477, speed: 2.098 steps/s
	current lr: 0.0000002
	lm_loss: 3.7551, ppl: 42.7403, loss: 3.7551
[train][1] progress: 1/1 step: 19, time: 0.477, speed: 2.095 steps/s
	current lr: 0.0000002
	lm_loss: 3.6387, ppl: 38.0433, loss: 3.6387
[train][1] progress: 1/1 step: 20, time: 0.466, speed: 2.144 steps/s
	current lr: 0.0000002
	lm_loss: 3.3849, ppl: 29.5139, loss: 3.3849
[train][1] progress: 1/1 step: 21, time: 0.470, speed: 2.126 steps/s
	current lr: 0.0000002
	lm_loss: 3.6185, ppl: 37.2828, loss: 3.6185
[train][1] progress: 1/1 step: 22, time: 0.473, speed: 2.114 steps/s
	current lr: 0.0000002
	lm_loss: 3.5974, ppl: 36.5046, loss: 3.5974
[train][1] progress: 1/1 step: 23, time: 0.463, speed: 2.158 steps/s
	current lr: 0.0000002
	lm_loss: 3.7483, ppl: 42.4484, loss: 3.7483
[train][1] progress: 1/1 step: 24, time: 0.468, speed: 2.136 steps/s
	current lr: 0.0000002
	lm_loss: 3.5335, ppl: 34.2421, loss: 3.5335
[train][1] progress: 1/1 step: 25, time: 0.466, speed: 2.145 steps/s
	current lr: 0.0000002
	lm_loss: 3.4049, ppl: 30.1101, loss: 3.4049
[train][1] progress: 1/1 step: 26, time: 0.464, speed: 2.156 steps/s
	current lr: 0.0000003
	lm_loss: 3.4505, ppl: 31.5172, loss: 3.4505
[train][1] progress: 1/1 step: 27, time: 0.476, speed: 2.100 steps/s
	current lr: 0.0000003
	lm_loss: 3.6839, ppl: 39.8020, loss: 3.6839
[train][1] progress: 1/1 step: 28, time: 0.470, speed: 2.128 steps/s
	current lr: 0.0000003
	lm_loss: 3.6511, ppl: 38.5163, loss: 3.6511
[train][1] progress: 1/1 step: 29, time: 0.468, speed: 2.135 steps/s
	current lr: 0.0000003
	lm_loss: 3.3910, ppl: 29.6964, loss: 3.3910
[train][1] progress: 1/1 step: 30, time: 0.473, speed: 2.115 steps/s
	current lr: 0.0000003
	lm_loss: 3.7546, ppl: 42.7160, loss: 3.7546
[train][1] progress: 1/1 step: 31, time: 0.475, speed: 2.104 steps/s
	current lr: 0.0000003
	lm_loss: 3.5497, ppl: 34.8029, loss: 3.5497
[train][1] progress: 1/1 step: 32, time: 0.478, speed: 2.091 steps/s
	current lr: 0.0000003
	lm_loss: 3.8804, ppl: 48.4454, loss: 3.8804
[train][1] progress: 1/1 step: 33, time: 0.471, speed: 2.124 steps/s
	current lr: 0.0000003
	lm_loss: 3.5678, ppl: 35.4379, loss: 3.5678
[train][1] progress: 1/1 step: 34, time: 0.463, speed: 2.160 steps/s
	current lr: 0.0000003
	lm_loss: 3.5331, ppl: 34.2293, loss: 3.5331
[train][1] progress: 1/1 step: 35, time: 0.471, speed: 2.122 steps/s
	current lr: 0.0000004
	lm_loss: 3.6524, ppl: 38.5653, loss: 3.6524
[train][1] progress: 1/1 step: 36, time: 0.469, speed: 2.131 steps/s
	current lr: 0.0000004
	lm_loss: 3.4181, ppl: 30.5101, loss: 3.4181
[train][1] progress: 1/1 step: 37, time: 0.475, speed: 2.103 steps/s
	current lr: 0.0000004
	lm_loss: 3.5628, ppl: 35.2631, loss: 3.5628
[train][1] progress: 1/1 step: 38, time: 0.470, speed: 2.127 steps/s
	current lr: 0.0000004
	lm_loss: 3.5889, ppl: 36.1947, loss: 3.5889
[train][1] progress: 1/1 step: 39, time: 0.468, speed: 2.138 steps/s
	current lr: 0.0000004
	lm_loss: 3.7632, ppl: 43.0869, loss: 3.7632
[train][1] progress: 1/1 step: 40, time: 0.463, speed: 2.159 steps/s
	current lr: 0.0000004
	lm_loss: 3.6876, ppl: 39.9487, loss: 3.6876
[train][1] progress: 1/1 step: 41, time: 0.467, speed: 2.140 steps/s
	current lr: 0.0000004
	lm_loss: 3.5555, ppl: 35.0053, loss: 3.5555
[train][1] progress: 1/1 step: 42, time: 0.470, speed: 2.129 steps/s
	current lr: 0.0000004
	lm_loss: 3.4826, ppl: 32.5427, loss: 3.4826
[train][1] progress: 1/1 step: 43, time: 0.478, speed: 2.090 steps/s
	current lr: 0.0000004
	lm_loss: 3.8803, ppl: 48.4390, loss: 3.8803
[train][1] progress: 1/1 step: 44, time: 0.466, speed: 2.146 steps/s
	current lr: 0.0000004
	lm_loss: 3.6052, ppl: 36.7877, loss: 3.6052
[train][1] progress: 1/1 step: 45, time: 0.465, speed: 2.149 steps/s
	current lr: 0.0000005
	lm_loss: 3.7293, ppl: 41.6489, loss: 3.7293
[train][1] progress: 1/1 step: 46, time: 0.466, speed: 2.145 steps/s
	current lr: 0.0000005
	lm_loss: 3.5575, ppl: 35.0744, loss: 3.5575
[train][1] progress: 1/1 step: 47, time: 0.474, speed: 2.112 steps/s
	current lr: 0.0000005
	lm_loss: 3.6116, ppl: 37.0256, loss: 3.6116
[train][1] progress: 1/1 step: 48, time: 0.465, speed: 2.152 steps/s
	current lr: 0.0000005
	lm_loss: 3.5679, ppl: 35.4406, loss: 3.5679
[train][1] progress: 1/1 step: 49, time: 0.478, speed: 2.092 steps/s
	current lr: 0.0000005
	lm_loss: 3.5979, ppl: 36.5225, loss: 3.5979
[train][1] progress: 1/1 step: 50, time: 0.469, speed: 2.132 steps/s
	current lr: 0.0000005
	lm_loss: 3.4842, ppl: 32.5979, loss: 3.4842
[train][1] progress: 1/1 step: 51, time: 0.477, speed: 2.098 steps/s
	current lr: 0.0000005
	lm_loss: 3.7160, ppl: 41.1014, loss: 3.7160
[train][1] progress: 1/1 step: 52, time: 0.468, speed: 2.139 steps/s
	current lr: 0.0000005
	lm_loss: 3.3887, ppl: 29.6277, loss: 3.3887
[train][1] progress: 1/1 step: 53, time: 0.471, speed: 2.122 steps/s
	current lr: 0.0000005
	lm_loss: 3.6567, ppl: 38.7336, loss: 3.6567
[train][1] progress: 1/1 step: 54, time: 0.466, speed: 2.148 steps/s
	current lr: 0.0000005
	lm_loss: 3.6601, ppl: 38.8654, loss: 3.6601
[train][1] progress: 1/1 step: 55, time: 0.537, speed: 1.864 steps/s
	current lr: 0.0000006
	lm_loss: 3.6057, ppl: 36.8060, loss: 3.6057
[train][1] progress: 1/1 step: 56, time: 0.478, speed: 2.092 steps/s
	current lr: 0.0000006
	lm_loss: 3.6729, ppl: 39.3674, loss: 3.6729
[train][1] progress: 1/1 step: 57, time: 0.469, speed: 2.132 steps/s
	current lr: 0.0000006
	lm_loss: 3.5147, ppl: 33.6044, loss: 3.5147
[train][1] progress: 1/1 step: 58, time: 0.469, speed: 2.131 steps/s
	current lr: 0.0000006
	lm_loss: 3.4011, ppl: 29.9968, loss: 3.4011
[train][1] progress: 1/1 step: 59, time: 0.468, speed: 2.136 steps/s
	current lr: 0.0000006
	lm_loss: 3.4955, ppl: 32.9653, loss: 3.4955
[train][1] progress: 1/1 step: 60, time: 0.468, speed: 2.137 steps/s
	current lr: 0.0000006
	lm_loss: 3.5693, ppl: 35.4920, loss: 3.5693
[train][1] progress: 1/1 step: 61, time: 0.475, speed: 2.107 steps/s
	current lr: 0.0000006
	lm_loss: 3.6647, ppl: 39.0448, loss: 3.6647
[train][1] progress: 1/1 step: 62, time: 0.477, speed: 2.096 steps/s
	current lr: 0.0000006
	lm_loss: 3.6325, ppl: 37.8087, loss: 3.6325
[train][1] progress: 1/1 step: 63, time: 0.477, speed: 2.098 steps/s
	current lr: 0.0000006
	lm_loss: 3.5685, ppl: 35.4617, loss: 3.5685
[train][1] progress: 1/1 step: 64, time: 0.468, speed: 2.135 steps/s
	current lr: 0.0000006
	lm_loss: 3.7403, ppl: 42.1118, loss: 3.7403
[train][1] progress: 1/1 step: 65, time: 0.479, speed: 2.088 steps/s
	current lr: 0.0000007
	lm_loss: 3.7462, ppl: 42.3604, loss: 3.7462
[train][1] progress: 1/1 step: 66, time: 0.479, speed: 2.087 steps/s
	current lr: 0.0000007
	lm_loss: 3.4875, ppl: 32.7053, loss: 3.4875
[train][1] progress: 1/1 step: 67, time: 0.470, speed: 2.129 steps/s
	current lr: 0.0000007
	lm_loss: 3.7348, ppl: 41.8815, loss: 3.7348
[train][1] progress: 1/1 step: 68, time: 0.467, speed: 2.141 steps/s
	current lr: 0.0000007
	lm_loss: 3.6580, ppl: 38.7847, loss: 3.6580
[train][1] progress: 1/1 step: 69, time: 0.467, speed: 2.140 steps/s
	current lr: 0.0000007
	lm_loss: 3.5649, ppl: 35.3362, loss: 3.5649
[train][1] progress: 1/1 step: 70, time: 0.469, speed: 2.130 steps/s
	current lr: 0.0000007
	lm_loss: 3.6820, ppl: 39.7246, loss: 3.6820
[train][1] progress: 1/1 step: 71, time: 0.466, speed: 2.147 steps/s
	current lr: 0.0000007
	lm_loss: 3.5786, ppl: 35.8241, loss: 3.5786
[train][1] progress: 1/1 step: 72, time: 0.467, speed: 2.139 steps/s
	current lr: 0.0000007
	lm_loss: 3.6606, ppl: 38.8829, loss: 3.6606
[train][1] progress: 1/1 step: 73, time: 0.467, speed: 2.142 steps/s
	current lr: 0.0000007
	lm_loss: 3.5331, ppl: 34.2292, loss: 3.5331
[train][1] progress: 1/1 step: 74, time: 0.475, speed: 2.103 steps/s
	current lr: 0.0000007
	lm_loss: 3.5449, ppl: 34.6351, loss: 3.5449
[train][1] progress: 1/1 step: 75, time: 0.467, speed: 2.142 steps/s
	current lr: 0.0000008
	lm_loss: 3.5230, ppl: 33.8861, loss: 3.5230
[train][1] progress: 1/1 step: 76, time: 0.470, speed: 2.130 steps/s
	current lr: 0.0000008
	lm_loss: 3.6352, ppl: 37.9094, loss: 3.6352
[train][1] progress: 1/1 step: 77, time: 0.484, speed: 2.066 steps/s
	current lr: 0.0000008
	lm_loss: 3.5511, ppl: 34.8521, loss: 3.5511
[train][1] progress: 1/1 step: 78, time: 0.466, speed: 2.144 steps/s
	current lr: 0.0000008
	lm_loss: 3.6182, ppl: 37.2704, loss: 3.6182
[train][1] progress: 1/1 step: 79, time: 0.478, speed: 2.094 steps/s
	current lr: 0.0000008
	lm_loss: 3.5724, ppl: 35.6012, loss: 3.5724
[train][1] progress: 1/1 step: 80, time: 0.480, speed: 2.082 steps/s
	current lr: 0.0000008
	lm_loss: 2.9165, ppl: 18.4771, loss: 2.9165
[train][1] progress: 1/1 step: 81, time: 0.466, speed: 2.148 steps/s
	current lr: 0.0000008
	lm_loss: 3.6005, ppl: 36.6179, loss: 3.6005
[train][1] progress: 1/1 step: 82, time: 0.470, speed: 2.126 steps/s
	current lr: 0.0000008
	lm_loss: 3.5820, ppl: 35.9463, loss: 3.5820
[train][1] progress: 1/1 step: 83, time: 0.469, speed: 2.130 steps/s
	current lr: 0.0000008
	lm_loss: 3.5399, ppl: 34.4628, loss: 3.5399
[train][1] progress: 1/1 step: 84, time: 0.467, speed: 2.139 steps/s
	current lr: 0.0000008
	lm_loss: 3.3333, ppl: 28.0296, loss: 3.3333
[train][1] progress: 1/1 step: 85, time: 0.477, speed: 2.096 steps/s
	current lr: 0.0000009
	lm_loss: 2.4151, ppl: 11.1904, loss: 2.4151
[train][1] progress: 1/1 step: 86, time: 0.475, speed: 2.106 steps/s
	current lr: 0.0000009
	lm_loss: 3.6985, ppl: 40.3874, loss: 3.6985
[train][1] progress: 1/1 step: 87, time: 0.470, speed: 2.127 steps/s
	current lr: 0.0000009
	lm_loss: 3.4527, ppl: 31.5855, loss: 3.4527
[train][1] progress: 1/1 step: 88, time: 0.470, speed: 2.128 steps/s
	current lr: 0.0000009
	lm_loss: 3.8572, ppl: 47.3330, loss: 3.8572
[train][1] progress: 1/1 step: 89, time: 0.475, speed: 2.103 steps/s
	current lr: 0.0000009
	lm_loss: 3.6333, ppl: 37.8386, loss: 3.6333
[train][1] progress: 1/1 step: 90, time: 0.466, speed: 2.146 steps/s
	current lr: 0.0000009
	lm_loss: 3.5818, ppl: 35.9377, loss: 3.5818
[train][1] progress: 1/1 step: 91, time: 0.522, speed: 1.915 steps/s
	current lr: 0.0000009
	lm_loss: 3.5716, ppl: 35.5750, loss: 3.5716
[train][1] progress: 1/1 step: 92, time: 0.476, speed: 2.101 steps/s
	current lr: 0.0000009
	lm_loss: 3.6852, ppl: 39.8549, loss: 3.6852
[train][1] progress: 1/1 step: 93, time: 0.469, speed: 2.132 steps/s
	current lr: 0.0000009
	lm_loss: 3.5137, ppl: 33.5714, loss: 3.5137
[train][1] progress: 1/1 step: 94, time: 0.481, speed: 2.078 steps/s
	current lr: 0.0000009
	lm_loss: 3.8350, ppl: 46.2945, loss: 3.8350
[train][1] progress: 1/1 step: 95, time: 0.481, speed: 2.081 steps/s
	current lr: 0.0000010
	lm_loss: 3.6663, ppl: 39.1070, loss: 3.6663
[train][1] progress: 1/1 step: 96, time: 0.477, speed: 2.094 steps/s
	current lr: 0.0000010
	lm_loss: 3.5416, ppl: 34.5220, loss: 3.5416
[train][1] progress: 1/1 step: 97, time: 0.475, speed: 2.105 steps/s
	current lr: 0.0000010
	lm_loss: 3.5444, ppl: 34.6178, loss: 3.5444
[train][1] progress: 1/1 step: 98, time: 0.468, speed: 2.136 steps/s
	current lr: 0.0000010
	lm_loss: 3.5642, ppl: 35.3101, loss: 3.5642
[train][1] progress: 1/1 step: 99, time: 0.472, speed: 2.120 steps/s
	current lr: 0.0000010
	lm_loss: 3.4851, ppl: 32.6242, loss: 3.4851
[train][1] progress: 1/1 step: 100, time: 0.476, speed: 2.101 steps/s
	current lr: 0.0000010
	lm_loss: 3.3994, ppl: 29.9466, loss: 3.3994
[train][1] progress: 1/1 step: 101, time: 0.470, speed: 2.129 steps/s
	current lr: 0.0000010
	lm_loss: 3.5260, ppl: 33.9866, loss: 3.5260
[train][1] progress: 1/1 step: 102, time: 0.469, speed: 2.130 steps/s
	current lr: 0.0000010
	lm_loss: 3.7052, ppl: 40.6574, loss: 3.7052
[train][1] progress: 1/1 step: 103, time: 0.469, speed: 2.131 steps/s
	current lr: 0.0000010
	lm_loss: 3.1452, ppl: 23.2235, loss: 3.1452
[train][1] progress: 1/1 step: 104, time: 0.470, speed: 2.128 steps/s
	current lr: 0.0000010
	lm_loss: 3.5036, ppl: 33.2362, loss: 3.5036
[train][1] progress: 1/1 step: 105, time: 0.543, speed: 1.840 steps/s
	current lr: 0.0000011
	lm_loss: 3.6513, ppl: 38.5238, loss: 3.6513
[train][1] progress: 1/1 step: 106, time: 0.473, speed: 2.113 steps/s
	current lr: 0.0000011
	lm_loss: 3.2768, ppl: 26.4905, loss: 3.2768
[train][1] progress: 1/1 step: 107, time: 0.467, speed: 2.142 steps/s
	current lr: 0.0000011
	lm_loss: 3.7084, ppl: 40.7885, loss: 3.7084
[train][1] progress: 1/1 step: 108, time: 0.472, speed: 2.117 steps/s
	current lr: 0.0000011
	lm_loss: 3.4302, ppl: 30.8816, loss: 3.4302
[train][1] progress: 1/1 step: 109, time: 0.066, speed: 15.185 steps/s
	current lr: 0.0000011
	lm_loss: 0.5494, ppl: 1.7322, loss: 0.5494
[train][1] progress: 1/1 step: 110, time: 0.467, speed: 2.140 steps/s
	current lr: 0.0000011
	lm_loss: 3.5689, ppl: 35.4775, loss: 3.5689
[train][1] progress: 1/1 step: 111, time: 0.471, speed: 2.123 steps/s
	current lr: 0.0000011
	lm_loss: 3.6596, ppl: 38.8474, loss: 3.6596
[train][1] progress: 1/1 step: 112, time: 0.470, speed: 2.129 steps/s
	current lr: 0.0000011
	lm_loss: 3.4447, ppl: 31.3349, loss: 3.4447
[train][1] progress: 1/1 step: 113, time: 0.475, speed: 2.105 steps/s
	current lr: 0.0000011
	lm_loss: 3.6468, ppl: 38.3535, loss: 3.6468
[train][1] progress: 1/1 step: 114, time: 0.478, speed: 2.093 steps/s
	current lr: 0.0000011
	lm_loss: 3.5675, ppl: 35.4263, loss: 3.5675
[train][1] progress: 1/1 step: 115, time: 0.469, speed: 2.132 steps/s
	current lr: 0.0000012
	lm_loss: 3.3701, ppl: 29.0801, loss: 3.3701
[train][1] progress: 1/1 step: 116, time: 0.467, speed: 2.140 steps/s
	current lr: 0.0000012
	lm_loss: 3.5104, ppl: 33.4616, loss: 3.5104
[train][1] progress: 1/1 step: 117, time: 0.475, speed: 2.103 steps/s
	current lr: 0.0000012
	lm_loss: 3.6128, ppl: 37.0695, loss: 3.6128
[train][1] progress: 1/1 step: 118, time: 0.475, speed: 2.105 steps/s
	current lr: 0.0000012
	lm_loss: 3.6397, ppl: 38.0799, loss: 3.6397
[train][1] progress: 1/1 step: 119, time: 0.470, speed: 2.127 steps/s
	current lr: 0.0000012
	lm_loss: 3.5299, ppl: 34.1214, loss: 3.5299
[train][1] progress: 1/1 step: 120, time: 0.468, speed: 2.136 steps/s
	current lr: 0.0000012
	lm_loss: 3.5980, ppl: 36.5234, loss: 3.5980
[train][1] progress: 1/1 step: 121, time: 0.476, speed: 2.101 steps/s
	current lr: 0.0000012
	lm_loss: 3.2847, ppl: 26.7009, loss: 3.2847
[train][1] progress: 1/1 step: 122, time: 0.475, speed: 2.106 steps/s
	current lr: 0.0000012
	lm_loss: 3.2302, ppl: 25.2838, loss: 3.2302
[train][1] progress: 1/1 step: 123, time: 0.467, speed: 2.143 steps/s
	current lr: 0.0000012
	lm_loss: 3.7213, ppl: 41.3199, loss: 3.7213
[train][1] progress: 1/1 step: 124, time: 0.466, speed: 2.144 steps/s
	current lr: 0.0000012
	lm_loss: 3.6349, ppl: 37.8987, loss: 3.6349
[train][1] progress: 1/1 step: 125, time: 0.482, speed: 2.073 steps/s
	current lr: 0.0000013
	lm_loss: 3.6610, ppl: 38.9010, loss: 3.6610
[train][1] progress: 1/1 step: 126, time: 0.467, speed: 2.140 steps/s
	current lr: 0.0000013
	lm_loss: 3.6355, ppl: 37.9225, loss: 3.6355
[train][1] progress: 1/1 step: 127, time: 0.470, speed: 2.130 steps/s
	current lr: 0.0000013
	lm_loss: 3.4886, ppl: 32.7398, loss: 3.4886
[train][1] progress: 1/1 step: 128, time: 0.469, speed: 2.134 steps/s
	current lr: 0.0000013
	lm_loss: 3.6611, ppl: 38.9041, loss: 3.6611
[train][1] progress: 1/1 step: 129, time: 0.459, speed: 2.180 steps/s
	current lr: 0.0000013
	lm_loss: 3.6180, ppl: 37.2612, loss: 3.6180
[train][1] progress: 1/1 step: 130, time: 0.469, speed: 2.133 steps/s
	current lr: 0.0000013
	lm_loss: 3.6432, ppl: 38.2122, loss: 3.6432
[train][1] progress: 1/1 step: 131, time: 0.479, speed: 2.090 steps/s
	current lr: 0.0000013
	lm_loss: 3.7034, ppl: 40.5854, loss: 3.7034
[train][1] progress: 1/1 step: 132, time: 0.480, speed: 2.082 steps/s
	current lr: 0.0000013
	lm_loss: 3.5976, ppl: 36.5108, loss: 3.5976
[train][1] progress: 1/1 step: 133, time: 0.477, speed: 2.094 steps/s
	current lr: 0.0000013
	lm_loss: 3.6419, ppl: 38.1630, loss: 3.6419
[train][1] progress: 1/1 step: 134, time: 0.471, speed: 2.124 steps/s
	current lr: 0.0000013
	lm_loss: 3.3985, ppl: 29.9199, loss: 3.3985
[train][1] progress: 1/1 step: 135, time: 0.478, speed: 2.091 steps/s
	current lr: 0.0000014
	lm_loss: 3.5432, ppl: 34.5779, loss: 3.5432
[train][1] progress: 1/1 step: 136, time: 0.470, speed: 2.129 steps/s
	current lr: 0.0000014
	lm_loss: 3.2657, ppl: 26.1993, loss: 3.2657
[train][1] progress: 1/1 step: 137, time: 0.472, speed: 2.121 steps/s
	current lr: 0.0000014
	lm_loss: 3.6279, ppl: 37.6349, loss: 3.6279
[train][1] progress: 1/1 step: 138, time: 0.476, speed: 2.102 steps/s
	current lr: 0.0000014
	lm_loss: 3.6449, ppl: 38.2779, loss: 3.6449
[train][1] progress: 1/1 step: 139, time: 0.479, speed: 2.087 steps/s
	current lr: 0.0000014
	lm_loss: 3.5957, ppl: 36.4416, loss: 3.5957
[train][1] progress: 1/1 step: 140, time: 0.476, speed: 2.102 steps/s
	current lr: 0.0000014
	lm_loss: 3.4248, ppl: 30.7168, loss: 3.4248
[train][1] progress: 1/1 step: 141, time: 0.472, speed: 2.117 steps/s
	current lr: 0.0000014
	lm_loss: 3.5081, ppl: 33.3862, loss: 3.5081
[train][1] progress: 1/1 step: 142, time: 0.479, speed: 2.088 steps/s
	current lr: 0.0000014
	lm_loss: 3.5625, ppl: 35.2525, loss: 3.5625
[train][1] progress: 1/1 step: 143, time: 0.471, speed: 2.125 steps/s
	current lr: 0.0000014
	lm_loss: 3.5327, ppl: 34.2148, loss: 3.5327
[train][1] progress: 1/1 step: 144, time: 0.477, speed: 2.098 steps/s
	current lr: 0.0000014
	lm_loss: 3.6135, ppl: 37.0971, loss: 3.6135
[train][1] progress: 1/1 step: 145, time: 0.467, speed: 2.141 steps/s
	current lr: 0.0000015
	lm_loss: 3.6574, ppl: 38.7586, loss: 3.6574
[train][1] progress: 1/1 step: 146, time: 0.472, speed: 2.117 steps/s
	current lr: 0.0000015
	lm_loss: 3.4164, ppl: 30.4585, loss: 3.4164
[train][1] progress: 1/1 step: 147, time: 0.470, speed: 2.126 steps/s
	current lr: 0.0000015
	lm_loss: 3.6130, ppl: 37.0776, loss: 3.6130
[train][1] progress: 1/1 step: 148, time: 0.468, speed: 2.138 steps/s
	current lr: 0.0000015
	lm_loss: 3.5418, ppl: 34.5277, loss: 3.5418
[train][1] progress: 1/1 step: 149, time: 0.485, speed: 2.061 steps/s
	current lr: 0.0000015
	lm_loss: 3.3985, ppl: 29.9186, loss: 3.3985
[train][1] progress: 1/1 step: 150, time: 0.466, speed: 2.145 steps/s
	current lr: 0.0000015
	lm_loss: 3.5025, ppl: 33.1986, loss: 3.5025
[train][1] progress: 1/1 step: 151, time: 0.475, speed: 2.107 steps/s
	current lr: 0.0000015
	lm_loss: 3.2835, ppl: 26.6678, loss: 3.2835
[train][1] progress: 1/1 step: 152, time: 0.471, speed: 2.125 steps/s
	current lr: 0.0000015
	lm_loss: 3.7364, ppl: 41.9476, loss: 3.7364
[train][1] progress: 1/1 step: 153, time: 0.480, speed: 2.085 steps/s
	current lr: 0.0000015
	lm_loss: 3.7595, ppl: 42.9251, loss: 3.7595
[train][1] progress: 1/1 step: 154, time: 0.481, speed: 2.078 steps/s
	current lr: 0.0000015
	lm_loss: 3.4286, ppl: 30.8325, loss: 3.4286
[train][1] progress: 1/1 step: 155, time: 0.483, speed: 2.070 steps/s
	current lr: 0.0000016
	lm_loss: 3.4649, ppl: 31.9727, loss: 3.4649
[train][1] progress: 1/1 step: 156, time: 0.477, speed: 2.094 steps/s
	current lr: 0.0000016
	lm_loss: 3.5646, ppl: 35.3243, loss: 3.5646
[train][1] progress: 1/1 step: 157, time: 0.472, speed: 2.117 steps/s
	current lr: 0.0000016
	lm_loss: 3.4574, ppl: 31.7334, loss: 3.4574
[train][1] progress: 1/1 step: 158, time: 0.476, speed: 2.099 steps/s
	current lr: 0.0000016
	lm_loss: 3.5107, ppl: 33.4712, loss: 3.5107
[train][1] progress: 1/1 step: 159, time: 0.471, speed: 2.122 steps/s
	current lr: 0.0000016
	lm_loss: 3.6868, ppl: 39.9174, loss: 3.6868
[train][1] progress: 1/1 step: 160, time: 0.479, speed: 2.086 steps/s
	current lr: 0.0000016
	lm_loss: 3.5563, ppl: 35.0339, loss: 3.5563
[train][1] progress: 1/1 step: 161, time: 0.470, speed: 2.130 steps/s
	current lr: 0.0000016
	lm_loss: 3.6667, ppl: 39.1224, loss: 3.6667
[train][1] progress: 1/1 step: 162, time: 0.470, speed: 2.129 steps/s
	current lr: 0.0000016
	lm_loss: 3.7515, ppl: 42.5847, loss: 3.7515
[train][1] progress: 1/1 step: 163, time: 0.480, speed: 2.082 steps/s
	current lr: 0.0000016
	lm_loss: 3.5855, ppl: 36.0698, loss: 3.5855
[train][1] progress: 1/1 step: 164, time: 0.479, speed: 2.087 steps/s
	current lr: 0.0000016
	lm_loss: 3.4866, ppl: 32.6752, loss: 3.4866
[train][1] progress: 1/1 step: 165, time: 0.473, speed: 2.116 steps/s
	current lr: 0.0000017
	lm_loss: 3.7788, ppl: 43.7651, loss: 3.7788
[train][1] progress: 1/1 step: 166, time: 0.471, speed: 2.122 steps/s
	current lr: 0.0000017
	lm_loss: 3.5835, ppl: 36.0006, loss: 3.5835
[train][1] progress: 1/1 step: 167, time: 0.468, speed: 2.138 steps/s
	current lr: 0.0000017
	lm_loss: 3.6022, ppl: 36.6802, loss: 3.6022
[train][1] progress: 1/1 step: 168, time: 0.471, speed: 2.122 steps/s
	current lr: 0.0000017
	lm_loss: 3.5079, ppl: 33.3781, loss: 3.5079
[train][1] progress: 1/1 step: 169, time: 0.469, speed: 2.133 steps/s
	current lr: 0.0000017
	lm_loss: 3.5435, ppl: 34.5866, loss: 3.5435
[train][1] progress: 1/1 step: 170, time: 0.477, speed: 2.097 steps/s
	current lr: 0.0000017
	lm_loss: 3.6102, ppl: 36.9717, loss: 3.6102
[train][1] progress: 1/1 step: 171, time: 0.472, speed: 2.118 steps/s
	current lr: 0.0000017
	lm_loss: 3.5270, ppl: 34.0229, loss: 3.5270
[train][1] progress: 1/1 step: 172, time: 0.477, speed: 2.095 steps/s
	current lr: 0.0000017
	lm_loss: 3.6457, ppl: 38.3103, loss: 3.6457
[train][1] progress: 1/1 step: 173, time: 0.474, speed: 2.112 steps/s
	current lr: 0.0000017
	lm_loss: 3.4221, ppl: 30.6336, loss: 3.4221
[train][1] progress: 1/1 step: 174, time: 0.478, speed: 2.093 steps/s
	current lr: 0.0000017
	lm_loss: 3.5756, ppl: 35.7175, loss: 3.5756
[train][1] progress: 1/1 step: 175, time: 0.471, speed: 2.125 steps/s
	current lr: 0.0000018
	lm_loss: 3.4961, ppl: 32.9875, loss: 3.4961
[train][1] progress: 1/1 step: 176, time: 0.470, speed: 2.127 steps/s
	current lr: 0.0000018
	lm_loss: 3.4547, ppl: 31.6473, loss: 3.4547
[train][1] progress: 1/1 step: 177, time: 0.474, speed: 2.109 steps/s
	current lr: 0.0000018
	lm_loss: 3.4965, ppl: 32.9988, loss: 3.4965
[train][1] progress: 1/1 step: 178, time: 0.467, speed: 2.142 steps/s
	current lr: 0.0000018
	lm_loss: 3.7354, ppl: 41.9052, loss: 3.7354
[train][1] progress: 1/1 step: 179, time: 0.470, speed: 2.129 steps/s
	current lr: 0.0000018
	lm_loss: 3.6215, ppl: 37.3944, loss: 3.6215
[train][1] progress: 1/1 step: 180, time: 0.474, speed: 2.111 steps/s
	current lr: 0.0000018
	lm_loss: 3.5550, ppl: 34.9866, loss: 3.5550
[train][1] progress: 1/1 step: 181, time: 0.471, speed: 2.124 steps/s
	current lr: 0.0000018
	lm_loss: 3.4780, ppl: 32.3950, loss: 3.4780
[train][1] progress: 1/1 step: 182, time: 0.483, speed: 2.069 steps/s
	current lr: 0.0000018
	lm_loss: 3.6145, ppl: 37.1336, loss: 3.6145
[train][1] progress: 1/1 step: 183, time: 0.473, speed: 2.115 steps/s
	current lr: 0.0000018
	lm_loss: 3.5991, ppl: 36.5666, loss: 3.5991
[train][1] progress: 1/1 step: 184, time: 0.472, speed: 2.120 steps/s
	current lr: 0.0000018
	lm_loss: 3.7209, ppl: 41.3024, loss: 3.7209
[train][1] progress: 1/1 step: 185, time: 0.478, speed: 2.091 steps/s
	current lr: 0.0000019
	lm_loss: 3.6525, ppl: 38.5696, loss: 3.6525
[train][1] progress: 1/1 step: 186, time: 0.479, speed: 2.089 steps/s
	current lr: 0.0000019
	lm_loss: 3.7031, ppl: 40.5732, loss: 3.7031
[train][1] progress: 1/1 step: 187, time: 0.482, speed: 2.073 steps/s
	current lr: 0.0000019
	lm_loss: 3.0624, ppl: 21.3785, loss: 3.0624
[train][1] progress: 1/1 step: 188, time: 0.467, speed: 2.140 steps/s
	current lr: 0.0000019
	lm_loss: 3.5344, ppl: 34.2742, loss: 3.5344
[train][1] progress: 1/1 step: 189, time: 0.483, speed: 2.070 steps/s
	current lr: 0.0000019
	lm_loss: 3.6876, ppl: 39.9492, loss: 3.6876
[train][1] progress: 1/1 step: 190, time: 0.478, speed: 2.094 steps/s
	current lr: 0.0000019
	lm_loss: 3.5170, ppl: 33.6847, loss: 3.5170
[train][1] progress: 1/1 step: 191, time: 0.471, speed: 2.123 steps/s
	current lr: 0.0000019
	lm_loss: 3.6687, ppl: 39.1991, loss: 3.6687
[train][1] progress: 1/1 step: 192, time: 0.480, speed: 2.081 steps/s
	current lr: 0.0000019
	lm_loss: 3.7011, ppl: 40.4905, loss: 3.7011
[train][1] progress: 1/1 step: 193, time: 0.492, speed: 2.031 steps/s
	current lr: 0.0000019
	lm_loss: 3.6268, ppl: 37.5933, loss: 3.6268
[train][1] progress: 1/1 step: 194, time: 0.478, speed: 2.091 steps/s
	current lr: 0.0000019
	lm_loss: 3.7051, ppl: 40.6543, loss: 3.7051
[train][1] progress: 1/1 step: 195, time: 0.487, speed: 2.055 steps/s
	current lr: 0.0000020
	lm_loss: 3.6078, ppl: 36.8849, loss: 3.6078
[train][1] progress: 1/1 step: 196, time: 0.509, speed: 1.966 steps/s
	current lr: 0.0000020
	lm_loss: 3.2863, ppl: 26.7426, loss: 3.2863
[train][1] progress: 1/1 step: 197, time: 0.469, speed: 2.132 steps/s
	current lr: 0.0000020
	lm_loss: 3.5724, ppl: 35.6028, loss: 3.5724
[train][1] progress: 1/1 step: 198, time: 0.479, speed: 2.086 steps/s
	current lr: 0.0000020
	lm_loss: 3.6236, ppl: 37.4707, loss: 3.6236
[train][1] progress: 1/1 step: 199, time: 0.465, speed: 2.149 steps/s
	current lr: 0.0000020
	lm_loss: 3.5976, ppl: 36.5092, loss: 3.5976
[train][1] progress: 1/1 step: 200, time: 0.481, speed: 2.080 steps/s
	current lr: 0.0000020
	lm_loss: 3.5863, ppl: 36.1018, loss: 3.5863
[train][1] progress: 1/1 step: 201, time: 0.468, speed: 2.135 steps/s
	current lr: 0.0000020
	lm_loss: 3.6700, ppl: 39.2519, loss: 3.6700
[train][1] progress: 1/1 step: 202, time: 0.479, speed: 2.087 steps/s
	current lr: 0.0000020
	lm_loss: 2.9551, ppl: 19.2037, loss: 2.9551
[train][1] progress: 1/1 step: 203, time: 0.477, speed: 2.095 steps/s
	current lr: 0.0000020
	lm_loss: 3.6339, ppl: 37.8615, loss: 3.6339
[train][1] progress: 1/1 step: 204, time: 0.477, speed: 2.098 steps/s
	current lr: 0.0000020
	lm_loss: 3.7386, ppl: 42.0393, loss: 3.7386
[train][1] progress: 1/1 step: 205, time: 0.480, speed: 2.084 steps/s
	current lr: 0.0000021
	lm_loss: 3.7745, ppl: 43.5753, loss: 3.7745
[train][1] progress: 1/1 step: 206, time: 0.469, speed: 2.131 steps/s
	current lr: 0.0000021
	lm_loss: 3.7358, ppl: 41.9200, loss: 3.7358
[train][1] progress: 1/1 step: 207, time: 0.472, speed: 2.120 steps/s
	current lr: 0.0000021
	lm_loss: 3.5850, ppl: 36.0533, loss: 3.5850
[train][1] progress: 1/1 step: 208, time: 0.481, speed: 2.081 steps/s
	current lr: 0.0000021
	lm_loss: 3.6211, ppl: 37.3792, loss: 3.6211
[train][1] progress: 1/1 step: 209, time: 0.471, speed: 2.124 steps/s
	current lr: 0.0000021
	lm_loss: 3.7091, ppl: 40.8164, loss: 3.7091
[train][1] progress: 1/1 step: 210, time: 0.480, speed: 2.082 steps/s
	current lr: 0.0000021
	lm_loss: 3.5291, ppl: 34.0918, loss: 3.5291
[train][1] progress: 1/1 step: 211, time: 0.471, speed: 2.124 steps/s
	current lr: 0.0000021
	lm_loss: 3.4939, ppl: 32.9132, loss: 3.4939
[train][1] progress: 1/1 step: 212, time: 0.472, speed: 2.119 steps/s
	current lr: 0.0000021
	lm_loss: 3.5014, ppl: 33.1616, loss: 3.5014
[train][1] progress: 1/1 step: 213, time: 0.618, speed: 1.617 steps/s
	current lr: 0.0000021
	lm_loss: 3.6991, ppl: 40.4117, loss: 3.6991
[train][1] progress: 1/1 step: 214, time: 0.662, speed: 1.510 steps/s
	current lr: 0.0000021
	lm_loss: 3.6574, ppl: 38.7606, loss: 3.6574
[train][1] progress: 1/1 step: 215, time: 0.695, speed: 1.438 steps/s
	current lr: 0.0000022
	lm_loss: 3.3875, ppl: 29.5908, loss: 3.3875
[train][1] progress: 1/1 step: 216, time: 0.476, speed: 2.103 steps/s
	current lr: 0.0000022
	lm_loss: 3.5729, ppl: 35.6207, loss: 3.5729
[train][1] progress: 1/1 step: 217, time: 0.476, speed: 2.102 steps/s
	current lr: 0.0000022
	lm_loss: 3.6496, ppl: 38.4600, loss: 3.6496
[train][1] progress: 1/1 step: 218, time: 0.479, speed: 2.090 steps/s
	current lr: 0.0000022
	lm_loss: 3.6137, ppl: 37.1044, loss: 3.6137
[train][1] progress: 1/1 step: 219, time: 0.467, speed: 2.143 steps/s
	current lr: 0.0000022
	lm_loss: 3.4922, ppl: 32.8573, loss: 3.4922
[train][1] progress: 1/1 step: 220, time: 0.472, speed: 2.119 steps/s
	current lr: 0.0000022
	lm_loss: 3.6923, ppl: 40.1387, loss: 3.6923
[train][1] progress: 1/1 step: 221, time: 0.478, speed: 2.094 steps/s
	current lr: 0.0000022
	lm_loss: 3.6361, ppl: 37.9443, loss: 3.6361
[train][1] progress: 1/1 step: 222, time: 0.470, speed: 2.129 steps/s
	current lr: 0.0000022
	lm_loss: 3.5793, ppl: 35.8496, loss: 3.5793
[train][1] progress: 1/1 step: 223, time: 0.481, speed: 2.077 steps/s
	current lr: 0.0000022
	lm_loss: 3.6071, ppl: 36.8583, loss: 3.6071
[train][1] progress: 1/1 step: 224, time: 0.544, speed: 1.838 steps/s
	current lr: 0.0000022
	lm_loss: 3.6444, ppl: 38.2592, loss: 3.6444
[train][1] progress: 1/1 step: 225, time: 0.480, speed: 2.085 steps/s
	current lr: 0.0000023
	lm_loss: 3.4762, ppl: 32.3350, loss: 3.4762
[train][1] progress: 1/1 step: 226, time: 0.471, speed: 2.122 steps/s
	current lr: 0.0000023
	lm_loss: 3.4382, ppl: 31.1301, loss: 3.4382
[train][1] progress: 1/1 step: 227, time: 0.473, speed: 2.114 steps/s
	current lr: 0.0000023
	lm_loss: 3.5519, ppl: 34.8797, loss: 3.5519
[train][1] progress: 1/1 step: 228, time: 0.469, speed: 2.134 steps/s
	current lr: 0.0000023
	lm_loss: 3.5636, ppl: 35.2906, loss: 3.5636
[train][1] progress: 1/1 step: 229, time: 0.469, speed: 2.132 steps/s
	current lr: 0.0000023
	lm_loss: 3.5337, ppl: 34.2490, loss: 3.5337
[train][1] progress: 1/1 step: 230, time: 0.472, speed: 2.118 steps/s
	current lr: 0.0000023
	lm_loss: 3.8421, ppl: 46.6242, loss: 3.8421
[train][1] progress: 1/1 step: 231, time: 0.474, speed: 2.112 steps/s
	current lr: 0.0000023
	lm_loss: 3.5750, ppl: 35.6955, loss: 3.5750
[train][1] progress: 1/1 step: 232, time: 0.471, speed: 2.121 steps/s
	current lr: 0.0000023
	lm_loss: 3.4872, ppl: 32.6931, loss: 3.4872
[train][1] progress: 1/1 step: 233, time: 0.466, speed: 2.148 steps/s
	current lr: 0.0000023
	lm_loss: 3.4753, ppl: 32.3077, loss: 3.4753
[train][1] progress: 1/1 step: 234, time: 0.486, speed: 2.057 steps/s
	current lr: 0.0000023
	lm_loss: 3.2321, ppl: 25.3331, loss: 3.2321
[train][1] progress: 1/1 step: 235, time: 0.470, speed: 2.128 steps/s
	current lr: 0.0000024
	lm_loss: 3.5584, ppl: 35.1079, loss: 3.5584
[train][1] progress: 1/1 step: 236, time: 0.469, speed: 2.133 steps/s
	current lr: 0.0000024
	lm_loss: 3.4763, ppl: 32.3412, loss: 3.4763
[train][1] progress: 1/1 step: 237, time: 0.477, speed: 2.096 steps/s
	current lr: 0.0000024
	lm_loss: 3.2911, ppl: 26.8719, loss: 3.2911
[train][1] progress: 1/1 step: 238, time: 0.474, speed: 2.108 steps/s
	current lr: 0.0000024
	lm_loss: 3.4738, ppl: 32.2576, loss: 3.4738
[train][1] progress: 1/1 step: 239, time: 0.471, speed: 2.123 steps/s
	current lr: 0.0000024
	lm_loss: 3.5719, ppl: 35.5832, loss: 3.5719
[train][1] progress: 1/1 step: 240, time: 0.480, speed: 2.085 steps/s
	current lr: 0.0000024
	lm_loss: 3.3610, ppl: 28.8179, loss: 3.3610
[train][1] progress: 1/1 step: 241, time: 0.472, speed: 2.118 steps/s
	current lr: 0.0000024
	lm_loss: 3.5290, ppl: 34.0892, loss: 3.5290
[train][1] progress: 1/1 step: 242, time: 0.473, speed: 2.113 steps/s
	current lr: 0.0000024
	lm_loss: 3.4535, ppl: 31.6107, loss: 3.4535
[train][1] progress: 1/1 step: 243, time: 0.473, speed: 2.116 steps/s
	current lr: 0.0000024
	lm_loss: 3.7010, ppl: 40.4895, loss: 3.7010
[train][1] progress: 1/1 step: 244, time: 0.470, speed: 2.128 steps/s
	current lr: 0.0000024
	lm_loss: 3.5458, ppl: 34.6659, loss: 3.5458
[train][1] progress: 1/1 step: 245, time: 0.468, speed: 2.138 steps/s
	current lr: 0.0000025
	lm_loss: 3.5813, ppl: 35.9204, loss: 3.5813
[train][1] progress: 1/1 step: 246, time: 0.471, speed: 2.121 steps/s
	current lr: 0.0000025
	lm_loss: 3.5385, ppl: 34.4156, loss: 3.5385
[train][1] progress: 1/1 step: 247, time: 0.471, speed: 2.122 steps/s
	current lr: 0.0000025
	lm_loss: 3.3692, ppl: 29.0553, loss: 3.3692
[train][1] progress: 1/1 step: 248, time: 0.472, speed: 2.120 steps/s
	current lr: 0.0000025
	lm_loss: 3.6265, ppl: 37.5825, loss: 3.6265
[train][1] progress: 1/1 step: 249, time: 0.462, speed: 2.164 steps/s
	current lr: 0.0000025
	lm_loss: 3.5021, ppl: 33.1850, loss: 3.5021
[train][1] progress: 1/1 step: 250, time: 0.477, speed: 2.098 steps/s
	current lr: 0.0000025
	lm_loss: 3.7269, ppl: 41.5519, loss: 3.7269
[train][1] progress: 1/1 step: 251, time: 0.477, speed: 2.097 steps/s
	current lr: 0.0000025
	lm_loss: 3.5527, ppl: 34.9085, loss: 3.5527
[train][1] progress: 1/1 step: 252, time: 0.474, speed: 2.109 steps/s
	current lr: 0.0000025
	lm_loss: 3.5656, ppl: 35.3599, loss: 3.5656
[train][1] progress: 1/1 step: 253, time: 0.472, speed: 2.117 steps/s
	current lr: 0.0000025
	lm_loss: 3.6625, ppl: 38.9570, loss: 3.6625
[train][1] progress: 1/1 step: 254, time: 0.477, speed: 2.095 steps/s
	current lr: 0.0000025
	lm_loss: 3.5157, ppl: 33.6392, loss: 3.5157
[train][1] progress: 1/1 step: 255, time: 0.480, speed: 2.084 steps/s
	current lr: 0.0000025
	lm_loss: 3.7136, ppl: 41.0017, loss: 3.7136
[train][1] progress: 1/1 step: 256, time: 0.485, speed: 2.062 steps/s
	current lr: 0.0000026
	lm_loss: 3.0681, ppl: 21.5016, loss: 3.0681
[train][1] progress: 1/1 step: 257, time: 0.477, speed: 2.097 steps/s
	current lr: 0.0000026
	lm_loss: 3.5204, ppl: 33.7968, loss: 3.5204
[train][1] progress: 1/1 step: 258, time: 0.471, speed: 2.121 steps/s
	current lr: 0.0000026
	lm_loss: 3.3862, ppl: 29.5533, loss: 3.3862
[train][1] progress: 1/1 step: 259, time: 0.469, speed: 2.133 steps/s
	current lr: 0.0000026
	lm_loss: 3.5675, ppl: 35.4279, loss: 3.5675
[train][1] progress: 1/1 step: 260, time: 0.477, speed: 2.095 steps/s
	current lr: 0.0000026
	lm_loss: 3.6066, ppl: 36.8421, loss: 3.6066
[train][1] progress: 1/1 step: 261, time: 0.469, speed: 2.134 steps/s
	current lr: 0.0000026
	lm_loss: 3.5737, ppl: 35.6471, loss: 3.5737
[train][1] progress: 1/1 step: 262, time: 0.488, speed: 2.050 steps/s
	current lr: 0.0000026
	lm_loss: 2.8859, ppl: 17.9198, loss: 2.8859
[train][1] progress: 1/1 step: 263, time: 0.479, speed: 2.086 steps/s
	current lr: 0.0000026
	lm_loss: 3.5997, ppl: 36.5856, loss: 3.5997
[train][1] progress: 1/1 step: 264, time: 0.482, speed: 2.075 steps/s
	current lr: 0.0000026
	lm_loss: 3.2758, ppl: 26.4650, loss: 3.2758
[train][1] progress: 1/1 step: 265, time: 0.482, speed: 2.075 steps/s
	current lr: 0.0000026
	lm_loss: 3.6551, ppl: 38.6719, loss: 3.6551
[train][1] progress: 1/1 step: 266, time: 0.471, speed: 2.123 steps/s
	current lr: 0.0000027
	lm_loss: 3.7005, ppl: 40.4665, loss: 3.7005
[train][1] progress: 1/1 step: 267, time: 0.480, speed: 2.083 steps/s
	current lr: 0.0000027
	lm_loss: 3.6559, ppl: 38.7030, loss: 3.6559
[train][1] progress: 1/1 step: 268, time: 0.470, speed: 2.128 steps/s
	current lr: 0.0000027
	lm_loss: 3.5770, ppl: 35.7666, loss: 3.5770
[train][1] progress: 1/1 step: 269, time: 0.483, speed: 2.072 steps/s
	current lr: 0.0000027
	lm_loss: 3.6591, ppl: 38.8246, loss: 3.6591
[train][1] progress: 1/1 step: 270, time: 0.472, speed: 2.117 steps/s
	current lr: 0.0000027
	lm_loss: 3.3953, ppl: 29.8227, loss: 3.3953
[train][1] progress: 1/1 step: 271, time: 0.470, speed: 2.126 steps/s
	current lr: 0.0000027
	lm_loss: 3.5148, ppl: 33.6077, loss: 3.5148
[train][1] progress: 1/1 step: 272, time: 0.477, speed: 2.095 steps/s
	current lr: 0.0000027
	lm_loss: 3.6000, ppl: 36.5969, loss: 3.6000
[train][1] progress: 1/1 step: 273, time: 0.484, speed: 2.067 steps/s
	current lr: 0.0000027
	lm_loss: 3.6463, ppl: 38.3324, loss: 3.6463
[train][1] progress: 1/1 step: 274, time: 0.478, speed: 2.092 steps/s
	current lr: 0.0000027
	lm_loss: 3.6250, ppl: 37.5253, loss: 3.6250
[train][1] progress: 1/1 step: 275, time: 0.472, speed: 2.118 steps/s
	current lr: 0.0000028
	lm_loss: 3.5414, ppl: 34.5148, loss: 3.5414
[train][1] progress: 1/1 step: 276, time: 0.480, speed: 2.082 steps/s
	current lr: 0.0000028
	lm_loss: 3.4444, ppl: 31.3258, loss: 3.4444
[train][1] progress: 1/1 step: 277, time: 0.478, speed: 2.091 steps/s
	current lr: 0.0000028
	lm_loss: 3.6127, ppl: 37.0668, loss: 3.6127
[train][1] progress: 1/1 step: 278, time: 0.481, speed: 2.078 steps/s
	current lr: 0.0000028
	lm_loss: 3.5938, ppl: 36.3726, loss: 3.5938
[train][1] progress: 1/1 step: 279, time: 0.552, speed: 1.811 steps/s
	current lr: 0.0000028
	lm_loss: 3.5640, ppl: 35.3048, loss: 3.5640
[train][1] progress: 1/1 step: 280, time: 0.468, speed: 2.135 steps/s
	current lr: 0.0000028
	lm_loss: 3.3989, ppl: 29.9299, loss: 3.3989
[train][1] progress: 1/1 step: 281, time: 0.471, speed: 2.123 steps/s
	current lr: 0.0000028
	lm_loss: 3.5653, ppl: 35.3483, loss: 3.5653
[train][1] progress: 1/1 step: 282, time: 0.480, speed: 2.082 steps/s
	current lr: 0.0000028
	lm_loss: 3.4888, ppl: 32.7477, loss: 3.4888
[train][1] progress: 1/1 step: 283, time: 0.470, speed: 2.126 steps/s
	current lr: 0.0000028
	lm_loss: 3.3003, ppl: 27.1202, loss: 3.3003
[train][1] progress: 1/1 step: 284, time: 0.479, speed: 2.088 steps/s
	current lr: 0.0000028
	lm_loss: 3.4756, ppl: 32.3162, loss: 3.4756
[train][1] progress: 1/1 step: 285, time: 0.468, speed: 2.138 steps/s
	current lr: 0.0000028
	lm_loss: 3.5389, ppl: 34.4297, loss: 3.5389
[train][1] progress: 1/1 step: 286, time: 0.477, speed: 2.099 steps/s
	current lr: 0.0000029
	lm_loss: 3.5599, ppl: 35.1591, loss: 3.5599
[train][1] progress: 1/1 step: 287, time: 0.469, speed: 2.133 steps/s
	current lr: 0.0000029
	lm_loss: 3.5726, ppl: 35.6096, loss: 3.5726
[train][1] progress: 1/1 step: 288, time: 0.467, speed: 2.140 steps/s
	current lr: 0.0000029
	lm_loss: 3.6440, ppl: 38.2454, loss: 3.6440
[train][1] progress: 1/1 step: 289, time: 0.469, speed: 2.134 steps/s
	current lr: 0.0000029
	lm_loss: 3.5695, ppl: 35.4995, loss: 3.5695
[train][1] progress: 1/1 step: 290, time: 0.481, speed: 2.078 steps/s
	current lr: 0.0000029
	lm_loss: 3.7083, ppl: 40.7827, loss: 3.7083
[train][1] progress: 1/1 step: 291, time: 0.468, speed: 2.137 steps/s
	current lr: 0.0000029
	lm_loss: 3.4787, ppl: 32.4178, loss: 3.4787
[train][1] progress: 1/1 step: 292, time: 0.466, speed: 2.148 steps/s
	current lr: 0.0000029
	lm_loss: 3.6698, ppl: 39.2459, loss: 3.6698
[train][1] progress: 1/1 step: 293, time: 0.479, speed: 2.089 steps/s
	current lr: 0.0000029
	lm_loss: 3.2659, ppl: 26.2028, loss: 3.2659
[train][1] progress: 1/1 step: 294, time: 0.472, speed: 2.118 steps/s
	current lr: 0.0000029
	lm_loss: 3.5624, ppl: 35.2467, loss: 3.5624
[train][1] progress: 1/1 step: 295, time: 0.472, speed: 2.119 steps/s
	current lr: 0.0000029
	lm_loss: 3.5659, ppl: 35.3703, loss: 3.5659
[train][1] progress: 1/1 step: 296, time: 0.471, speed: 2.123 steps/s
	current lr: 0.0000030
	lm_loss: 3.6197, ppl: 37.3257, loss: 3.6197
[train][1] progress: 1/1 step: 297, time: 0.476, speed: 2.100 steps/s
	current lr: 0.0000030
	lm_loss: 3.6648, ppl: 39.0501, loss: 3.6648
[train][1] progress: 1/1 step: 298, time: 0.487, speed: 2.051 steps/s
	current lr: 0.0000030
	lm_loss: 3.5984, ppl: 36.5401, loss: 3.5984
[train][1] progress: 1/1 step: 299, time: 0.478, speed: 2.093 steps/s
	current lr: 0.0000030
	lm_loss: 3.6398, ppl: 38.0854, loss: 3.6398
[train][1] progress: 1/1 step: 300, time: 0.475, speed: 2.104 steps/s
	current lr: 0.0000030
	lm_loss: 3.8232, ppl: 45.7508, loss: 3.8232
[train][1] progress: 1/1 step: 301, time: 0.481, speed: 2.080 steps/s
	current lr: 0.0000030
	lm_loss: 3.5814, ppl: 35.9223, loss: 3.5814
[train][1] progress: 1/1 step: 302, time: 0.479, speed: 2.086 steps/s
	current lr: 0.0000030
	lm_loss: 3.6315, ppl: 37.7693, loss: 3.6315
[train][1] progress: 1/1 step: 303, time: 0.479, speed: 2.088 steps/s
	current lr: 0.0000030
	lm_loss: 3.6056, ppl: 36.8028, loss: 3.6056
[train][1] progress: 1/1 step: 304, time: 0.469, speed: 2.130 steps/s
	current lr: 0.0000030
	lm_loss: 3.4840, ppl: 32.5909, loss: 3.4840
[train][1] progress: 1/1 step: 305, time: 0.474, speed: 2.110 steps/s
	current lr: 0.0000031
	lm_loss: 3.6727, ppl: 39.3577, loss: 3.6727
[train][1] progress: 1/1 step: 306, time: 0.472, speed: 2.120 steps/s
	current lr: 0.0000031
	lm_loss: 3.3615, ppl: 28.8331, loss: 3.3615
[train][1] progress: 1/1 step: 307, time: 0.467, speed: 2.143 steps/s
	current lr: 0.0000031
	lm_loss: 3.7074, ppl: 40.7492, loss: 3.7074
[train][1] progress: 1/1 step: 308, time: 0.470, speed: 2.127 steps/s
	current lr: 0.0000031
	lm_loss: 3.5675, ppl: 35.4288, loss: 3.5675
[train][1] progress: 1/1 step: 309, time: 0.478, speed: 2.091 steps/s
	current lr: 0.0000031
	lm_loss: 3.5143, ppl: 33.5929, loss: 3.5143
[train][1] progress: 1/1 step: 310, time: 0.472, speed: 2.120 steps/s
	current lr: 0.0000031
	lm_loss: 3.4881, ppl: 32.7247, loss: 3.4881
[train][1] progress: 1/1 step: 311, time: 0.476, speed: 2.101 steps/s
	current lr: 0.0000031
	lm_loss: 3.6744, ppl: 39.4260, loss: 3.6744
[train][1] progress: 1/1 step: 312, time: 0.478, speed: 2.093 steps/s
	current lr: 0.0000031
	lm_loss: 3.6782, ppl: 39.5758, loss: 3.6782
[train][1] progress: 1/1 step: 313, time: 0.479, speed: 2.088 steps/s
	current lr: 0.0000031
	lm_loss: 3.6678, ppl: 39.1645, loss: 3.6678
[train][1] progress: 1/1 step: 314, time: 0.473, speed: 2.113 steps/s
	current lr: 0.0000031
	lm_loss: 3.7996, ppl: 44.6834, loss: 3.7996
[train][1] progress: 1/1 step: 315, time: 0.473, speed: 2.115 steps/s
	current lr: 0.0000031
	lm_loss: 3.6477, ppl: 38.3846, loss: 3.6477
[train][1] progress: 1/1 step: 316, time: 0.466, speed: 2.144 steps/s
	current lr: 0.0000032
	lm_loss: 3.6108, ppl: 36.9961, loss: 3.6108
[train][1] progress: 1/1 step: 317, time: 0.471, speed: 2.123 steps/s
	current lr: 0.0000032
	lm_loss: 3.4931, ppl: 32.8866, loss: 3.4931
[train][1] progress: 1/1 step: 318, time: 0.472, speed: 2.119 steps/s
	current lr: 0.0000032
	lm_loss: 3.7511, ppl: 42.5679, loss: 3.7511
[train][1] progress: 1/1 step: 319, time: 0.480, speed: 2.084 steps/s
	current lr: 0.0000032
	lm_loss: 3.7384, ppl: 42.0310, loss: 3.7384
[train][1] progress: 1/1 step: 320, time: 0.479, speed: 2.086 steps/s
	current lr: 0.0000032
	lm_loss: 3.6520, ppl: 38.5525, loss: 3.6520
[train][1] progress: 1/1 step: 321, time: 0.471, speed: 2.125 steps/s
	current lr: 0.0000032
	lm_loss: 3.6252, ppl: 37.5328, loss: 3.6252
[train][1] progress: 1/1 step: 322, time: 0.472, speed: 2.119 steps/s
	current lr: 0.0000032
	lm_loss: 3.6328, ppl: 37.8178, loss: 3.6328
[train][1] progress: 1/1 step: 323, time: 0.469, speed: 2.133 steps/s
	current lr: 0.0000032
	lm_loss: 3.5233, ppl: 33.8977, loss: 3.5233
[train][1] progress: 1/1 step: 324, time: 0.471, speed: 2.124 steps/s
	current lr: 0.0000032
	lm_loss: 3.4412, ppl: 31.2251, loss: 3.4412
[train][1] progress: 1/1 step: 325, time: 0.468, speed: 2.136 steps/s
	current lr: 0.0000032
	lm_loss: 3.4382, ppl: 31.1298, loss: 3.4382
[train][1] progress: 1/1 step: 326, time: 0.470, speed: 2.130 steps/s
	current lr: 0.0000033
	lm_loss: 3.5682, ppl: 35.4518, loss: 3.5682
[train][1] progress: 1/1 step: 327, time: 0.483, speed: 2.072 steps/s
	current lr: 0.0000033
	lm_loss: 3.6700, ppl: 39.2522, loss: 3.6700
[train][1] progress: 1/1 step: 328, time: 0.482, speed: 2.075 steps/s
	current lr: 0.0000033
	lm_loss: 3.5533, ppl: 34.9296, loss: 3.5533
[train][1] progress: 1/1 step: 329, time: 0.481, speed: 2.079 steps/s
	current lr: 0.0000033
	lm_loss: 3.5131, ppl: 33.5510, loss: 3.5131
[train][1] progress: 1/1 step: 330, time: 0.480, speed: 2.082 steps/s
	current lr: 0.0000033
	lm_loss: 3.5097, ppl: 33.4375, loss: 3.5097
[train][1] progress: 1/1 step: 331, time: 0.478, speed: 2.091 steps/s
	current lr: 0.0000033
	lm_loss: 3.5437, ppl: 34.5939, loss: 3.5437
[train][1] progress: 1/1 step: 332, time: 0.475, speed: 2.105 steps/s
	current lr: 0.0000033
	lm_loss: 3.6466, ppl: 38.3425, loss: 3.6466
[train][1] progress: 1/1 step: 333, time: 0.472, speed: 2.118 steps/s
	current lr: 0.0000033
	lm_loss: 3.5697, ppl: 35.5073, loss: 3.5697
[train][1] progress: 1/1 step: 334, time: 0.476, speed: 2.101 steps/s
	current lr: 0.0000033
	lm_loss: 3.5881, ppl: 36.1649, loss: 3.5881
[train][1] progress: 1/1 step: 335, time: 0.479, speed: 2.086 steps/s
	current lr: 0.0000034
	lm_loss: 3.6350, ppl: 37.9021, loss: 3.6350
[train][1] progress: 1/1 step: 336, time: 0.483, speed: 2.070 steps/s
	current lr: 0.0000034
	lm_loss: 3.4806, ppl: 32.4792, loss: 3.4806
[train][1] progress: 1/1 step: 337, time: 0.471, speed: 2.123 steps/s
	current lr: 0.0000034
	lm_loss: 3.3961, ppl: 29.8476, loss: 3.3961
[train][1] progress: 1/1 step: 338, time: 0.469, speed: 2.133 steps/s
	current lr: 0.0000034
	lm_loss: 3.6112, ppl: 37.0093, loss: 3.6112
[train][1] progress: 1/1 step: 339, time: 0.479, speed: 2.090 steps/s
	current lr: 0.0000034
	lm_loss: 3.5301, ppl: 34.1261, loss: 3.5301
[train][1] progress: 1/1 step: 340, time: 0.481, speed: 2.080 steps/s
	current lr: 0.0000034
	lm_loss: 3.6192, ppl: 37.3088, loss: 3.6192
[train][1] progress: 1/1 step: 341, time: 0.470, speed: 2.128 steps/s
	current lr: 0.0000034
	lm_loss: 3.6445, ppl: 38.2641, loss: 3.6445
[train][1] progress: 1/1 step: 342, time: 0.468, speed: 2.135 steps/s
	current lr: 0.0000034
	lm_loss: 3.5137, ppl: 33.5712, loss: 3.5137
[train][1] progress: 1/1 step: 343, time: 0.472, speed: 2.119 steps/s
	current lr: 0.0000034
	lm_loss: 3.4729, ppl: 32.2288, loss: 3.4729
[train][1] progress: 1/1 step: 344, time: 0.476, speed: 2.100 steps/s
	current lr: 0.0000034
	lm_loss: 3.6295, ppl: 37.6937, loss: 3.6295
[train][1] progress: 1/1 step: 345, time: 0.475, speed: 2.106 steps/s
	current lr: 0.0000034
	lm_loss: 3.5974, ppl: 36.5043, loss: 3.5974
[train][1] progress: 1/1 step: 346, time: 0.476, speed: 2.100 steps/s
	current lr: 0.0000035
	lm_loss: 3.3022, ppl: 27.1714, loss: 3.3022
[train][1] progress: 1/1 step: 347, time: 0.483, speed: 2.069 steps/s
	current lr: 0.0000035
	lm_loss: 3.7002, ppl: 40.4537, loss: 3.7002
[train][1] progress: 1/1 step: 348, time: 0.479, speed: 2.089 steps/s
	current lr: 0.0000035
	lm_loss: 3.5844, ppl: 36.0306, loss: 3.5844
[train][1] progress: 1/1 step: 349, time: 0.474, speed: 2.108 steps/s
	current lr: 0.0000035
	lm_loss: 3.5303, ppl: 34.1343, loss: 3.5303
[train][1] progress: 1/1 step: 350, time: 0.481, speed: 2.081 steps/s
	current lr: 0.0000035
	lm_loss: 3.6645, ppl: 39.0355, loss: 3.6645
[train][1] progress: 1/1 step: 351, time: 0.470, speed: 2.129 steps/s
	current lr: 0.0000035
	lm_loss: 3.6224, ppl: 37.4258, loss: 3.6224
[train][1] progress: 1/1 step: 352, time: 0.470, speed: 2.128 steps/s
	current lr: 0.0000035
	lm_loss: 3.5024, ppl: 33.1965, loss: 3.5024
[train][1] progress: 1/1 step: 353, time: 0.474, speed: 2.112 steps/s
	current lr: 0.0000035
	lm_loss: 3.5539, ppl: 34.9505, loss: 3.5539
[train][1] progress: 1/1 step: 354, time: 0.473, speed: 2.116 steps/s
	current lr: 0.0000035
	lm_loss: 3.7012, ppl: 40.4957, loss: 3.7012
[train][1] progress: 1/1 step: 355, time: 0.480, speed: 2.082 steps/s
	current lr: 0.0000035
	lm_loss: 3.6641, ppl: 39.0224, loss: 3.6641
[train][1] progress: 1/1 step: 356, time: 0.478, speed: 2.093 steps/s
	current lr: 0.0000036
	lm_loss: 3.5782, ppl: 35.8074, loss: 3.5782
[train][1] progress: 1/1 step: 357, time: 0.472, speed: 2.117 steps/s
	current lr: 0.0000036
	lm_loss: 3.6979, ppl: 40.3607, loss: 3.6979
[train][1] progress: 1/1 step: 358, time: 0.473, speed: 2.113 steps/s
	current lr: 0.0000036
	lm_loss: 3.1056, ppl: 22.3227, loss: 3.1056
[train][1] progress: 1/1 step: 359, time: 0.472, speed: 2.119 steps/s
	current lr: 0.0000036
	lm_loss: 3.5471, ppl: 34.7128, loss: 3.5471
[train][1] progress: 1/1 step: 360, time: 0.470, speed: 2.126 steps/s
	current lr: 0.0000036
	lm_loss: 3.6902, ppl: 40.0535, loss: 3.6902
[train][1] progress: 1/1 step: 361, time: 0.473, speed: 2.115 steps/s
	current lr: 0.0000036
	lm_loss: 3.7142, ppl: 41.0245, loss: 3.7142
[train][1] progress: 1/1 step: 362, time: 0.468, speed: 2.139 steps/s
	current lr: 0.0000036
	lm_loss: 3.5807, ppl: 35.8972, loss: 3.5807
[train][1] progress: 1/1 step: 363, time: 0.475, speed: 2.105 steps/s
	current lr: 0.0000036
	lm_loss: 3.7758, ppl: 43.6338, loss: 3.7758
[train][1] progress: 1/1 step: 364, time: 0.478, speed: 2.092 steps/s
	current lr: 0.0000036
	lm_loss: 3.2259, ppl: 25.1774, loss: 3.2259
[train][1] progress: 1/1 step: 365, time: 0.474, speed: 2.112 steps/s
	current lr: 0.0000037
	lm_loss: 3.4484, ppl: 31.4512, loss: 3.4484
[train][1] progress: 1/1 step: 366, time: 0.471, speed: 2.122 steps/s
	current lr: 0.0000037
	lm_loss: 3.1914, ppl: 24.3215, loss: 3.1914
[train][1] progress: 1/1 step: 367, time: 0.483, speed: 2.071 steps/s
	current lr: 0.0000037
	lm_loss: 3.6534, ppl: 38.6063, loss: 3.6534
[train][1] progress: 1/1 step: 368, time: 0.479, speed: 2.086 steps/s
	current lr: 0.0000037
	lm_loss: 3.5059, ppl: 33.3113, loss: 3.5059
[train][1] progress: 1/1 step: 369, time: 0.473, speed: 2.116 steps/s
	current lr: 0.0000037
	lm_loss: 3.7061, ppl: 40.6964, loss: 3.7061
[train][1] progress: 1/1 step: 370, time: 0.480, speed: 2.082 steps/s
	current lr: 0.0000037
	lm_loss: 3.5514, ppl: 34.8627, loss: 3.5514
[train][1] progress: 1/1 step: 371, time: 0.481, speed: 2.078 steps/s
	current lr: 0.0000037
	lm_loss: 3.7438, ppl: 42.2586, loss: 3.7438
[train][1] progress: 1/1 step: 372, time: 0.469, speed: 2.133 steps/s
	current lr: 0.0000037
	lm_loss: 3.6727, ppl: 39.3569, loss: 3.6727
[train][1] progress: 1/1 step: 373, time: 0.484, speed: 2.068 steps/s
	current lr: 0.0000037
	lm_loss: 3.6421, ppl: 38.1719, loss: 3.6421
[train][1] progress: 1/1 step: 374, time: 0.470, speed: 2.127 steps/s
	current lr: 0.0000037
	lm_loss: 3.4588, ppl: 31.7782, loss: 3.4588
[train][1] progress: 1/1 step: 375, time: 0.468, speed: 2.135 steps/s
	current lr: 0.0000037
	lm_loss: 3.5284, ppl: 34.0702, loss: 3.5284
[train][1] progress: 1/1 step: 376, time: 0.471, speed: 2.121 steps/s
	current lr: 0.0000038
	lm_loss: 3.6250, ppl: 37.5238, loss: 3.6250
[train][1] progress: 1/1 step: 377, time: 0.482, speed: 2.076 steps/s
	current lr: 0.0000038
	lm_loss: 3.6222, ppl: 37.4190, loss: 3.6222
[train][1] progress: 1/1 step: 378, time: 0.469, speed: 2.131 steps/s
	current lr: 0.0000038
	lm_loss: 3.6658, ppl: 39.0888, loss: 3.6658
[train][1] progress: 1/1 step: 379, time: 0.468, speed: 2.138 steps/s
	current lr: 0.0000038
	lm_loss: 3.7431, ppl: 42.2303, loss: 3.7431
[train][1] progress: 1/1 step: 380, time: 0.479, speed: 2.088 steps/s
	current lr: 0.0000038
	lm_loss: 3.6912, ppl: 40.0947, loss: 3.6912
[train][1] progress: 1/1 step: 381, time: 0.478, speed: 2.091 steps/s
	current lr: 0.0000038
	lm_loss: 3.5829, ppl: 35.9793, loss: 3.5829
[train][1] progress: 1/1 step: 382, time: 0.470, speed: 2.128 steps/s
	current lr: 0.0000038
	lm_loss: 3.6196, ppl: 37.3238, loss: 3.6196
[train][1] progress: 1/1 step: 383, time: 0.483, speed: 2.070 steps/s
	current lr: 0.0000038
	lm_loss: 3.5311, ppl: 34.1617, loss: 3.5311
[train][1] progress: 1/1 step: 384, time: 0.473, speed: 2.114 steps/s
	current lr: 0.0000038
	lm_loss: 3.5795, ppl: 35.8573, loss: 3.5795
[train][1] progress: 1/1 step: 385, time: 0.470, speed: 2.127 steps/s
	current lr: 0.0000039
	lm_loss: 3.4453, ppl: 31.3534, loss: 3.4453
[train][1] progress: 1/1 step: 386, time: 0.485, speed: 2.063 steps/s
	current lr: 0.0000039
	lm_loss: 3.0840, ppl: 21.8449, loss: 3.0840
[train][1] progress: 1/1 step: 387, time: 0.478, speed: 2.093 steps/s
	current lr: 0.0000039
	lm_loss: 3.6350, ppl: 37.9027, loss: 3.6350
[train][1] progress: 1/1 step: 388, time: 0.468, speed: 2.135 steps/s
	current lr: 0.0000039
	lm_loss: 3.5051, ppl: 33.2832, loss: 3.5051
[train][1] progress: 1/1 step: 389, time: 0.473, speed: 2.115 steps/s
	current lr: 0.0000039
	lm_loss: 3.4680, ppl: 32.0725, loss: 3.4680
[train][1] progress: 1/1 step: 390, time: 0.475, speed: 2.104 steps/s
	current lr: 0.0000039
	lm_loss: 3.4702, ppl: 32.1429, loss: 3.4702
[train][1] progress: 1/1 step: 391, time: 0.481, speed: 2.078 steps/s
	current lr: 0.0000039
	lm_loss: 3.5185, ppl: 33.7352, loss: 3.5185
[train][1] progress: 1/1 step: 392, time: 0.473, speed: 2.113 steps/s
	current lr: 0.0000039
	lm_loss: 3.4459, ppl: 31.3701, loss: 3.4459
[train][1] progress: 1/1 step: 393, time: 0.474, speed: 2.110 steps/s
	current lr: 0.0000039
	lm_loss: 3.6203, ppl: 37.3506, loss: 3.6203
[train][1] progress: 1/1 step: 394, time: 0.475, speed: 2.106 steps/s
	current lr: 0.0000039
	lm_loss: 3.7786, ppl: 43.7535, loss: 3.7786
[train][1] progress: 1/1 step: 395, time: 0.476, speed: 2.100 steps/s
	current lr: 0.0000040
	lm_loss: 3.5868, ppl: 36.1191, loss: 3.5868
[train][1] progress: 1/1 step: 396, time: 0.477, speed: 2.095 steps/s
	current lr: 0.0000040
	lm_loss: 3.6611, ppl: 38.9034, loss: 3.6611
[train][1] progress: 1/1 step: 397, time: 0.474, speed: 2.109 steps/s
	current lr: 0.0000040
	lm_loss: 3.8121, ppl: 45.2456, loss: 3.8121
[train][1] progress: 1/1 step: 398, time: 0.474, speed: 2.110 steps/s
	current lr: 0.0000040
	lm_loss: 3.6179, ppl: 37.2602, loss: 3.6179
[train][1] progress: 1/1 step: 399, time: 0.477, speed: 2.096 steps/s
	current lr: 0.0000040
	lm_loss: 3.5445, ppl: 34.6233, loss: 3.5445
[train][1] progress: 1/1 step: 400, time: 0.472, speed: 2.120 steps/s
	current lr: 0.0000040
	lm_loss: 3.6644, ppl: 39.0336, loss: 3.6644
[train][1] progress: 1/1 step: 401, time: 0.478, speed: 2.090 steps/s
	current lr: 0.0000040
	lm_loss: 3.3608, ppl: 28.8132, loss: 3.3608
[train][1] progress: 1/1 step: 402, time: 0.480, speed: 2.085 steps/s
	current lr: 0.0000040
	lm_loss: 3.5805, ppl: 35.8912, loss: 3.5805
[train][1] progress: 1/1 step: 403, time: 0.470, speed: 2.130 steps/s
	current lr: 0.0000040
	lm_loss: 3.6327, ppl: 37.8159, loss: 3.6327
[train][1] progress: 1/1 step: 404, time: 0.470, speed: 2.128 steps/s
	current lr: 0.0000040
	lm_loss: 3.4937, ppl: 32.9071, loss: 3.4937
[train][1] progress: 1/1 step: 405, time: 0.473, speed: 2.112 steps/s
	current lr: 0.0000040
	lm_loss: 3.6621, ppl: 38.9421, loss: 3.6621
[train][1] progress: 1/1 step: 406, time: 0.474, speed: 2.110 steps/s
	current lr: 0.0000041
	lm_loss: 3.6681, ppl: 39.1777, loss: 3.6681
[train][1] progress: 1/1 step: 407, time: 0.480, speed: 2.083 steps/s
	current lr: 0.0000041
	lm_loss: 3.5264, ppl: 34.0016, loss: 3.5264
[train][1] progress: 1/1 step: 408, time: 0.483, speed: 2.068 steps/s
	current lr: 0.0000041
	lm_loss: 3.4752, ppl: 32.3055, loss: 3.4752
[train][1] progress: 1/1 step: 409, time: 0.477, speed: 2.096 steps/s
	current lr: 0.0000041
	lm_loss: 3.6011, ppl: 36.6374, loss: 3.6011
[train][1] progress: 1/1 step: 410, time: 0.477, speed: 2.097 steps/s
	current lr: 0.0000041
	lm_loss: 3.5819, ppl: 35.9410, loss: 3.5819
[train][1] progress: 1/1 step: 411, time: 0.480, speed: 2.083 steps/s
	current lr: 0.0000041
	lm_loss: 3.5015, ppl: 33.1641, loss: 3.5015
[train][1] progress: 1/1 step: 412, time: 0.467, speed: 2.139 steps/s
	current lr: 0.0000041
	lm_loss: 3.5190, ppl: 33.7503, loss: 3.5190
[train][1] progress: 1/1 step: 413, time: 0.469, speed: 2.132 steps/s
	current lr: 0.0000041
	lm_loss: 3.3859, ppl: 29.5459, loss: 3.3859
[train][1] progress: 1/1 step: 414, time: 0.471, speed: 2.125 steps/s
	current lr: 0.0000041
	lm_loss: 3.6180, ppl: 37.2618, loss: 3.6180
[train][1] progress: 1/1 step: 415, time: 0.476, speed: 2.101 steps/s
	current lr: 0.0000042
	lm_loss: 3.7193, ppl: 41.2352, loss: 3.7193
[train][1] progress: 1/1 step: 416, time: 0.471, speed: 2.125 steps/s
	current lr: 0.0000042
	lm_loss: 3.6263, ppl: 37.5736, loss: 3.6263
[train][1] progress: 1/1 step: 417, time: 0.485, speed: 2.062 steps/s
	current lr: 0.0000042
	lm_loss: 3.7304, ppl: 41.6958, loss: 3.7304
[train][1] progress: 1/1 step: 418, time: 0.473, speed: 2.114 steps/s
	current lr: 0.0000042
	lm_loss: 3.7165, ppl: 41.1205, loss: 3.7165
[train][1] progress: 1/1 step: 419, time: 0.475, speed: 2.107 steps/s
	current lr: 0.0000042
	lm_loss: 3.3846, ppl: 29.5065, loss: 3.3846
[train][1] progress: 1/1 step: 420, time: 0.474, speed: 2.110 steps/s
	current lr: 0.0000042
	lm_loss: 3.3858, ppl: 29.5409, loss: 3.3858
[train][1] progress: 1/1 step: 421, time: 0.470, speed: 2.129 steps/s
	current lr: 0.0000042
	lm_loss: 3.5982, ppl: 36.5341, loss: 3.5982
[train][1] progress: 1/1 step: 422, time: 0.480, speed: 2.083 steps/s
	current lr: 0.0000042
	lm_loss: 3.7680, ppl: 43.2930, loss: 3.7680
[train][1] progress: 1/1 step: 423, time: 0.471, speed: 2.123 steps/s
	current lr: 0.0000042
	lm_loss: 3.4009, ppl: 29.9902, loss: 3.4009
[train][1] progress: 1/1 step: 424, time: 0.471, speed: 2.125 steps/s
	current lr: 0.0000042
	lm_loss: 3.5042, ppl: 33.2562, loss: 3.5042
[train][1] progress: 1/1 step: 425, time: 0.482, speed: 2.074 steps/s
	current lr: 0.0000043
	lm_loss: 3.7486, ppl: 42.4598, loss: 3.7486
[train][1] progress: 1/1 step: 426, time: 0.482, speed: 2.074 steps/s
	current lr: 0.0000043
	lm_loss: 3.5394, ppl: 34.4478, loss: 3.5394
[train][1] progress: 1/1 step: 427, time: 0.481, speed: 2.079 steps/s
	current lr: 0.0000043
	lm_loss: 3.5413, ppl: 34.5121, loss: 3.5413
[train][1] progress: 1/1 step: 428, time: 0.481, speed: 2.077 steps/s
	current lr: 0.0000043
	lm_loss: 3.5304, ppl: 34.1367, loss: 3.5304
[train][1] progress: 1/1 step: 429, time: 0.470, speed: 2.128 steps/s
	current lr: 0.0000043
	lm_loss: 3.5981, ppl: 36.5272, loss: 3.5981
[train][1] progress: 1/1 step: 430, time: 0.481, speed: 2.078 steps/s
	current lr: 0.0000043
	lm_loss: 3.6357, ppl: 37.9286, loss: 3.6357
[train][1] progress: 1/1 step: 431, time: 0.468, speed: 2.138 steps/s
	current lr: 0.0000043
	lm_loss: 3.7138, ppl: 41.0103, loss: 3.7138
[train][1] progress: 1/1 step: 432, time: 0.476, speed: 2.101 steps/s
	current lr: 0.0000043
	lm_loss: 3.5226, ppl: 33.8727, loss: 3.5226
[train][1] progress: 1/1 step: 433, time: 0.479, speed: 2.086 steps/s
	current lr: 0.0000043
	lm_loss: 3.6114, ppl: 37.0193, loss: 3.6114
[train][1] progress: 1/1 step: 434, time: 0.474, speed: 2.112 steps/s
	current lr: 0.0000043
	lm_loss: 3.4254, ppl: 30.7359, loss: 3.4254
[train][1] progress: 1/1 step: 435, time: 0.480, speed: 2.081 steps/s
	current lr: 0.0000043
	lm_loss: 3.6193, ppl: 37.3120, loss: 3.6193
[train][1] progress: 1/1 step: 436, time: 0.468, speed: 2.138 steps/s
	current lr: 0.0000044
	lm_loss: 3.4522, ppl: 31.5689, loss: 3.4522
[train][1] progress: 1/1 step: 437, time: 0.478, speed: 2.092 steps/s
	current lr: 0.0000044
	lm_loss: 3.6636, ppl: 39.0021, loss: 3.6636
[train][1] progress: 1/1 step: 438, time: 0.471, speed: 2.124 steps/s
	current lr: 0.0000044
	lm_loss: 3.7282, ppl: 41.6035, loss: 3.7282
[train][1] progress: 1/1 step: 439, time: 0.471, speed: 2.123 steps/s
	current lr: 0.0000044
	lm_loss: 3.4130, ppl: 30.3575, loss: 3.4130
[train][1] progress: 1/1 step: 440, time: 0.473, speed: 2.116 steps/s
	current lr: 0.0000044
	lm_loss: 3.4955, ppl: 32.9670, loss: 3.4955
[train][1] progress: 1/1 step: 441, time: 0.476, speed: 2.101 steps/s
	current lr: 0.0000044
	lm_loss: 3.4928, ppl: 32.8765, loss: 3.4928
[train][1] progress: 1/1 step: 442, time: 0.472, speed: 2.121 steps/s
	current lr: 0.0000044
	lm_loss: 3.7188, ppl: 41.2132, loss: 3.7188
[train][1] progress: 1/1 step: 443, time: 0.472, speed: 2.118 steps/s
	current lr: 0.0000044
	lm_loss: 3.6212, ppl: 37.3827, loss: 3.6212
[train][1] progress: 1/1 step: 444, time: 0.472, speed: 2.117 steps/s
	current lr: 0.0000044
	lm_loss: 3.6143, ppl: 37.1240, loss: 3.6143
[train][1] progress: 1/1 step: 445, time: 0.474, speed: 2.112 steps/s
	current lr: 0.0000045
	lm_loss: 3.7082, ppl: 40.7811, loss: 3.7082
[train][1] progress: 1/1 step: 446, time: 0.472, speed: 2.120 steps/s
	current lr: 0.0000045
	lm_loss: 3.5850, ppl: 36.0529, loss: 3.5850
[train][1] progress: 1/1 step: 447, time: 0.480, speed: 2.085 steps/s
	current lr: 0.0000045
	lm_loss: 3.6273, ppl: 37.6108, loss: 3.6273
[train][1] progress: 1/1 step: 448, time: 0.477, speed: 2.095 steps/s
	current lr: 0.0000045
	lm_loss: 3.5737, ppl: 35.6498, loss: 3.5737
[train][1] progress: 1/1 step: 449, time: 0.477, speed: 2.095 steps/s
	current lr: 0.0000045
	lm_loss: 3.6797, ppl: 39.6356, loss: 3.6797
[train][1] progress: 1/1 step: 450, time: 0.481, speed: 2.081 steps/s
	current lr: 0.0000045
	lm_loss: 3.4647, ppl: 31.9657, loss: 3.4647
[train][1] progress: 1/1 step: 451, time: 0.473, speed: 2.115 steps/s
	current lr: 0.0000045
	lm_loss: 3.4891, ppl: 32.7577, loss: 3.4891
[train][1] progress: 1/1 step: 452, time: 0.473, speed: 2.115 steps/s
	current lr: 0.0000045
	lm_loss: 3.5148, ppl: 33.6086, loss: 3.5148
[train][1] progress: 1/1 step: 453, time: 0.478, speed: 2.092 steps/s
	current lr: 0.0000045
	lm_loss: 3.4633, ppl: 31.9236, loss: 3.4633
[train][1] progress: 1/1 step: 454, time: 0.474, speed: 2.112 steps/s
	current lr: 0.0000045
	lm_loss: 3.1368, ppl: 23.0291, loss: 3.1368
[train][1] progress: 1/1 step: 455, time: 0.474, speed: 2.109 steps/s
	current lr: 0.0000046
	lm_loss: 3.4983, ppl: 33.0592, loss: 3.4983
[train][1] progress: 1/1 step: 456, time: 0.480, speed: 2.085 steps/s
	current lr: 0.0000046
	lm_loss: 3.6253, ppl: 37.5343, loss: 3.6253
[train][1] progress: 1/1 step: 457, time: 0.495, speed: 2.022 steps/s
	current lr: 0.0000046
	lm_loss: 3.6677, ppl: 39.1613, loss: 3.6677
[train][1] progress: 1/1 step: 458, time: 0.491, speed: 2.039 steps/s
	current lr: 0.0000046
	lm_loss: 3.4896, ppl: 32.7719, loss: 3.4896
[train][1] progress: 1/1 step: 459, time: 0.485, speed: 2.060 steps/s
	current lr: 0.0000046
	lm_loss: 3.5758, ppl: 35.7248, loss: 3.5758
[train][1] progress: 1/1 step: 460, time: 0.480, speed: 2.085 steps/s
	current lr: 0.0000046
	lm_loss: 3.4380, ppl: 31.1250, loss: 3.4380
[train][1] progress: 1/1 step: 461, time: 0.477, speed: 2.096 steps/s
	current lr: 0.0000046
	lm_loss: 3.5682, ppl: 35.4544, loss: 3.5682
[train][1] progress: 1/1 step: 462, time: 0.473, speed: 2.113 steps/s
	current lr: 0.0000046
	lm_loss: 3.5035, ppl: 33.2316, loss: 3.5035
[train][1] progress: 1/1 step: 463, time: 0.479, speed: 2.088 steps/s
	current lr: 0.0000046
	lm_loss: 3.6481, ppl: 38.4022, loss: 3.6481
[train][1] progress: 1/1 step: 464, time: 0.478, speed: 2.094 steps/s
	current lr: 0.0000046
	lm_loss: 3.5028, ppl: 33.2096, loss: 3.5028
[train][1] progress: 1/1 step: 465, time: 0.471, speed: 2.121 steps/s
	current lr: 0.0000046
	lm_loss: 3.5008, ppl: 33.1426, loss: 3.5008
[train][1] progress: 1/1 step: 466, time: 0.469, speed: 2.131 steps/s
	current lr: 0.0000047
	lm_loss: 3.5689, ppl: 35.4765, loss: 3.5689
[train][1] progress: 1/1 step: 467, time: 0.486, speed: 2.058 steps/s
	current lr: 0.0000047
	lm_loss: 3.2734, ppl: 26.4017, loss: 3.2734
[train][1] progress: 1/1 step: 468, time: 0.462, speed: 2.164 steps/s
	current lr: 0.0000047
	lm_loss: 2.4273, ppl: 11.3283, loss: 2.4273
[train][1] progress: 1/1 step: 469, time: 0.478, speed: 2.094 steps/s
	current lr: 0.0000047
	lm_loss: 3.5037, ppl: 33.2394, loss: 3.5037
[train][1] progress: 1/1 step: 470, time: 0.484, speed: 2.065 steps/s
	current lr: 0.0000047
	lm_loss: 3.3830, ppl: 29.4604, loss: 3.3830
[train][1] progress: 1/1 step: 471, time: 0.471, speed: 2.125 steps/s
	current lr: 0.0000047
	lm_loss: 3.6738, ppl: 39.4002, loss: 3.6738
[train][1] progress: 1/1 step: 472, time: 0.472, speed: 2.117 steps/s
	current lr: 0.0000047
	lm_loss: 3.7629, ppl: 43.0748, loss: 3.7629
[train][1] progress: 1/1 step: 473, time: 0.477, speed: 2.097 steps/s
	current lr: 0.0000047
	lm_loss: 3.4637, ppl: 31.9355, loss: 3.4637
[train][1] progress: 1/1 step: 474, time: 0.489, speed: 2.045 steps/s
	current lr: 0.0000047
	lm_loss: 3.1957, ppl: 24.4276, loss: 3.1957
[train][1] progress: 1/1 step: 475, time: 0.484, speed: 2.065 steps/s
	current lr: 0.0000048
	lm_loss: 3.5443, ppl: 34.6155, loss: 3.5443
[train][1] progress: 1/1 step: 476, time: 0.483, speed: 2.072 steps/s
	current lr: 0.0000048
	lm_loss: 3.6000, ppl: 36.5977, loss: 3.6000
[train][1] progress: 1/1 step: 477, time: 0.475, speed: 2.106 steps/s
	current lr: 0.0000048
	lm_loss: 3.6403, ppl: 38.1051, loss: 3.6403
[train][1] progress: 1/1 step: 478, time: 0.470, speed: 2.126 steps/s
	current lr: 0.0000048
	lm_loss: 3.6574, ppl: 38.7602, loss: 3.6574
[train][1] progress: 1/1 step: 479, time: 0.468, speed: 2.136 steps/s
	current lr: 0.0000048
	lm_loss: 3.6239, ppl: 37.4825, loss: 3.6239
[train][1] progress: 1/1 step: 480, time: 0.469, speed: 2.130 steps/s
	current lr: 0.0000048
	lm_loss: 3.6904, ppl: 40.0614, loss: 3.6904
[train][1] progress: 1/1 step: 481, time: 0.479, speed: 2.088 steps/s
	current lr: 0.0000048
	lm_loss: 3.5344, ppl: 34.2739, loss: 3.5344
[train][1] progress: 1/1 step: 482, time: 0.478, speed: 2.091 steps/s
	current lr: 0.0000048
	lm_loss: 3.6092, ppl: 36.9362, loss: 3.6092
[train][1] progress: 1/1 step: 483, time: 0.478, speed: 2.091 steps/s
	current lr: 0.0000048
	lm_loss: 3.6126, ppl: 37.0624, loss: 3.6126
[train][1] progress: 1/1 step: 484, time: 0.470, speed: 2.127 steps/s
	current lr: 0.0000048
	lm_loss: 3.6205, ppl: 37.3554, loss: 3.6205
[train][1] progress: 1/1 step: 485, time: 0.472, speed: 2.121 steps/s
	current lr: 0.0000049
	lm_loss: 3.5709, ppl: 35.5487, loss: 3.5709
[train][1] progress: 1/1 step: 486, time: 0.470, speed: 2.128 steps/s
	current lr: 0.0000049
	lm_loss: 3.6473, ppl: 38.3714, loss: 3.6473
[train][1] progress: 1/1 step: 487, time: 0.475, speed: 2.105 steps/s
	current lr: 0.0000049
	lm_loss: 3.5590, ppl: 35.1293, loss: 3.5590
[train][1] progress: 1/1 step: 488, time: 0.468, speed: 2.136 steps/s
	current lr: 0.0000049
	lm_loss: 3.4165, ppl: 30.4630, loss: 3.4165
[train][1] progress: 1/1 step: 489, time: 0.469, speed: 2.133 steps/s
	current lr: 0.0000049
	lm_loss: 3.6205, ppl: 37.3547, loss: 3.6205
[train][1] progress: 1/1 step: 490, time: 0.471, speed: 2.125 steps/s
	current lr: 0.0000049
	lm_loss: 3.5204, ppl: 33.7973, loss: 3.5204
[train][1] progress: 1/1 step: 491, time: 0.557, speed: 1.796 steps/s
	current lr: 0.0000049
	lm_loss: 3.5951, ppl: 36.4183, loss: 3.5951
[train][1] progress: 1/1 step: 492, time: 0.903, speed: 1.108 steps/s
	current lr: 0.0000049
	lm_loss: 3.6327, ppl: 37.8155, loss: 3.6327
[train][1] progress: 1/1 step: 493, time: 0.473, speed: 2.116 steps/s
	current lr: 0.0000049
	lm_loss: 3.6377, ppl: 38.0056, loss: 3.6377
[train][1] progress: 1/1 step: 494, time: 0.472, speed: 2.120 steps/s
	current lr: 0.0000049
	lm_loss: 3.4842, ppl: 32.5954, loss: 3.4842
[train][1] progress: 1/1 step: 495, time: 0.471, speed: 2.123 steps/s
	current lr: 0.0000049
	lm_loss: 3.4303, ppl: 30.8849, loss: 3.4303
[train][1] progress: 1/1 step: 496, time: 0.478, speed: 2.090 steps/s
	current lr: 0.0000050
	lm_loss: 3.6116, ppl: 37.0267, loss: 3.6116
[train][1] progress: 1/1 step: 497, time: 0.470, speed: 2.125 steps/s
	current lr: 0.0000050
	lm_loss: 3.6099, ppl: 36.9612, loss: 3.6099
[train][1] progress: 1/1 step: 498, time: 0.472, speed: 2.120 steps/s
	current lr: 0.0000050
	lm_loss: 3.5887, ppl: 36.1874, loss: 3.5887
[train][1] progress: 1/1 step: 499, time: 0.470, speed: 2.128 steps/s
	current lr: 0.0000050
	lm_loss: 3.5894, ppl: 36.2114, loss: 3.5894
[train][1] progress: 1/1 step: 500, time: 0.473, speed: 2.114 steps/s
	current lr: 0.0000050
	lm_loss: 3.5654, ppl: 35.3524, loss: 3.5654
[train][1] progress: 1/1 step: 501, time: 0.473, speed: 2.116 steps/s
	current lr: 0.0000050
	lm_loss: 3.6311, ppl: 37.7551, loss: 3.6311
[train][1] progress: 1/1 step: 502, time: 0.470, speed: 2.128 steps/s
	current lr: 0.0000050
	lm_loss: 3.5837, ppl: 36.0053, loss: 3.5837
[train][1] progress: 1/1 step: 503, time: 0.480, speed: 2.084 steps/s
	current lr: 0.0000050
	lm_loss: 3.6167, ppl: 37.2134, loss: 3.6167
[train][1] progress: 1/1 step: 504, time: 0.478, speed: 2.094 steps/s
	current lr: 0.0000050
	lm_loss: 3.4431, ppl: 31.2826, loss: 3.4431
[train][1] progress: 1/1 step: 505, time: 0.481, speed: 2.078 steps/s
	current lr: 0.0000051
	lm_loss: 3.7479, ppl: 42.4311, loss: 3.7479
[train][1] progress: 1/1 step: 506, time: 0.475, speed: 2.104 steps/s
	current lr: 0.0000051
	lm_loss: 3.2629, ppl: 26.1260, loss: 3.2629
[train][1] progress: 1/1 step: 507, time: 0.471, speed: 2.122 steps/s
	current lr: 0.0000051
	lm_loss: 3.5337, ppl: 34.2518, loss: 3.5337
[train][1] progress: 1/1 step: 508, time: 0.475, speed: 2.104 steps/s
	current lr: 0.0000051
	lm_loss: 3.4797, ppl: 32.4502, loss: 3.4797
[train][1] progress: 1/1 step: 509, time: 0.468, speed: 2.136 steps/s
	current lr: 0.0000051
	lm_loss: 3.6202, ppl: 37.3449, loss: 3.6202
[train][1] progress: 1/1 step: 510, time: 0.475, speed: 2.103 steps/s
	current lr: 0.0000051
	lm_loss: 3.7958, ppl: 44.5148, loss: 3.7958
[train][1] progress: 1/1 step: 511, time: 0.483, speed: 2.070 steps/s
	current lr: 0.0000051
	lm_loss: 3.7343, ppl: 41.8603, loss: 3.7343
[train][1] progress: 1/1 step: 512, time: 0.473, speed: 2.112 steps/s
	current lr: 0.0000051
	lm_loss: 3.6860, ppl: 39.8870, loss: 3.6860
[train][1] progress: 1/1 step: 513, time: 0.473, speed: 2.116 steps/s
	current lr: 0.0000051
	lm_loss: 3.6307, ppl: 37.7398, loss: 3.6307
[train][1] progress: 1/1 step: 514, time: 0.474, speed: 2.110 steps/s
	current lr: 0.0000051
	lm_loss: 3.7221, ppl: 41.3493, loss: 3.7221
[train][1] progress: 1/1 step: 515, time: 0.471, speed: 2.125 steps/s
	current lr: 0.0000052
	lm_loss: 3.5577, ppl: 35.0815, loss: 3.5577
[train][1] progress: 1/1 step: 516, time: 0.480, speed: 2.084 steps/s
	current lr: 0.0000052
	lm_loss: 3.5638, ppl: 35.2956, loss: 3.5638
[train][1] progress: 1/1 step: 517, time: 0.474, speed: 2.111 steps/s
	current lr: 0.0000052
	lm_loss: 3.4478, ppl: 31.4322, loss: 3.4478
[train][1] progress: 1/1 step: 518, time: 0.494, speed: 2.024 steps/s
	current lr: 0.0000052
	lm_loss: 3.5222, ppl: 33.8602, loss: 3.5222
[train][1] progress: 1/1 step: 519, time: 0.479, speed: 2.090 steps/s
	current lr: 0.0000052
	lm_loss: 3.5907, ppl: 36.2588, loss: 3.5907
[train][1] progress: 1/1 step: 520, time: 0.467, speed: 2.139 steps/s
	current lr: 0.0000052
	lm_loss: 3.5805, ppl: 35.8920, loss: 3.5805
[train][1] progress: 1/1 step: 521, time: 0.483, speed: 2.072 steps/s
	current lr: 0.0000052
	lm_loss: 3.5194, ppl: 33.7642, loss: 3.5194
[train][1] progress: 1/1 step: 522, time: 0.472, speed: 2.120 steps/s
	current lr: 0.0000052
	lm_loss: 3.4204, ppl: 30.5809, loss: 3.4204
[train][1] progress: 1/1 step: 523, time: 0.485, speed: 2.063 steps/s
	current lr: 0.0000052
	lm_loss: 3.5866, ppl: 36.1095, loss: 3.5866
[train][1] progress: 1/1 step: 524, time: 0.473, speed: 2.116 steps/s
	current lr: 0.0000052
	lm_loss: 3.5479, ppl: 34.7392, loss: 3.5479
[train][1] progress: 1/1 step: 525, time: 0.485, speed: 2.062 steps/s
	current lr: 0.0000053
	lm_loss: 3.0191, ppl: 20.4729, loss: 3.0191
[train][1] progress: 1/1 step: 526, time: 0.472, speed: 2.121 steps/s
	current lr: 0.0000053
	lm_loss: 3.6469, ppl: 38.3562, loss: 3.6469
[train][1] progress: 1/1 step: 527, time: 0.473, speed: 2.115 steps/s
	current lr: 0.0000053
	lm_loss: 3.6133, ppl: 37.0880, loss: 3.6133
[train][1] progress: 1/1 step: 528, time: 0.474, speed: 2.111 steps/s
	current lr: 0.0000053
	lm_loss: 3.3951, ppl: 29.8182, loss: 3.3951
[train][1] progress: 1/1 step: 529, time: 0.479, speed: 2.089 steps/s
	current lr: 0.0000053
	lm_loss: 3.5429, ppl: 34.5674, loss: 3.5429
[train][1] progress: 1/1 step: 530, time: 0.470, speed: 2.128 steps/s
	current lr: 0.0000053
	lm_loss: 3.4347, ppl: 31.0227, loss: 3.4347
[train][1] progress: 1/1 step: 531, time: 0.480, speed: 2.082 steps/s
	current lr: 0.0000053
	lm_loss: 3.3802, ppl: 29.3778, loss: 3.3802
[train][1] progress: 1/1 step: 532, time: 0.482, speed: 2.073 steps/s
	current lr: 0.0000053
	lm_loss: 3.8200, ppl: 45.6044, loss: 3.8200
[train][1] progress: 1/1 step: 533, time: 0.470, speed: 2.126 steps/s
	current lr: 0.0000053
	lm_loss: 3.5846, ppl: 36.0391, loss: 3.5846
[train][1] progress: 1/1 step: 534, time: 0.478, speed: 2.093 steps/s
	current lr: 0.0000053
	lm_loss: 3.3980, ppl: 29.9037, loss: 3.3980
[train][1] progress: 1/1 step: 535, time: 0.484, speed: 2.064 steps/s
	current lr: 0.0000054
	lm_loss: 3.4980, ppl: 33.0505, loss: 3.4980
[train][1] progress: 1/1 step: 536, time: 0.479, speed: 2.087 steps/s
	current lr: 0.0000054
	lm_loss: 3.6002, ppl: 36.6071, loss: 3.6002
[train][1] progress: 1/1 step: 537, time: 0.474, speed: 2.112 steps/s
	current lr: 0.0000054
	lm_loss: 3.6098, ppl: 36.9603, loss: 3.6098
[train][1] progress: 1/1 step: 538, time: 0.472, speed: 2.121 steps/s
	current lr: 0.0000054
	lm_loss: 3.5315, ppl: 34.1753, loss: 3.5315
[train][1] progress: 1/1 step: 539, time: 0.474, speed: 2.110 steps/s
	current lr: 0.0000054
	lm_loss: 3.5330, ppl: 34.2274, loss: 3.5330
[train][1] progress: 1/1 step: 540, time: 0.493, speed: 2.029 steps/s
	current lr: 0.0000054
	lm_loss: 3.3228, ppl: 27.7387, loss: 3.3228
[train][1] progress: 1/1 step: 541, time: 0.479, speed: 2.086 steps/s
	current lr: 0.0000054
	lm_loss: 3.4711, ppl: 32.1722, loss: 3.4711
[train][1] progress: 1/1 step: 542, time: 0.469, speed: 2.131 steps/s
	current lr: 0.0000054
	lm_loss: 3.3303, ppl: 27.9480, loss: 3.3303
[train][1] progress: 1/1 step: 543, time: 0.477, speed: 2.095 steps/s
	current lr: 0.0000054
	lm_loss: 3.5523, ppl: 34.8925, loss: 3.5523
[train][1] progress: 1/1 step: 544, time: 0.483, speed: 2.071 steps/s
	current lr: 0.0000054
	lm_loss: 3.4953, ppl: 32.9604, loss: 3.4953
[train][1] progress: 1/1 step: 545, time: 0.474, speed: 2.111 steps/s
	current lr: 0.0000055
	lm_loss: 3.4685, ppl: 32.0883, loss: 3.4685
[train][1] progress: 1/1 step: 546, time: 0.470, speed: 2.128 steps/s
	current lr: 0.0000055
	lm_loss: 3.6673, ppl: 39.1451, loss: 3.6673
[train][1] progress: 1/1 step: 547, time: 0.477, speed: 2.095 steps/s
	current lr: 0.0000055
	lm_loss: 3.3330, ppl: 28.0210, loss: 3.3330
[train][1] progress: 1/1 step: 548, time: 0.485, speed: 2.061 steps/s
	current lr: 0.0000055
	lm_loss: 3.2620, ppl: 26.1020, loss: 3.2620
[train][1] progress: 1/1 step: 549, time: 0.478, speed: 2.092 steps/s
	current lr: 0.0000055
	lm_loss: 3.5607, ppl: 35.1888, loss: 3.5607
[train][1] progress: 1/1 step: 550, time: 0.473, speed: 2.115 steps/s
	current lr: 0.0000055
	lm_loss: 3.2972, ppl: 27.0356, loss: 3.2972
[train][1] progress: 1/1 step: 551, time: 0.471, speed: 2.122 steps/s
	current lr: 0.0000055
	lm_loss: 3.4538, ppl: 31.6215, loss: 3.4538
[train][1] progress: 1/1 step: 552, time: 0.469, speed: 2.133 steps/s
	current lr: 0.0000055
	lm_loss: 3.6045, ppl: 36.7637, loss: 3.6045
[train][1] progress: 1/1 step: 553, time: 0.480, speed: 2.084 steps/s
	current lr: 0.0000055
	lm_loss: 3.6055, ppl: 36.8013, loss: 3.6055
[train][1] progress: 1/1 step: 554, time: 0.483, speed: 2.069 steps/s
	current lr: 0.0000055
	lm_loss: 3.5330, ppl: 34.2268, loss: 3.5330
[train][1] progress: 1/1 step: 555, time: 0.472, speed: 2.118 steps/s
	current lr: 0.0000055
	lm_loss: 3.4533, ppl: 31.6037, loss: 3.4533
[train][1] progress: 1/1 step: 556, time: 0.481, speed: 2.078 steps/s
	current lr: 0.0000056
	lm_loss: 3.5639, ppl: 35.3000, loss: 3.5639
[train][1] progress: 1/1 step: 557, time: 0.488, speed: 2.050 steps/s
	current lr: 0.0000056
	lm_loss: 3.1604, ppl: 23.5808, loss: 3.1604
[train][1] progress: 1/1 step: 558, time: 0.481, speed: 2.081 steps/s
	current lr: 0.0000056
	lm_loss: 3.5862, ppl: 36.0952, loss: 3.5862
[train][1] progress: 1/1 step: 559, time: 0.489, speed: 2.047 steps/s
	current lr: 0.0000056
	lm_loss: 3.0746, ppl: 21.6418, loss: 3.0746
[train][1] progress: 1/1 step: 560, time: 0.477, speed: 2.095 steps/s
	current lr: 0.0000056
	lm_loss: 3.5317, ppl: 34.1813, loss: 3.5317
[train][1] progress: 1/1 step: 561, time: 0.477, speed: 2.097 steps/s
	current lr: 0.0000056
	lm_loss: 3.6479, ppl: 38.3930, loss: 3.6479
[train][1] progress: 1/1 step: 562, time: 0.473, speed: 2.112 steps/s
	current lr: 0.0000056
	lm_loss: 3.6720, ppl: 39.3322, loss: 3.6720
[train][1] progress: 1/1 step: 563, time: 0.473, speed: 2.114 steps/s
	current lr: 0.0000056
	lm_loss: 3.5729, ppl: 35.6194, loss: 3.5729
[train][1] progress: 1/1 step: 564, time: 0.478, speed: 2.093 steps/s
	current lr: 0.0000056
	lm_loss: 3.5628, ppl: 35.2610, loss: 3.5628
[train][1] progress: 1/1 step: 565, time: 0.469, speed: 2.130 steps/s
	current lr: 0.0000057
	lm_loss: 3.4760, ppl: 32.3294, loss: 3.4760
[train][1] progress: 1/1 step: 566, time: 0.472, speed: 2.118 steps/s
	current lr: 0.0000057
	lm_loss: 3.5407, ppl: 34.4900, loss: 3.5407
[train][1] progress: 1/1 step: 567, time: 0.469, speed: 2.132 steps/s
	current lr: 0.0000057
	lm_loss: 3.7006, ppl: 40.4698, loss: 3.7006
[train][1] progress: 1/1 step: 568, time: 0.484, speed: 2.066 steps/s
	current lr: 0.0000057
	lm_loss: 3.5237, ppl: 33.9101, loss: 3.5237
[train][1] progress: 1/1 step: 569, time: 0.473, speed: 2.115 steps/s
	current lr: 0.0000057
	lm_loss: 3.5546, ppl: 34.9738, loss: 3.5546
[train][1] progress: 1/1 step: 570, time: 0.469, speed: 2.134 steps/s
	current lr: 0.0000057
	lm_loss: 3.5714, ppl: 35.5670, loss: 3.5714
[train][1] progress: 1/1 step: 571, time: 0.471, speed: 2.123 steps/s
	current lr: 0.0000057
	lm_loss: 3.5304, ppl: 34.1385, loss: 3.5304
[train][1] progress: 1/1 step: 572, time: 0.483, speed: 2.072 steps/s
	current lr: 0.0000057
	lm_loss: 3.4826, ppl: 32.5448, loss: 3.4826
[train][1] progress: 1/1 step: 573, time: 0.474, speed: 2.109 steps/s
	current lr: 0.0000057
	lm_loss: 3.6521, ppl: 38.5563, loss: 3.6521
[train][1] progress: 1/1 step: 574, time: 0.481, speed: 2.079 steps/s
	current lr: 0.0000057
	lm_loss: 3.4626, ppl: 31.9011, loss: 3.4626
[train][1] progress: 1/1 step: 575, time: 0.481, speed: 2.077 steps/s
	current lr: 0.0000058
	lm_loss: 3.5810, ppl: 35.9086, loss: 3.5810
[train][1] progress: 1/1 step: 576, time: 0.480, speed: 2.083 steps/s
	current lr: 0.0000058
	lm_loss: 3.6246, ppl: 37.5104, loss: 3.6246
[train][1] progress: 1/1 step: 577, time: 0.467, speed: 2.140 steps/s
	current lr: 0.0000058
	lm_loss: 3.5490, ppl: 34.7769, loss: 3.5490
[train][1] progress: 1/1 step: 578, time: 0.477, speed: 2.097 steps/s
	current lr: 0.0000058
	lm_loss: 3.5964, ppl: 36.4668, loss: 3.5964
[train][1] progress: 1/1 step: 579, time: 0.481, speed: 2.079 steps/s
	current lr: 0.0000058
	lm_loss: 3.6009, ppl: 36.6302, loss: 3.6009
[train][1] progress: 1/1 step: 580, time: 0.484, speed: 2.068 steps/s
	current lr: 0.0000058
	lm_loss: 3.6830, ppl: 39.7658, loss: 3.6830
[train][1] progress: 1/1 step: 581, time: 0.472, speed: 2.121 steps/s
	current lr: 0.0000058
	lm_loss: 3.5669, ppl: 35.4062, loss: 3.5669
[train][1] progress: 1/1 step: 582, time: 0.472, speed: 2.118 steps/s
	current lr: 0.0000058
	lm_loss: 3.5782, ppl: 35.8079, loss: 3.5782
[train][1] progress: 1/1 step: 583, time: 0.470, speed: 2.128 steps/s
	current lr: 0.0000058
	lm_loss: 3.3557, ppl: 28.6650, loss: 3.3557
[train][1] progress: 1/1 step: 584, time: 0.483, speed: 2.072 steps/s
	current lr: 0.0000058
	lm_loss: 3.8277, ppl: 45.9572, loss: 3.8277
[train][1] progress: 1/1 step: 585, time: 0.475, speed: 2.106 steps/s
	current lr: 0.0000059
	lm_loss: 3.7089, ppl: 40.8097, loss: 3.7089
[train][1] progress: 1/1 step: 586, time: 0.470, speed: 2.126 steps/s
	current lr: 0.0000059
	lm_loss: 3.5858, ppl: 36.0831, loss: 3.5858
[train][1] progress: 1/1 step: 587, time: 0.472, speed: 2.117 steps/s
	current lr: 0.0000059
	lm_loss: 3.5851, ppl: 36.0558, loss: 3.5851
[train][1] progress: 1/1 step: 588, time: 0.480, speed: 2.084 steps/s
	current lr: 0.0000059
	lm_loss: 3.5323, ppl: 34.2011, loss: 3.5323
[train][1] progress: 1/1 step: 589, time: 0.473, speed: 2.112 steps/s
	current lr: 0.0000059
	lm_loss: 3.5665, ppl: 35.3930, loss: 3.5665
[train][1] progress: 1/1 step: 590, time: 0.478, speed: 2.094 steps/s
	current lr: 0.0000059
	lm_loss: 3.0973, ppl: 22.1375, loss: 3.0973
[train][1] progress: 1/1 step: 591, time: 0.186, speed: 5.372 steps/s
	current lr: 0.0000059
	lm_loss: 1.5479, ppl: 4.7015, loss: 1.5479
[train][1] progress: 1/1 step: 592, time: 0.477, speed: 2.096 steps/s
	current lr: 0.0000059
	lm_loss: 3.6809, ppl: 39.6818, loss: 3.6809
[train][1] progress: 1/1 step: 593, time: 0.488, speed: 2.048 steps/s
	current lr: 0.0000059
	lm_loss: 3.6137, ppl: 37.1015, loss: 3.6137
[train][1] progress: 1/1 step: 594, time: 0.471, speed: 2.123 steps/s
	current lr: 0.0000059
	lm_loss: 3.7514, ppl: 42.5796, loss: 3.7514
[train][1] progress: 1/1 step: 595, time: 0.483, speed: 2.070 steps/s
	current lr: 0.0000060
	lm_loss: 3.5893, ppl: 36.2095, loss: 3.5893
[train][1] progress: 1/1 step: 596, time: 0.470, speed: 2.128 steps/s
	current lr: 0.0000060
	lm_loss: 3.5371, ppl: 34.3667, loss: 3.5371
[train][1] progress: 1/1 step: 597, time: 0.479, speed: 2.090 steps/s
	current lr: 0.0000060
	lm_loss: 3.5620, ppl: 35.2351, loss: 3.5620
[train][1] progress: 1/1 step: 598, time: 0.514, speed: 1.946 steps/s
	current lr: 0.0000060
	lm_loss: 3.6080, ppl: 36.8920, loss: 3.6080
[train][1] progress: 1/1 step: 599, time: 0.474, speed: 2.111 steps/s
	current lr: 0.0000060
	lm_loss: 3.7522, ppl: 42.6154, loss: 3.7522
[train][1] progress: 1/1 step: 600, time: 0.471, speed: 2.123 steps/s
	current lr: 0.0000060
	lm_loss: 3.8027, ppl: 44.8211, loss: 3.8027
[train][1] progress: 1/1 step: 601, time: 0.480, speed: 2.081 steps/s
	current lr: 0.0000060
	lm_loss: 3.6109, ppl: 36.9997, loss: 3.6109
[train][1] progress: 1/1 step: 602, time: 0.478, speed: 2.092 steps/s
	current lr: 0.0000060
	lm_loss: 3.5931, ppl: 36.3449, loss: 3.5931
[train][1] progress: 1/1 step: 603, time: 0.479, speed: 2.087 steps/s
	current lr: 0.0000060
	lm_loss: 3.7163, ppl: 41.1124, loss: 3.7163
[train][1] progress: 1/1 step: 604, time: 0.473, speed: 2.114 steps/s
	current lr: 0.0000060
	lm_loss: 3.6254, ppl: 37.5382, loss: 3.6254
[train][1] progress: 1/1 step: 605, time: 0.478, speed: 2.094 steps/s
	current lr: 0.0000061
	lm_loss: 3.4736, ppl: 32.2527, loss: 3.4736
[train][1] progress: 1/1 step: 606, time: 0.469, speed: 2.133 steps/s
	current lr: 0.0000061
	lm_loss: 3.6182, ppl: 37.2709, loss: 3.6182
[train][1] progress: 1/1 step: 607, time: 0.481, speed: 2.080 steps/s
	current lr: 0.0000061
	lm_loss: 3.6848, ppl: 39.8372, loss: 3.6848
[train][1] progress: 1/1 step: 608, time: 0.473, speed: 2.112 steps/s
	current lr: 0.0000061
	lm_loss: 3.7044, ppl: 40.6267, loss: 3.7044
[train][1] progress: 1/1 step: 609, time: 0.481, speed: 2.078 steps/s
	current lr: 0.0000061
	lm_loss: 3.5623, ppl: 35.2428, loss: 3.5623
[train][1] progress: 1/1 step: 610, time: 0.469, speed: 2.132 steps/s
	current lr: 0.0000061
	lm_loss: 3.5555, ppl: 35.0060, loss: 3.5555
[train][1] progress: 1/1 step: 611, time: 0.473, speed: 2.114 steps/s
	current lr: 0.0000061
	lm_loss: 3.4539, ppl: 31.6220, loss: 3.4539
[train][1] progress: 1/1 step: 612, time: 0.472, speed: 2.120 steps/s
	current lr: 0.0000061
	lm_loss: 3.6029, ppl: 36.7061, loss: 3.6029
[train][1] progress: 1/1 step: 613, time: 0.470, speed: 2.128 steps/s
	current lr: 0.0000061
	lm_loss: 3.5427, ppl: 34.5613, loss: 3.5427
[train][1] progress: 1/1 step: 614, time: 0.471, speed: 2.125 steps/s
	current lr: 0.0000061
	lm_loss: 3.6184, ppl: 37.2765, loss: 3.6184
[train][1] progress: 1/1 step: 615, time: 0.474, speed: 2.110 steps/s
	current lr: 0.0000061
	lm_loss: 3.2718, ppl: 26.3584, loss: 3.2718
[train][1] progress: 1/1 step: 616, time: 0.478, speed: 2.090 steps/s
	current lr: 0.0000062
	lm_loss: 3.6245, ppl: 37.5058, loss: 3.6245
[train][1] progress: 1/1 step: 617, time: 0.476, speed: 2.100 steps/s
	current lr: 0.0000062
	lm_loss: 3.3689, ppl: 29.0467, loss: 3.3689
[train][1] progress: 1/1 step: 618, time: 0.472, speed: 2.121 steps/s
	current lr: 0.0000062
	lm_loss: 3.4335, ppl: 30.9852, loss: 3.4335
[train][1] progress: 1/1 step: 619, time: 0.470, speed: 2.130 steps/s
	current lr: 0.0000062
	lm_loss: 3.3914, ppl: 29.7062, loss: 3.3914
[train][1] progress: 1/1 step: 620, time: 0.474, speed: 2.108 steps/s
	current lr: 0.0000062
	lm_loss: 3.5869, ppl: 36.1218, loss: 3.5869
[train][1] progress: 1/1 step: 621, time: 0.471, speed: 2.122 steps/s
	current lr: 0.0000062
	lm_loss: 3.5961, ppl: 36.4541, loss: 3.5961
[train][1] progress: 1/1 step: 622, time: 0.474, speed: 2.109 steps/s
	current lr: 0.0000062
	lm_loss: 3.5996, ppl: 36.5848, loss: 3.5996
[train][1] progress: 1/1 step: 623, time: 0.479, speed: 2.089 steps/s
	current lr: 0.0000062
	lm_loss: 3.5129, ppl: 33.5459, loss: 3.5129
[train][1] progress: 1/1 step: 624, time: 0.479, speed: 2.088 steps/s
	current lr: 0.0000062
	lm_loss: 3.6985, ppl: 40.3864, loss: 3.6985
[train][1] progress: 1/1 step: 625, time: 0.473, speed: 2.113 steps/s
	current lr: 0.0000063
	lm_loss: 3.4219, ppl: 30.6290, loss: 3.4219
[train][1] progress: 1/1 step: 626, time: 0.478, speed: 2.093 steps/s
	current lr: 0.0000063
	lm_loss: 3.6816, ppl: 39.7083, loss: 3.6816
[train][1] progress: 1/1 step: 627, time: 0.467, speed: 2.141 steps/s
	current lr: 0.0000063
	lm_loss: 3.4788, ppl: 32.4219, loss: 3.4788
[train][1] progress: 1/1 step: 628, time: 0.484, speed: 2.068 steps/s
	current lr: 0.0000063
	lm_loss: 3.5767, ppl: 35.7548, loss: 3.5767
[train][1] progress: 1/1 step: 629, time: 0.473, speed: 2.112 steps/s
	current lr: 0.0000063
	lm_loss: 3.5592, ppl: 35.1351, loss: 3.5592
[train][1] progress: 1/1 step: 630, time: 0.482, speed: 2.073 steps/s
	current lr: 0.0000063
	lm_loss: 3.5847, ppl: 36.0439, loss: 3.5847
[train][1] progress: 1/1 step: 631, time: 0.470, speed: 2.129 steps/s
	current lr: 0.0000063
	lm_loss: 3.4730, ppl: 32.2334, loss: 3.4730
[train][1] progress: 1/1 step: 632, time: 0.474, speed: 2.110 steps/s
	current lr: 0.0000063
	lm_loss: 3.5535, ppl: 34.9349, loss: 3.5535
[train][1] progress: 1/1 step: 633, time: 0.470, speed: 2.127 steps/s
	current lr: 0.0000063
	lm_loss: 3.5906, ppl: 36.2547, loss: 3.5906
[train][1] progress: 1/1 step: 634, time: 0.475, speed: 2.107 steps/s
	current lr: 0.0000063
	lm_loss: 3.4747, ppl: 32.2878, loss: 3.4747
[train][1] progress: 1/1 step: 635, time: 0.484, speed: 2.068 steps/s
	current lr: 0.0000064
	lm_loss: 3.6889, ppl: 40.0012, loss: 3.6889
[train][1] progress: 1/1 step: 636, time: 0.479, speed: 2.089 steps/s
	current lr: 0.0000064
	lm_loss: 3.6357, ppl: 37.9292, loss: 3.6357
[train][1] progress: 1/1 step: 637, time: 0.474, speed: 2.108 steps/s
	current lr: 0.0000064
	lm_loss: 3.5847, ppl: 36.0414, loss: 3.5847
[train][1] progress: 1/1 step: 638, time: 0.481, speed: 2.081 steps/s
	current lr: 0.0000064
	lm_loss: 3.5647, ppl: 35.3282, loss: 3.5647
[train][1] progress: 1/1 step: 639, time: 0.477, speed: 2.097 steps/s
	current lr: 0.0000064
	lm_loss: 3.5931, ppl: 36.3478, loss: 3.5931
[train][1] progress: 1/1 step: 640, time: 0.480, speed: 2.081 steps/s
	current lr: 0.0000064
	lm_loss: 3.5351, ppl: 34.2990, loss: 3.5351
[train][1] progress: 1/1 step: 641, time: 0.479, speed: 2.086 steps/s
	current lr: 0.0000064
	lm_loss: 3.5171, ppl: 33.6872, loss: 3.5171
[train][1] progress: 1/1 step: 642, time: 0.470, speed: 2.129 steps/s
	current lr: 0.0000064
	lm_loss: 3.6047, ppl: 36.7711, loss: 3.6047
[train][1] progress: 1/1 step: 643, time: 0.472, speed: 2.117 steps/s
	current lr: 0.0000064
	lm_loss: 3.5303, ppl: 34.1359, loss: 3.5303
[train][1] progress: 1/1 step: 644, time: 0.481, speed: 2.080 steps/s
	current lr: 0.0000064
	lm_loss: 3.4100, ppl: 30.2650, loss: 3.4100
[train][1] progress: 1/1 step: 645, time: 0.470, speed: 2.128 steps/s
	current lr: 0.0000065
	lm_loss: 3.5620, ppl: 35.2326, loss: 3.5620
[train][1] progress: 1/1 step: 646, time: 0.475, speed: 2.106 steps/s
	current lr: 0.0000065
	lm_loss: 3.6546, ppl: 38.6538, loss: 3.6546
[train][1] progress: 1/1 step: 647, time: 0.473, speed: 2.113 steps/s
	current lr: 0.0000065
	lm_loss: 3.6184, ppl: 37.2789, loss: 3.6184
[train][1] progress: 1/1 step: 648, time: 0.478, speed: 2.094 steps/s
	current lr: 0.0000065
	lm_loss: 3.6875, ppl: 39.9438, loss: 3.6875
[train][1] progress: 1/1 step: 649, time: 0.482, speed: 2.077 steps/s
	current lr: 0.0000065
	lm_loss: 3.5441, ppl: 34.6091, loss: 3.5441
[train][1] progress: 1/1 step: 650, time: 0.479, speed: 2.086 steps/s
	current lr: 0.0000065
	lm_loss: 3.5727, ppl: 35.6122, loss: 3.5727
[train][1] progress: 1/1 step: 651, time: 0.472, speed: 2.117 steps/s
	current lr: 0.0000065
	lm_loss: 3.7137, ppl: 41.0063, loss: 3.7137
[train][1] progress: 1/1 step: 652, time: 0.482, speed: 2.077 steps/s
	current lr: 0.0000065
	lm_loss: 3.6455, ppl: 38.3016, loss: 3.6455
[train][1] progress: 1/1 step: 653, time: 0.471, speed: 2.122 steps/s
	current lr: 0.0000065
	lm_loss: 3.6529, ppl: 38.5870, loss: 3.6529
[train][1] progress: 1/1 step: 654, time: 0.479, speed: 2.090 steps/s
	current lr: 0.0000065
	lm_loss: 3.7187, ppl: 41.2128, loss: 3.7187
[train][1] progress: 1/1 step: 655, time: 0.473, speed: 2.113 steps/s
	current lr: 0.0000066
	lm_loss: 3.4548, ppl: 31.6530, loss: 3.4548
[train][1] progress: 1/1 step: 656, time: 0.469, speed: 2.130 steps/s
	current lr: 0.0000066
	lm_loss: 3.6096, ppl: 36.9503, loss: 3.6096
[train][1] progress: 1/1 step: 657, time: 0.470, speed: 2.129 steps/s
	current lr: 0.0000066
	lm_loss: 3.4628, ppl: 31.9047, loss: 3.4628
[train][1] progress: 1/1 step: 658, time: 0.471, speed: 2.124 steps/s
	current lr: 0.0000066
	lm_loss: 3.6293, ppl: 37.6852, loss: 3.6293
[train][1] progress: 1/1 step: 659, time: 0.473, speed: 2.114 steps/s
	current lr: 0.0000066
	lm_loss: 3.7438, ppl: 42.2579, loss: 3.7438
[train][1] progress: 1/1 step: 660, time: 0.475, speed: 2.106 steps/s
	current lr: 0.0000066
	lm_loss: 3.4077, ppl: 30.1971, loss: 3.4077
[train][1] progress: 1/1 step: 661, time: 0.480, speed: 2.085 steps/s
	current lr: 0.0000066
	lm_loss: 3.4811, ppl: 32.4952, loss: 3.4811
[train][1] progress: 1/1 step: 662, time: 0.468, speed: 2.137 steps/s
	current lr: 0.0000066
	lm_loss: 3.4772, ppl: 32.3679, loss: 3.4772
[train][1] progress: 1/1 step: 663, time: 0.471, speed: 2.121 steps/s
	current lr: 0.0000066
	lm_loss: 3.6131, ppl: 37.0819, loss: 3.6131
[train][1] progress: 1/1 step: 664, time: 0.468, speed: 2.135 steps/s
	current lr: 0.0000066
	lm_loss: 3.6348, ppl: 37.8928, loss: 3.6348
[train][1] progress: 1/1 step: 665, time: 0.470, speed: 2.126 steps/s
	current lr: 0.0000066
	lm_loss: 3.4704, ppl: 32.1482, loss: 3.4704
[train][1] progress: 1/1 step: 666, time: 0.470, speed: 2.126 steps/s
	current lr: 0.0000067
	lm_loss: 3.6046, ppl: 36.7656, loss: 3.6046
[train][1] progress: 1/1 step: 667, time: 0.471, speed: 2.124 steps/s
	current lr: 0.0000067
	lm_loss: 3.7170, ppl: 41.1422, loss: 3.7170
[train][1] progress: 1/1 step: 668, time: 0.470, speed: 2.126 steps/s
	current lr: 0.0000067
	lm_loss: 3.6643, ppl: 39.0270, loss: 3.6643
[train][1] progress: 1/1 step: 669, time: 0.473, speed: 2.113 steps/s
	current lr: 0.0000067
	lm_loss: 3.6725, ppl: 39.3494, loss: 3.6725
[train][1] progress: 1/1 step: 670, time: 0.470, speed: 2.129 steps/s
	current lr: 0.0000067
	lm_loss: 3.5111, ppl: 33.4836, loss: 3.5111
[train][1] progress: 1/1 step: 671, time: 0.483, speed: 2.071 steps/s
	current lr: 0.0000067
	lm_loss: 3.6446, ppl: 38.2664, loss: 3.6446
[train][1] progress: 1/1 step: 672, time: 0.472, speed: 2.120 steps/s
	current lr: 0.0000067
	lm_loss: 3.5755, ppl: 35.7135, loss: 3.5755
[train][1] progress: 1/1 step: 673, time: 0.480, speed: 2.081 steps/s
	current lr: 0.0000067
	lm_loss: 3.5116, ppl: 33.5011, loss: 3.5116
[train][1] progress: 1/1 step: 674, time: 0.483, speed: 2.072 steps/s
	current lr: 0.0000067
	lm_loss: 3.8430, ppl: 46.6655, loss: 3.8430
[train][1] progress: 1/1 step: 675, time: 0.484, speed: 2.067 steps/s
	current lr: 0.0000068
	lm_loss: 3.3150, ppl: 27.5220, loss: 3.3150
[train][1] progress: 1/1 step: 676, time: 0.476, speed: 2.103 steps/s
	current lr: 0.0000068
	lm_loss: 3.6532, ppl: 38.5969, loss: 3.6532
[train][1] progress: 1/1 step: 677, time: 0.477, speed: 2.098 steps/s
	current lr: 0.0000068
	lm_loss: 2.9843, ppl: 19.7735, loss: 2.9843
[train][1] progress: 1/1 step: 678, time: 0.470, speed: 2.128 steps/s
	current lr: 0.0000068
	lm_loss: 3.5821, ppl: 35.9497, loss: 3.5821
[train][1] progress: 1/1 step: 679, time: 0.471, speed: 2.124 steps/s
	current lr: 0.0000068
	lm_loss: 3.6974, ppl: 40.3417, loss: 3.6974
[train][1] progress: 1/1 step: 680, time: 0.471, speed: 2.124 steps/s
	current lr: 0.0000068
	lm_loss: 3.6748, ppl: 39.4397, loss: 3.6748
[train][1] progress: 1/1 step: 681, time: 0.481, speed: 2.081 steps/s
	current lr: 0.0000068
	lm_loss: 3.6102, ppl: 36.9742, loss: 3.6102
[train][1] progress: 1/1 step: 682, time: 0.478, speed: 2.092 steps/s
	current lr: 0.0000068
	lm_loss: 3.6336, ppl: 37.8480, loss: 3.6336
[train][1] progress: 1/1 step: 683, time: 0.472, speed: 2.118 steps/s
	current lr: 0.0000068
	lm_loss: 3.4750, ppl: 32.2965, loss: 3.4750
[train][1] progress: 1/1 step: 684, time: 0.479, speed: 2.088 steps/s
	current lr: 0.0000068
	lm_loss: 3.7603, ppl: 42.9625, loss: 3.7603
[train][1] progress: 1/1 step: 685, time: 0.471, speed: 2.123 steps/s
	current lr: 0.0000068
	lm_loss: 3.3960, ppl: 29.8454, loss: 3.3960
[train][1] progress: 1/1 step: 686, time: 0.482, speed: 2.074 steps/s
	current lr: 0.0000069
	lm_loss: 3.5841, ppl: 36.0220, loss: 3.5841
[train][1] progress: 1/1 step: 687, time: 0.482, speed: 2.073 steps/s
	current lr: 0.0000069
	lm_loss: 3.5112, ppl: 33.4875, loss: 3.5112
[train][1] progress: 1/1 step: 688, time: 0.472, speed: 2.120 steps/s
	current lr: 0.0000069
	lm_loss: 3.5448, ppl: 34.6345, loss: 3.5448
[train][1] progress: 1/1 step: 689, time: 0.472, speed: 2.119 steps/s
	current lr: 0.0000069
	lm_loss: 3.5359, ppl: 34.3272, loss: 3.5359
[train][1] progress: 1/1 step: 690, time: 0.475, speed: 2.103 steps/s
	current lr: 0.0000069
	lm_loss: 3.6028, ppl: 36.7021, loss: 3.6028
[train][1] progress: 1/1 step: 691, time: 0.469, speed: 2.132 steps/s
	current lr: 0.0000069
	lm_loss: 3.6425, ppl: 38.1888, loss: 3.6425
[train][1] progress: 1/1 step: 692, time: 0.481, speed: 2.079 steps/s
	current lr: 0.0000069
	lm_loss: 3.6063, ppl: 36.8312, loss: 3.6063
[train][1] progress: 1/1 step: 693, time: 0.480, speed: 2.084 steps/s
	current lr: 0.0000069
	lm_loss: 3.6250, ppl: 37.5238, loss: 3.6250
[train][1] progress: 1/1 step: 694, time: 0.480, speed: 2.082 steps/s
	current lr: 0.0000069
	lm_loss: 3.4124, ppl: 30.3371, loss: 3.4124
[train][1] progress: 1/1 step: 695, time: 0.468, speed: 2.136 steps/s
	current lr: 0.0000070
	lm_loss: 3.6107, ppl: 36.9928, loss: 3.6107
[train][1] progress: 1/1 step: 696, time: 0.471, speed: 2.125 steps/s
	current lr: 0.0000070
	lm_loss: 3.6849, ppl: 39.8415, loss: 3.6849
[train][1] progress: 1/1 step: 697, time: 0.470, speed: 2.130 steps/s
	current lr: 0.0000070
	lm_loss: 3.5833, ppl: 35.9916, loss: 3.5833
[train][1] progress: 1/1 step: 698, time: 0.470, speed: 2.126 steps/s
	current lr: 0.0000070
	lm_loss: 3.6368, ppl: 37.9708, loss: 3.6368
[train][1] progress: 1/1 step: 699, time: 0.482, speed: 2.075 steps/s
	current lr: 0.0000070
	lm_loss: 3.7189, ppl: 41.2179, loss: 3.7189
[train][1] progress: 1/1 step: 700, time: 0.482, speed: 2.076 steps/s
	current lr: 0.0000070
	lm_loss: 3.6744, ppl: 39.4257, loss: 3.6744
[train][1] progress: 1/1 step: 701, time: 0.474, speed: 2.108 steps/s
	current lr: 0.0000070
	lm_loss: 3.5588, ppl: 35.1220, loss: 3.5588
[train][1] progress: 1/1 step: 702, time: 0.468, speed: 2.135 steps/s
	current lr: 0.0000070
	lm_loss: 3.6290, ppl: 37.6741, loss: 3.6290
[train][1] progress: 1/1 step: 703, time: 0.474, speed: 2.110 steps/s
	current lr: 0.0000070
	lm_loss: 3.5754, ppl: 35.7093, loss: 3.5754
[train][1] progress: 1/1 step: 704, time: 0.475, speed: 2.106 steps/s
	current lr: 0.0000070
	lm_loss: 3.5386, ppl: 34.4171, loss: 3.5386
[train][1] progress: 1/1 step: 705, time: 0.477, speed: 2.095 steps/s
	current lr: 0.0000071
	lm_loss: 3.5528, ppl: 34.9094, loss: 3.5528
[train][1] progress: 1/1 step: 706, time: 0.469, speed: 2.134 steps/s
	current lr: 0.0000071
	lm_loss: 3.5006, ppl: 33.1338, loss: 3.5006
[train][1] progress: 1/1 step: 707, time: 0.494, speed: 2.024 steps/s
	current lr: 0.0000071
	lm_loss: 3.6824, ppl: 39.7407, loss: 3.6824
[train][1] progress: 1/1 step: 708, time: 0.475, speed: 2.105 steps/s
	current lr: 0.0000071
	lm_loss: 3.3001, ppl: 27.1154, loss: 3.3001
[train][1] progress: 1/1 step: 709, time: 0.473, speed: 2.112 steps/s
	current lr: 0.0000071
	lm_loss: 3.4869, ppl: 32.6844, loss: 3.4869
[train][1] progress: 1/1 step: 710, time: 0.478, speed: 2.094 steps/s
	current lr: 0.0000071
	lm_loss: 3.6036, ppl: 36.7298, loss: 3.6036
[train][1] progress: 1/1 step: 711, time: 0.474, speed: 2.108 steps/s
	current lr: 0.0000071
	lm_loss: 3.6767, ppl: 39.5153, loss: 3.6767
[train][1] progress: 1/1 step: 712, time: 0.474, speed: 2.109 steps/s
	current lr: 0.0000071
	lm_loss: 3.2871, ppl: 26.7660, loss: 3.2871
[train][1] progress: 1/1 step: 713, time: 0.481, speed: 2.078 steps/s
	current lr: 0.0000071
	lm_loss: 3.6705, ppl: 39.2699, loss: 3.6705
[train][1] progress: 1/1 step: 714, time: 0.481, speed: 2.079 steps/s
	current lr: 0.0000071
	lm_loss: 3.4772, ppl: 32.3677, loss: 3.4772
[train][1] progress: 1/1 step: 715, time: 0.471, speed: 2.124 steps/s
	current lr: 0.0000072
	lm_loss: 3.6593, ppl: 38.8348, loss: 3.6593
[train][1] progress: 1/1 step: 716, time: 0.482, speed: 2.076 steps/s
	current lr: 0.0000072
	lm_loss: 3.6412, ppl: 38.1376, loss: 3.6412
[train][1] progress: 1/1 step: 717, time: 0.476, speed: 2.101 steps/s
	current lr: 0.0000072
	lm_loss: 3.5402, ppl: 34.4745, loss: 3.5402
[train][1] progress: 1/1 step: 718, time: 0.474, speed: 2.110 steps/s
	current lr: 0.0000072
	lm_loss: 3.4348, ppl: 31.0259, loss: 3.4348
[train][1] progress: 1/1 step: 719, time: 0.469, speed: 2.130 steps/s
	current lr: 0.0000072
	lm_loss: 3.5818, ppl: 35.9381, loss: 3.5818
[train][1] progress: 1/1 step: 720, time: 0.474, speed: 2.111 steps/s
	current lr: 0.0000072
	lm_loss: 3.5273, ppl: 34.0330, loss: 3.5273
[train][1] progress: 1/1 step: 721, time: 0.473, speed: 2.116 steps/s
	current lr: 0.0000072
	lm_loss: 3.4771, ppl: 32.3654, loss: 3.4771
[train][1] progress: 1/1 step: 722, time: 0.469, speed: 2.131 steps/s
	current lr: 0.0000072
	lm_loss: 3.4663, ppl: 32.0166, loss: 3.4663
[train][1] progress: 1/1 step: 723, time: 0.478, speed: 2.094 steps/s
	current lr: 0.0000072
	lm_loss: 3.3411, ppl: 28.2514, loss: 3.3411
[train][1] progress: 1/1 step: 724, time: 0.475, speed: 2.107 steps/s
	current lr: 0.0000072
	lm_loss: 3.6011, ppl: 36.6401, loss: 3.6011
[train][1] progress: 1/1 step: 725, time: 0.475, speed: 2.105 steps/s
	current lr: 0.0000072
	lm_loss: 3.5643, ppl: 35.3146, loss: 3.5643
[train][1] progress: 1/1 step: 726, time: 0.471, speed: 2.122 steps/s
	current lr: 0.0000073
	lm_loss: 3.6617, ppl: 38.9293, loss: 3.6617
[train][1] progress: 1/1 step: 727, time: 0.482, speed: 2.077 steps/s
	current lr: 0.0000073
	lm_loss: 3.7441, ppl: 42.2693, loss: 3.7441
[train][1] progress: 1/1 step: 728, time: 0.479, speed: 2.088 steps/s
	current lr: 0.0000073
	lm_loss: 3.6717, ppl: 39.3170, loss: 3.6717
[train][1] progress: 1/1 step: 729, time: 0.479, speed: 2.089 steps/s
	current lr: 0.0000073
	lm_loss: 3.6181, ppl: 37.2651, loss: 3.6181
[train][1] progress: 1/1 step: 730, time: 0.472, speed: 2.121 steps/s
	current lr: 0.0000073
	lm_loss: 3.4612, ppl: 31.8551, loss: 3.4612
[train][1] progress: 1/1 step: 731, time: 0.487, speed: 2.053 steps/s
	current lr: 0.0000073
	lm_loss: 3.7656, ppl: 43.1912, loss: 3.7656
[train][1] progress: 1/1 step: 732, time: 0.471, speed: 2.125 steps/s
	current lr: 0.0000073
	lm_loss: 3.5258, ppl: 33.9823, loss: 3.5258
[train][1] progress: 1/1 step: 733, time: 0.472, speed: 2.118 steps/s
	current lr: 0.0000073
	lm_loss: 3.5512, ppl: 34.8540, loss: 3.5512
[train][1] progress: 1/1 step: 734, time: 0.474, speed: 2.112 steps/s
	current lr: 0.0000073
	lm_loss: 3.5165, ppl: 33.6651, loss: 3.5165
[train][1] progress: 1/1 step: 735, time: 0.470, speed: 2.129 steps/s
	current lr: 0.0000074
	lm_loss: 3.5016, ppl: 33.1683, loss: 3.5016
[train][1] progress: 1/1 step: 736, time: 0.473, speed: 2.115 steps/s
	current lr: 0.0000074
	lm_loss: 3.5949, ppl: 36.4136, loss: 3.5949
[train][1] progress: 1/1 step: 737, time: 0.477, speed: 2.098 steps/s
	current lr: 0.0000074
	lm_loss: 3.7439, ppl: 42.2605, loss: 3.7439
[train][1] progress: 1/1 step: 738, time: 0.479, speed: 2.086 steps/s
	current lr: 0.0000074
	lm_loss: 3.5481, ppl: 34.7467, loss: 3.5481
[train][1] progress: 1/1 step: 739, time: 0.470, speed: 2.126 steps/s
	current lr: 0.0000074
	lm_loss: 3.5856, ppl: 36.0756, loss: 3.5856
[train][1] progress: 1/1 step: 740, time: 0.469, speed: 2.130 steps/s
	current lr: 0.0000074
	lm_loss: 3.5252, ppl: 33.9600, loss: 3.5252
[train][1] progress: 1/1 step: 741, time: 0.475, speed: 2.106 steps/s
	current lr: 0.0000074
	lm_loss: 3.5808, ppl: 35.9036, loss: 3.5808
[train][1] progress: 1/1 step: 742, time: 0.477, speed: 2.096 steps/s
	current lr: 0.0000074
	lm_loss: 3.2692, ppl: 26.2891, loss: 3.2692
[train][1] progress: 1/1 step: 743, time: 0.475, speed: 2.106 steps/s
	current lr: 0.0000074
	lm_loss: 3.7037, ppl: 40.5969, loss: 3.7037
[train][1] progress: 1/1 step: 744, time: 0.481, speed: 2.077 steps/s
	current lr: 0.0000074
	lm_loss: 3.5888, ppl: 36.1924, loss: 3.5888
[train][1] progress: 1/1 step: 745, time: 0.472, speed: 2.117 steps/s
	current lr: 0.0000074
	lm_loss: 3.6146, ppl: 37.1378, loss: 3.6146
[train][1] progress: 1/1 step: 746, time: 0.479, speed: 2.089 steps/s
	current lr: 0.0000075
	lm_loss: 3.0466, ppl: 21.0437, loss: 3.0466
[train][1] progress: 1/1 step: 747, time: 0.468, speed: 2.137 steps/s
	current lr: 0.0000075
	lm_loss: 3.5086, ppl: 33.4016, loss: 3.5086
[train][1] progress: 1/1 step: 748, time: 0.473, speed: 2.113 steps/s
	current lr: 0.0000075
	lm_loss: 3.5670, ppl: 35.4116, loss: 3.5670
[train][1] progress: 1/1 step: 749, time: 0.471, speed: 2.124 steps/s
	current lr: 0.0000075
	lm_loss: 3.5185, ppl: 33.7332, loss: 3.5185
[train][1] progress: 1/1 step: 750, time: 0.481, speed: 2.081 steps/s
	current lr: 0.0000075
	lm_loss: 3.6161, ppl: 37.1915, loss: 3.6161
[train][1] progress: 1/1 step: 751, time: 0.472, speed: 2.121 steps/s
	current lr: 0.0000075
	lm_loss: 3.5337, ppl: 34.2518, loss: 3.5337
[train][1] progress: 1/1 step: 752, time: 0.481, speed: 2.080 steps/s
	current lr: 0.0000075
	lm_loss: 3.7469, ppl: 42.3887, loss: 3.7469
[train][1] progress: 1/1 step: 753, time: 0.473, speed: 2.114 steps/s
	current lr: 0.0000075
	lm_loss: 3.6602, ppl: 38.8676, loss: 3.6602
[train][1] progress: 1/1 step: 754, time: 0.482, speed: 2.075 steps/s
	current lr: 0.0000075
	lm_loss: 3.6125, ppl: 37.0573, loss: 3.6125
[train][1] progress: 1/1 step: 755, time: 0.479, speed: 2.090 steps/s
	current lr: 0.0000076
	lm_loss: 3.5999, ppl: 36.5962, loss: 3.5999
[train][1] progress: 1/1 step: 756, time: 0.480, speed: 2.085 steps/s
	current lr: 0.0000076
	lm_loss: 3.6382, ppl: 38.0220, loss: 3.6382
[train][1] progress: 1/1 step: 757, time: 0.476, speed: 2.099 steps/s
	current lr: 0.0000076
	lm_loss: 3.7008, ppl: 40.4817, loss: 3.7008
[train][1] progress: 1/1 step: 758, time: 0.472, speed: 2.118 steps/s
	current lr: 0.0000076
	lm_loss: 3.6708, ppl: 39.2842, loss: 3.6708
[train][1] progress: 1/1 step: 759, time: 0.482, speed: 2.076 steps/s
	current lr: 0.0000076
	lm_loss: 3.4415, ppl: 31.2329, loss: 3.4415
[train][1] progress: 1/1 step: 760, time: 0.469, speed: 2.131 steps/s
	current lr: 0.0000076
	lm_loss: 3.5938, ppl: 36.3730, loss: 3.5938
[train][1] progress: 1/1 step: 761, time: 0.472, speed: 2.118 steps/s
	current lr: 0.0000076
	lm_loss: 3.5877, ppl: 36.1523, loss: 3.5877
[train][1] progress: 1/1 step: 762, time: 0.470, speed: 2.126 steps/s
	current lr: 0.0000076
	lm_loss: 3.5122, ppl: 33.5214, loss: 3.5122
[train][1] progress: 1/1 step: 763, time: 0.472, speed: 2.118 steps/s
	current lr: 0.0000076
	lm_loss: 3.6257, ppl: 37.5523, loss: 3.6257
[train][1] progress: 1/1 step: 764, time: 0.475, speed: 2.105 steps/s
	current lr: 0.0000076
	lm_loss: 3.6041, ppl: 36.7500, loss: 3.6041
[train][1] progress: 1/1 step: 765, time: 0.481, speed: 2.079 steps/s
	current lr: 0.0000077
	lm_loss: 3.6933, ppl: 40.1760, loss: 3.6933
[train][1] progress: 1/1 step: 766, time: 0.525, speed: 1.904 steps/s
	current lr: 0.0000077
	lm_loss: 3.4912, ppl: 32.8261, loss: 3.4912
[train][1] progress: 1/1 step: 767, time: 0.484, speed: 2.067 steps/s
	current lr: 0.0000077
	lm_loss: 3.4748, ppl: 32.2912, loss: 3.4748
[train][1] progress: 1/1 step: 768, time: 0.480, speed: 2.083 steps/s
	current lr: 0.0000077
	lm_loss: 3.5120, ppl: 33.5161, loss: 3.5120
[train][1] progress: 1/1 step: 769, time: 0.472, speed: 2.121 steps/s
	current lr: 0.0000077
	lm_loss: 3.2208, ppl: 25.0486, loss: 3.2208
[train][1] progress: 1/1 step: 770, time: 0.775, speed: 1.291 steps/s
	current lr: 0.0000077
	lm_loss: 3.5513, ppl: 34.8598, loss: 3.5513
[train][1] progress: 1/1 step: 771, time: 0.677, speed: 1.477 steps/s
	current lr: 0.0000077
	lm_loss: 3.5868, ppl: 36.1172, loss: 3.5868
[train][1] progress: 1/1 step: 772, time: 0.473, speed: 2.112 steps/s
	current lr: 0.0000077
	lm_loss: 3.6809, ppl: 39.6829, loss: 3.6809
[train][1] progress: 1/1 step: 773, time: 0.485, speed: 2.064 steps/s
	current lr: 0.0000077
	lm_loss: 3.1889, ppl: 24.2620, loss: 3.1889
[train][1] progress: 1/1 step: 774, time: 0.481, speed: 2.080 steps/s
	current lr: 0.0000077
	lm_loss: 3.6828, ppl: 39.7595, loss: 3.6828
[train][1] progress: 1/1 step: 775, time: 0.479, speed: 2.087 steps/s
	current lr: 0.0000077
	lm_loss: 3.7427, ppl: 42.2107, loss: 3.7427
[train][1] progress: 1/1 step: 776, time: 0.478, speed: 2.094 steps/s
	current lr: 0.0000078
	lm_loss: 3.3962, ppl: 29.8514, loss: 3.3962
[train][1] progress: 1/1 step: 777, time: 0.477, speed: 2.096 steps/s
	current lr: 0.0000078
	lm_loss: 3.2503, ppl: 25.7981, loss: 3.2503
[train][1] progress: 1/1 step: 778, time: 0.480, speed: 2.085 steps/s
	current lr: 0.0000078
	lm_loss: 3.5261, ppl: 33.9906, loss: 3.5261
[train][1] progress: 1/1 step: 779, time: 0.469, speed: 2.133 steps/s
	current lr: 0.0000078
	lm_loss: 3.3367, ppl: 28.1260, loss: 3.3367
[train][1] progress: 1/1 step: 780, time: 0.471, speed: 2.122 steps/s
	current lr: 0.0000078
	lm_loss: 3.5316, ppl: 34.1790, loss: 3.5316
[train][1] progress: 1/1 step: 781, time: 0.472, speed: 2.118 steps/s
	current lr: 0.0000078
	lm_loss: 3.6241, ppl: 37.4924, loss: 3.6241
[train][1] progress: 1/1 step: 782, time: 0.474, speed: 2.109 steps/s
	current lr: 0.0000078
	lm_loss: 3.5684, ppl: 35.4607, loss: 3.5684
[train][1] progress: 1/1 step: 783, time: 0.469, speed: 2.130 steps/s
	current lr: 0.0000078
	lm_loss: 3.4640, ppl: 31.9444, loss: 3.4640
[train][1] progress: 1/1 step: 784, time: 0.482, speed: 2.074 steps/s
	current lr: 0.0000078
	lm_loss: 3.6605, ppl: 38.8809, loss: 3.6605
[train][1] progress: 1/1 step: 785, time: 0.472, speed: 2.120 steps/s
	current lr: 0.0000078
	lm_loss: 3.4148, ppl: 30.4122, loss: 3.4148
[train][1] progress: 1/1 step: 786, time: 0.476, speed: 2.100 steps/s
	current lr: 0.0000079
	lm_loss: 3.4023, ppl: 30.0328, loss: 3.4023
[train][1] progress: 1/1 step: 787, time: 0.484, speed: 2.064 steps/s
	current lr: 0.0000079
	lm_loss: 3.6184, ppl: 37.2767, loss: 3.6184
[train][1] progress: 1/1 step: 788, time: 0.476, speed: 2.100 steps/s
	current lr: 0.0000079
	lm_loss: 3.5675, ppl: 35.4279, loss: 3.5675
[train][1] progress: 1/1 step: 789, time: 0.477, speed: 2.097 steps/s
	current lr: 0.0000079
	lm_loss: 3.1978, ppl: 24.4794, loss: 3.1978
[train][1] progress: 1/1 step: 790, time: 0.474, speed: 2.110 steps/s
	current lr: 0.0000079
	lm_loss: 3.5972, ppl: 36.4945, loss: 3.5972
[train][1] progress: 1/1 step: 791, time: 0.474, speed: 2.110 steps/s
	current lr: 0.0000079
	lm_loss: 3.4526, ppl: 31.5828, loss: 3.4526
[train][1] progress: 1/1 step: 792, time: 0.476, speed: 2.103 steps/s
	current lr: 0.0000079
	lm_loss: 3.3415, ppl: 28.2618, loss: 3.3415
[train][1] progress: 1/1 step: 793, time: 0.472, speed: 2.121 steps/s
	current lr: 0.0000079
	lm_loss: 3.5784, ppl: 35.8167, loss: 3.5784
[train][1] progress: 1/1 step: 794, time: 0.478, speed: 2.094 steps/s
	current lr: 0.0000079
	lm_loss: 3.8574, ppl: 47.3405, loss: 3.8574
[train][1] progress: 1/1 step: 795, time: 0.471, speed: 2.123 steps/s
	current lr: 0.0000080
	lm_loss: 3.6663, ppl: 39.1088, loss: 3.6663
[train][1] progress: 1/1 step: 796, time: 0.465, speed: 2.149 steps/s
	current lr: 0.0000080
	lm_loss: 3.3839, ppl: 29.4846, loss: 3.3839
[train][1] progress: 1/1 step: 797, time: 0.473, speed: 2.115 steps/s
	current lr: 0.0000080
	lm_loss: 3.7883, ppl: 44.1797, loss: 3.7883
[train][1] progress: 1/1 step: 798, time: 0.474, speed: 2.109 steps/s
	current lr: 0.0000080
	lm_loss: 3.5193, ppl: 33.7613, loss: 3.5193
[train][1] progress: 1/1 step: 799, time: 0.472, speed: 2.119 steps/s
	current lr: 0.0000080
	lm_loss: 3.7000, ppl: 40.4475, loss: 3.7000
[train][1] progress: 1/1 step: 800, time: 0.483, speed: 2.070 steps/s
	current lr: 0.0000080
	lm_loss: 3.5331, ppl: 34.2313, loss: 3.5331
[train][1] progress: 1/1 step: 801, time: 0.474, speed: 2.109 steps/s
	current lr: 0.0000080
	lm_loss: 3.4637, ppl: 31.9353, loss: 3.4637
[train][1] progress: 1/1 step: 802, time: 0.471, speed: 2.123 steps/s
	current lr: 0.0000080
	lm_loss: 3.3887, ppl: 29.6274, loss: 3.3887
[train][1] progress: 1/1 step: 803, time: 0.479, speed: 2.086 steps/s
	current lr: 0.0000080
	lm_loss: 2.9903, ppl: 19.8913, loss: 2.9903
[train][1] progress: 1/1 step: 804, time: 0.475, speed: 2.107 steps/s
	current lr: 0.0000080
	lm_loss: 3.5447, ppl: 34.6285, loss: 3.5447
[train][1] progress: 1/1 step: 805, time: 0.555, speed: 1.802 steps/s
	current lr: 0.0000080
	lm_loss: 3.5124, ppl: 33.5296, loss: 3.5124
[train][1] progress: 1/1 step: 806, time: 0.470, speed: 2.130 steps/s
	current lr: 0.0000081
	lm_loss: 3.5393, ppl: 34.4442, loss: 3.5393
[train][1] progress: 1/1 step: 807, time: 0.479, speed: 2.088 steps/s
	current lr: 0.0000081
	lm_loss: 3.5612, ppl: 35.2038, loss: 3.5612
[train][1] progress: 1/1 step: 808, time: 0.477, speed: 2.098 steps/s
	current lr: 0.0000081
	lm_loss: 3.5888, ppl: 36.1921, loss: 3.5888
[train][1] progress: 1/1 step: 809, time: 0.467, speed: 2.140 steps/s
	current lr: 0.0000081
	lm_loss: 3.5152, ppl: 33.6220, loss: 3.5152
[train][1] progress: 1/1 step: 810, time: 0.481, speed: 2.078 steps/s
	current lr: 0.0000081
	lm_loss: 3.7232, ppl: 41.3973, loss: 3.7232
[train][1] progress: 1/1 step: 811, time: 0.480, speed: 2.082 steps/s
	current lr: 0.0000081
	lm_loss: 3.6089, ppl: 36.9270, loss: 3.6089
[train][1] progress: 1/1 step: 812, time: 0.467, speed: 2.143 steps/s
	current lr: 0.0000081
	lm_loss: 3.8383, ppl: 46.4448, loss: 3.8383
[train][1] progress: 1/1 step: 813, time: 0.471, speed: 2.125 steps/s
	current lr: 0.0000081
	lm_loss: 3.4424, ppl: 31.2626, loss: 3.4424
[train][1] progress: 1/1 step: 814, time: 0.471, speed: 2.123 steps/s
	current lr: 0.0000081
	lm_loss: 3.5130, ppl: 33.5490, loss: 3.5130
[train][1] progress: 1/1 step: 815, time: 0.471, speed: 2.123 steps/s
	current lr: 0.0000082
	lm_loss: 3.4950, ppl: 32.9488, loss: 3.4950
[train][1] progress: 1/1 step: 816, time: 0.479, speed: 2.088 steps/s
	current lr: 0.0000082
	lm_loss: 3.4919, ppl: 32.8471, loss: 3.4919
[train][1] progress: 1/1 step: 817, time: 0.468, speed: 2.135 steps/s
	current lr: 0.0000082
	lm_loss: 3.4861, ppl: 32.6577, loss: 3.4861
[train][1] progress: 1/1 step: 818, time: 0.470, speed: 2.127 steps/s
	current lr: 0.0000082
	lm_loss: 3.3799, ppl: 29.3681, loss: 3.3799
[train][1] progress: 1/1 step: 819, time: 0.477, speed: 2.097 steps/s
	current lr: 0.0000082
	lm_loss: 3.7146, ppl: 41.0409, loss: 3.7146
[train][1] progress: 1/1 step: 820, time: 0.478, speed: 2.092 steps/s
	current lr: 0.0000082
	lm_loss: 3.5686, ppl: 35.4658, loss: 3.5686
[train][1] progress: 1/1 step: 821, time: 0.468, speed: 2.135 steps/s
	current lr: 0.0000082
	lm_loss: 3.4296, ppl: 30.8656, loss: 3.4296
[train][1] progress: 1/1 step: 822, time: 0.472, speed: 2.121 steps/s
	current lr: 0.0000082
	lm_loss: 3.7035, ppl: 40.5889, loss: 3.7035
[train][1] progress: 1/1 step: 823, time: 0.469, speed: 2.134 steps/s
	current lr: 0.0000082
	lm_loss: 3.4820, ppl: 32.5237, loss: 3.4820
[train][1] progress: 1/1 step: 824, time: 0.472, speed: 2.118 steps/s
	current lr: 0.0000082
	lm_loss: 3.6996, ppl: 40.4314, loss: 3.6996
[train][1] progress: 1/1 step: 825, time: 0.480, speed: 2.084 steps/s
	current lr: 0.0000083
	lm_loss: 3.5794, ppl: 35.8510, loss: 3.5794
[train][1] progress: 1/1 step: 826, time: 0.473, speed: 2.113 steps/s
	current lr: 0.0000083
	lm_loss: 3.7433, ppl: 42.2390, loss: 3.7433
[train][1] progress: 1/1 step: 827, time: 0.473, speed: 2.116 steps/s
	current lr: 0.0000083
	lm_loss: 3.6602, ppl: 38.8692, loss: 3.6602
[train][1] progress: 1/1 step: 828, time: 0.474, speed: 2.111 steps/s
	current lr: 0.0000083
	lm_loss: 3.5264, ppl: 34.0025, loss: 3.5264
[train][1] progress: 1/1 step: 829, time: 0.481, speed: 2.079 steps/s
	current lr: 0.0000083
	lm_loss: 3.8219, ppl: 45.6897, loss: 3.8219
[train][1] progress: 1/1 step: 830, time: 0.469, speed: 2.131 steps/s
	current lr: 0.0000083
	lm_loss: 3.6902, ppl: 40.0536, loss: 3.6902
[train][1] progress: 1/1 step: 831, time: 0.480, speed: 2.084 steps/s
	current lr: 0.0000083
	lm_loss: 3.5839, ppl: 36.0143, loss: 3.5839
[train][1] progress: 1/1 step: 832, time: 0.473, speed: 2.115 steps/s
	current lr: 0.0000083
	lm_loss: 3.5580, ppl: 35.0937, loss: 3.5580
[train][1] progress: 1/1 step: 833, time: 0.474, speed: 2.108 steps/s
	current lr: 0.0000083
	lm_loss: 3.3682, ppl: 29.0262, loss: 3.3682
[train][1] progress: 1/1 step: 834, time: 0.486, speed: 2.058 steps/s
	current lr: 0.0000083
	lm_loss: 2.6229, ppl: 13.7761, loss: 2.6229
[train][1] progress: 1/1 step: 835, time: 0.473, speed: 2.114 steps/s
	current lr: 0.0000083
	lm_loss: 3.5402, ppl: 34.4744, loss: 3.5402
[train][1] progress: 1/1 step: 836, time: 0.474, speed: 2.110 steps/s
	current lr: 0.0000084
	lm_loss: 3.4647, ppl: 31.9667, loss: 3.4647
[train][1] progress: 1/1 step: 837, time: 0.487, speed: 2.055 steps/s
	current lr: 0.0000084
	lm_loss: 3.5851, ppl: 36.0564, loss: 3.5851
[train][1] progress: 1/1 step: 838, time: 0.470, speed: 2.130 steps/s
	current lr: 0.0000084
	lm_loss: 3.6841, ppl: 39.8083, loss: 3.6841
[train][1] progress: 1/1 step: 839, time: 0.477, speed: 2.094 steps/s
	current lr: 0.0000084
	lm_loss: 3.6667, ppl: 39.1209, loss: 3.6667
[train][1] progress: 1/1 step: 840, time: 0.476, speed: 2.103 steps/s
	current lr: 0.0000084
	lm_loss: 3.4969, ppl: 33.0124, loss: 3.4969
[train][1] progress: 1/1 step: 841, time: 0.472, speed: 2.118 steps/s
	current lr: 0.0000084
	lm_loss: 3.4813, ppl: 32.5028, loss: 3.4813
[train][1] progress: 1/1 step: 842, time: 0.480, speed: 2.083 steps/s
	current lr: 0.0000084
	lm_loss: 3.6100, ppl: 36.9673, loss: 3.6100
[train][1] progress: 1/1 step: 843, time: 0.474, speed: 2.111 steps/s
	current lr: 0.0000084
	lm_loss: 3.2321, ppl: 25.3332, loss: 3.2321
[train][1] progress: 1/1 step: 844, time: 0.471, speed: 2.123 steps/s
	current lr: 0.0000084
	lm_loss: 3.7229, ppl: 41.3839, loss: 3.7229
[train][1] progress: 1/1 step: 845, time: 0.481, speed: 2.080 steps/s
	current lr: 0.0000084
	lm_loss: 3.5746, ppl: 35.6790, loss: 3.5746
[train][1] progress: 1/1 step: 846, time: 0.476, speed: 2.100 steps/s
	current lr: 0.0000085
	lm_loss: 3.6949, ppl: 40.2397, loss: 3.6949
[train][1] progress: 1/1 step: 847, time: 0.400, speed: 2.499 steps/s
	current lr: 0.0000085
	lm_loss: 1.8400, ppl: 6.2968, loss: 1.8400
[train][1] progress: 1/1 step: 848, time: 0.481, speed: 2.078 steps/s
	current lr: 0.0000085
	lm_loss: 3.5798, ppl: 35.8673, loss: 3.5798
[train][1] progress: 1/1 step: 849, time: 0.474, speed: 2.108 steps/s
	current lr: 0.0000085
	lm_loss: 3.5818, ppl: 35.9382, loss: 3.5818
[train][1] progress: 1/1 step: 850, time: 0.467, speed: 2.142 steps/s
	current lr: 0.0000085
	lm_loss: 3.6845, ppl: 39.8240, loss: 3.6845
[train][1] progress: 1/1 step: 851, time: 0.469, speed: 2.133 steps/s
	current lr: 0.0000085
	lm_loss: 3.6370, ppl: 37.9771, loss: 3.6370
[train][1] progress: 1/1 step: 852, time: 0.480, speed: 2.084 steps/s
	current lr: 0.0000085
	lm_loss: 3.5738, ppl: 35.6511, loss: 3.5738
[train][1] progress: 1/1 step: 853, time: 0.478, speed: 2.091 steps/s
	current lr: 0.0000085
	lm_loss: 3.4854, ppl: 32.6339, loss: 3.4854
[train][1] progress: 1/1 step: 854, time: 0.480, speed: 2.082 steps/s
	current lr: 0.0000085
	lm_loss: 3.6967, ppl: 40.3158, loss: 3.6967
[train][1] progress: 1/1 step: 855, time: 0.480, speed: 2.083 steps/s
	current lr: 0.0000086
	lm_loss: 3.5914, ppl: 36.2832, loss: 3.5914
[train][1] progress: 1/1 step: 856, time: 0.481, speed: 2.081 steps/s
	current lr: 0.0000086
	lm_loss: 3.6185, ppl: 37.2815, loss: 3.6185
[train][1] progress: 1/1 step: 857, time: 0.470, speed: 2.126 steps/s
	current lr: 0.0000086
	lm_loss: 3.5401, ppl: 34.4714, loss: 3.5401
[train][1] progress: 1/1 step: 858, time: 0.475, speed: 2.105 steps/s
	current lr: 0.0000086
	lm_loss: 3.3775, ppl: 29.2988, loss: 3.3775
[train][1] progress: 1/1 step: 859, time: 0.470, speed: 2.128 steps/s
	current lr: 0.0000086
	lm_loss: 3.4370, ppl: 31.0949, loss: 3.4370
[train][1] progress: 1/1 step: 860, time: 0.488, speed: 2.047 steps/s
	current lr: 0.0000086
	lm_loss: 3.1055, ppl: 22.3208, loss: 3.1055
[train][1] progress: 1/1 step: 861, time: 0.470, speed: 2.127 steps/s
	current lr: 0.0000086
	lm_loss: 3.6736, ppl: 39.3918, loss: 3.6736
[train][1] progress: 1/1 step: 862, time: 0.472, speed: 2.118 steps/s
	current lr: 0.0000086
	lm_loss: 3.5563, ppl: 35.0316, loss: 3.5563
[train][1] progress: 1/1 step: 863, time: 0.470, speed: 2.126 steps/s
	current lr: 0.0000086
	lm_loss: 3.5573, ppl: 35.0667, loss: 3.5573
[train][1] progress: 1/1 step: 864, time: 0.468, speed: 2.134 steps/s
	current lr: 0.0000086
	lm_loss: 3.6053, ppl: 36.7939, loss: 3.6053
[train][1] progress: 1/1 step: 865, time: 0.472, speed: 2.121 steps/s
	current lr: 0.0000086
	lm_loss: 3.5271, ppl: 34.0243, loss: 3.5271
[train][1] progress: 1/1 step: 866, time: 0.472, speed: 2.117 steps/s
	current lr: 0.0000087
	lm_loss: 3.4583, ppl: 31.7634, loss: 3.4583
[train][1] progress: 1/1 step: 867, time: 0.480, speed: 2.082 steps/s
	current lr: 0.0000087
	lm_loss: 3.5988, ppl: 36.5550, loss: 3.5988
[train][1] progress: 1/1 step: 868, time: 0.472, speed: 2.117 steps/s
	current lr: 0.0000087
	lm_loss: 3.8561, ppl: 47.2824, loss: 3.8561
[train][1] progress: 1/1 step: 869, time: 0.476, speed: 2.102 steps/s
	current lr: 0.0000087
	lm_loss: 3.4387, ppl: 31.1460, loss: 3.4387
[train][1] progress: 1/1 step: 870, time: 0.473, speed: 2.114 steps/s
	current lr: 0.0000087
	lm_loss: 3.5917, ppl: 36.2950, loss: 3.5917
[train][1] progress: 1/1 step: 871, time: 0.483, speed: 2.071 steps/s
	current lr: 0.0000087
	lm_loss: 3.5132, ppl: 33.5556, loss: 3.5132
[train][1] progress: 1/1 step: 872, time: 0.471, speed: 2.125 steps/s
	current lr: 0.0000087
	lm_loss: 3.5553, ppl: 34.9981, loss: 3.5553
[train][1] progress: 1/1 step: 873, time: 0.476, speed: 2.102 steps/s
	current lr: 0.0000087
	lm_loss: 3.3058, ppl: 27.2698, loss: 3.3058
[train][1] progress: 1/1 step: 874, time: 0.478, speed: 2.094 steps/s
	current lr: 0.0000087
	lm_loss: 3.4064, ppl: 30.1552, loss: 3.4064
[train][1] progress: 1/1 step: 875, time: 0.482, speed: 2.074 steps/s
	current lr: 0.0000088
	lm_loss: 3.5161, ppl: 33.6544, loss: 3.5161
[train][1] progress: 1/1 step: 876, time: 0.473, speed: 2.112 steps/s
	current lr: 0.0000088
	lm_loss: 3.5593, ppl: 35.1381, loss: 3.5593
[train][1] progress: 1/1 step: 877, time: 0.473, speed: 2.114 steps/s
	current lr: 0.0000088
	lm_loss: 3.6392, ppl: 38.0599, loss: 3.6392
[train][1] progress: 1/1 step: 878, time: 0.481, speed: 2.077 steps/s
	current lr: 0.0000088
	lm_loss: 2.9412, ppl: 18.9387, loss: 2.9412
[train][1] progress: 1/1 step: 879, time: 0.480, speed: 2.085 steps/s
	current lr: 0.0000088
	lm_loss: 3.5786, ppl: 35.8224, loss: 3.5786
[train][1] progress: 1/1 step: 880, time: 0.472, speed: 2.121 steps/s
	current lr: 0.0000088
	lm_loss: 3.5688, ppl: 35.4735, loss: 3.5688
[train][1] progress: 1/1 step: 881, time: 0.472, speed: 2.119 steps/s
	current lr: 0.0000088
	lm_loss: 3.6103, ppl: 36.9778, loss: 3.6103
[train][1] progress: 1/1 step: 882, time: 0.469, speed: 2.133 steps/s
	current lr: 0.0000088
	lm_loss: 3.6673, ppl: 39.1475, loss: 3.6673
[train][1] progress: 1/1 step: 883, time: 0.473, speed: 2.114 steps/s
	current lr: 0.0000088
	lm_loss: 3.4401, ppl: 31.1909, loss: 3.4401
[train][1] progress: 1/1 step: 884, time: 0.471, speed: 2.123 steps/s
	current lr: 0.0000088
	lm_loss: 3.5732, ppl: 35.6310, loss: 3.5732
[train][1] progress: 1/1 step: 885, time: 0.483, speed: 2.072 steps/s
	current lr: 0.0000089
	lm_loss: 3.6785, ppl: 39.5860, loss: 3.6785
[train][1] progress: 1/1 step: 886, time: 0.474, speed: 2.110 steps/s
	current lr: 0.0000089
	lm_loss: 3.4926, ppl: 32.8711, loss: 3.4926
[train][1] progress: 1/1 step: 887, time: 0.469, speed: 2.132 steps/s
	current lr: 0.0000089
	lm_loss: 3.6351, ppl: 37.9069, loss: 3.6351
[train][1] progress: 1/1 step: 888, time: 0.484, speed: 2.067 steps/s
	current lr: 0.0000089
	lm_loss: 3.7938, ppl: 44.4247, loss: 3.7938
[train][1] progress: 1/1 step: 889, time: 0.474, speed: 2.111 steps/s
	current lr: 0.0000089
	lm_loss: 3.5482, ppl: 34.7509, loss: 3.5482
[train][1] progress: 1/1 step: 890, time: 0.485, speed: 2.062 steps/s
	current lr: 0.0000089
	lm_loss: 3.5300, ppl: 34.1256, loss: 3.5300
[train][1] progress: 1/1 step: 891, time: 0.477, speed: 2.098 steps/s
	current lr: 0.0000089
	lm_loss: 3.6169, ppl: 37.2217, loss: 3.6169
[train][1] progress: 1/1 step: 892, time: 0.476, speed: 2.103 steps/s
	current lr: 0.0000089
	lm_loss: 3.6213, ppl: 37.3866, loss: 3.6213
[train][1] progress: 1/1 step: 893, time: 0.478, speed: 2.092 steps/s
	current lr: 0.0000089
	lm_loss: 3.3992, ppl: 29.9396, loss: 3.3992
[train][1] progress: 1/1 step: 894, time: 0.475, speed: 2.107 steps/s
	current lr: 0.0000089
	lm_loss: 3.4024, ppl: 30.0364, loss: 3.4024
[train][1] progress: 1/1 step: 895, time: 0.481, speed: 2.079 steps/s
	current lr: 0.0000089
	lm_loss: 3.7088, ppl: 40.8061, loss: 3.7088
[train][1] progress: 1/1 step: 896, time: 0.473, speed: 2.113 steps/s
	current lr: 0.0000090
	lm_loss: 3.5286, ppl: 34.0747, loss: 3.5286
[train][1] progress: 1/1 step: 897, time: 0.472, speed: 2.119 steps/s
	current lr: 0.0000090
	lm_loss: 3.5902, ppl: 36.2424, loss: 3.5902
[train][1] progress: 1/1 step: 898, time: 0.470, speed: 2.129 steps/s
	current lr: 0.0000090
	lm_loss: 3.6015, ppl: 36.6546, loss: 3.6015
[train][1] progress: 1/1 step: 899, time: 0.479, speed: 2.086 steps/s
	current lr: 0.0000090
	lm_loss: 3.5819, ppl: 35.9428, loss: 3.5819
[train][1] progress: 1/1 step: 900, time: 0.481, speed: 2.080 steps/s
	current lr: 0.0000090
	lm_loss: 3.7924, ppl: 44.3617, loss: 3.7924
[train][1] progress: 1/1 step: 901, time: 0.476, speed: 2.100 steps/s
	current lr: 0.0000090
	lm_loss: 3.5817, ppl: 35.9355, loss: 3.5817
[train][1] progress: 1/1 step: 902, time: 0.470, speed: 2.129 steps/s
	current lr: 0.0000090
	lm_loss: 3.5087, ppl: 33.4036, loss: 3.5087
[train][1] progress: 1/1 step: 903, time: 0.473, speed: 2.113 steps/s
	current lr: 0.0000090
	lm_loss: 3.6165, ppl: 37.2085, loss: 3.6165
[train][1] progress: 1/1 step: 904, time: 0.469, speed: 2.132 steps/s
	current lr: 0.0000090
	lm_loss: 3.6606, ppl: 38.8846, loss: 3.6606
[train][1] progress: 1/1 step: 905, time: 0.476, speed: 2.102 steps/s
	current lr: 0.0000090
	lm_loss: 3.5534, ppl: 34.9316, loss: 3.5534
[train][1] progress: 1/1 step: 906, time: 0.477, speed: 2.095 steps/s
	current lr: 0.0000091
	lm_loss: 3.6639, ppl: 39.0129, loss: 3.6639
[train][1] progress: 1/1 step: 907, time: 0.469, speed: 2.131 steps/s
	current lr: 0.0000091
	lm_loss: 3.6052, ppl: 36.7905, loss: 3.6052
[train][1] progress: 1/1 step: 908, time: 0.474, speed: 2.112 steps/s
	current lr: 0.0000091
	lm_loss: 3.4601, ppl: 31.8208, loss: 3.4601
[train][1] progress: 1/1 step: 909, time: 0.482, speed: 2.075 steps/s
	current lr: 0.0000091
	lm_loss: 3.6732, ppl: 39.3793, loss: 3.6732
[train][1] progress: 1/1 step: 910, time: 0.474, speed: 2.109 steps/s
	current lr: 0.0000091
	lm_loss: 3.5195, ppl: 33.7662, loss: 3.5195
[train][1] progress: 1/1 step: 911, time: 0.469, speed: 2.134 steps/s
	current lr: 0.0000091
	lm_loss: 3.6162, ppl: 37.1969, loss: 3.6162
[train][1] progress: 1/1 step: 912, time: 0.471, speed: 2.124 steps/s
	current lr: 0.0000091
	lm_loss: 3.6710, ppl: 39.2920, loss: 3.6710
[train][1] progress: 1/1 step: 913, time: 0.480, speed: 2.085 steps/s
	current lr: 0.0000091
	lm_loss: 3.5326, ppl: 34.2144, loss: 3.5326
[train][1] progress: 1/1 step: 914, time: 0.476, speed: 2.101 steps/s
	current lr: 0.0000091
	lm_loss: 3.6142, ppl: 37.1228, loss: 3.6142
[train][1] progress: 1/1 step: 915, time: 0.475, speed: 2.105 steps/s
	current lr: 0.0000092
	lm_loss: 3.5685, ppl: 35.4620, loss: 3.5685
[train][1] progress: 1/1 step: 916, time: 0.479, speed: 2.088 steps/s
	current lr: 0.0000092
	lm_loss: 3.5218, ppl: 33.8461, loss: 3.5218
[train][1] progress: 1/1 step: 917, time: 0.478, speed: 2.091 steps/s
	current lr: 0.0000092
	lm_loss: 3.5493, ppl: 34.7888, loss: 3.5493
[train][1] progress: 1/1 step: 918, time: 0.470, speed: 2.126 steps/s
	current lr: 0.0000092
	lm_loss: 3.4157, ppl: 30.4397, loss: 3.4157
[train][1] progress: 1/1 step: 919, time: 0.474, speed: 2.110 steps/s
	current lr: 0.0000092
	lm_loss: 3.3553, ppl: 28.6531, loss: 3.3553
[train][1] progress: 1/1 step: 920, time: 0.476, speed: 2.103 steps/s
	current lr: 0.0000092
	lm_loss: 3.3242, ppl: 27.7766, loss: 3.3242
[train][1] progress: 1/1 step: 921, time: 0.481, speed: 2.079 steps/s
	current lr: 0.0000092
	lm_loss: 3.6236, ppl: 37.4722, loss: 3.6236
[train][1] progress: 1/1 step: 922, time: 0.467, speed: 2.142 steps/s
	current lr: 0.0000092
	lm_loss: 3.5925, ppl: 36.3262, loss: 3.5925
[train][1] progress: 1/1 step: 923, time: 0.471, speed: 2.122 steps/s
	current lr: 0.0000092
	lm_loss: 3.6643, ppl: 39.0270, loss: 3.6643
[train][1] progress: 1/1 step: 924, time: 0.472, speed: 2.118 steps/s
	current lr: 0.0000092
	lm_loss: 3.5200, ppl: 33.7858, loss: 3.5200
[train][1] progress: 1/1 step: 925, time: 0.474, speed: 2.108 steps/s
	current lr: 0.0000092
	lm_loss: 3.6842, ppl: 39.8135, loss: 3.6842
[train][1] progress: 1/1 step: 926, time: 0.491, speed: 2.035 steps/s
	current lr: 0.0000093
	lm_loss: 3.5451, ppl: 34.6448, loss: 3.5451
[train][1] progress: 1/1 step: 927, time: 0.480, speed: 2.085 steps/s
	current lr: 0.0000093
	lm_loss: 3.3492, ppl: 28.4804, loss: 3.3492
[train][1] progress: 1/1 step: 928, time: 0.469, speed: 2.130 steps/s
	current lr: 0.0000093
	lm_loss: 3.5580, ppl: 35.0921, loss: 3.5580
[train][1] progress: 1/1 step: 929, time: 0.470, speed: 2.129 steps/s
	current lr: 0.0000093
	lm_loss: 3.4435, ppl: 31.2961, loss: 3.4435
[train][1] progress: 1/1 step: 930, time: 0.474, speed: 2.110 steps/s
	current lr: 0.0000093
	lm_loss: 3.6173, ppl: 37.2376, loss: 3.6173
[train][1] progress: 1/1 step: 931, time: 0.466, speed: 2.144 steps/s
	current lr: 0.0000093
	lm_loss: 3.6241, ppl: 37.4905, loss: 3.6241
[train][1] progress: 1/1 step: 932, time: 0.479, speed: 2.089 steps/s
	current lr: 0.0000093
	lm_loss: 3.0783, ppl: 21.7209, loss: 3.0783
[train][1] progress: 1/1 step: 933, time: 0.482, speed: 2.073 steps/s
	current lr: 0.0000093
	lm_loss: 3.6442, ppl: 38.2533, loss: 3.6442
[train][1] progress: 1/1 step: 934, time: 0.479, speed: 2.090 steps/s
	current lr: 0.0000093
	lm_loss: 3.6335, ppl: 37.8468, loss: 3.6335
[train][1] progress: 1/1 step: 935, time: 0.471, speed: 2.125 steps/s
	current lr: 0.0000094
	lm_loss: 3.5543, ppl: 34.9619, loss: 3.5543
[train][1] progress: 1/1 step: 936, time: 0.470, speed: 2.129 steps/s
	current lr: 0.0000094
	lm_loss: 3.5242, ppl: 33.9262, loss: 3.5242
[train][1] progress: 1/1 step: 937, time: 0.476, speed: 2.101 steps/s
	current lr: 0.0000094
	lm_loss: 3.4367, ppl: 31.0854, loss: 3.4367
[train][1] progress: 1/1 step: 938, time: 0.484, speed: 2.068 steps/s
	current lr: 0.0000094
	lm_loss: 3.5763, ppl: 35.7416, loss: 3.5763
[train][1] progress: 1/1 step: 939, time: 0.475, speed: 2.107 steps/s
	current lr: 0.0000094
	lm_loss: 3.3615, ppl: 28.8327, loss: 3.3615
[train][1] progress: 1/1 step: 940, time: 0.482, speed: 2.073 steps/s
	current lr: 0.0000094
	lm_loss: 3.4760, ppl: 32.3307, loss: 3.4760
[train][1] progress: 1/1 step: 941, time: 0.469, speed: 2.133 steps/s
	current lr: 0.0000094
	lm_loss: 3.6289, ppl: 37.6718, loss: 3.6289
[train][1] progress: 1/1 step: 942, time: 0.479, speed: 2.088 steps/s
	current lr: 0.0000094
	lm_loss: 3.7319, ppl: 41.7584, loss: 3.7319
[train][1] progress: 1/1 step: 943, time: 0.475, speed: 2.107 steps/s
	current lr: 0.0000094
	lm_loss: 3.3013, ppl: 27.1489, loss: 3.3013
[train][1] progress: 1/1 step: 944, time: 0.468, speed: 2.135 steps/s
	current lr: 0.0000094
	lm_loss: 3.6523, ppl: 38.5632, loss: 3.6523
[train][1] progress: 1/1 step: 945, time: 0.480, speed: 2.083 steps/s
	current lr: 0.0000095
	lm_loss: 3.5936, ppl: 36.3651, loss: 3.5936
[train][1] progress: 1/1 step: 946, time: 0.471, speed: 2.122 steps/s
	current lr: 0.0000095
	lm_loss: 3.6041, ppl: 36.7474, loss: 3.6041
[train][1] progress: 1/1 step: 947, time: 0.472, speed: 2.121 steps/s
	current lr: 0.0000095
	lm_loss: 3.5948, ppl: 36.4072, loss: 3.5948
[train][1] progress: 1/1 step: 948, time: 0.474, speed: 2.111 steps/s
	current lr: 0.0000095
	lm_loss: 3.4018, ppl: 30.0174, loss: 3.4018
[train][1] progress: 1/1 step: 949, time: 0.482, speed: 2.076 steps/s
	current lr: 0.0000095
	lm_loss: 3.4910, ppl: 32.8186, loss: 3.4910
[train][1] progress: 1/1 step: 950, time: 0.476, speed: 2.100 steps/s
	current lr: 0.0000095
	lm_loss: 3.3571, ppl: 28.7069, loss: 3.3571
[train][1] progress: 1/1 step: 951, time: 0.471, speed: 2.121 steps/s
	current lr: 0.0000095
	lm_loss: 3.6696, ppl: 39.2366, loss: 3.6696
[train][1] progress: 1/1 step: 952, time: 0.482, speed: 2.074 steps/s
	current lr: 0.0000095
	lm_loss: 3.6069, ppl: 36.8527, loss: 3.6069
[train][1] progress: 1/1 step: 953, time: 0.473, speed: 2.113 steps/s
	current lr: 0.0000095
	lm_loss: 3.5744, ppl: 35.6735, loss: 3.5744
[train][1] progress: 1/1 step: 954, time: 0.473, speed: 2.112 steps/s
	current lr: 0.0000095
	lm_loss: 3.5034, ppl: 33.2286, loss: 3.5034
[train][1] progress: 1/1 step: 955, time: 0.474, speed: 2.108 steps/s
	current lr: 0.0000095
	lm_loss: 3.5333, ppl: 34.2381, loss: 3.5333
[train][1] progress: 1/1 step: 956, time: 0.471, speed: 2.121 steps/s
	current lr: 0.0000096
	lm_loss: 3.6611, ppl: 38.9039, loss: 3.6611
[train][1] progress: 1/1 step: 957, time: 0.473, speed: 2.114 steps/s
	current lr: 0.0000096
	lm_loss: 3.4680, ppl: 32.0737, loss: 3.4680
[train][1] progress: 1/1 step: 958, time: 0.472, speed: 2.118 steps/s
	current lr: 0.0000096
	lm_loss: 3.3806, ppl: 29.3873, loss: 3.3806
[train][1] progress: 1/1 step: 959, time: 0.470, speed: 2.126 steps/s
	current lr: 0.0000096
	lm_loss: 3.6891, ppl: 40.0084, loss: 3.6891
[train][1] progress: 1/1 step: 960, time: 0.476, speed: 2.101 steps/s
	current lr: 0.0000096
	lm_loss: 3.6005, ppl: 36.6175, loss: 3.6005
[train][1] progress: 1/1 step: 961, time: 0.474, speed: 2.109 steps/s
	current lr: 0.0000096
	lm_loss: 3.5849, ppl: 36.0497, loss: 3.5849
[train][1] progress: 1/1 step: 962, time: 0.469, speed: 2.131 steps/s
	current lr: 0.0000096
	lm_loss: 3.5008, ppl: 33.1415, loss: 3.5008
[train][1] progress: 1/1 step: 963, time: 0.481, speed: 2.081 steps/s
	current lr: 0.0000096
	lm_loss: 3.6255, ppl: 37.5427, loss: 3.6255
[train][1] progress: 1/1 step: 964, time: 0.469, speed: 2.131 steps/s
	current lr: 0.0000096
	lm_loss: 3.6480, ppl: 38.3964, loss: 3.6480
[train][1] progress: 1/1 step: 965, time: 0.485, speed: 2.062 steps/s
	current lr: 0.0000096
	lm_loss: 3.7358, ppl: 41.9211, loss: 3.7358
[train][1] progress: 1/1 step: 966, time: 0.482, speed: 2.073 steps/s
	current lr: 0.0000097
	lm_loss: 3.4792, ppl: 32.4352, loss: 3.4792
[train][1] progress: 1/1 step: 967, time: 0.476, speed: 2.102 steps/s
	current lr: 0.0000097
	lm_loss: 3.6117, ppl: 37.0293, loss: 3.6117
[train][1] progress: 1/1 step: 968, time: 0.473, speed: 2.114 steps/s
	current lr: 0.0000097
	lm_loss: 3.4375, ppl: 31.1105, loss: 3.4375
[train][1] progress: 1/1 step: 969, time: 0.479, speed: 2.090 steps/s
	current lr: 0.0000097
	lm_loss: 3.7260, ppl: 41.5147, loss: 3.7260
[train][1] progress: 1/1 step: 970, time: 0.479, speed: 2.088 steps/s
	current lr: 0.0000097
	lm_loss: 3.6647, ppl: 39.0446, loss: 3.6647
[train][1] progress: 1/1 step: 971, time: 0.476, speed: 2.101 steps/s
	current lr: 0.0000097
	lm_loss: 3.3056, ppl: 27.2651, loss: 3.3056
[train][1] progress: 1/1 step: 972, time: 0.483, speed: 2.071 steps/s
	current lr: 0.0000097
	lm_loss: 3.7674, ppl: 43.2657, loss: 3.7674
[train][1] progress: 1/1 step: 973, time: 0.480, speed: 2.085 steps/s
	current lr: 0.0000097
	lm_loss: 3.6200, ppl: 37.3371, loss: 3.6200
[train][1] progress: 1/1 step: 974, time: 0.474, speed: 2.109 steps/s
	current lr: 0.0000097
	lm_loss: 3.5532, ppl: 34.9252, loss: 3.5532
[train][1] progress: 1/1 step: 975, time: 0.470, speed: 2.130 steps/s
	current lr: 0.0000098
	lm_loss: 3.5134, ppl: 33.5636, loss: 3.5134
[train][1] progress: 1/1 step: 976, time: 0.469, speed: 2.130 steps/s
	current lr: 0.0000098
	lm_loss: 3.4674, ppl: 32.0548, loss: 3.4674
[train][1] progress: 1/1 step: 977, time: 0.472, speed: 2.119 steps/s
	current lr: 0.0000098
	lm_loss: 3.5531, ppl: 34.9198, loss: 3.5531
[train][1] progress: 1/1 step: 978, time: 0.479, speed: 2.087 steps/s
	current lr: 0.0000098
	lm_loss: 3.4809, ppl: 32.4875, loss: 3.4809
[train][1] progress: 1/1 step: 979, time: 0.487, speed: 2.055 steps/s
	current lr: 0.0000098
	lm_loss: 3.5871, ppl: 36.1287, loss: 3.5871
[train][1] progress: 1/1 step: 980, time: 0.481, speed: 2.081 steps/s
	current lr: 0.0000098
	lm_loss: 3.5864, ppl: 36.1026, loss: 3.5864
[train][1] progress: 1/1 step: 981, time: 0.487, speed: 2.055 steps/s
	current lr: 0.0000098
	lm_loss: 3.3850, ppl: 29.5170, loss: 3.3850
[train][1] progress: 1/1 step: 982, time: 0.480, speed: 2.084 steps/s
	current lr: 0.0000098
	lm_loss: 3.6465, ppl: 38.3406, loss: 3.6465
[train][1] progress: 1/1 step: 983, time: 0.469, speed: 2.134 steps/s
	current lr: 0.0000098
	lm_loss: 3.5823, ppl: 35.9544, loss: 3.5823
[train][1] progress: 1/1 step: 984, time: 0.485, speed: 2.062 steps/s
	current lr: 0.0000098
	lm_loss: 3.6248, ppl: 37.5188, loss: 3.6248
[train][1] progress: 1/1 step: 985, time: 0.481, speed: 2.078 steps/s
	current lr: 0.0000098
	lm_loss: 3.5185, ppl: 33.7337, loss: 3.5185
[train][1] progress: 1/1 step: 986, time: 0.480, speed: 2.082 steps/s
	current lr: 0.0000099
	lm_loss: 3.6551, ppl: 38.6700, loss: 3.6551
[train][1] progress: 1/1 step: 987, time: 0.481, speed: 2.080 steps/s
	current lr: 0.0000099
	lm_loss: 3.6267, ppl: 37.5882, loss: 3.6267
[train][1] progress: 1/1 step: 988, time: 0.475, speed: 2.106 steps/s
	current lr: 0.0000099
	lm_loss: 3.6623, ppl: 38.9510, loss: 3.6623
[train][1] progress: 1/1 step: 989, time: 0.473, speed: 2.116 steps/s
	current lr: 0.0000099
	lm_loss: 3.5986, ppl: 36.5476, loss: 3.5986
[train][1] progress: 1/1 step: 990, time: 0.471, speed: 2.122 steps/s
	current lr: 0.0000099
	lm_loss: 3.5830, ppl: 35.9813, loss: 3.5830
[train][1] progress: 1/1 step: 991, time: 0.472, speed: 2.119 steps/s
	current lr: 0.0000099
	lm_loss: 3.6109, ppl: 37.0010, loss: 3.6109
[train][1] progress: 1/1 step: 992, time: 0.480, speed: 2.082 steps/s
	current lr: 0.0000099
	lm_loss: 3.4788, ppl: 32.4203, loss: 3.4788
[train][1] progress: 1/1 step: 993, time: 0.466, speed: 2.147 steps/s
	current lr: 0.0000099
	lm_loss: 3.6404, ppl: 38.1052, loss: 3.6404
[train][1] progress: 1/1 step: 994, time: 0.468, speed: 2.137 steps/s
	current lr: 0.0000099
	lm_loss: 3.6253, ppl: 37.5376, loss: 3.6253
[train][1] progress: 1/1 step: 995, time: 0.473, speed: 2.114 steps/s
	current lr: 0.0000099
	lm_loss: 3.5995, ppl: 36.5806, loss: 3.5995
[train][1] progress: 1/1 step: 996, time: 0.484, speed: 2.066 steps/s
	current lr: 0.0000100
	lm_loss: 3.5366, ppl: 34.3488, loss: 3.5366
[train][1] progress: 1/1 step: 997, time: 0.480, speed: 2.082 steps/s
	current lr: 0.0000100
	lm_loss: 3.8318, ppl: 46.1443, loss: 3.8318
[train][1] progress: 1/1 step: 998, time: 0.480, speed: 2.081 steps/s
	current lr: 0.0000100
	lm_loss: 3.6416, ppl: 38.1540, loss: 3.6416
[train][1] progress: 1/1 step: 999, time: 0.481, speed: 2.079 steps/s
	current lr: 0.0000100
	lm_loss: 3.6516, ppl: 38.5369, loss: 3.6516
[train][1] progress: 1/1 step: 1000, time: 0.479, speed: 2.089 steps/s
	current lr: 0.0000100
	lm_loss: 3.6426, ppl: 38.1896, loss: 3.6426
================================================================================
Evaluation:
	step 1:lm_loss: 3.5996, ppl: 36.5853, loss: 3.5996
	step 2:lm_loss: 3.5487, ppl: 34.7664, loss: 3.5479
	step 3:lm_loss: 3.5902, ppl: 36.2409, loss: 3.5982
	step 4:lm_loss: 3.5313, ppl: 34.1692, loss: 3.5458
	step 5:lm_loss: 3.5101, ppl: 33.4526, loss: 3.5260
	step 6:lm_loss: 3.5160, ppl: 33.6485, loss: 3.5293
	step 7:lm_loss: 3.5161, ppl: 33.6514, loss: 3.5276
	step 8:lm_loss: 3.4954, ppl: 32.9646, loss: 3.5148
	step 9:lm_loss: 3.4987, ppl: 33.0737, loss: 3.5160
	step 10:lm_loss: 3.5095, ppl: 33.4331, loss: 3.5270
	step 11:lm_loss: 3.4908, ppl: 32.8118, loss: 3.5142
	step 12:lm_loss: 3.4964, ppl: 32.9971, loss: 3.5184
	step 13:lm_loss: 3.4918, ppl: 32.8438, loss: 3.5140
	step 14:lm_loss: 3.4869, ppl: 32.6848, loss: 3.5076
	step 15:lm_loss: 3.4756, ppl: 32.3187, loss: 3.4982
	step 16:lm_loss: 3.4423, ppl: 31.2599, loss: 3.4761
	step 17:lm_loss: 3.4459, ppl: 31.3727, loss: 3.4781
	step 18:lm_loss: 3.4482, ppl: 31.4434, loss: 3.4785
	step 19:lm_loss: 3.4478, ppl: 31.4322, loss: 3.4769
	step 20:lm_loss: 3.4550, ppl: 31.6582, loss: 3.4850
	step 21:lm_loss: 3.4587, ppl: 31.7743, loss: 3.4872
	step 22:lm_loss: 3.4530, ppl: 31.5962, loss: 3.4810
	step 23:lm_loss: 3.4499, ppl: 31.4971, loss: 3.4765
	step 24:lm_loss: 3.4485, ppl: 31.4521, loss: 3.4738
	step 25:lm_loss: 3.4610, ppl: 31.8484, loss: 3.4925
	step 26:lm_loss: 3.4716, ppl: 32.1887, loss: 3.5066
	step 27:lm_loss: 3.4713, ppl: 32.1790, loss: 3.5050
	step 28:lm_loss: 3.4753, ppl: 32.3079, loss: 3.5070
	step 29:lm_loss: 3.4772, ppl: 32.3687, loss: 3.5080
	step 30:lm_loss: 3.4801, ppl: 32.4634, loss: 3.5098
	step 31:lm_loss: 3.4801, ppl: 32.4618, loss: 3.5090
	step 32:lm_loss: 3.4793, ppl: 32.4364, loss: 3.5075
	step 33:lm_loss: 3.4803, ppl: 32.4680, loss: 3.5075
	step 34:lm_loss: 3.4827, ppl: 32.5483, loss: 3.5097
	step 35:lm_loss: 3.4801, ppl: 32.4625, loss: 3.5065
	step 36:lm_loss: 3.4801, ppl: 32.4630, loss: 3.5058
	step 37:lm_loss: 3.4816, ppl: 32.5118, loss: 3.5068
	step 38:lm_loss: 3.4797, ppl: 32.4512, loss: 3.5045
	step 39:lm_loss: 3.4815, ppl: 32.5084, loss: 3.5059
	step 40:lm_loss: 3.4788, ppl: 32.4209, loss: 3.5030
	step 41:lm_loss: 3.4815, ppl: 32.5088, loss: 3.5056
	step 42:lm_loss: 3.4835, ppl: 32.5741, loss: 3.5075
	step 43:lm_loss: 3.4870, ppl: 32.6863, loss: 3.5109
	step 44:lm_loss: 3.4820, ppl: 32.5263, loss: 3.5063
	step 45:lm_loss: 3.4782, ppl: 32.4024, loss: 3.5034
	step 46:lm_loss: 3.4667, ppl: 32.0303, loss: 3.4960
	step 47:lm_loss: 3.4699, ppl: 32.1339, loss: 3.4987
	step 48:lm_loss: 3.4693, ppl: 32.1156, loss: 3.4978
	step 49:lm_loss: 3.4690, ppl: 32.1037, loss: 3.4969
	step 50:lm_loss: 3.4562, ppl: 31.6954, loss: 3.4897
	step 51:lm_loss: 3.4527, ppl: 31.5869, loss: 3.4865
	step 52:lm_loss: 3.4499, ppl: 31.4977, loss: 3.4844
	step 53:lm_loss: 3.4513, ppl: 31.5424, loss: 3.4853
	step 54:lm_loss: 3.4544, ppl: 31.6393, loss: 3.4883
	step 55:lm_loss: 3.4575, ppl: 31.7369, loss: 3.4913
	step 56:lm_loss: 3.4595, ppl: 31.8011, loss: 3.4932
	step 57:lm_loss: 3.4596, ppl: 31.8028, loss: 3.4927
	step 58:lm_loss: 3.4585, ppl: 31.7689, loss: 3.4908
	step 59:lm_loss: 3.4581, ppl: 31.7573, loss: 3.4899
	step 60:lm_loss: 3.4572, ppl: 31.7268, loss: 3.4886
	step 61:lm_loss: 3.4566, ppl: 31.7100, loss: 3.4878
	step 62:lm_loss: 3.4581, ppl: 31.7566, loss: 3.4888
	step 63:lm_loss: 3.4603, ppl: 31.8268, loss: 3.4906
	step 64:lm_loss: 3.4608, ppl: 31.8411, loss: 3.4906
	step 65:lm_loss: 3.4616, ppl: 31.8669, loss: 3.4908
	step 66:lm_loss: 3.4620, ppl: 31.8814, loss: 3.4908
	step 67:lm_loss: 3.4628, ppl: 31.9048, loss: 3.4911
	step 68:lm_loss: 3.4633, ppl: 31.9215, loss: 3.4912
	step 69:lm_loss: 3.4644, ppl: 31.9579, loss: 3.4918
	step 70:lm_loss: 3.4655, ppl: 31.9923, loss: 3.4925
	step 71:lm_loss: 3.4641, ppl: 31.9470, loss: 3.4904
	step 72:lm_loss: 3.4619, ppl: 31.8786, loss: 3.4885
	step 73:lm_loss: 3.4633, ppl: 31.9223, loss: 3.4897
	step 74:lm_loss: 3.4625, ppl: 31.8966, loss: 3.4882
	step 75:lm_loss: 3.4620, ppl: 31.8816, loss: 3.4876
	step 76:lm_loss: 3.4638, ppl: 31.9387, loss: 3.4895
	step 77:lm_loss: 3.4627, ppl: 31.9022, loss: 3.4885
	step 78:lm_loss: 3.4642, ppl: 31.9500, loss: 3.4900
	step 79:lm_loss: 3.4644, ppl: 31.9580, loss: 3.4899
	step 80:lm_loss: 3.4651, ppl: 31.9801, loss: 3.4902
	step 81:lm_loss: 3.4668, ppl: 32.0330, loss: 3.4917
	step 82:lm_loss: 3.4645, ppl: 31.9599, loss: 3.4898
	step 83:lm_loss: 3.4644, ppl: 31.9575, loss: 3.4894
	step 84:lm_loss: 3.4655, ppl: 31.9924, loss: 3.4903
	step 85:lm_loss: 3.4657, ppl: 31.9979, loss: 3.4902
	step 86:lm_loss: 3.4691, ppl: 32.1090, loss: 3.4958
	step 87:lm_loss: 3.4692, ppl: 32.1121, loss: 3.4956
	step 88:lm_loss: 3.4692, ppl: 32.1108, loss: 3.4952
	step 89:lm_loss: 3.4683, ppl: 32.0837, loss: 3.4942
	step 90:lm_loss: 3.4685, ppl: 32.0895, loss: 3.4941
	step 91:lm_loss: 3.4695, ppl: 32.1203, loss: 3.4949
	step 92:lm_loss: 3.4686, ppl: 32.0930, loss: 3.4941
	step 93:lm_loss: 3.4658, ppl: 32.0024, loss: 3.4916
	step 94:lm_loss: 3.4658, ppl: 32.0017, loss: 3.4913
	step 95:lm_loss: 3.4654, ppl: 31.9904, loss: 3.4908
	step 96:lm_loss: 3.4614, ppl: 31.8600, loss: 3.4880
	step 97:lm_loss: 3.4634, ppl: 31.9238, loss: 3.4905
	step 98:lm_loss: 3.4639, ppl: 31.9421, loss: 3.4909
	step 99:lm_loss: 3.4628, ppl: 31.9053, loss: 3.4898
	step 100:lm_loss: 3.4633, ppl: 31.9224, loss: 3.4901
	step 101:lm_loss: 3.4628, ppl: 31.9070, loss: 3.4895
	step 102:lm_loss: 3.4648, ppl: 31.9691, loss: 3.4917
	step 103:lm_loss: 3.4652, ppl: 31.9831, loss: 3.4919
	step 104:lm_loss: 3.4635, ppl: 31.9290, loss: 3.4903
	step 105:lm_loss: 3.4644, ppl: 31.9577, loss: 3.4910
	step 106:lm_loss: 3.4649, ppl: 31.9729, loss: 3.4913
	step 107:lm_loss: 3.4654, ppl: 31.9895, loss: 3.4916
	step 108:lm_loss: 3.4661, ppl: 32.0120, loss: 3.4921
	step 109:lm_loss: 3.4600, ppl: 31.8159, loss: 3.4887
	step 110:lm_loss: 3.4608, ppl: 31.8433, loss: 3.4895
	step 111:lm_loss: 3.4619, ppl: 31.8777, loss: 3.4904
	step 112:lm_loss: 3.4632, ppl: 31.9203, loss: 3.4917
	step 113:lm_loss: 3.4628, ppl: 31.9060, loss: 3.4910
	step 114:lm_loss: 3.4626, ppl: 31.8998, loss: 3.4906
	step 115:lm_loss: 3.4631, ppl: 31.9146, loss: 3.4908
	step 116:lm_loss: 3.4633, ppl: 31.9206, loss: 3.4907
	step 117:lm_loss: 3.4631, ppl: 31.9143, loss: 3.4904
	step 118:lm_loss: 3.4644, ppl: 31.9584, loss: 3.4917
	step 119:lm_loss: 3.4643, ppl: 31.9526, loss: 3.4912
	step 120:lm_loss: 3.4651, ppl: 31.9808, loss: 3.4918
	step 121:lm_loss: 3.4652, ppl: 31.9817, loss: 3.4917
	step 122:lm_loss: 3.4651, ppl: 31.9792, loss: 3.4913
	step 123:lm_loss: 3.4666, ppl: 32.0277, loss: 3.4929
	step 124:lm_loss: 3.4676, ppl: 32.0602, loss: 3.4935
	step 125:lm_loss: 3.4676, ppl: 32.0611, loss: 3.4933
	step 126:lm_loss: 3.4684, ppl: 32.0840, loss: 3.4938
	step 127:lm_loss: 3.4627, ppl: 31.9030, loss: 3.4909
	step 128:lm_loss: 3.4630, ppl: 31.9111, loss: 3.4909
	step 129:lm_loss: 3.4632, ppl: 31.9188, loss: 3.4910
	step 130:lm_loss: 3.4619, ppl: 31.8765, loss: 3.4898
	step 131:lm_loss: 3.4620, ppl: 31.8808, loss: 3.4898
	step 132:lm_loss: 3.4623, ppl: 31.8907, loss: 3.4898
	step 133:lm_loss: 3.4611, ppl: 31.8518, loss: 3.4888
	step 134:lm_loss: 3.4611, ppl: 31.8521, loss: 3.4885
	step 135:lm_loss: 3.4617, ppl: 31.8705, loss: 3.4889
	step 136:lm_loss: 3.4615, ppl: 31.8661, loss: 3.4885
	step 137:lm_loss: 3.4618, ppl: 31.8733, loss: 3.4886
	step 138:lm_loss: 3.4628, ppl: 31.9073, loss: 3.4896
	step 139:lm_loss: 3.4627, ppl: 31.9030, loss: 3.4893
	step 140:lm_loss: 3.4626, ppl: 31.9013, loss: 3.4891
	step 141:lm_loss: 3.4626, ppl: 31.8986, loss: 3.4888
	step 142:lm_loss: 3.4632, ppl: 31.9198, loss: 3.4893
	step 143:lm_loss: 3.4641, ppl: 31.9469, loss: 3.4901
	step 144:lm_loss: 3.4650, ppl: 31.9780, loss: 3.4910
	step 145:lm_loss: 3.4648, ppl: 31.9701, loss: 3.4907
	step 146:lm_loss: 3.4654, ppl: 31.9908, loss: 3.4911
	step 147:lm_loss: 3.4650, ppl: 31.9779, loss: 3.4907
	step 148:lm_loss: 3.4654, ppl: 31.9878, loss: 3.4908
	step 149:lm_loss: 3.4642, ppl: 31.9512, loss: 3.4899
	step 150:lm_loss: 3.4645, ppl: 31.9602, loss: 3.4900
	step 151:lm_loss: 3.4639, ppl: 31.9404, loss: 3.4892
	step 152:lm_loss: 3.4647, ppl: 31.9671, loss: 3.4900
	step 153:lm_loss: 3.4649, ppl: 31.9721, loss: 3.4900
	step 154:lm_loss: 3.4647, ppl: 31.9666, loss: 3.4897
	step 155:lm_loss: 3.4649, ppl: 31.9729, loss: 3.4898
	step 156:lm_loss: 3.4650, ppl: 31.9772, loss: 3.4898
	step 157:lm_loss: 3.4649, ppl: 31.9744, loss: 3.4896
	step 158:lm_loss: 3.4647, ppl: 31.9682, loss: 3.4892
	step 159:lm_loss: 3.4660, ppl: 32.0089, loss: 3.4908
	step 160:lm_loss: 3.4643, ppl: 31.9541, loss: 3.4897
	step 161:lm_loss: 3.4645, ppl: 31.9608, loss: 3.4898
	step 162:lm_loss: 3.4665, ppl: 32.0235, loss: 3.4926
	step 163:lm_loss: 3.4666, ppl: 32.0268, loss: 3.4925
	step 164:lm_loss: 3.4661, ppl: 32.0116, loss: 3.4920
	step 165:lm_loss: 3.4658, ppl: 32.0025, loss: 3.4917
	step 166:lm_loss: 3.4659, ppl: 32.0065, loss: 3.4916
	step 167:lm_loss: 3.4662, ppl: 32.0137, loss: 3.4917
	step 168:lm_loss: 3.4662, ppl: 32.0146, loss: 3.4916
	step 169:lm_loss: 3.4662, ppl: 32.0144, loss: 3.4915
	step 170:lm_loss: 3.4659, ppl: 32.0061, loss: 3.4911
	step 171:lm_loss: 3.4660, ppl: 32.0075, loss: 3.4910
	step 172:lm_loss: 3.4664, ppl: 32.0208, loss: 3.4913
	step 173:lm_loss: 3.4672, ppl: 32.0458, loss: 3.4919
	step 174:lm_loss: 3.4672, ppl: 32.0471, loss: 3.4918
	step 175:lm_loss: 3.4666, ppl: 32.0273, loss: 3.4911
	step 176:lm_loss: 3.4663, ppl: 32.0167, loss: 3.4905
	step 177:lm_loss: 3.4665, ppl: 32.0238, loss: 3.4906
	step 178:lm_loss: 3.4675, ppl: 32.0559, loss: 3.4918
	step 179:lm_loss: 3.4683, ppl: 32.0808, loss: 3.4926
	step 180:lm_loss: 3.4682, ppl: 32.0798, loss: 3.4924
	step 181:lm_loss: 3.4679, ppl: 32.0702, loss: 3.4920
	step 182:lm_loss: 3.4676, ppl: 32.0583, loss: 3.4915
	step 183:lm_loss: 3.4685, ppl: 32.0899, loss: 3.4926
	step 184:lm_loss: 3.4670, ppl: 32.0405, loss: 3.4914
	step 185:lm_loss: 3.4677, ppl: 32.0631, loss: 3.4921
	step 186:lm_loss: 3.4678, ppl: 32.0648, loss: 3.4920
	step 187:lm_loss: 3.4683, ppl: 32.0814, loss: 3.4924
	step 188:lm_loss: 3.4690, ppl: 32.1059, loss: 3.4931
	step 189:lm_loss: 3.4694, ppl: 32.1181, loss: 3.4933
	step 190:lm_loss: 3.4672, ppl: 32.0458, loss: 3.4928
	step 191:lm_loss: 3.4673, ppl: 32.0493, loss: 3.4928
	step 192:lm_loss: 3.4680, ppl: 32.0730, loss: 3.4935
	step 193:lm_loss: 3.4678, ppl: 32.0658, loss: 3.4931
	step 194:lm_loss: 3.4661, ppl: 32.0106, loss: 3.4921
	step 195:lm_loss: 3.4669, ppl: 32.0376, loss: 3.4927
	step 196:lm_loss: 3.4661, ppl: 32.0117, loss: 3.4922
	step 197:lm_loss: 3.4669, ppl: 32.0374, loss: 3.4930
	step 198:lm_loss: 3.4683, ppl: 32.0831, loss: 3.4951
	step 199:lm_loss: 3.4688, ppl: 32.0972, loss: 3.4954
	step 200:lm_loss: 3.4697, ppl: 32.1260, loss: 3.4961
	step 201:lm_loss: 3.4691, ppl: 32.1079, loss: 3.4955
	step 202:lm_loss: 3.4692, ppl: 32.1098, loss: 3.4955
	step 203:lm_loss: 3.4693, ppl: 32.1135, loss: 3.4954
	step 204:lm_loss: 3.4695, ppl: 32.1215, loss: 3.4956
	step 205:lm_loss: 3.4697, ppl: 32.1278, loss: 3.4957
	step 206:lm_loss: 3.4702, ppl: 32.1430, loss: 3.4960
	step 207:lm_loss: 3.4701, ppl: 32.1411, loss: 3.4958
	step 208:lm_loss: 3.4709, ppl: 32.1642, loss: 3.4965
	step 209:lm_loss: 3.4710, ppl: 32.1674, loss: 3.4965
	step 210:lm_loss: 3.4712, ppl: 32.1761, loss: 3.4967
	step 211:lm_loss: 3.4711, ppl: 32.1730, loss: 3.4964
	step 212:lm_loss: 3.4717, ppl: 32.1908, loss: 3.4970
	step 213:lm_loss: 3.4714, ppl: 32.1804, loss: 3.4967
	step 214:lm_loss: 3.4718, ppl: 32.1942, loss: 3.4971
	step 215:lm_loss: 3.4723, ppl: 32.2092, loss: 3.4975
	step 216:lm_loss: 3.4725, ppl: 32.2169, loss: 3.4976
	step 217:lm_loss: 3.4728, ppl: 32.2256, loss: 3.4978
	step 218:lm_loss: 3.4728, ppl: 32.2266, loss: 3.4977
	step 219:lm_loss: 3.4722, ppl: 32.2064, loss: 3.4972
	step 220:lm_loss: 3.4723, ppl: 32.2098, loss: 3.4972
	step 221:lm_loss: 3.4721, ppl: 32.2044, loss: 3.4969
	step 222:lm_loss: 3.4725, ppl: 32.2162, loss: 3.4972
	step 223:lm_loss: 3.4725, ppl: 32.2183, loss: 3.4972
	step 224:lm_loss: 3.4726, ppl: 32.2213, loss: 3.4971
	step 225:lm_loss: 3.4731, ppl: 32.2360, loss: 3.4974
	step 226:lm_loss: 3.4738, ppl: 32.2597, loss: 3.4981
	step 227:lm_loss: 3.4736, ppl: 32.2522, loss: 3.4978
	step 228:lm_loss: 3.4721, ppl: 32.2047, loss: 3.4966
	step 229:lm_loss: 3.4731, ppl: 32.2355, loss: 3.4977
	step 230:lm_loss: 3.4722, ppl: 32.2082, loss: 3.4970
	step 231:lm_loss: 3.4725, ppl: 32.2171, loss: 3.4972
	step 232:lm_loss: 3.4683, ppl: 32.0825, loss: 3.4953
	step 233:lm_loss: 3.4690, ppl: 32.1057, loss: 3.4960
	step 234:lm_loss: 3.4686, ppl: 32.0932, loss: 3.4956
	step 235:lm_loss: 3.4687, ppl: 32.0963, loss: 3.4956
	step 236:lm_loss: 3.4684, ppl: 32.0845, loss: 3.4949
	step 237:lm_loss: 3.4687, ppl: 32.0965, loss: 3.4953
	step 238:lm_loss: 3.4690, ppl: 32.1035, loss: 3.4954
	step 239:lm_loss: 3.4692, ppl: 32.1119, loss: 3.4955
	step 240:lm_loss: 3.4698, ppl: 32.1315, loss: 3.4963
	step 241:lm_loss: 3.4701, ppl: 32.1409, loss: 3.4965
	step 242:lm_loss: 3.4703, ppl: 32.1467, loss: 3.4966
	step 243:lm_loss: 3.4707, ppl: 32.1578, loss: 3.4969
	step 244:lm_loss: 3.4705, ppl: 32.1542, loss: 3.4967
	step 245:lm_loss: 3.4694, ppl: 32.1185, loss: 3.4959
	step 246:lm_loss: 3.4697, ppl: 32.1264, loss: 3.4960
	step 247:lm_loss: 3.4692, ppl: 32.1096, loss: 3.4956
	step 248:lm_loss: 3.4695, ppl: 32.1210, loss: 3.4959
	step 249:lm_loss: 3.4695, ppl: 32.1205, loss: 3.4958
	step 250:lm_loss: 3.4693, ppl: 32.1146, loss: 3.4955
	step 251:lm_loss: 3.4692, ppl: 32.1112, loss: 3.4952
	step 252:lm_loss: 3.4681, ppl: 32.0770, loss: 3.4944
	step 253:lm_loss: 3.4688, ppl: 32.0990, loss: 3.4952
	step 254:lm_loss: 3.4694, ppl: 32.1189, loss: 3.4959
	step 255:lm_loss: 3.4702, ppl: 32.1430, loss: 3.4966
	step 256:lm_loss: 3.4708, ppl: 32.1638, loss: 3.4973
	step 257:lm_loss: 3.4708, ppl: 32.1640, loss: 3.4972
	step 258:lm_loss: 3.4711, ppl: 32.1705, loss: 3.4973
	step 259:lm_loss: 3.4713, ppl: 32.1787, loss: 3.4974
	step 260:lm_loss: 3.4717, ppl: 32.1905, loss: 3.4977
	step 261:lm_loss: 3.4718, ppl: 32.1958, loss: 3.4978
	step 262:lm_loss: 3.4718, ppl: 32.1959, loss: 3.4977
	step 263:lm_loss: 3.4709, ppl: 32.1642, loss: 3.4968
	step 264:lm_loss: 3.4713, ppl: 32.1777, loss: 3.4971
	step 265:lm_loss: 3.4710, ppl: 32.1675, loss: 3.4968
	step 266:lm_loss: 3.4710, ppl: 32.1691, loss: 3.4967
	step 267:lm_loss: 3.4711, ppl: 32.1723, loss: 3.4967
	step 268:lm_loss: 3.4713, ppl: 32.1797, loss: 3.4968
	step 269:lm_loss: 3.4709, ppl: 32.1645, loss: 3.4965
	step 270:lm_loss: 3.4700, ppl: 32.1369, loss: 3.4957
	step 271:lm_loss: 3.4704, ppl: 32.1496, loss: 3.4960
	step 272:lm_loss: 3.4693, ppl: 32.1152, loss: 3.4951
	step 273:lm_loss: 3.4690, ppl: 32.1059, loss: 3.4947
	step 274:lm_loss: 3.4685, ppl: 32.0894, loss: 3.4943
	step 275:lm_loss: 3.4687, ppl: 32.0948, loss: 3.4944
	step 276:lm_loss: 3.4677, ppl: 32.0630, loss: 3.4937
	step 277:lm_loss: 3.4678, ppl: 32.0653, loss: 3.4937
	step 278:lm_loss: 3.4673, ppl: 32.0512, loss: 3.4933
	step 279:lm_loss: 3.4673, ppl: 32.0515, loss: 3.4932
	step 280:lm_loss: 3.4680, ppl: 32.0733, loss: 3.4939
	step 281:lm_loss: 3.4682, ppl: 32.0796, loss: 3.4940
	step 282:lm_loss: 3.4684, ppl: 32.0857, loss: 3.4941
	step 283:lm_loss: 3.4685, ppl: 32.0880, loss: 3.4941
	step 284:lm_loss: 3.4676, ppl: 32.0591, loss: 3.4933
	step 285:lm_loss: 3.4677, ppl: 32.0621, loss: 3.4934
	step 286:lm_loss: 3.4673, ppl: 32.0516, loss: 3.4928
	step 287:lm_loss: 3.4674, ppl: 32.0537, loss: 3.4928
	step 288:lm_loss: 3.4675, ppl: 32.0568, loss: 3.4928
	step 289:lm_loss: 3.4677, ppl: 32.0619, loss: 3.4929
	step 290:lm_loss: 3.4640, ppl: 31.9441, loss: 3.4914
	step 291:lm_loss: 3.4640, ppl: 31.9443, loss: 3.4913
	step 292:lm_loss: 3.4642, ppl: 31.9521, loss: 3.4915
	step 293:lm_loss: 3.4641, ppl: 31.9482, loss: 3.4913
	step 294:lm_loss: 3.4636, ppl: 31.9329, loss: 3.4908
	step 295:lm_loss: 3.4641, ppl: 31.9483, loss: 3.4912
	step 296:lm_loss: 3.4637, ppl: 31.9355, loss: 3.4910
	step 297:lm_loss: 3.4637, ppl: 31.9353, loss: 3.4909
	step 298:lm_loss: 3.4633, ppl: 31.9227, loss: 3.4904
	step 299:lm_loss: 3.4635, ppl: 31.9299, loss: 3.4905
	step 300:lm_loss: 3.4630, ppl: 31.9133, loss: 3.4899
	step 301:lm_loss: 3.4632, ppl: 31.9191, loss: 3.4900
	step 302:lm_loss: 3.4631, ppl: 31.9170, loss: 3.4899
	step 303:lm_loss: 3.4640, ppl: 31.9450, loss: 3.4911
	step 304:lm_loss: 3.4636, ppl: 31.9329, loss: 3.4909
	step 305:lm_loss: 3.4636, ppl: 31.9309, loss: 3.4907
	step 306:lm_loss: 3.4624, ppl: 31.8932, loss: 3.4897
	step 307:lm_loss: 3.4624, ppl: 31.8932, loss: 3.4897
	step 308:lm_loss: 3.4619, ppl: 31.8784, loss: 3.4891
	step 309:lm_loss: 3.4625, ppl: 31.8972, loss: 3.4899
	step 310:lm_loss: 3.4625, ppl: 31.8954, loss: 3.4897
	step 311:lm_loss: 3.4622, ppl: 31.8886, loss: 3.4894
	step 312:lm_loss: 3.4622, ppl: 31.8871, loss: 3.4893
	step 313:lm_loss: 3.4624, ppl: 31.8939, loss: 3.4895
	step 314:lm_loss: 3.4626, ppl: 31.8983, loss: 3.4895
	step 315:lm_loss: 3.4631, ppl: 31.9155, loss: 3.4903
	step 316:lm_loss: 3.4634, ppl: 31.9268, loss: 3.4906
	step 317:lm_loss: 3.4635, ppl: 31.9291, loss: 3.4906
	step 318:lm_loss: 3.4640, ppl: 31.9456, loss: 3.4910
	step 319:lm_loss: 3.4631, ppl: 31.9152, loss: 3.4903
	step 320:lm_loss: 3.4631, ppl: 31.9146, loss: 3.4902
	step 321:lm_loss: 3.4629, ppl: 31.9107, loss: 3.4899
	step 322:lm_loss: 3.4636, ppl: 31.9330, loss: 3.4906
	step 323:lm_loss: 3.4630, ppl: 31.9139, loss: 3.4900
	step 324:lm_loss: 3.4629, ppl: 31.9100, loss: 3.4898
	step 325:lm_loss: 3.4627, ppl: 31.9024, loss: 3.4895
	step 326:lm_loss: 3.4629, ppl: 31.9092, loss: 3.4896
	step 327:lm_loss: 3.4627, ppl: 31.9030, loss: 3.4894
	step 328:lm_loss: 3.4630, ppl: 31.9125, loss: 3.4897
	step 329:lm_loss: 3.4630, ppl: 31.9117, loss: 3.4895
	step 330:lm_loss: 3.4627, ppl: 31.9038, loss: 3.4892
	step 331:lm_loss: 3.4625, ppl: 31.8982, loss: 3.4889
	step 332:lm_loss: 3.4608, ppl: 31.8413, loss: 3.4879
	step 333:lm_loss: 3.4609, ppl: 31.8450, loss: 3.4879
	step 334:lm_loss: 3.4606, ppl: 31.8348, loss: 3.4875
	step 335:lm_loss: 3.4603, ppl: 31.8271, loss: 3.4873
	step 336:lm_loss: 3.4608, ppl: 31.8432, loss: 3.4879
	step 337:lm_loss: 3.4599, ppl: 31.8141, loss: 3.4872
	step 338:lm_loss: 3.4599, ppl: 31.8141, loss: 3.4872
	step 339:lm_loss: 3.4589, ppl: 31.7806, loss: 3.4866
	step 340:lm_loss: 3.4592, ppl: 31.7928, loss: 3.4869
	step 341:lm_loss: 3.4593, ppl: 31.7943, loss: 3.4868
	step 342:lm_loss: 3.4597, ppl: 31.8086, loss: 3.4872
	step 343:lm_loss: 3.4598, ppl: 31.8112, loss: 3.4872
	step 344:lm_loss: 3.4603, ppl: 31.8275, loss: 3.4879
	step 345:lm_loss: 3.4604, ppl: 31.8290, loss: 3.4878
	step 346:lm_loss: 3.4599, ppl: 31.8142, loss: 3.4875
	step 347:lm_loss: 3.4602, ppl: 31.8246, loss: 3.4877
	step 348:lm_loss: 3.4603, ppl: 31.8270, loss: 3.4877
	step 349:lm_loss: 3.4605, ppl: 31.8318, loss: 3.4878
	step 350:lm_loss: 3.4604, ppl: 31.8302, loss: 3.4877
	step 351:lm_loss: 3.4603, ppl: 31.8271, loss: 3.4875
	step 352:lm_loss: 3.4605, ppl: 31.8318, loss: 3.4876
	step 353:lm_loss: 3.4607, ppl: 31.8401, loss: 3.4878
	step 354:lm_loss: 3.4603, ppl: 31.8250, loss: 3.4873
	step 355:lm_loss: 3.4606, ppl: 31.8353, loss: 3.4876
	step 356:lm_loss: 3.4605, ppl: 31.8341, loss: 3.4875
	step 357:lm_loss: 3.4607, ppl: 31.8390, loss: 3.4876
	step 358:lm_loss: 3.4609, ppl: 31.8456, loss: 3.4877
	step 359:lm_loss: 3.4604, ppl: 31.8281, loss: 3.4872
	step 360:lm_loss: 3.4600, ppl: 31.8181, loss: 3.4868
	step 361:lm_loss: 3.4606, ppl: 31.8349, loss: 3.4875
	step 362:lm_loss: 3.4610, ppl: 31.8487, loss: 3.4879
	step 363:lm_loss: 3.4614, ppl: 31.8621, loss: 3.4882
	step 364:lm_loss: 3.4613, ppl: 31.8582, loss: 3.4881
	step 365:lm_loss: 3.4611, ppl: 31.8507, loss: 3.4877
	step 366:lm_loss: 3.4606, ppl: 31.8352, loss: 3.4873
	step 367:lm_loss: 3.4607, ppl: 31.8398, loss: 3.4874
	step 368:lm_loss: 3.4607, ppl: 31.8401, loss: 3.4873
	step 369:lm_loss: 3.4606, ppl: 31.8373, loss: 3.4872
	step 370:lm_loss: 3.4606, ppl: 31.8372, loss: 3.4871
	step 371:lm_loss: 3.4607, ppl: 31.8388, loss: 3.4871
	step 372:lm_loss: 3.4610, ppl: 31.8472, loss: 3.4873
	step 373:lm_loss: 3.4616, ppl: 31.8675, loss: 3.4880
	step 374:lm_loss: 3.4619, ppl: 31.8777, loss: 3.4883
	step 375:lm_loss: 3.4619, ppl: 31.8782, loss: 3.4882
	step 376:lm_loss: 3.4624, ppl: 31.8946, loss: 3.4889
	step 377:lm_loss: 3.4625, ppl: 31.8962, loss: 3.4889
	step 378:lm_loss: 3.4629, ppl: 31.9108, loss: 3.4894
	step 379:lm_loss: 3.4627, ppl: 31.9036, loss: 3.4892
	step 380:lm_loss: 3.4626, ppl: 31.8994, loss: 3.4890
	step 381:lm_loss: 3.4624, ppl: 31.8946, loss: 3.4887
	step 382:lm_loss: 3.4623, ppl: 31.8899, loss: 3.4885
	step 383:lm_loss: 3.4626, ppl: 31.8995, loss: 3.4887
	step 384:lm_loss: 3.4628, ppl: 31.9062, loss: 3.4889
	step 385:lm_loss: 3.4624, ppl: 31.8942, loss: 3.4886
	step 386:lm_loss: 3.4623, ppl: 31.8906, loss: 3.4884
	step 387:lm_loss: 3.4620, ppl: 31.8807, loss: 3.4880
	step 388:lm_loss: 3.4624, ppl: 31.8926, loss: 3.4884
	step 389:lm_loss: 3.4618, ppl: 31.8744, loss: 3.4878
	step 390:lm_loss: 3.4617, ppl: 31.8705, loss: 3.4876
	step 391:lm_loss: 3.4617, ppl: 31.8720, loss: 3.4876
	step 392:lm_loss: 3.4615, ppl: 31.8635, loss: 3.4873
	step 393:lm_loss: 3.4603, ppl: 31.8270, loss: 3.4866
	step 394:lm_loss: 3.4609, ppl: 31.8458, loss: 3.4874
	step 395:lm_loss: 3.4606, ppl: 31.8367, loss: 3.4871
	step 396:lm_loss: 3.4607, ppl: 31.8395, loss: 3.4871
	step 397:lm_loss: 3.4608, ppl: 31.8424, loss: 3.4872
	step 398:lm_loss: 3.4603, ppl: 31.8261, loss: 3.4868
	step 399:lm_loss: 3.4607, ppl: 31.8384, loss: 3.4871
	step 400:lm_loss: 3.4608, ppl: 31.8427, loss: 3.4872
	step 401:lm_loss: 3.4611, ppl: 31.8524, loss: 3.4873
	step 402:lm_loss: 3.4613, ppl: 31.8575, loss: 3.4875
	step 403:lm_loss: 3.4614, ppl: 31.8612, loss: 3.4875
	step 404:lm_loss: 3.4613, ppl: 31.8572, loss: 3.4873
	step 405:lm_loss: 3.4611, ppl: 31.8520, loss: 3.4872
	step 406:lm_loss: 3.4601, ppl: 31.8201, loss: 3.4866
	step 407:lm_loss: 3.4604, ppl: 31.8287, loss: 3.4869
	step 408:lm_loss: 3.4603, ppl: 31.8252, loss: 3.4867
	step 409:lm_loss: 3.4602, ppl: 31.8246, loss: 3.4866
	step 410:lm_loss: 3.4603, ppl: 31.8251, loss: 3.4865
	step 411:lm_loss: 3.4607, ppl: 31.8393, loss: 3.4869
	step 412:lm_loss: 3.4609, ppl: 31.8447, loss: 3.4871
	step 413:lm_loss: 3.4611, ppl: 31.8533, loss: 3.4873
	step 414:lm_loss: 3.4613, ppl: 31.8598, loss: 3.4875
	step 415:lm_loss: 3.4615, ppl: 31.8647, loss: 3.4876
	step 416:lm_loss: 3.4613, ppl: 31.8599, loss: 3.4874
	step 417:lm_loss: 3.4612, ppl: 31.8542, loss: 3.4871
	step 418:lm_loss: 3.4613, ppl: 31.8585, loss: 3.4872
	step 419:lm_loss: 3.4615, ppl: 31.8661, loss: 3.4874
	step 420:lm_loss: 3.4617, ppl: 31.8708, loss: 3.4874
	step 421:lm_loss: 3.4610, ppl: 31.8481, loss: 3.4869
	step 422:lm_loss: 3.4613, ppl: 31.8576, loss: 3.4871
	step 423:lm_loss: 3.4613, ppl: 31.8574, loss: 3.4870
	step 424:lm_loss: 3.4613, ppl: 31.8573, loss: 3.4869
	step 425:lm_loss: 3.4612, ppl: 31.8554, loss: 3.4869
	step 426:lm_loss: 3.4612, ppl: 31.8549, loss: 3.4868
	step 427:lm_loss: 3.4615, ppl: 31.8660, loss: 3.4871
	step 428:lm_loss: 3.4616, ppl: 31.8680, loss: 3.4871
	step 429:lm_loss: 3.4615, ppl: 31.8637, loss: 3.4869
	step 430:lm_loss: 3.4613, ppl: 31.8579, loss: 3.4867
	step 431:lm_loss: 3.4615, ppl: 31.8633, loss: 3.4868
	step 432:lm_loss: 3.4613, ppl: 31.8584, loss: 3.4866
	step 433:lm_loss: 3.4611, ppl: 31.8504, loss: 3.4864
	step 434:lm_loss: 3.4608, ppl: 31.8424, loss: 3.4861
	step 435:lm_loss: 3.4610, ppl: 31.8474, loss: 3.4862
	step 436:lm_loss: 3.4612, ppl: 31.8566, loss: 3.4865
	step 437:lm_loss: 3.4605, ppl: 31.8338, loss: 3.4860
	step 438:lm_loss: 3.4604, ppl: 31.8288, loss: 3.4857
	step 439:lm_loss: 3.4603, ppl: 31.8276, loss: 3.4856
	step 440:lm_loss: 3.4602, ppl: 31.8243, loss: 3.4855
	step 441:lm_loss: 3.4604, ppl: 31.8305, loss: 3.4856
	step 442:lm_loss: 3.4610, ppl: 31.8496, loss: 3.4864
	step 443:lm_loss: 3.4610, ppl: 31.8496, loss: 3.4863
	step 444:lm_loss: 3.4610, ppl: 31.8474, loss: 3.4862
	step 445:lm_loss: 3.4611, ppl: 31.8517, loss: 3.4863
	step 446:lm_loss: 3.4605, ppl: 31.8325, loss: 3.4858
	step 447:lm_loss: 3.4607, ppl: 31.8390, loss: 3.4859
	step 448:lm_loss: 3.4604, ppl: 31.8286, loss: 3.4856
Traceback (most recent call last):
  File "./train.py", line 175, in <module>
    train(args)
  File "./train.py", line 113, in train
    evaluate(task, model, valid_generator, args, dev_count, gpu_id, step)
  File "./train.py", line 131, in evaluate
    part_outputs = task.eval_step(model, data)
  File "/home/aistudio/ldk/Knover/tasks/task_base.py", line 37, in eval_step
    outputs = model.eval_step(inputs)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 288, in eval_step
    self.eval_fetch_dict)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 266, in _execute
    fetch_vars = self.exe.run(program, feed, fetch_list, **kwargs)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1066, in run
    return_merged=return_merged)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1154, in _run_impl
    use_program_cache=use_program_cache)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1229, in _run_program
    fetch_var_name)
KeyboardInterrupt
{
  "is_distributed": false,
  "save_path": "./output",
  "train_file": "data/train.txt",
  "valid_file": "data/valid.txt",
  "start_step": 0,
  "num_epochs": 20,
  "log_steps": 100,
  "validation_steps": 1000,
  "save_steps": 1000,
  "Model": {
    "model": "UnifiedTransformer",
    "config_path": "./config/12L.json",
    "init_checkpoint": "",
    "init_pretraining_params": "12L",
    "learning_rate": 1e-05,
    "warmup_steps": 1000,
    "weight_decay": 0.01,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": true,
    "amp_loss_scaling": 12800,
    "max_seq_len": 512,
    "weight_sharing": true,
    "mem_efficient": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": null,
    "topk": 10,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "DialogGeneration",
    "do_generation": false,
    "is_cn": false,
    "nsp_inference_model_path": null,
    "nsp_attention_style": "bidirectional",
    "ranking_score": "decode_score"
  },
  "Reader": {
    "max_src_len": 384,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": true,
    "batch_size": 8192,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  }
}
W1023 16:10:23.557477 11499 device_context.cc:252] Please NOTE: device: 0, CUDA Capability: 70, Driver API Version: 11.0, Runtime API Version: 9.0
W1023 16:10:23.562058 11499 device_context.cc:260] device: 0, cuDNN Version: 7.6.
Load pretraining parameters from 12L.
[train][1] progress: 1/1 step: 100, time: 49.634, speed: 2.015 steps/s
	current lr: 0.0000010
	lm_loss: 3.4119, ppl: 30.3230, loss: 3.4119
[train][1] progress: 1/1 step: 200, time: 47.022, speed: 2.127 steps/s
	current lr: 0.0000020
	lm_loss: 3.5859, ppl: 36.0857, loss: 3.5859
[train][1] progress: 1/1 step: 300, time: 47.966, speed: 2.085 steps/s
	current lr: 0.0000030
	lm_loss: 3.8151, ppl: 45.3799, loss: 3.8151
[train][1] progress: 1/1 step: 400, time: 47.624, speed: 2.100 steps/s
	current lr: 0.0000040
	lm_loss: 3.6771, ppl: 39.5305, loss: 3.6771
[train][1] progress: 1/1 step: 500, time: 48.379, speed: 2.067 steps/s
	current lr: 0.0000050
	lm_loss: 3.5606, ppl: 35.1825, loss: 3.5606
[train][1] progress: 1/1 step: 600, time: 47.597, speed: 2.101 steps/s
	current lr: 0.0000060
	lm_loss: 3.8130, ppl: 45.2870, loss: 3.8130
[train][1] progress: 1/1 step: 700, time: 47.646, speed: 2.099 steps/s
	current lr: 0.0000070
	lm_loss: 3.6791, ppl: 39.6127, loss: 3.6791
[train][1] progress: 1/1 step: 800, time: 48.363, speed: 2.068 steps/s
	current lr: 0.0000080
	lm_loss: 3.5349, ppl: 34.2908, loss: 3.5349
[train][1] progress: 1/1 step: 900, time: 47.697, speed: 2.097 steps/s
	current lr: 0.0000090
	lm_loss: 3.7971, ppl: 44.5707, loss: 3.7971
[train][1] progress: 1/1 step: 1000, time: 47.673, speed: 2.098 steps/s
	current lr: 0.0000100
	lm_loss: 3.6298, ppl: 37.7046, loss: 3.6298
================================================================================
Evaluation:
	step 100:lm_loss: 3.4635, ppl: 31.9275, loss: 3.4903
	step 200:lm_loss: 3.4698, ppl: 32.1295, loss: 3.4962
	step 300:lm_loss: 3.4631, ppl: 31.9165, loss: 3.4901
	step 400:lm_loss: 3.4609, ppl: 31.8467, loss: 3.4873
	step 500:lm_loss: 3.4614, ppl: 31.8625, loss: 3.4852
	step 600:lm_loss: 3.4618, ppl: 31.8745, loss: 3.4847
	step 700:lm_loss: 3.4598, ppl: 31.8116, loss: 3.4829
	step 800:lm_loss: 3.4579, ppl: 31.7506, loss: 3.4813
	step 900:lm_loss: 3.4560, ppl: 31.6915, loss: 3.4798
	step 1000:lm_loss: 3.4580, ppl: 31.7523, loss: 3.4813
	step 1100:lm_loss: 3.4563, ppl: 31.6995, loss: 3.4802
	step 1200:lm_loss: 3.4543, ppl: 31.6351, loss: 3.4790
	step 1300:lm_loss: 3.4565, ppl: 31.7043, loss: 3.4807
	step 1400:lm_loss: 3.4587, ppl: 31.7760, loss: 3.4828
	step 1500:lm_loss: 3.4590, ppl: 31.7848, loss: 3.4835
	step 1600:lm_loss: 3.4608, ppl: 31.8426, loss: 3.4848
	step 1700:lm_loss: 3.4622, ppl: 31.8872, loss: 3.4857
	step 1800:lm_loss: 3.4679, ppl: 32.0686, loss: 3.4898
	step 1900:lm_loss: 3.4738, ppl: 32.2579, loss: 3.4941
	step 2000:lm_loss: 3.4792, ppl: 32.4334, loss: 3.4978
	step 2100:lm_loss: 3.4835, ppl: 32.5731, loss: 3.5008
	step 2200:lm_loss: 3.4845, ppl: 32.6056, loss: 3.5005
	step 2300:lm_loss: 3.4842, ppl: 32.5976, loss: 3.4989
	step 2400:lm_loss: 3.4839, ppl: 32.5858, loss: 3.4974
	step 2500:lm_loss: 3.4836, ppl: 32.5766, loss: 3.4961
	step 2600:lm_loss: 3.4760, ppl: 32.3299, loss: 3.4900
Traceback (most recent call last):
  File "./train.py", line 175, in <module>
    train(args)
  File "./train.py", line 113, in train
    evaluate(task, model, valid_generator, args, dev_count, gpu_id, step)
  File "./train.py", line 131, in evaluate
    part_outputs = task.eval_step(model, data)
  File "/home/aistudio/ldk/Knover/tasks/task_base.py", line 37, in eval_step
    outputs = model.eval_step(inputs)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 288, in eval_step
    self.eval_fetch_dict)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 266, in _execute
    fetch_vars = self.exe.run(program, feed, fetch_list, **kwargs)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1066, in run
    return_merged=return_merged)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1154, in _run_impl
    use_program_cache=use_program_cache)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1229, in _run_program
    fetch_var_name)
KeyboardInterrupt
{
  "is_distributed": false,
  "save_path": "./output",
  "train_file": "data/train.txt",
  "valid_file": "data/valid.txt",
  "start_step": 0,
  "num_epochs": 20,
  "log_steps": 100,
  "validation_steps": 1000,
  "save_steps": 1000,
  "Model": {
    "model": "Plato",
    "config_path": "./config/12L.json",
    "init_checkpoint": "",
    "init_pretraining_params": "12L",
    "learning_rate": 1e-05,
    "warmup_steps": 1000,
    "weight_decay": 0.01,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": true,
    "amp_loss_scaling": 12800,
    "max_seq_len": 512,
    "weight_sharing": true,
    "mem_efficient": false,
    "use_bow": true,
    "use_entropy": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": null,
    "topk": 10,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "DialogGeneration",
    "do_generation": false,
    "is_cn": false,
    "nsp_inference_model_path": null,
    "nsp_attention_style": "bidirectional",
    "ranking_score": "decode_score"
  },
  "Reader": {
    "max_src_len": 384,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": true,
    "batch_size": 8192,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  }
}
Traceback (most recent call last):
  File "./train.py", line 175, in <module>
    train(args)
  File "./train.py", line 78, in train
    model = models.create_model(args, place)
  File "/home/aistudio/ldk/Knover/models/__init__.py", line 49, in create_model
    return MODEL_REGISTRY[args.model](args, place)
  File "/home/aistudio/ldk/Knover/models/plato.py", line 49, in __init__
    super(Plato, self).__init__(args, place)
  File "/home/aistudio/ldk/Knover/models/unified_transformer.py", line 93, in __init__
    super(UnifiedTransformer, self).__init__(args, place)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 74, in __init__
    self._build_programs()
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 121, in _build_programs
    outputs = self.forward(inputs)
  File "/home/aistudio/ldk/Knover/models/plato.py", line 172, in forward
    name=self.latent_emb_name, initializer=self.param_initializer))
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/tensor.py", line 142, in create_parameter
    numpy.int64), 'create_parameter')
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/data_feeder.py", line 97, in check_type
    (input_name, op_name, expected_type, type(input), extra_message))
TypeError: The type of 'item of shape' in create_parameter must be (<class 'int'>, <class 'numpy.uint8'>, <class 'numpy.int8'>, <class 'numpy.int16'>, <class 'numpy.int32'>, <class 'numpy.int64'>), but received <class 'NoneType'>. 
Traceback (most recent call last):
  File "./train.py", line 173, in <module>
    args = setup_args()
  File "./train.py", line 50, in setup_args
    models.add_cmdline_args(parser)
  File "/home/aistudio/ldk/Knover/models/__init__.py", line 65, in add_cmdline_args
    raise ValueError(f"Unknown model type: {args.model}")
ValueError: Unknown model type: P1lato
{
  "is_distributed": false,
  "save_path": "./output",
  "train_file": "data/train.txt",
  "valid_file": "data/valid.txt",
  "start_step": 0,
  "num_epochs": 20,
  "log_steps": 100,
  "validation_steps": 1000,
  "save_steps": 1000,
  "Model": {
    "model": "Plato",
    "config_path": "./config/12L.json",
    "init_checkpoint": "",
    "init_pretraining_params": "12L",
    "learning_rate": 1e-05,
    "warmup_steps": 1000,
    "weight_decay": 0.01,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": true,
    "amp_loss_scaling": 12800,
    "max_seq_len": 512,
    "weight_sharing": true,
    "mem_efficient": false,
    "use_bow": true,
    "use_entropy": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": null,
    "topk": 10,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "DialogGeneration",
    "do_generation": false,
    "is_cn": false,
    "nsp_inference_model_path": null,
    "nsp_attention_style": "bidirectional",
    "ranking_score": "decode_score"
  },
  "Reader": {
    "max_src_len": 384,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": true,
    "batch_size": 8192,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  }
}
Traceback (most recent call last):
  File "./train.py", line 175, in <module>
    train(args)
  File "./train.py", line 78, in train
    model = models.create_model(args, place)
  File "/home/aistudio/ldk/Knover/models/__init__.py", line 49, in create_model
    return MODEL_REGISTRY[args.model](args, place)
  File "/home/aistudio/ldk/Knover/models/plato.py", line 49, in __init__
    super(Plato, self).__init__(args, place)
  File "/home/aistudio/ldk/Knover/models/unified_transformer.py", line 93, in __init__
    super(UnifiedTransformer, self).__init__(args, place)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 74, in __init__
    self._build_programs()
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 121, in _build_programs
    outputs = self.forward(inputs)
  File "/home/aistudio/ldk/Knover/models/plato.py", line 172, in forward
    name=self.latent_emb_name, initializer=self.param_initializer))
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/tensor.py", line 142, in create_parameter
    numpy.int64), 'create_parameter')
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/data_feeder.py", line 97, in check_type
    (input_name, op_name, expected_type, type(input), extra_message))
TypeError: The type of 'item of shape' in create_parameter must be (<class 'int'>, <class 'numpy.uint8'>, <class 'numpy.int8'>, <class 'numpy.int16'>, <class 'numpy.int32'>, <class 'numpy.int64'>), but received <class 'NoneType'>. 
{
  "is_distributed": true,
  "save_path": "./output",
  "train_file": "data/train.txt",
  "valid_file": "data/valid.txt",
  "start_step": 0,
  "num_epochs": 20,
  "log_steps": 100,
  "validation_steps": 1000,
  "save_steps": 1000,
  "Model": {
    "model": "Plato",
    "config_path": "./config/12L.json",
    "init_checkpoint": "",
    "init_pretraining_params": "12L",
    "learning_rate": 1e-05,
    "warmup_steps": 1000,
    "weight_decay": 0.01,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": true,
    "amp_loss_scaling": 12800,
    "max_seq_len": 512,
    "weight_sharing": true,
    "mem_efficient": false,
    "use_bow": true,
    "use_entropy": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": null,
    "topk": 10,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "DialogGeneration",
    "do_generation": false,
    "is_cn": false,
    "nsp_inference_model_path": null,
    "nsp_attention_style": "bidirectional",
    "ranking_score": "decode_score"
  },
  "Reader": {
    "max_src_len": 384,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": true,
    "batch_size": 8192,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  }
}
Traceback (most recent call last):
  File "./train.py", line 175, in <module>
    train(args)
  File "./train.py", line 78, in train
    model = models.create_model(args, place)
  File "/home/aistudio/ldk/Knover/models/__init__.py", line 49, in create_model
    return MODEL_REGISTRY[args.model](args, place)
  File "/home/aistudio/ldk/Knover/models/plato.py", line 49, in __init__
    super(Plato, self).__init__(args, place)
  File "/home/aistudio/ldk/Knover/models/unified_transformer.py", line 93, in __init__
    super(UnifiedTransformer, self).__init__(args, place)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 74, in __init__
    self._build_programs()
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 121, in _build_programs
    outputs = self.forward(inputs)
  File "/home/aistudio/ldk/Knover/models/plato.py", line 172, in forward
    name=self.latent_emb_name, initializer=self.param_initializer))
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/tensor.py", line 142, in create_parameter
    numpy.int64), 'create_parameter')
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/data_feeder.py", line 97, in check_type
    (input_name, op_name, expected_type, type(input), extra_message))
TypeError: The type of 'item of shape' in create_parameter must be (<class 'int'>, <class 'numpy.uint8'>, <class 'numpy.int8'>, <class 'numpy.int16'>, <class 'numpy.int32'>, <class 'numpy.int64'>), but received <class 'NoneType'>. 
{
  "is_distributed": true,
  "save_path": "./output",
  "train_file": "data/train.txt",
  "valid_file": "data/valid.txt",
  "start_step": 0,
  "num_epochs": 20,
  "log_steps": 100,
  "validation_steps": 1000,
  "save_steps": 1000,
  "Model": {
    "model": "Plato",
    "config_path": "./config/12L.json",
    "init_checkpoint": "",
    "init_pretraining_params": "12L",
    "learning_rate": 1e-05,
    "warmup_steps": 1000,
    "weight_decay": 0.01,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": true,
    "amp_loss_scaling": 12800,
    "max_seq_len": 512,
    "weight_sharing": true,
    "mem_efficient": false,
    "use_bow": true,
    "use_entropy": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": null,
    "topk": 10,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "DialogGeneration",
    "do_generation": false,
    "is_cn": false,
    "nsp_inference_model_path": null,
    "nsp_attention_style": "bidirectional",
    "ranking_score": "decode_score"
  },
  "Reader": {
    "max_src_len": 384,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": true,
    "batch_size": 8192,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  }
}
Traceback (most recent call last):
  File "./train.py", line 175, in <module>
    train(args)
  File "./train.py", line 78, in train
    model = models.create_model(args, place)
  File "/home/aistudio/ldk/Knover/models/__init__.py", line 49, in create_model
    return MODEL_REGISTRY[args.model](args, place)
  File "/home/aistudio/ldk/Knover/models/plato.py", line 49, in __init__
    super(Plato, self).__init__(args, place)
  File "/home/aistudio/ldk/Knover/models/unified_transformer.py", line 93, in __init__
    super(UnifiedTransformer, self).__init__(args, place)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 74, in __init__
    self._build_programs()
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 121, in _build_programs
    outputs = self.forward(inputs)
  File "/home/aistudio/ldk/Knover/models/plato.py", line 172, in forward
    name=self.latent_emb_name, initializer=self.param_initializer))
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/tensor.py", line 142, in create_parameter
    numpy.int64), 'create_parameter')
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/data_feeder.py", line 97, in check_type
    (input_name, op_name, expected_type, type(input), extra_message))
TypeError: The type of 'item of shape' in create_parameter must be (<class 'int'>, <class 'numpy.uint8'>, <class 'numpy.int8'>, <class 'numpy.int16'>, <class 'numpy.int32'>, <class 'numpy.int64'>), but received <class 'NoneType'>. 
{
  "is_distributed": true,
  "save_path": "./output",
  "train_file": "data/train.txt",
  "valid_file": "data/valid.txt",
  "start_step": 0,
  "num_epochs": 20,
  "log_steps": 100,
  "validation_steps": 1000,
  "save_steps": 1000,
  "Model": {
    "model": "Plato",
    "config_path": "./config/12L.json",
    "init_checkpoint": "",
    "init_pretraining_params": "12L",
    "learning_rate": 1e-05,
    "warmup_steps": 1000,
    "weight_decay": 0.01,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": true,
    "amp_loss_scaling": 12800,
    "max_seq_len": 512,
    "weight_sharing": true,
    "mem_efficient": false,
    "use_bow": true,
    "use_entropy": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": null,
    "topk": 10,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "DialogGeneration",
    "do_generation": false,
    "is_cn": false,
    "nsp_inference_model_path": null,
    "nsp_attention_style": "bidirectional",
    "ranking_score": "decode_score"
  },
  "Reader": {
    "max_src_len": 384,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": true,
    "batch_size": 8192,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  }
}
Traceback (most recent call last):
  File "./train.py", line 175, in <module>
    train(args)
  File "./train.py", line 78, in train
    model = models.create_model(args, place)
  File "/home/aistudio/ldk/Knover/models/__init__.py", line 49, in create_model
    return MODEL_REGISTRY[args.model](args, place)
  File "/home/aistudio/ldk/Knover/models/plato.py", line 49, in __init__
    super(Plato, self).__init__(args, place)
  File "/home/aistudio/ldk/Knover/models/unified_transformer.py", line 93, in __init__
    super(UnifiedTransformer, self).__init__(args, place)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 74, in __init__
    self._build_programs()
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 121, in _build_programs
    outputs = self.forward(inputs)
  File "/home/aistudio/ldk/Knover/models/plato.py", line 172, in forward
    name=self.latent_emb_name, initializer=self.param_initializer))
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/tensor.py", line 142, in create_parameter
    numpy.int64), 'create_parameter')
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/data_feeder.py", line 97, in check_type
    (input_name, op_name, expected_type, type(input), extra_message))
TypeError: The type of 'item of shape' in create_parameter must be (<class 'int'>, <class 'numpy.uint8'>, <class 'numpy.int8'>, <class 'numpy.int16'>, <class 'numpy.int32'>, <class 'numpy.int64'>), but received <class 'NoneType'>. 
{
  "is_distributed": false,
  "save_path": "./output",
  "train_file": "data/train.txt",
  "valid_file": "data/valid.txt",
  "start_step": 0,
  "num_epochs": 20,
  "log_steps": 100,
  "validation_steps": 1000,
  "save_steps": 1000,
  "Model": {
    "model": "Plato",
    "config_path": "./config/12L.json",
    "init_checkpoint": "",
    "init_pretraining_params": "12L",
    "learning_rate": 1e-05,
    "warmup_steps": 1000,
    "weight_decay": 0.01,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": true,
    "amp_loss_scaling": 12800,
    "max_seq_len": 512,
    "weight_sharing": true,
    "mem_efficient": false,
    "use_bow": true,
    "use_entropy": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": null,
    "topk": 10,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "DialogGeneration",
    "do_generation": false,
    "is_cn": false,
    "nsp_inference_model_path": null,
    "nsp_attention_style": "bidirectional",
    "ranking_score": "decode_score"
  },
  "Reader": {
    "max_src_len": 384,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": true,
    "batch_size": 8192,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  }
}
Traceback (most recent call last):
  File "./train.py", line 175, in <module>
    train(args)
  File "./train.py", line 78, in train
    model = models.create_model(args, place)
  File "/home/aistudio/ldk/Knover/models/__init__.py", line 49, in create_model
    return MODEL_REGISTRY[args.model](args, place)
  File "/home/aistudio/ldk/Knover/models/plato.py", line 49, in __init__
    super(Plato, self).__init__(args, place)
  File "/home/aistudio/ldk/Knover/models/unified_transformer.py", line 93, in __init__
    super(UnifiedTransformer, self).__init__(args, place)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 74, in __init__
    self._build_programs()
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 121, in _build_programs
    outputs = self.forward(inputs)
  File "/home/aistudio/ldk/Knover/models/plato.py", line 172, in forward
    name=self.latent_emb_name, initializer=self.param_initializer))
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/tensor.py", line 142, in create_parameter
    numpy.int64), 'create_parameter')
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/data_feeder.py", line 97, in check_type
    (input_name, op_name, expected_type, type(input), extra_message))
TypeError: The type of 'item of shape' in create_parameter must be (<class 'int'>, <class 'numpy.uint8'>, <class 'numpy.int8'>, <class 'numpy.int16'>, <class 'numpy.int32'>, <class 'numpy.int64'>), but received <class 'NoneType'>. 
{
  "is_distributed": false,
  "save_path": "./output",
  "train_file": "data/train.txt",
  "valid_file": "data/valid.txt",
  "start_step": 0,
  "num_epochs": 20,
  "log_steps": 100,
  "validation_steps": 1000,
  "save_steps": 1000,
  "Model": {
    "model": "Plato",
    "config_path": "./config/12L.json",
    "init_checkpoint": "",
    "init_pretraining_params": "12L",
    "learning_rate": 1e-05,
    "warmup_steps": 1000,
    "weight_decay": 0.01,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": true,
    "amp_loss_scaling": 12800,
    "max_seq_len": 512,
    "weight_sharing": true,
    "mem_efficient": false,
    "use_bow": true,
    "use_entropy": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": null,
    "topk": 10,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "DialogGeneration",
    "do_generation": false,
    "is_cn": false,
    "nsp_inference_model_path": null,
    "nsp_attention_style": "bidirectional",
    "ranking_score": "decode_score"
  },
  "Reader": {
    "max_src_len": 384,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": true,
    "batch_size": 8192,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  }
}
Traceback (most recent call last):
  File "./train.py", line 175, in <module>
    train(args)
  File "./train.py", line 78, in train
    model = models.create_model(args, place)
  File "/home/aistudio/ldk/Knover/models/__init__.py", line 49, in create_model
    return MODEL_REGISTRY[args.model](args, place)
  File "/home/aistudio/ldk/Knover/models/plato.py", line 49, in __init__
    super(Plato, self).__init__(args, place)
  File "/home/aistudio/ldk/Knover/models/unified_transformer.py", line 93, in __init__
    super(UnifiedTransformer, self).__init__(args, place)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 74, in __init__
    self._build_programs()
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 121, in _build_programs
    outputs = self.forward(inputs)
  File "/home/aistudio/ldk/Knover/models/plato.py", line 172, in forward
    name=self.latent_emb_name, initializer=self.param_initializer))
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/tensor.py", line 142, in create_parameter
    numpy.int64), 'create_parameter')
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/data_feeder.py", line 97, in check_type
    (input_name, op_name, expected_type, type(input), extra_message))
TypeError: The type of 'item of shape' in create_parameter must be (<class 'int'>, <class 'numpy.uint8'>, <class 'numpy.int8'>, <class 'numpy.int16'>, <class 'numpy.int32'>, <class 'numpy.int64'>), but received <class 'NoneType'>. 
{
  "is_distributed": false,
  "save_path": "./output",
  "train_file": "data/train.txt",
  "valid_file": "data/valid.txt",
  "start_step": 0,
  "num_epochs": 20,
  "log_steps": 100,
  "validation_steps": 1000,
  "save_steps": 1000,
  "Model": {
    "model": "Plato",
    "config_path": "./config/12L.json",
    "init_checkpoint": "",
    "init_pretraining_params": "12L",
    "learning_rate": 1e-05,
    "warmup_steps": 1000,
    "weight_decay": 0.01,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": true,
    "amp_loss_scaling": 12800,
    "max_seq_len": 512,
    "weight_sharing": true,
    "mem_efficient": false,
    "use_bow": true,
    "use_entropy": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": null,
    "topk": 10,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "DialogGeneration",
    "do_generation": false,
    "is_cn": false,
    "nsp_inference_model_path": null,
    "nsp_attention_style": "bidirectional",
    "ranking_score": "decode_score"
  },
  "Reader": {
    "max_src_len": 384,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": true,
    "batch_size": 8192,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  }
}
Traceback (most recent call last):
  File "./train.py", line 175, in <module>
    train(args)
  File "./train.py", line 78, in train
    model = models.create_model(args, place)
  File "/home/aistudio/ldk/Knover/models/__init__.py", line 49, in create_model
    return MODEL_REGISTRY[args.model](args, place)
  File "/home/aistudio/ldk/Knover/models/plato.py", line 49, in __init__
    super(Plato, self).__init__(args, place)
  File "/home/aistudio/ldk/Knover/models/unified_transformer.py", line 93, in __init__
    super(UnifiedTransformer, self).__init__(args, place)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 74, in __init__
    self._build_programs()
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 121, in _build_programs
    outputs = self.forward(inputs)
  File "/home/aistudio/ldk/Knover/models/plato.py", line 172, in forward
    name=self.latent_emb_name, initializer=self.param_initializer))
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/tensor.py", line 142, in create_parameter
    numpy.int64), 'create_parameter')
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/data_feeder.py", line 97, in check_type
    (input_name, op_name, expected_type, type(input), extra_message))
TypeError: The type of 'item of shape' in create_parameter must be (<class 'int'>, <class 'numpy.uint8'>, <class 'numpy.int8'>, <class 'numpy.int16'>, <class 'numpy.int32'>, <class 'numpy.int64'>), but received <class 'NoneType'>. 
{
  "is_distributed": false,
  "save_path": "./output",
  "train_file": "data/train.txt",
  "valid_file": "data/valid.txt",
  "start_step": 0,
  "num_epochs": 20,
  "log_steps": 100,
  "validation_steps": 200,
  "save_steps": 500,
  "Model": {
    "model": "UnifiedTransformer",
    "config_path": "./config/12L.json",
    "init_checkpoint": "",
    "init_pretraining_params": "12L",
    "learning_rate": 1e-05,
    "warmup_steps": 1000,
    "weight_decay": 0.01,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": true,
    "amp_loss_scaling": 12800,
    "max_seq_len": 512,
    "weight_sharing": true,
    "mem_efficient": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": null,
    "topk": 10,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "DialogGeneration",
    "do_generation": false,
    "is_cn": false,
    "nsp_inference_model_path": null,
    "nsp_attention_style": "bidirectional",
    "ranking_score": "decode_score"
  },
  "Reader": {
    "max_src_len": 384,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": true,
    "batch_size": 8192,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  }
}
W1023 17:00:02.001513 14434 device_context.cc:252] Please NOTE: device: 0, CUDA Capability: 70, Driver API Version: 11.0, Runtime API Version: 9.0
W1023 17:00:02.006120 14434 device_context.cc:260] device: 0, cuDNN Version: 7.6.
Load pretraining parameters from 12L.
[train][1] progress: 1/1 step: 100, time: 49.591, speed: 2.016 steps/s
	current lr: 0.0000010
	lm_loss: 3.4042, ppl: 30.0910, loss: 3.4042
[train][1] progress: 1/1 step: 200, time: 46.921, speed: 2.131 steps/s
	current lr: 0.0000020
	lm_loss: 3.5669, ppl: 35.4057, loss: 3.5669
================================================================================
Evaluation:
	step 100:lm_loss: 3.4917, ppl: 32.8431, loss: 3.5165
	step 200:lm_loss: 3.4792, ppl: 32.4336, loss: 3.5083
	step 300:lm_loss: 3.4730, ppl: 32.2330, loss: 3.5009
	step 400:lm_loss: 3.4762, ppl: 32.3372, loss: 3.5017
	step 500:lm_loss: 3.4770, ppl: 32.3623, loss: 3.5025
	step 600:lm_loss: 3.4746, ppl: 32.2846, loss: 3.5012
	step 700:lm_loss: 3.4755, ppl: 32.3141, loss: 3.5009
	step 800:lm_loss: 3.4777, ppl: 32.3849, loss: 3.5030
Traceback (most recent call last):
  File "./train.py", line 175, in <module>
    train(args)
  File "./train.py", line 113, in train
    evaluate(task, model, valid_generator, args, dev_count, gpu_id, step)
  File "./train.py", line 131, in evaluate
    part_outputs = task.eval_step(model, data)
  File "/home/aistudio/ldk/Knover/tasks/task_base.py", line 37, in eval_step
    outputs = model.eval_step(inputs)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 288, in eval_step
    self.eval_fetch_dict)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 266, in _execute
    fetch_vars = self.exe.run(program, feed, fetch_list, **kwargs)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1066, in run
    return_merged=return_merged)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1154, in _run_impl
    use_program_cache=use_program_cache)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1229, in _run_program
    fetch_var_name)
KeyboardInterrupt
{
  "is_distributed": false,
  "save_path": "./output",
  "train_file": "data/train.txt",
  "valid_file": "data/valid.txt",
  "start_step": 0,
  "num_epochs": 20,
  "log_steps": 100,
  "validation_steps": 1000,
  "save_steps": 500,
  "Model": {
    "model": "UnifiedTransformer",
    "config_path": "./config/12L.json",
    "init_checkpoint": "",
    "init_pretraining_params": "12L",
    "learning_rate": 1e-05,
    "warmup_steps": 1000,
    "weight_decay": 0.01,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": true,
    "amp_loss_scaling": 12800,
    "max_seq_len": 512,
    "weight_sharing": true,
    "mem_efficient": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": null,
    "topk": 10,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "DialogGeneration",
    "do_generation": false,
    "is_cn": false,
    "nsp_inference_model_path": null,
    "nsp_attention_style": "bidirectional",
    "ranking_score": "decode_score"
  },
  "Reader": {
    "max_src_len": 384,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": true,
    "batch_size": 8192,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  }
}
W1023 17:04:45.774399 14691 device_context.cc:252] Please NOTE: device: 0, CUDA Capability: 70, Driver API Version: 11.0, Runtime API Version: 9.0
W1023 17:04:45.779026 14691 device_context.cc:260] device: 0, cuDNN Version: 7.6.
Load pretraining parameters from 12L.
[train][1] progress: 1/1 step: 100, time: 49.788, speed: 2.009 steps/s
	current lr: 0.0000010
	lm_loss: 3.4090, ppl: 30.2350, loss: 3.4090
[train][1] progress: 1/1 step: 200, time: 47.039, speed: 2.126 steps/s
	current lr: 0.0000020
	lm_loss: 3.5793, ppl: 35.8501, loss: 3.5793
[train][1] progress: 1/1 step: 300, time: 48.239, speed: 2.073 steps/s
	current lr: 0.0000030
	lm_loss: 3.8278, ppl: 45.9635, loss: 3.8278
[train][1] progress: 1/1 step: 400, time: 47.652, speed: 2.099 steps/s
	current lr: 0.0000040
	lm_loss: 3.6612, ppl: 38.9077, loss: 3.6612
[train][1] progress: 1/1 step: 500, time: 48.503, speed: 2.062 steps/s
	current lr: 0.0000050
	lm_loss: 3.5660, ppl: 35.3739, loss: 3.5660
[train][1] progress: 1/1 step: 600, time: 47.322, speed: 2.113 steps/s
	current lr: 0.0000060
	lm_loss: 3.8169, ppl: 45.4653, loss: 3.8169
[train][1] progress: 1/1 step: 700, time: 47.547, speed: 2.103 steps/s
	current lr: 0.0000070
	lm_loss: 3.6819, ppl: 39.7226, loss: 3.6819
[train][1] progress: 1/1 step: 800, time: 48.521, speed: 2.061 steps/s
	current lr: 0.0000080
	lm_loss: 3.5298, ppl: 34.1155, loss: 3.5298
[train][1] progress: 1/1 step: 900, time: 47.592, speed: 2.101 steps/s
	current lr: 0.0000090
	lm_loss: 3.7863, ppl: 44.0945, loss: 3.7863
[train][1] progress: 1/1 step: 1000, time: 47.592, speed: 2.101 steps/s
	current lr: 0.0000100
	lm_loss: 3.6369, ppl: 37.9755, loss: 3.6369
================================================================================
Evaluation:
	step 100:lm_loss: 3.4633, ppl: 31.9237, loss: 3.4901
	step 200:lm_loss: 3.4697, ppl: 32.1268, loss: 3.4961
	step 300:lm_loss: 3.4630, ppl: 31.9133, loss: 3.4900
	step 400:lm_loss: 3.4608, ppl: 31.8431, loss: 3.4872
	step 500:lm_loss: 3.4613, ppl: 31.8590, loss: 3.4851
Traceback (most recent call last):
  File "./train.py", line 175, in <module>
    train(args)
  File "./train.py", line 113, in train
    evaluate(task, model, valid_generator, args, dev_count, gpu_id, step)
  File "./train.py", line 131, in evaluate
    part_outputs = task.eval_step(model, data)
  File "/home/aistudio/ldk/Knover/tasks/task_base.py", line 37, in eval_step
    outputs = model.eval_step(inputs)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 288, in eval_step
    self.eval_fetch_dict)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 266, in _execute
    fetch_vars = self.exe.run(program, feed, fetch_list, **kwargs)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1066, in run
    return_merged=return_merged)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1154, in _run_impl
    use_program_cache=use_program_cache)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1224, in _run_program
    fetch_var_name=fetch_var_name)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 580, in _add_feed_fetch_ops
    tmp_program = program.clone()
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/framework.py", line 4151, in clone
    p._copy_param_info_from(self)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/framework.py", line 4581, in _copy_param_info_from
    self.global_block()._copy_param_info_from(other.global_block())
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/framework.py", line 2793, in _copy_param_info_from
    lod_level=v.lod_level,
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/framework.py", line 1386, in lod_level
    @property
KeyboardInterrupt
{
  "is_distributed": false,
  "save_path": "./output",
  "train_file": "data/train.txt",
  "valid_file": "data/valid.txt",
  "start_step": 0,
  "num_epochs": 20,
  "log_steps": 100,
  "validation_steps": 1000,
  "save_steps": 500,
  "Model": {
    "model": "UnifiedTransformer",
    "config_path": "./config/12L.json",
    "init_checkpoint": "",
    "init_pretraining_params": "12L",
    "learning_rate": 1e-05,
    "warmup_steps": 1000,
    "weight_decay": 0.01,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": true,
    "amp_loss_scaling": 12800,
    "max_seq_len": 512,
    "weight_sharing": true,
    "mem_efficient": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": null,
    "topk": 10,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "DialogGeneration",
    "do_generation": false,
    "is_cn": false,
    "nsp_inference_model_path": null,
    "nsp_attention_style": "bidirectional",
    "ranking_score": "decode_score"
  },
  "Reader": {
    "max_src_len": 384,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": true,
    "batch_size": 8192,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  }
}
W1023 17:14:55.126647 15133 device_context.cc:252] Please NOTE: device: 0, CUDA Capability: 70, Driver API Version: 11.0, Runtime API Version: 9.0
W1023 17:14:55.131211 15133 device_context.cc:260] device: 0, cuDNN Version: 7.6.
Load pretraining parameters from 12L.
Traceback (most recent call last):
  File "./train.py", line 175, in <module>
    train(args)
  File "./train.py", line 99, in train
    outputs = task.train_step(model, data)
  File "/home/aistudio/ldk/Knover/tasks/task_base.py", line 31, in train_step
    outputs = model.train_step(inputs)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 278, in train_step
    use_program_cache=True)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 266, in _execute
    fetch_vars = self.exe.run(program, feed, fetch_list, **kwargs)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1066, in run
    return_merged=return_merged)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1154, in _run_impl
    use_program_cache=use_program_cache)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1232, in _run_program
    False)
KeyboardInterrupt
{
  "is_distributed": false,
  "save_path": "./output",
  "train_file": "data/train.txt",
  "valid_file": "data/valid.txt",
  "start_step": 0,
  "num_epochs": 20,
  "log_steps": 100,
  "validation_steps": 1000,
  "save_steps": 500,
  "Model": {
    "model": "UnifiedTransformer",
    "config_path": "./config/12L.json",
    "init_checkpoint": "",
    "init_pretraining_params": "12L",
    "learning_rate": 1e-05,
    "warmup_steps": 1000,
    "weight_decay": 0.01,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": true,
    "amp_loss_scaling": 12800,
    "max_seq_len": 512,
    "weight_sharing": true,
    "mem_efficient": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": null,
    "topk": 10,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "DialogGeneration",
    "do_generation": true,
    "is_cn": false,
    "nsp_inference_model_path": null,
    "nsp_attention_style": "bidirectional",
    "ranking_score": "decode_score"
  },
  "Reader": {
    "max_src_len": 384,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": true,
    "batch_size": 8192,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  }
}
W1023 17:16:57.316824 15284 device_context.cc:252] Please NOTE: device: 0, CUDA Capability: 70, Driver API Version: 11.0, Runtime API Version: 9.0
W1023 17:16:57.321417 15284 device_context.cc:260] device: 0, cuDNN Version: 7.6.
Load pretraining parameters from 12L.
Traceback (most recent call last):
  File "./train.py", line 175, in <module>
    train(args)
  File "./train.py", line 99, in train
    outputs = task.train_step(model, data)
  File "/home/aistudio/ldk/Knover/tasks/task_base.py", line 31, in train_step
    outputs = model.train_step(inputs)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 278, in train_step
    use_program_cache=True)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 266, in _execute
    fetch_vars = self.exe.run(program, feed, fetch_list, **kwargs)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1066, in run
    return_merged=return_merged)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1154, in _run_impl
    use_program_cache=use_program_cache)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1232, in _run_program
    False)
KeyboardInterrupt
{
  "is_distributed": false,
  "save_path": "./output",
  "train_file": "data/train.txt",
  "valid_file": "data/valid.txt",
  "start_step": 0,
  "num_epochs": 20,
  "log_steps": 100,
  "validation_steps": 1000,
  "save_steps": 500,
  "Model": {
    "model": "Plato",
    "config_path": "./config/12L.json",
    "init_checkpoint": "",
    "init_pretraining_params": "12L",
    "learning_rate": 1e-05,
    "warmup_steps": 1000,
    "weight_decay": 0.01,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": true,
    "amp_loss_scaling": 12800,
    "max_seq_len": 512,
    "weight_sharing": true,
    "mem_efficient": false,
    "use_bow": true,
    "use_entropy": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": null,
    "topk": 10,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "DialogGeneration",
    "do_generation": true,
    "is_cn": false,
    "nsp_inference_model_path": null,
    "nsp_attention_style": "bidirectional",
    "ranking_score": "decode_score"
  },
  "Reader": {
    "max_src_len": 384,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": true,
    "batch_size": 8192,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  }
}
Traceback (most recent call last):
  File "./train.py", line 175, in <module>
    train(args)
  File "./train.py", line 78, in train
    model = models.create_model(args, place)
  File "/home/aistudio/ldk/Knover/models/__init__.py", line 49, in create_model
    return MODEL_REGISTRY[args.model](args, place)
  File "/home/aistudio/ldk/Knover/models/plato.py", line 49, in __init__
    super(Plato, self).__init__(args, place)
  File "/home/aistudio/ldk/Knover/models/unified_transformer.py", line 93, in __init__
    super(UnifiedTransformer, self).__init__(args, place)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 74, in __init__
    self._build_programs()
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 121, in _build_programs
    outputs = self.forward(inputs)
  File "/home/aistudio/ldk/Knover/models/plato.py", line 172, in forward
    name=self.latent_emb_name, initializer=self.param_initializer))
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/tensor.py", line 142, in create_parameter
    numpy.int64), 'create_parameter')
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/data_feeder.py", line 97, in check_type
    (input_name, op_name, expected_type, type(input), extra_message))
TypeError: The type of 'item of shape' in create_parameter must be (<class 'int'>, <class 'numpy.uint8'>, <class 'numpy.int8'>, <class 'numpy.int16'>, <class 'numpy.int32'>, <class 'numpy.int64'>), but received <class 'NoneType'>. 
{
  "is_distributed": false,
  "save_path": "./output",
  "train_file": "data/train.txt",
  "valid_file": "data/valid.txt",
  "start_step": 0,
  "num_epochs": 20,
  "log_steps": 100,
  "validation_steps": 1000,
  "save_steps": 500,
  "Model": {
    "model": "UnifiedTransformer",
    "config_path": "./config/12L.json",
    "init_checkpoint": "",
    "init_pretraining_params": "12L",
    "learning_rate": 1e-05,
    "warmup_steps": 1000,
    "weight_decay": 0.01,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": true,
    "amp_loss_scaling": 12800,
    "max_seq_len": 512,
    "weight_sharing": true,
    "mem_efficient": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": null,
    "topk": 10,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "DialogGeneration",
    "do_generation": true,
    "is_cn": false,
    "nsp_inference_model_path": null,
    "nsp_attention_style": "bidirectional",
    "ranking_score": "decode_score"
  },
  "Reader": {
    "max_src_len": 384,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": true,
    "batch_size": 8192,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  }
}
W1023 17:18:41.947686 15478 device_context.cc:252] Please NOTE: device: 0, CUDA Capability: 70, Driver API Version: 11.0, Runtime API Version: 9.0
W1023 17:18:41.952308 15478 device_context.cc:260] device: 0, cuDNN Version: 7.6.
Load pretraining parameters from 12L.
[train][1] progress: 1/1 step: 100, time: 49.571, speed: 2.017 steps/s
	current lr: 0.0000010
	lm_loss: 3.4166, ppl: 30.4647, loss: 3.4166
[train][1] progress: 1/1 step: 200, time: 46.879, speed: 2.133 steps/s
	current lr: 0.0000020
	lm_loss: 3.5912, ppl: 36.2768, loss: 3.5912
[train][1] progress: 1/1 step: 300, time: 47.995, speed: 2.084 steps/s
	current lr: 0.0000030
	lm_loss: 3.8232, ppl: 45.7486, loss: 3.8232
[train][1] progress: 1/1 step: 400, time: 47.544, speed: 2.103 steps/s
	current lr: 0.0000040
	lm_loss: 3.6649, ppl: 39.0505, loss: 3.6649
[train][1] progress: 1/1 step: 500, time: 47.968, speed: 2.085 steps/s
	current lr: 0.0000050
	lm_loss: 3.5616, ppl: 35.2181, loss: 3.5616
[train][1] progress: 1/1 step: 600, time: 47.259, speed: 2.116 steps/s
	current lr: 0.0000060
	lm_loss: 3.8131, ppl: 45.2908, loss: 3.8131
[train][1] progress: 1/1 step: 700, time: 47.524, speed: 2.104 steps/s
	current lr: 0.0000070
	lm_loss: 3.6874, ppl: 39.9399, loss: 3.6874
[train][1] progress: 1/1 step: 800, time: 48.307, speed: 2.070 steps/s
	current lr: 0.0000080
	lm_loss: 3.5261, ppl: 33.9898, loss: 3.5261
[train][1] progress: 1/1 step: 900, time: 47.530, speed: 2.104 steps/s
	current lr: 0.0000090
	lm_loss: 3.7971, ppl: 44.5723, loss: 3.7971
Traceback (most recent call last):
  File "./train.py", line 175, in <module>
    train(args)
  File "./train.py", line 99, in train
    outputs = task.train_step(model, data)
  File "/home/aistudio/ldk/Knover/tasks/task_base.py", line 31, in train_step
    outputs = model.train_step(inputs)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 278, in train_step
    use_program_cache=True)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 266, in _execute
    fetch_vars = self.exe.run(program, feed, fetch_list, **kwargs)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1066, in run
    return_merged=return_merged)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1154, in _run_impl
    use_program_cache=use_program_cache)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1232, in _run_program
    False)
KeyboardInterrupt
{
  "is_distributed": false,
  "save_path": "./output",
  "train_file": "data/train.txt",
  "valid_file": "data/valid.txt",
  "start_step": 0,
  "num_epochs": 20,
  "log_steps": 100,
  "validation_steps": 1000,
  "save_steps": 500,
  "Model": {
    "model": "Plato",
    "config_path": "./config/12L.json",
    "init_checkpoint": "",
    "init_pretraining_params": "12L",
    "learning_rate": 1e-05,
    "warmup_steps": 1000,
    "weight_decay": 0.01,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": true,
    "amp_loss_scaling": 12800,
    "max_seq_len": 512,
    "weight_sharing": true,
    "mem_efficient": false,
    "use_bow": true,
    "use_entropy": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": null,
    "topk": 10,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "DialogGeneration",
    "do_generation": true,
    "is_cn": false,
    "nsp_inference_model_path": null,
    "nsp_attention_style": "bidirectional",
    "ranking_score": "decode_score"
  },
  "Reader": {
    "max_src_len": 384,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": true,
    "batch_size": 8192,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  }
}
Traceback (most recent call last):
  File "./train.py", line 175, in <module>
    train(args)
  File "./train.py", line 78, in train
    model = models.create_model(args, place)
  File "/home/aistudio/ldk/Knover/models/__init__.py", line 49, in create_model
    return MODEL_REGISTRY[args.model](args, place)
  File "/home/aistudio/ldk/Knover/models/plato.py", line 49, in __init__
    super(Plato, self).__init__(args, place)
  File "/home/aistudio/ldk/Knover/models/unified_transformer.py", line 93, in __init__
    super(UnifiedTransformer, self).__init__(args, place)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 74, in __init__
    self._build_programs()
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 121, in _build_programs
    outputs = self.forward(inputs)
  File "/home/aistudio/ldk/Knover/models/plato.py", line 172, in forward
    name=self.latent_emb_name, initializer=self.param_initializer))
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/tensor.py", line 142, in create_parameter
    numpy.int64), 'create_parameter')
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/data_feeder.py", line 97, in check_type
    (input_name, op_name, expected_type, type(input), extra_message))
TypeError: The type of 'item of shape' in create_parameter must be (<class 'int'>, <class 'numpy.uint8'>, <class 'numpy.int8'>, <class 'numpy.int16'>, <class 'numpy.int32'>, <class 'numpy.int64'>), but received <class 'NoneType'>. 
{
  "is_distributed": false,
  "save_path": "./output",
  "train_file": "data/train.txt",
  "valid_file": "data/valid.txt",
  "start_step": 0,
  "num_epochs": 20,
  "log_steps": 100,
  "validation_steps": 1000,
  "save_steps": 500,
  "Model": {
    "model": "Plato",
    "config_path": "./config/12L.json",
    "init_checkpoint": "./output",
    "init_pretraining_params": "12L",
    "learning_rate": 1e-05,
    "warmup_steps": 1000,
    "weight_decay": 0.01,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": true,
    "amp_loss_scaling": 12800,
    "max_seq_len": 512,
    "weight_sharing": true,
    "mem_efficient": false,
    "use_bow": true,
    "use_entropy": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": null,
    "topk": 10,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "DialogGeneration",
    "do_generation": true,
    "is_cn": false,
    "nsp_inference_model_path": null,
    "nsp_attention_style": "bidirectional",
    "ranking_score": "decode_score"
  },
  "Reader": {
    "max_src_len": 384,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": true,
    "batch_size": 8192,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  }
}
Traceback (most recent call last):
  File "./train.py", line 175, in <module>
    train(args)
  File "./train.py", line 78, in train
    model = models.create_model(args, place)
  File "/home/aistudio/ldk/Knover/models/__init__.py", line 49, in create_model
    return MODEL_REGISTRY[args.model](args, place)
  File "/home/aistudio/ldk/Knover/models/plato.py", line 49, in __init__
    super(Plato, self).__init__(args, place)
  File "/home/aistudio/ldk/Knover/models/unified_transformer.py", line 93, in __init__
    super(UnifiedTransformer, self).__init__(args, place)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 74, in __init__
    self._build_programs()
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 121, in _build_programs
    outputs = self.forward(inputs)
  File "/home/aistudio/ldk/Knover/models/plato.py", line 172, in forward
    name=self.latent_emb_name, initializer=self.param_initializer))
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/tensor.py", line 142, in create_parameter
    numpy.int64), 'create_parameter')
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/data_feeder.py", line 97, in check_type
    (input_name, op_name, expected_type, type(input), extra_message))
TypeError: The type of 'item of shape' in create_parameter must be (<class 'int'>, <class 'numpy.uint8'>, <class 'numpy.int8'>, <class 'numpy.int16'>, <class 'numpy.int32'>, <class 'numpy.int64'>), but received <class 'NoneType'>. 
{
  "is_distributed": false,
  "save_path": "./output",
  "train_file": "data/train.txt",
  "valid_file": "data/valid.txt",
  "start_step": 0,
  "num_epochs": 20,
  "log_steps": 100,
  "validation_steps": 1000,
  "save_steps": 500,
  "Model": {
    "model": "Plato",
    "config_path": "./config/12L.json",
    "init_checkpoint": "",
    "init_pretraining_params": "12L",
    "learning_rate": 1e-05,
    "warmup_steps": 1000,
    "weight_decay": 0.01,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": true,
    "amp_loss_scaling": 12800,
    "max_seq_len": 512,
    "weight_sharing": true,
    "mem_efficient": false,
    "use_bow": true,
    "use_entropy": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "latent_type_size": 20,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": null,
    "topk": 10,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "DialogGeneration",
    "do_generation": true,
    "is_cn": false,
    "nsp_inference_model_path": null,
    "nsp_attention_style": "bidirectional",
    "ranking_score": "decode_score"
  },
  "Reader": {
    "max_src_len": 384,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": true,
    "batch_size": 8192,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  }
}
{'is_distributed': False, 'save_path': './output', 'train_file': 'data/train.txt', 'valid_file': 'data/valid.txt', 'start_step': 0, 'num_epochs': 20, 'log_steps': 100, 'validation_steps': 1000, 'save_steps': 500, 'Model': {'model': 'Plato', 'config_path': './config/12L.json', 'init_checkpoint': '', 'init_pretraining_params': '12L', 'learning_rate': 1e-05, 'warmup_steps': 1000, 'weight_decay': 0.01, 'max_grad_norm': 0.1, 'use_recompute': False, 'use_amp': True, 'amp_loss_scaling': 12800, 'max_seq_len': 512, 'weight_sharing': True, 'mem_efficient': False, 'use_bow': True, 'use_entropy': False, 'pre_encoder_cmd': 'd', 'preprocess_cmd': 'n', 'postprocess_cmd': 'da', 'post_cls_cmd': 'n', 'cls_bias': True, 'attention_probs_dropout_prob': 0.1, 'hidden_act': 'gelu', 'hidden_dropout_prob': 0.1, 'hidden_size': 768, 'initializer_range': 0.02, 'max_position_embeddings': 512, 'latent_type_size': 20, 'num_attention_heads': 12, 'num_hidden_layers': 12, 'type_vocab_size': 2, 'role_type_size': 32, 'vocab_size': 30004}, 'Generator': {'min_dec_len': 1, 'max_dec_len': 64, 'decoding_strategy': 'topk_sampling', 'temperature': 1.0, 'ignore_unk': True, 'num_samples': None, 'topk': 10, 'topp': 0.9, 'beam_size': 10, 'length_average': True, 'length_penalty': 0.0}, 'Task': {'task': 'DialogGeneration', 'do_generation': True, 'is_cn': False, 'nsp_inference_model_path': None, 'nsp_attention_style': 'bidirectional', 'ranking_score': 'decode_score'}, 'Reader': {'max_src_len': 384, 'max_tgt_len': 128, 'truncate_first_turn': False, 'file_format': 'file', 'data_format': 'numerical', 'in_tokens': True, 'batch_size': 8192, 'continuous_position': True, 'random_seed': 11, 'sort_pool_size': 65536}, 'Tokenizer': {'tokenizer': 'SentencePieceTokenizer', 'vocab_path': './config/vocab.txt', 'do_lower_case': False, 'spm_model_file': './config/spm.model'}, 'pad_id': 0, 'bos_id': 1, 'eos_id': 2, 'unk_id': 0, 'mask_id': 30000}
W1024 10:37:52.652227   343 device_context.cc:252] Please NOTE: device: 0, CUDA Capability: 70, Driver API Version: 10.1, Runtime API Version: 9.0
W1024 10:37:52.657208   343 device_context.cc:260] device: 0, cuDNN Version: 7.6.
Load pretraining parameters from 12L.
[train][1] progress: 1/1 step: 100, time: 99.803, speed: 1.002 steps/s
	current lr: 0.0000010
	lm_loss: 3.4257, ppl: 30.7427, loss: 14.3882, bow_loss: 10.9625, entropy_loss: -2.9835
[train][1] progress: 1/1 step: 200, time: 97.258, speed: 1.028 steps/s
	current lr: 0.0000020
	lm_loss: 3.5867, ppl: 36.1139, loss: 12.3028, bow_loss: 8.7161, entropy_loss: -2.9834
[train][1] progress: 1/1 step: 300, time: 98.584, speed: 1.014 steps/s
	current lr: 0.0000030
	lm_loss: 3.8268, ppl: 45.9132, loss: 11.2764, bow_loss: 7.4497, entropy_loss: -2.9814
[train][1] progress: 1/1 step: 400, time: 98.593, speed: 1.014 steps/s
	current lr: 0.0000040
	lm_loss: 3.6761, ppl: 39.4931, loss: 10.7495, bow_loss: 7.0734, entropy_loss: -2.9832
[train][1] progress: 1/1 step: 500, time: 98.803, speed: 1.012 steps/s
	current lr: 0.0000050
	lm_loss: 3.5618, ppl: 35.2279, loss: 10.4331, bow_loss: 6.8713, entropy_loss: -2.9795
[train][1] progress: 1/1 step: 600, time: 98.122, speed: 1.019 steps/s
	current lr: 0.0000060
	lm_loss: 3.8227, ppl: 45.7264, loss: 10.9147, bow_loss: 7.0920, entropy_loss: -2.9766
[train][1] progress: 1/1 step: 700, time: 98.469, speed: 1.016 steps/s
	current lr: 0.0000070
	lm_loss: 3.6934, ppl: 40.1799, loss: 10.5083, bow_loss: 6.8149, entropy_loss: -2.9758
[train][1] progress: 1/1 step: 800, time: 98.700, speed: 1.013 steps/s
	current lr: 0.0000080
	lm_loss: 3.5360, ppl: 34.3291, loss: 10.3030, bow_loss: 6.7670, entropy_loss: -2.9648
[train][1] progress: 1/1 step: 900, time: 98.455, speed: 1.016 steps/s
	current lr: 0.0000090
	lm_loss: 3.8006, ppl: 44.7282, loss: 10.6954, bow_loss: 6.8948, entropy_loss: -2.9664
[train][1] progress: 1/1 step: 1000, time: 98.442, speed: 1.016 steps/s
	current lr: 0.0000100
	lm_loss: 3.6328, ppl: 37.8189, loss: 10.3775, bow_loss: 6.7447, entropy_loss: -2.9567
================================================================================
Evaluation:
	step 100:lm_loss: 3.4664, ppl: 32.0214, loss: 10.2352, bow_loss: 6.7452, entropy_loss: -2.9501
	step 200:lm_loss: 3.4724, ppl: 32.2140, loss: 10.2425, bow_loss: 6.7452, entropy_loss: -2.9500
	step 300:lm_loss: 3.4650, ppl: 31.9757, loss: 10.2322, bow_loss: 6.7418, entropy_loss: -2.9500
	step 400:lm_loss: 3.4612, ppl: 31.8566, loss: 10.2266, bow_loss: 6.7414, entropy_loss: -2.9501
	step 500:lm_loss: 3.4607, ppl: 31.8397, loss: 10.2233, bow_loss: 6.7424, entropy_loss: -2.9500
	step 600:lm_loss: 3.4605, ppl: 31.8344, loss: 10.2225, bow_loss: 6.7428, entropy_loss: -2.9501
	step 700:lm_loss: 3.4586, ppl: 31.7717, loss: 10.2225, bow_loss: 6.7443, entropy_loss: -2.9500
	step 800:lm_loss: 3.4565, ppl: 31.7074, loss: 10.2195, bow_loss: 6.7430, entropy_loss: -2.9500
	step 900:lm_loss: 3.4549, ppl: 31.6550, loss: 10.2177, bow_loss: 6.7424, entropy_loss: -2.9500
	step 1000:lm_loss: 3.4569, ppl: 31.7188, loss: 10.2189, bow_loss: 6.7417, entropy_loss: -2.9500
	step 1100:lm_loss: 3.4555, ppl: 31.6744, loss: 10.2177, bow_loss: 6.7419, entropy_loss: -2.9500
	step 1200:lm_loss: 3.4538, ppl: 31.6208, loss: 10.2162, bow_loss: 6.7409, entropy_loss: -2.9500
	step 1300:lm_loss: 3.4563, ppl: 31.6987, loss: 10.2195, bow_loss: 6.7426, entropy_loss: -2.9500
	step 1400:lm_loss: 3.4588, ppl: 31.7778, loss: 10.2222, bow_loss: 6.7427, entropy_loss: -2.9500
	step 1500:lm_loss: 3.4593, ppl: 31.7941, loss: 10.2229, bow_loss: 6.7424, entropy_loss: -2.9500
	step 1600:lm_loss: 3.4613, ppl: 31.8570, loss: 10.2246, bow_loss: 6.7427, entropy_loss: -2.9500
	step 1700:lm_loss: 3.4628, ppl: 31.9077, loss: 10.2282, bow_loss: 6.7460, entropy_loss: -2.9500
	step 1800:lm_loss: 3.4687, ppl: 32.0945, loss: 10.2390, bow_loss: 6.7550, entropy_loss: -2.9500
	step 1900:lm_loss: 3.4747, ppl: 32.2888, loss: 10.2501, bow_loss: 6.7640, entropy_loss: -2.9501
	step 2000:lm_loss: 3.4803, ppl: 32.4687, loss: 10.2593, bow_loss: 6.7714, entropy_loss: -2.9501
	step 2100:lm_loss: 3.4847, ppl: 32.6125, loss: 10.2669, bow_loss: 6.7776, entropy_loss: -2.9501
	step 2200:lm_loss: 3.4859, ppl: 32.6517, loss: 10.2620, bow_loss: 6.7764, entropy_loss: -2.9503
	step 2300:lm_loss: 3.4859, ppl: 32.6519, loss: 10.2486, bow_loss: 6.7708, entropy_loss: -2.9507
	step 2400:lm_loss: 3.4857, ppl: 32.6464, loss: 10.2405, bow_loss: 6.7672, entropy_loss: -2.9509
	step 2500:lm_loss: 3.4856, ppl: 32.6425, loss: 10.2353, bow_loss: 6.7657, entropy_loss: -2.9511
	step 2600:lm_loss: 3.4781, ppl: 32.3975, loss: 10.2327, bow_loss: 6.7701, entropy_loss: -2.9511
	step 2700:lm_loss: 3.4692, ppl: 32.1114, loss: 10.2311, bow_loss: 6.7766, entropy_loss: -2.9511
	step 2800:lm_loss: 3.4595, ppl: 31.8026, loss: 10.2285, bow_loss: 6.7825, entropy_loss: -2.9512
	step 2900:lm_loss: 3.4505, ppl: 31.5167, loss: 10.2256, bow_loss: 6.7877, entropy_loss: -2.9512
	step 3000:lm_loss: 3.4428, ppl: 31.2734, loss: 10.2238, bow_loss: 6.7932, entropy_loss: -2.9513
	step 3100:lm_loss: 3.4336, ppl: 30.9890, loss: 10.2198, bow_loss: 6.7980, entropy_loss: -2.9513
	step 3200:lm_loss: 3.4251, ppl: 30.7263, loss: 10.2176, bow_loss: 6.8033, entropy_loss: -2.9513
	step 3300:lm_loss: 3.4177, ppl: 30.4989, loss: 10.2156, bow_loss: 6.8081, entropy_loss: -2.9513
	step 3400:lm_loss: 3.4109, ppl: 30.2921, loss: 10.2145, bow_loss: 6.8136, entropy_loss: -2.9514
	step 3500:lm_loss: 3.4042, ppl: 30.0900, loss: 10.2139, bow_loss: 6.8194, entropy_loss: -2.9515
	step 3600:lm_loss: 3.3998, ppl: 29.9591, loss: 10.2146, bow_loss: 6.8249, entropy_loss: -2.9515
	step 3700:lm_loss: 3.3949, ppl: 29.8126, loss: 10.2149, bow_loss: 6.8300, entropy_loss: -2.9516
	step 3800:lm_loss: 3.3890, ppl: 29.6362, loss: 10.2136, bow_loss: 6.8347, entropy_loss: -2.9516
	step 3900:lm_loss: 3.3834, ppl: 29.4702, loss: 10.2121, bow_loss: 6.8394, entropy_loss: -2.9517
[Evaluation][1000]lm_loss: 3.3810, ppl: 29.4011, loss: 10.2122, bow_loss: 6.8421, entropy_loss: -2.9517
	time cost: 1600.115
================================================================================
[train][1] progress: 1/1 step: 1100, time: 98.298, speed: 1.017 steps/s
	current lr: 0.0000095
	lm_loss: 3.3476, ppl: 28.4341, loss: 10.2028, bow_loss: 6.8552, entropy_loss: -2.9443
[train][1] progress: 1/1 step: 1200, time: 98.544, speed: 1.015 steps/s
	current lr: 0.0000091
	lm_loss: 3.5316, ppl: 34.1794, loss: 10.2429, bow_loss: 6.7113, entropy_loss: -2.9499
[train][1] progress: 1/1 step: 1300, time: 97.960, speed: 1.021 steps/s
	current lr: 0.0000088
	lm_loss: 3.4497, ppl: 31.4906, loss: 10.1648, bow_loss: 6.7151, entropy_loss: -2.9361
[train][1] progress: 1/1 step: 1400, time: 98.602, speed: 1.014 steps/s
	current lr: 0.0000085
	lm_loss: 3.6185, ppl: 37.2815, loss: 10.3821, bow_loss: 6.7636, entropy_loss: -2.9224
[train][1] progress: 1/1 step: 1500, time: 98.127, speed: 1.019 steps/s
	current lr: 0.0000082
	lm_loss: 3.7083, ppl: 40.7834, loss: 10.4564, bow_loss: 6.7481, entropy_loss: -2.9189
[train][1] progress: 1/1 step: 1600, time: 98.528, speed: 1.015 steps/s
	current lr: 0.0000079
	lm_loss: 3.7511, ppl: 42.5700, loss: 10.5122, bow_loss: 6.7611, entropy_loss: -2.9159
[train][1] progress: 1/1 step: 1700, time: 98.687, speed: 1.013 steps/s
	current lr: 0.0000077
	lm_loss: 3.6410, ppl: 38.1292, loss: 10.3222, bow_loss: 6.6812, entropy_loss: -2.9255
[train][1] progress: 1/1 step: 1800, time: 98.759, speed: 1.013 steps/s
	current lr: 0.0000075
	lm_loss: 3.4974, ppl: 33.0295, loss: 10.1758, bow_loss: 6.6784, entropy_loss: -2.9050
[train][1] progress: 1/1 step: 1900, time: 99.050, speed: 1.010 steps/s
	current lr: 0.0000073
	lm_loss: 3.6092, ppl: 36.9358, loss: 10.2967, bow_loss: 6.6876, entropy_loss: -2.9004
[train][1] progress: 1/1 step: 2000, time: 98.733, speed: 1.013 steps/s
	current lr: 0.0000071
	lm_loss: 3.3796, ppl: 29.3595, loss: 10.0358, bow_loss: 6.6561, entropy_loss: -2.8901
================================================================================
Evaluation:
	step 100:lm_loss: 3.4821, ppl: 32.5266, loss: 10.1558, bow_loss: 6.6408, entropy_loss: -2.8804
	step 200:lm_loss: 3.4700, ppl: 32.1370, loss: 10.1392, bow_loss: 6.6352, entropy_loss: -2.8802
	step 300:lm_loss: 3.4654, ppl: 31.9896, loss: 10.1349, bow_loss: 6.6361, entropy_loss: -2.8800
	step 400:lm_loss: 3.4596, ppl: 31.8029, loss: 10.1289, bow_loss: 6.6377, entropy_loss: -2.8797
	step 500:lm_loss: 3.4594, ppl: 31.7982, loss: 10.1243, bow_loss: 6.6362, entropy_loss: -2.8799
	step 600:lm_loss: 3.4562, ppl: 31.6964, loss: 10.1215, bow_loss: 6.6355, entropy_loss: -2.8800
	step 700:lm_loss: 3.4567, ppl: 31.7117, loss: 10.1216, bow_loss: 6.6359, entropy_loss: -2.8800
	step 800:lm_loss: 3.4549, ppl: 31.6547, loss: 10.1195, bow_loss: 6.6357, entropy_loss: -2.8800
	step 900:lm_loss: 3.4536, ppl: 31.6154, loss: 10.1182, bow_loss: 6.6349, entropy_loss: -2.8800
	step 1000:lm_loss: 3.4534, ppl: 31.6077, loss: 10.1176, bow_loss: 6.6352, entropy_loss: -2.8801
	step 1100:lm_loss: 3.4512, ppl: 31.5367, loss: 10.1148, bow_loss: 6.6351, entropy_loss: -2.8800
	step 1200:lm_loss: 3.4492, ppl: 31.4741, loss: 10.1127, bow_loss: 6.6351, entropy_loss: -2.8800
	step 1300:lm_loss: 3.4486, ppl: 31.4565, loss: 10.1122, bow_loss: 6.6346, entropy_loss: -2.8800
	step 1400:lm_loss: 3.4489, ppl: 31.4654, loss: 10.1122, bow_loss: 6.6347, entropy_loss: -2.8801
	step 1500:lm_loss: 3.4474, ppl: 31.4174, loss: 10.1114, bow_loss: 6.6338, entropy_loss: -2.8800
	step 1600:lm_loss: 3.4468, ppl: 31.4007, loss: 10.1103, bow_loss: 6.6337, entropy_loss: -2.8801
	step 1700:lm_loss: 3.4480, ppl: 31.4382, loss: 10.1125, bow_loss: 6.6363, entropy_loss: -2.8801
	step 1800:lm_loss: 3.4532, ppl: 31.6013, loss: 10.1220, bow_loss: 6.6439, entropy_loss: -2.8802
	step 1900:lm_loss: 3.4598, ppl: 31.8114, loss: 10.1333, bow_loss: 6.6538, entropy_loss: -2.8803
	step 2000:lm_loss: 3.4656, ppl: 31.9956, loss: 10.1424, bow_loss: 6.6611, entropy_loss: -2.8805
	step 2100:lm_loss: 3.4703, ppl: 32.1480, loss: 10.1510, bow_loss: 6.6683, entropy_loss: -2.8806
	step 2200:lm_loss: 3.4720, ppl: 32.2026, loss: 10.1470, bow_loss: 6.6669, entropy_loss: -2.8811
	step 2300:lm_loss: 3.4728, ppl: 32.2273, loss: 10.1411, bow_loss: 6.6641, entropy_loss: -2.8818
	step 2400:lm_loss: 3.4736, ppl: 32.2535, loss: 10.1343, bow_loss: 6.6615, entropy_loss: -2.8824
	step 2500:lm_loss: 3.4743, ppl: 32.2754, loss: 10.1285, bow_loss: 6.6583, entropy_loss: -2.8830
	step 2600:lm_loss: 3.4679, ppl: 32.0705, loss: 10.1261, bow_loss: 6.6621, entropy_loss: -2.8832
	step 2700:lm_loss: 3.4582, ppl: 31.7598, loss: 10.1215, bow_loss: 6.6665, entropy_loss: -2.8834
	step 2800:lm_loss: 3.4500, ppl: 31.4995, loss: 10.1181, bow_loss: 6.6707, entropy_loss: -2.8836
	step 2900:lm_loss: 3.4412, ppl: 31.2251, loss: 10.1154, bow_loss: 6.6755, entropy_loss: -2.8838
	step 3000:lm_loss: 3.4327, ppl: 30.9595, loss: 10.1128, bow_loss: 6.6805, entropy_loss: -2.8840
	step 3100:lm_loss: 3.4247, ppl: 30.7149, loss: 10.1099, bow_loss: 6.6848, entropy_loss: -2.8842
	step 3200:lm_loss: 3.4176, ppl: 30.4968, loss: 10.1074, bow_loss: 6.6888, entropy_loss: -2.8844
	step 3300:lm_loss: 3.4088, ppl: 30.2298, loss: 10.1038, bow_loss: 6.6929, entropy_loss: -2.8845
	step 3400:lm_loss: 3.4028, ppl: 30.0482, loss: 10.1022, bow_loss: 6.6968, entropy_loss: -2.8848
	step 3500:lm_loss: 3.3971, ppl: 29.8772, loss: 10.1001, bow_loss: 6.7006, entropy_loss: -2.8850
	step 3600:lm_loss: 3.3927, ppl: 29.7473, loss: 10.1000, bow_loss: 6.7048, entropy_loss: -2.8853
	step 3700:lm_loss: 3.3876, ppl: 29.5955, loss: 10.0987, bow_loss: 6.7088, entropy_loss: -2.8855
	step 3800:lm_loss: 3.3827, ppl: 29.4496, loss: 10.0979, bow_loss: 6.7127, entropy_loss: -2.8857
	step 3900:lm_loss: 3.3769, ppl: 29.2810, loss: 10.0967, bow_loss: 6.7169, entropy_loss: -2.8859
[Evaluation][2000]lm_loss: 3.3740, ppl: 29.1955, loss: 10.0958, bow_loss: 6.7190, entropy_loss: -2.8861
	time cost: 1607.881
================================================================================
[train][1] progress: 1/1 step: 2100, time: 97.832, speed: 1.022 steps/s
	current lr: 0.0000069
	lm_loss: 3.5060, ppl: 33.3158, loss: 10.2496, bow_loss: 6.7436, entropy_loss: -2.8658
[train][1] progress: 1/1 step: 2200, time: 98.347, speed: 1.017 steps/s
	current lr: 0.0000067
	lm_loss: 3.4237, ppl: 30.6813, loss: 10.1101, bow_loss: 6.6865, entropy_loss: -2.8604
[train][1] progress: 1/1 step: 2300, time: 97.969, speed: 1.021 steps/s
	current lr: 0.0000066
	lm_loss: 3.5738, ppl: 35.6510, loss: 10.1428, bow_loss: 6.5690, entropy_loss: -2.8636
[train][1] progress: 1/1 step: 2400, time: 97.081, speed: 1.030 steps/s
	current lr: 0.0000065
	lm_loss: 3.5733, ppl: 35.6349, loss: 10.2363, bow_loss: 6.6630, entropy_loss: -2.8631
[train][1] progress: 1/1 step: 2500, time: 98.112, speed: 1.019 steps/s
	current lr: 0.0000063
	lm_loss: 3.3609, ppl: 28.8151, loss: 9.9969, bow_loss: 6.6360, entropy_loss: -2.8389
[train][1] progress: 1/1 step: 2600, time: 97.985, speed: 1.021 steps/s
	current lr: 0.0000062
	lm_loss: 3.6731, ppl: 39.3739, loss: 10.3217, bow_loss: 6.6486, entropy_loss: -2.8276
[train][1] progress: 1/1 step: 2700, time: 97.134, speed: 1.030 steps/s
	current lr: 0.0000061
	lm_loss: 3.5122, ppl: 33.5225, loss: 10.1930, bow_loss: 6.6807, entropy_loss: -2.8035
[train][1] progress: 1/1 step: 2800, time: 98.006, speed: 1.020 steps/s
	current lr: 0.0000060
	lm_loss: 3.4758, ppl: 32.3226, loss: 10.1913, bow_loss: 6.7155, entropy_loss: -2.7762
[train][1] progress: 1/1 step: 2900, time: 97.718, speed: 1.023 steps/s
	current lr: 0.0000059
	lm_loss: 3.5686, ppl: 35.4665, loss: 10.1404, bow_loss: 6.5718, entropy_loss: -2.7694
[train][1] progress: 1/1 step: 3000, time: 98.183, speed: 1.019 steps/s
	current lr: 0.0000058
	lm_loss: 3.6872, ppl: 39.9334, loss: 10.2909, bow_loss: 6.6037, entropy_loss: -2.7551
================================================================================
Evaluation:
	step 100:lm_loss: 3.4510, ppl: 31.5307, loss: 10.0894, bow_loss: 6.6012, entropy_loss: -2.7227
	step 200:lm_loss: 3.4632, ppl: 31.9189, loss: 10.0951, bow_loss: 6.6036, entropy_loss: -2.7230
	step 300:lm_loss: 3.4609, ppl: 31.8461, loss: 10.0923, bow_loss: 6.6064, entropy_loss: -2.7223
	step 400:lm_loss: 3.4587, ppl: 31.7748, loss: 10.0868, bow_loss: 6.6054, entropy_loss: -2.7223
	step 500:lm_loss: 3.4574, ppl: 31.7329, loss: 10.0838, bow_loss: 6.6054, entropy_loss: -2.7226
	step 600:lm_loss: 3.4521, ppl: 31.5651, loss: 10.0787, bow_loss: 6.6048, entropy_loss: -2.7226
	step 700:lm_loss: 3.4507, ppl: 31.5211, loss: 10.0771, bow_loss: 6.6055, entropy_loss: -2.7225
	step 800:lm_loss: 3.4516, ppl: 31.5503, loss: 10.0803, bow_loss: 6.6063, entropy_loss: -2.7227
	step 900:lm_loss: 3.4499, ppl: 31.4971, loss: 10.0781, bow_loss: 6.6057, entropy_loss: -2.7227
	step 1000:lm_loss: 3.4486, ppl: 31.4574, loss: 10.0762, bow_loss: 6.6050, entropy_loss: -2.7228
	step 1100:lm_loss: 3.4471, ppl: 31.4089, loss: 10.0744, bow_loss: 6.6049, entropy_loss: -2.7227
	step 1200:lm_loss: 3.4458, ppl: 31.3696, loss: 10.0732, bow_loss: 6.6047, entropy_loss: -2.7227
	step 1300:lm_loss: 3.4456, ppl: 31.3616, loss: 10.0737, bow_loss: 6.6052, entropy_loss: -2.7227
	step 1400:lm_loss: 3.4451, ppl: 31.3468, loss: 10.0727, bow_loss: 6.6049, entropy_loss: -2.7226
	step 1500:lm_loss: 3.4448, ppl: 31.3362, loss: 10.0720, bow_loss: 6.6044, entropy_loss: -2.7227
	step 1600:lm_loss: 3.4439, ppl: 31.3078, loss: 10.0713, bow_loss: 6.6036, entropy_loss: -2.7228
	step 1700:lm_loss: 3.4450, ppl: 31.3424, loss: 10.0737, bow_loss: 6.6066, entropy_loss: -2.7228
	step 1800:lm_loss: 3.4494, ppl: 31.4824, loss: 10.0824, bow_loss: 6.6144, entropy_loss: -2.7230
	step 1900:lm_loss: 3.4555, ppl: 31.6740, loss: 10.0916, bow_loss: 6.6219, entropy_loss: -2.7234
	step 2000:lm_loss: 3.4614, ppl: 31.8611, loss: 10.1010, bow_loss: 6.6293, entropy_loss: -2.7238
	step 2100:lm_loss: 3.4664, ppl: 32.0214, loss: 10.1100, bow_loss: 6.6363, entropy_loss: -2.7240
	step 2200:lm_loss: 3.4694, ppl: 32.1166, loss: 10.1096, bow_loss: 6.6366, entropy_loss: -2.7254
	step 2300:lm_loss: 3.4706, ppl: 32.1562, loss: 10.1058, bow_loss: 6.6344, entropy_loss: -2.7266
	step 2400:lm_loss: 3.4716, ppl: 32.1897, loss: 10.0980, bow_loss: 6.6301, entropy_loss: -2.7285
	step 2500:lm_loss: 3.4725, ppl: 32.2185, loss: 10.0917, bow_loss: 6.6267, entropy_loss: -2.7300
	step 2600:lm_loss: 3.4661, ppl: 32.0123, loss: 10.0871, bow_loss: 6.6290, entropy_loss: -2.7309
	step 2700:lm_loss: 3.4567, ppl: 31.7119, loss: 10.0828, bow_loss: 6.6330, entropy_loss: -2.7314
	step 2800:lm_loss: 3.4480, ppl: 31.4382, loss: 10.0790, bow_loss: 6.6371, entropy_loss: -2.7318
	step 2900:lm_loss: 3.4395, ppl: 31.1716, loss: 10.0757, bow_loss: 6.6412, entropy_loss: -2.7321
	step 3000:lm_loss: 3.4315, ppl: 30.9239, loss: 10.0724, bow_loss: 6.6446, entropy_loss: -2.7325
	step 3100:lm_loss: 3.4247, ppl: 30.7144, loss: 10.0695, bow_loss: 6.6480, entropy_loss: -2.7330
	step 3200:lm_loss: 3.4150, ppl: 30.4158, loss: 10.0643, bow_loss: 6.6515, entropy_loss: -2.7334
	step 3300:lm_loss: 3.4078, ppl: 30.1999, loss: 10.0613, bow_loss: 6.6547, entropy_loss: -2.7337
	step 3400:lm_loss: 3.4017, ppl: 30.0156, loss: 10.0589, bow_loss: 6.6582, entropy_loss: -2.7342
	step 3500:lm_loss: 3.3957, ppl: 29.8356, loss: 10.0571, bow_loss: 6.6616, entropy_loss: -2.7346
	step 3600:lm_loss: 3.3914, ppl: 29.7064, loss: 10.0560, bow_loss: 6.6651, entropy_loss: -2.7353
	step 3700:lm_loss: 3.3868, ppl: 29.5702, loss: 10.0544, bow_loss: 6.6680, entropy_loss: -2.7359
	step 3800:lm_loss: 3.3816, ppl: 29.4183, loss: 10.0523, bow_loss: 6.6713, entropy_loss: -2.7366
	step 3900:lm_loss: 3.3768, ppl: 29.2775, loss: 10.0506, bow_loss: 6.6743, entropy_loss: -2.7370
[Evaluation][3000]lm_loss: 3.3738, ppl: 29.1897, loss: 10.0493, bow_loss: 6.6759, entropy_loss: -2.7373
	time cost: 1594.746
================================================================================
[train][1] progress: 1/1 step: 3100, time: 97.753, speed: 1.023 steps/s
	current lr: 0.0000057
	lm_loss: 3.6243, ppl: 37.4977, loss: 10.2635, bow_loss: 6.6392, entropy_loss: -2.7625
[train][1] progress: 1/1 step: 3200, time: 97.878, speed: 1.022 steps/s
	current lr: 0.0000056
	lm_loss: 3.4943, ppl: 32.9272, loss: 10.0694, bow_loss: 6.5751, entropy_loss: -2.6919
[train][1] progress: 1/1 step: 3300, time: 97.125, speed: 1.030 steps/s
	current lr: 0.0000055
	lm_loss: 3.5673, ppl: 35.4197, loss: 10.1857, bow_loss: 6.6184, entropy_loss: -2.7162
[train][1] progress: 1/1 step: 3400, time: 97.092, speed: 1.030 steps/s
	current lr: 0.0000054
	lm_loss: 3.5005, ppl: 33.1323, loss: 10.1116, bow_loss: 6.6111, entropy_loss: -2.6819
[train][1] progress: 1/1 step: 3500, time: 97.987, speed: 1.021 steps/s
	current lr: 0.0000053
	lm_loss: 3.6240, ppl: 37.4865, loss: 10.1303, bow_loss: 6.5063, entropy_loss: -2.7164
[train][1] progress: 1/1 step: 3600, time: 98.152, speed: 1.019 steps/s
	current lr: 0.0000053
	lm_loss: 3.5388, ppl: 34.4272, loss: 10.1468, bow_loss: 6.6079, entropy_loss: -2.6962
[train][1] progress: 1/1 step: 3700, time: 97.930, speed: 1.021 steps/s
	current lr: 0.0000052
	lm_loss: 3.6011, ppl: 36.6386, loss: 10.3349, bow_loss: 6.7338, entropy_loss: -2.6590
[train][1] progress: 1/1 step: 3800, time: 97.916, speed: 1.021 steps/s
	current lr: 0.0000051
	lm_loss: 3.3960, ppl: 29.8439, loss: 10.0357, bow_loss: 6.6398, entropy_loss: -2.6871
[train][1] progress: 1/1 step: 3900, time: 97.682, speed: 1.024 steps/s
	current lr: 0.0000051
	lm_loss: 3.5882, ppl: 36.1677, loss: 10.2572, bow_loss: 6.6690, entropy_loss: -2.6495
[train][1] progress: 1/1 step: 4000, time: 97.920, speed: 1.021 steps/s
	current lr: 0.0000050
	lm_loss: 3.2545, ppl: 25.9076, loss: 9.9466, bow_loss: 6.6921, entropy_loss: -2.5711
================================================================================
Evaluation:
	step 100:lm_loss: 3.4450, ppl: 31.3423, loss: 10.0463, bow_loss: 6.5818, entropy_loss: -2.5947
	step 200:lm_loss: 3.4501, ppl: 31.5042, loss: 10.0556, bow_loss: 6.5781, entropy_loss: -2.5960
	step 300:lm_loss: 3.4609, ppl: 31.8448, loss: 10.0623, bow_loss: 6.5791, entropy_loss: -2.5969
	step 400:lm_loss: 3.4519, ppl: 31.5608, loss: 10.0547, bow_loss: 6.5789, entropy_loss: -2.5973
	step 500:lm_loss: 3.4548, ppl: 31.6527, loss: 10.0544, bow_loss: 6.5791, entropy_loss: -2.5976
	step 600:lm_loss: 3.4539, ppl: 31.6222, loss: 10.0518, bow_loss: 6.5790, entropy_loss: -2.5972
	step 700:lm_loss: 3.4524, ppl: 31.5750, loss: 10.0498, bow_loss: 6.5791, entropy_loss: -2.5970
	step 800:lm_loss: 3.4528, ppl: 31.5883, loss: 10.0522, bow_loss: 6.5806, entropy_loss: -2.5973
	step 900:lm_loss: 3.4475, ppl: 31.4228, loss: 10.0472, bow_loss: 6.5787, entropy_loss: -2.5973
	step 1000:lm_loss: 3.4459, ppl: 31.3708, loss: 10.0450, bow_loss: 6.5784, entropy_loss: -2.5971
	step 1100:lm_loss: 3.4465, ppl: 31.3893, loss: 10.0454, bow_loss: 6.5787, entropy_loss: -2.5972
	step 1200:lm_loss: 3.4459, ppl: 31.3702, loss: 10.0448, bow_loss: 6.5787, entropy_loss: -2.5979
	step 1300:lm_loss: 3.4439, ppl: 31.3092, loss: 10.0432, bow_loss: 6.5786, entropy_loss: -2.5975
	step 1400:lm_loss: 3.4448, ppl: 31.3357, loss: 10.0438, bow_loss: 6.5789, entropy_loss: -2.5975
	step 1500:lm_loss: 3.4446, ppl: 31.3321, loss: 10.0436, bow_loss: 6.5793, entropy_loss: -2.5975
	step 1600:lm_loss: 3.4442, ppl: 31.3175, loss: 10.0422, bow_loss: 6.5779, entropy_loss: -2.5976
	step 1700:lm_loss: 3.4452, ppl: 31.3501, loss: 10.0455, bow_loss: 6.5810, entropy_loss: -2.5977
	step 1800:lm_loss: 3.4519, ppl: 31.5608, loss: 10.0569, bow_loss: 6.5899, entropy_loss: -2.5981
	step 1900:lm_loss: 3.4575, ppl: 31.7363, loss: 10.0658, bow_loss: 6.5975, entropy_loss: -2.5986
	step 2000:lm_loss: 3.4626, ppl: 31.9013, loss: 10.0753, bow_loss: 6.6047, entropy_loss: -2.5992
	step 2100:lm_loss: 3.4677, ppl: 32.0623, loss: 10.0833, bow_loss: 6.6109, entropy_loss: -2.5995
	step 2200:lm_loss: 3.4693, ppl: 32.1145, loss: 10.0784, bow_loss: 6.6090, entropy_loss: -2.6022
	step 2300:lm_loss: 3.4713, ppl: 32.1789, loss: 10.0741, bow_loss: 6.6061, entropy_loss: -2.6048
	step 2400:lm_loss: 3.4727, ppl: 32.2230, loss: 10.0703, bow_loss: 6.6041, entropy_loss: -2.6069
	step 2500:lm_loss: 3.4737, ppl: 32.2574, loss: 10.0664, bow_loss: 6.6021, entropy_loss: -2.6087
	step 2600:lm_loss: 3.4670, ppl: 32.0406, loss: 10.0613, bow_loss: 6.6041, entropy_loss: -2.6099
	step 2700:lm_loss: 3.4588, ppl: 31.7794, loss: 10.0584, bow_loss: 6.6083, entropy_loss: -2.6102
	step 2800:lm_loss: 3.4508, ppl: 31.5242, loss: 10.0550, bow_loss: 6.6120, entropy_loss: -2.6107
	step 2900:lm_loss: 3.4413, ppl: 31.2290, loss: 10.0497, bow_loss: 6.6151, entropy_loss: -2.6113
	step 3000:lm_loss: 3.4330, ppl: 30.9694, loss: 10.0457, bow_loss: 6.6185, entropy_loss: -2.6118
	step 3100:lm_loss: 3.4249, ppl: 30.7209, loss: 10.0423, bow_loss: 6.6220, entropy_loss: -2.6123
	step 3200:lm_loss: 3.4177, ppl: 30.4983, loss: 10.0390, bow_loss: 6.6253, entropy_loss: -2.6129
	step 3300:lm_loss: 3.4096, ppl: 30.2544, loss: 10.0358, bow_loss: 6.6288, entropy_loss: -2.6133
	step 3400:lm_loss: 3.4037, ppl: 30.0747, loss: 10.0333, bow_loss: 6.6322, entropy_loss: -2.6141
	step 3500:lm_loss: 3.3976, ppl: 29.8928, loss: 10.0315, bow_loss: 6.6359, entropy_loss: -2.6149
	step 3600:lm_loss: 3.3926, ppl: 29.7445, loss: 10.0296, bow_loss: 6.6391, entropy_loss: -2.6155
	step 3700:lm_loss: 3.3876, ppl: 29.5946, loss: 10.0277, bow_loss: 6.6419, entropy_loss: -2.6162
	step 3800:lm_loss: 3.3825, ppl: 29.4454, loss: 10.0249, bow_loss: 6.6444, entropy_loss: -2.6171
	step 3900:lm_loss: 3.3792, ppl: 29.3469, loss: 10.0243, bow_loss: 6.6470, entropy_loss: -2.6177
[Evaluation][4000]lm_loss: 3.3764, ppl: 29.2649, loss: 10.0229, bow_loss: 6.6485, entropy_loss: -2.6180
	time cost: 1599.919
================================================================================
[train][1] progress: 1/1 step: 4100, time: 97.026, speed: 1.031 steps/s
	current lr: 0.0000049
	lm_loss: 3.5024, ppl: 33.1964, loss: 10.1443, bow_loss: 6.6419, entropy_loss: -2.5892
[train][1] progress: 1/1 step: 4200, time: 98.026, speed: 1.020 steps/s
	current lr: 0.0000049
	lm_loss: 3.6305, ppl: 37.7301, loss: 10.3090, bow_loss: 6.6785, entropy_loss: -2.5976
[train][1] progress: 1/1 step: 4300, time: 98.022, speed: 1.020 steps/s
	current lr: 0.0000048
	lm_loss: 3.4241, ppl: 30.6941, loss: 9.9401, bow_loss: 6.5160, entropy_loss: -2.6010
[train][1] progress: 1/1 step: 4400, time: 97.039, speed: 1.031 steps/s
	current lr: 0.0000048
	lm_loss: 3.6247, ppl: 37.5132, loss: 10.2471, bow_loss: 6.6224, entropy_loss: -2.5165
[train][1] progress: 1/1 step: 4500, time: 97.641, speed: 1.024 steps/s
	current lr: 0.0000047
	lm_loss: 2.8841, ppl: 17.8876, loss: 9.3130, bow_loss: 6.4289, entropy_loss: -2.4795
[train][1] progress: 1/1 step: 4600, time: 98.040, speed: 1.020 steps/s
	current lr: 0.0000047
	lm_loss: 3.5483, ppl: 34.7550, loss: 10.1259, bow_loss: 6.5776, entropy_loss: -2.5563
[train][1] progress: 1/1 step: 4700, time: 98.159, speed: 1.019 steps/s
	current lr: 0.0000046
	lm_loss: 3.3092, ppl: 27.3634, loss: 9.9310, bow_loss: 6.6218, entropy_loss: -2.4331
[train][1] progress: 1/1 step: 4800, time: 97.042, speed: 1.030 steps/s
	current lr: 0.0000046
	lm_loss: 3.5722, ppl: 35.5948, loss: 10.2160, bow_loss: 6.6439, entropy_loss: -2.4385
[train][1] progress: 1/1 step: 4900, time: 98.089, speed: 1.019 steps/s
	current lr: 0.0000045
	lm_loss: 3.6375, ppl: 37.9984, loss: 10.2070, bow_loss: 6.5695, entropy_loss: -2.4602
[train][1] progress: 1/1 step: 5000, time: 98.026, speed: 1.020 steps/s
	current lr: 0.0000045
	lm_loss: 3.6608, ppl: 38.8915, loss: 10.3175, bow_loss: 6.6567, entropy_loss: -2.4436
================================================================================
Evaluation:
	step 100:lm_loss: 3.4519, ppl: 31.5589, loss: 10.0288, bow_loss: 6.5510, entropy_loss: -2.4024
	step 200:lm_loss: 3.4534, ppl: 31.6090, loss: 10.0365, bow_loss: 6.5535, entropy_loss: -2.4063
	step 300:lm_loss: 3.4598, ppl: 31.8100, loss: 10.0459, bow_loss: 6.5588, entropy_loss: -2.4067
	step 400:lm_loss: 3.4524, ppl: 31.5749, loss: 10.0336, bow_loss: 6.5566, entropy_loss: -2.4045
	step 500:lm_loss: 3.4491, ppl: 31.4717, loss: 10.0298, bow_loss: 6.5569, entropy_loss: -2.4051
	step 600:lm_loss: 3.4498, ppl: 31.4950, loss: 10.0325, bow_loss: 6.5576, entropy_loss: -2.4051
	step 700:lm_loss: 3.4506, ppl: 31.5189, loss: 10.0333, bow_loss: 6.5586, entropy_loss: -2.4060
	step 800:lm_loss: 3.4489, ppl: 31.4657, loss: 10.0311, bow_loss: 6.5590, entropy_loss: -2.4050
	step 900:lm_loss: 3.4494, ppl: 31.4804, loss: 10.0313, bow_loss: 6.5585, entropy_loss: -2.4059
	step 1000:lm_loss: 3.4468, ppl: 31.4013, loss: 10.0280, bow_loss: 6.5579, entropy_loss: -2.4052
	step 1100:lm_loss: 3.4444, ppl: 31.3240, loss: 10.0254, bow_loss: 6.5571, entropy_loss: -2.4052
	step 1200:lm_loss: 3.4443, ppl: 31.3224, loss: 10.0259, bow_loss: 6.5570, entropy_loss: -2.4056
	step 1300:lm_loss: 3.4435, ppl: 31.2958, loss: 10.0253, bow_loss: 6.5571, entropy_loss: -2.4055
	step 1400:lm_loss: 3.4420, ppl: 31.2484, loss: 10.0240, bow_loss: 6.5573, entropy_loss: -2.4051
	step 1500:lm_loss: 3.4421, ppl: 31.2519, loss: 10.0239, bow_loss: 6.5569, entropy_loss: -2.4054
	step 1600:lm_loss: 3.4417, ppl: 31.2390, loss: 10.0234, bow_loss: 6.5568, entropy_loss: -2.4055
	step 1700:lm_loss: 3.4437, ppl: 31.3014, loss: 10.0270, bow_loss: 6.5601, entropy_loss: -2.4053
	step 1800:lm_loss: 3.4508, ppl: 31.5263, loss: 10.0384, bow_loss: 6.5683, entropy_loss: -2.4065
	step 1900:lm_loss: 3.4554, ppl: 31.6722, loss: 10.0455, bow_loss: 6.5749, entropy_loss: -2.4069
	step 2000:lm_loss: 3.4603, ppl: 31.8259, loss: 10.0532, bow_loss: 6.5812, entropy_loss: -2.4076
	step 2100:lm_loss: 3.4656, ppl: 31.9944, loss: 10.0633, bow_loss: 6.5890, entropy_loss: -2.4082
	step 2200:lm_loss: 3.4680, ppl: 32.0729, loss: 10.0627, bow_loss: 6.5894, entropy_loss: -2.4107
	step 2300:lm_loss: 3.4694, ppl: 32.1181, loss: 10.0566, bow_loss: 6.5856, entropy_loss: -2.4149
	step 2400:lm_loss: 3.4708, ppl: 32.1629, loss: 10.0516, bow_loss: 6.5825, entropy_loss: -2.4186
	step 2500:lm_loss: 3.4723, ppl: 32.2119, loss: 10.0484, bow_loss: 6.5809, entropy_loss: -2.4216
	step 2600:lm_loss: 3.4666, ppl: 32.0273, loss: 10.0442, bow_loss: 6.5828, entropy_loss: -2.4233
	step 2700:lm_loss: 3.4576, ppl: 31.7405, loss: 10.0406, bow_loss: 6.5868, entropy_loss: -2.4242
	step 2800:lm_loss: 3.4492, ppl: 31.4744, loss: 10.0364, bow_loss: 6.5899, entropy_loss: -2.4248
	step 2900:lm_loss: 3.4405, ppl: 31.2015, loss: 10.0318, bow_loss: 6.5930, entropy_loss: -2.4255
	step 3000:lm_loss: 3.4325, ppl: 30.9543, loss: 10.0275, bow_loss: 6.5960, entropy_loss: -2.4267
	step 3100:lm_loss: 3.4251, ppl: 30.7250, loss: 10.0242, bow_loss: 6.5994, entropy_loss: -2.4276
	step 3200:lm_loss: 3.4172, ppl: 30.4843, loss: 10.0204, bow_loss: 6.6025, entropy_loss: -2.4279
	step 3300:lm_loss: 3.4091, ppl: 30.2392, loss: 10.0161, bow_loss: 6.6049, entropy_loss: -2.4283
	step 3400:lm_loss: 3.4014, ppl: 30.0055, loss: 10.0127, bow_loss: 6.6085, entropy_loss: -2.4290
	step 3500:lm_loss: 3.3975, ppl: 29.8906, loss: 10.0109, bow_loss: 6.6106, entropy_loss: -2.4299
	step 3600:lm_loss: 3.3931, ppl: 29.7592, loss: 10.0092, bow_loss: 6.6135, entropy_loss: -2.4311
	step 3700:lm_loss: 3.3884, ppl: 29.6183, loss: 10.0070, bow_loss: 6.6157, entropy_loss: -2.4320
	step 3800:lm_loss: 3.3832, ppl: 29.4661, loss: 10.0042, bow_loss: 6.6182, entropy_loss: -2.4333
	step 3900:lm_loss: 3.3790, ppl: 29.3412, loss: 10.0027, bow_loss: 6.6208, entropy_loss: -2.4340
[Evaluation][5000]lm_loss: 3.3760, ppl: 29.2527, loss: 10.0014, bow_loss: 6.6224, entropy_loss: -2.4344
	time cost: 1599.894
================================================================================
[train][1] progress: 1/1 step: 5100, time: 97.767, speed: 1.023 steps/s
	current lr: 0.0000044
	lm_loss: 3.5945, ppl: 36.3959, loss: 10.2866, bow_loss: 6.6922, entropy_loss: -2.4187
[train][1] progress: 1/1 step: 5200, time: 97.939, speed: 1.021 steps/s
	current lr: 0.0000044
	lm_loss: 3.5500, ppl: 34.8119, loss: 10.1680, bow_loss: 6.6181, entropy_loss: -2.3250
[train][1] progress: 1/1 step: 5300, time: 97.290, speed: 1.028 steps/s
	current lr: 0.0000043
	lm_loss: 3.4576, ppl: 31.7417, loss: 10.0512, bow_loss: 6.5936, entropy_loss: -2.3545
[train][1] progress: 1/1 step: 5400, time: 97.951, speed: 1.021 steps/s
	current lr: 0.0000043
	lm_loss: 3.6632, ppl: 38.9840, loss: 10.2211, bow_loss: 6.5580, entropy_loss: -2.4491
[train][1] progress: 1/1 step: 5500, time: 98.183, speed: 1.019 steps/s
	current lr: 0.0000043
	lm_loss: 3.6283, ppl: 37.6500, loss: 10.1636, bow_loss: 6.5353, entropy_loss: -2.4438
[train][1] progress: 1/1 step: 5600, time: 97.660, speed: 1.024 steps/s
	current lr: 0.0000042
	lm_loss: 3.5025, ppl: 33.1982, loss: 10.0383, bow_loss: 6.5358, entropy_loss: -2.2960
[train][1] progress: 1/1 step: 5700, time: 97.017, speed: 1.031 steps/s
	current lr: 0.0000042
	lm_loss: 3.5013, ppl: 33.1577, loss: 10.0252, bow_loss: 6.5239, entropy_loss: -2.3128
[train][1] progress: 1/1 step: 5800, time: 98.066, speed: 1.020 steps/s
	current lr: 0.0000042
	lm_loss: 3.4837, ppl: 32.5807, loss: 10.0771, bow_loss: 6.5934, entropy_loss: -2.1779
[train][1] progress: 1/1 step: 5900, time: 98.040, speed: 1.020 steps/s
	current lr: 0.0000041
	lm_loss: 3.2953, ppl: 26.9849, loss: 9.8065, bow_loss: 6.5112, entropy_loss: -2.1037
[train][1] progress: 1/1 step: 6000, time: 98.041, speed: 1.020 steps/s
	current lr: 0.0000041
	lm_loss: 3.5972, ppl: 36.4965, loss: 10.1700, bow_loss: 6.5728, entropy_loss: -2.2508
================================================================================
Evaluation:
	step 100:lm_loss: 3.4646, ppl: 31.9648, loss: 10.0300, bow_loss: 6.5388, entropy_loss: -2.2490
	step 200:lm_loss: 3.4656, ppl: 31.9948, loss: 10.0298, bow_loss: 6.5434, entropy_loss: -2.2420
	step 300:lm_loss: 3.4550, ppl: 31.6573, loss: 10.0244, bow_loss: 6.5414, entropy_loss: -2.2426
	step 400:lm_loss: 3.4499, ppl: 31.4980, loss: 10.0149, bow_loss: 6.5395, entropy_loss: -2.2427
	step 500:lm_loss: 3.4482, ppl: 31.4433, loss: 10.0115, bow_loss: 6.5409, entropy_loss: -2.2414
	step 600:lm_loss: 3.4508, ppl: 31.5251, loss: 10.0162, bow_loss: 6.5432, entropy_loss: -2.2419
	step 700:lm_loss: 3.4519, ppl: 31.5596, loss: 10.0190, bow_loss: 6.5443, entropy_loss: -2.2427
	step 800:lm_loss: 3.4483, ppl: 31.4470, loss: 10.0145, bow_loss: 6.5442, entropy_loss: -2.2412
	step 900:lm_loss: 3.4467, ppl: 31.3963, loss: 10.0121, bow_loss: 6.5434, entropy_loss: -2.2409
	step 1000:lm_loss: 3.4442, ppl: 31.3182, loss: 10.0094, bow_loss: 6.5430, entropy_loss: -2.2411
	step 1100:lm_loss: 3.4433, ppl: 31.2901, loss: 10.0081, bow_loss: 6.5422, entropy_loss: -2.2412
	step 1200:lm_loss: 3.4431, ppl: 31.2826, loss: 10.0076, bow_loss: 6.5424, entropy_loss: -2.2414
	step 1300:lm_loss: 3.4412, ppl: 31.2246, loss: 10.0067, bow_loss: 6.5424, entropy_loss: -2.2419
	step 1400:lm_loss: 3.4415, ppl: 31.2342, loss: 10.0065, bow_loss: 6.5424, entropy_loss: -2.2417
	step 1500:lm_loss: 3.4413, ppl: 31.2286, loss: 10.0058, bow_loss: 6.5427, entropy_loss: -2.2412
	step 1600:lm_loss: 3.4397, ppl: 31.1764, loss: 10.0046, bow_loss: 6.5418, entropy_loss: -2.2415
	step 1700:lm_loss: 3.4419, ppl: 31.2453, loss: 10.0079, bow_loss: 6.5439, entropy_loss: -2.2418
	step 1800:lm_loss: 3.4488, ppl: 31.4640, loss: 10.0197, bow_loss: 6.5526, entropy_loss: -2.2429
	step 1900:lm_loss: 3.4539, ppl: 31.6221, loss: 10.0288, bow_loss: 6.5598, entropy_loss: -2.2441
	step 2000:lm_loss: 3.4594, ppl: 31.7972, loss: 10.0373, bow_loss: 6.5674, entropy_loss: -2.2446
	step 2100:lm_loss: 3.4648, ppl: 31.9690, loss: 10.0462, bow_loss: 6.5744, entropy_loss: -2.2451
	step 2200:lm_loss: 3.4671, ppl: 32.0438, loss: 10.0441, bow_loss: 6.5733, entropy_loss: -2.2487
	step 2300:lm_loss: 3.4690, ppl: 32.1041, loss: 10.0387, bow_loss: 6.5700, entropy_loss: -2.2541
	step 2400:lm_loss: 3.4705, ppl: 32.1513, loss: 10.0339, bow_loss: 6.5667, entropy_loss: -2.2587
	step 2500:lm_loss: 3.4724, ppl: 32.2124, loss: 10.0316, bow_loss: 6.5655, entropy_loss: -2.2626
	step 2600:lm_loss: 3.4668, ppl: 32.0352, loss: 10.0286, bow_loss: 6.5677, entropy_loss: -2.2643
	step 2700:lm_loss: 3.4587, ppl: 31.7742, loss: 10.0247, bow_loss: 6.5710, entropy_loss: -2.2656
	step 2800:lm_loss: 3.4504, ppl: 31.5139, loss: 10.0205, bow_loss: 6.5742, entropy_loss: -2.2668
	step 2900:lm_loss: 3.4415, ppl: 31.2345, loss: 10.0165, bow_loss: 6.5777, entropy_loss: -2.2667
	step 3000:lm_loss: 3.4339, ppl: 30.9959, loss: 10.0125, bow_loss: 6.5806, entropy_loss: -2.2675
	step 3100:lm_loss: 3.4260, ppl: 30.7543, loss: 10.0087, bow_loss: 6.5836, entropy_loss: -2.2678
	step 3200:lm_loss: 3.4171, ppl: 30.4819, loss: 10.0040, bow_loss: 6.5865, entropy_loss: -2.2683
	step 3300:lm_loss: 3.4098, ppl: 30.2582, loss: 10.0005, bow_loss: 6.5897, entropy_loss: -2.2689
	step 3400:lm_loss: 3.4031, ppl: 30.0582, loss: 9.9970, bow_loss: 6.5926, entropy_loss: -2.2701
	step 3500:lm_loss: 3.3989, ppl: 29.9314, loss: 9.9952, bow_loss: 6.5954, entropy_loss: -2.2716
	step 3600:lm_loss: 3.3928, ppl: 29.7481, loss: 9.9930, bow_loss: 6.5988, entropy_loss: -2.2723
	step 3700:lm_loss: 3.3887, ppl: 29.6276, loss: 9.9912, bow_loss: 6.6013, entropy_loss: -2.2731
	step 3800:lm_loss: 3.3841, ppl: 29.4911, loss: 9.9890, bow_loss: 6.6035, entropy_loss: -2.2742
	step 3900:lm_loss: 3.3793, ppl: 29.3501, loss: 9.9867, bow_loss: 6.6058, entropy_loss: -2.2753
[Evaluation][6000]lm_loss: 3.3770, ppl: 29.2840, loss: 9.9857, bow_loss: 6.6070, entropy_loss: -2.2760
	time cost: 1599.808
================================================================================
[train][1] progress: 1/1 step: 6100, time: 97.064, speed: 1.030 steps/s
	current lr: 0.0000040
	lm_loss: 3.5826, ppl: 35.9663, loss: 10.0857, bow_loss: 6.5031, entropy_loss: -2.3085
[train][1] progress: 1/1 step: 6200, time: 97.243, speed: 1.028 steps/s
	current lr: 0.0000040
	lm_loss: 3.5537, ppl: 34.9425, loss: 10.1888, bow_loss: 6.6351, entropy_loss: -2.2317
[train][1] progress: 1/1 step: 6300, time: 97.985, speed: 1.021 steps/s
	current lr: 0.0000040
	lm_loss: 3.6941, ppl: 40.2109, loss: 10.2728, bow_loss: 6.5786, entropy_loss: -2.3596
[train][1] progress: 1/1 step: 6400, time: 98.101, speed: 1.019 steps/s
	current lr: 0.0000040
	lm_loss: 3.6197, ppl: 37.3256, loss: 10.1427, bow_loss: 6.5230, entropy_loss: -2.1505
[train][1] progress: 1/1 step: 6500, time: 97.941, speed: 1.021 steps/s
	current lr: 0.0000039
	lm_loss: 3.4826, ppl: 32.5455, loss: 10.0689, bow_loss: 6.5862, entropy_loss: -2.1113
[train][1] progress: 1/1 step: 6600, time: 97.219, speed: 1.029 steps/s
	current lr: 0.0000039
	lm_loss: 2.9289, ppl: 18.7072, loss: 9.3489, bow_loss: 6.4200, entropy_loss: -2.0078
[train][1] progress: 1/1 step: 6700, time: 98.082, speed: 1.020 steps/s
	current lr: 0.0000039
	lm_loss: 3.6964, ppl: 40.3003, loss: 10.3700, bow_loss: 6.6737, entropy_loss: -2.1541
[train][1] progress: 1/1 step: 6800, time: 97.912, speed: 1.021 steps/s
	current lr: 0.0000038
	lm_loss: 3.5354, ppl: 34.3092, loss: 10.1767, bow_loss: 6.6413, entropy_loss: -2.0284
[train][1] progress: 1/1 step: 6900, time: 98.046, speed: 1.020 steps/s
	current lr: 0.0000038
	lm_loss: 3.5625, ppl: 35.2519, loss: 10.1801, bow_loss: 6.6175, entropy_loss: -1.9708
[train][1] progress: 1/1 step: 7000, time: 96.634, speed: 1.035 steps/s
	current lr: 0.0000038
	lm_loss: 3.5028, ppl: 33.2067, loss: 10.0533, bow_loss: 6.5506, entropy_loss: -1.9333
================================================================================
Evaluation:
	step 100:lm_loss: 3.4573, ppl: 31.7327, loss: 10.0161, bow_loss: 6.5339, entropy_loss: -2.0727
	step 200:lm_loss: 3.4519, ppl: 31.5610, loss: 10.0042, bow_loss: 6.5257, entropy_loss: -2.0786
	step 300:lm_loss: 3.4591, ppl: 31.7894, loss: 10.0167, bow_loss: 6.5285, entropy_loss: -2.0888
	step 400:lm_loss: 3.4567, ppl: 31.7110, loss: 10.0131, bow_loss: 6.5286, entropy_loss: -2.0908
	step 500:lm_loss: 3.4493, ppl: 31.4790, loss: 10.0047, bow_loss: 6.5290, entropy_loss: -2.0866
	step 600:lm_loss: 3.4489, ppl: 31.4644, loss: 10.0050, bow_loss: 6.5279, entropy_loss: -2.0886
	step 700:lm_loss: 3.4453, ppl: 31.3522, loss: 10.0026, bow_loss: 6.5296, entropy_loss: -2.0854
	step 800:lm_loss: 3.4474, ppl: 31.4171, loss: 10.0031, bow_loss: 6.5298, entropy_loss: -2.0855
	step 900:lm_loss: 3.4457, ppl: 31.3663, loss: 10.0010, bow_loss: 6.5295, entropy_loss: -2.0869
	step 1000:lm_loss: 3.4416, ppl: 31.2366, loss: 9.9974, bow_loss: 6.5281, entropy_loss: -2.0855
	step 1100:lm_loss: 3.4433, ppl: 31.2885, loss: 9.9981, bow_loss: 6.5282, entropy_loss: -2.0873
	step 1200:lm_loss: 3.4427, ppl: 31.2716, loss: 9.9981, bow_loss: 6.5283, entropy_loss: -2.0878
	step 1300:lm_loss: 3.4410, ppl: 31.2197, loss: 9.9958, bow_loss: 6.5280, entropy_loss: -2.0874
	step 1400:lm_loss: 3.4413, ppl: 31.2260, loss: 9.9958, bow_loss: 6.5283, entropy_loss: -2.0870
	step 1500:lm_loss: 3.4428, ppl: 31.2737, loss: 9.9972, bow_loss: 6.5287, entropy_loss: -2.0875
	step 1600:lm_loss: 3.4408, ppl: 31.2127, loss: 9.9958, bow_loss: 6.5278, entropy_loss: -2.0870
	step 1700:lm_loss: 3.4422, ppl: 31.2554, loss: 9.9981, bow_loss: 6.5302, entropy_loss: -2.0878
	step 1800:lm_loss: 3.4490, ppl: 31.4696, loss: 10.0097, bow_loss: 6.5389, entropy_loss: -2.0890
	step 1900:lm_loss: 3.4547, ppl: 31.6498, loss: 10.0197, bow_loss: 6.5468, entropy_loss: -2.0898
	step 2000:lm_loss: 3.4583, ppl: 31.7628, loss: 10.0265, bow_loss: 6.5524, entropy_loss: -2.0909
	step 2100:lm_loss: 3.4645, ppl: 31.9610, loss: 10.0357, bow_loss: 6.5603, entropy_loss: -2.0913
	step 2200:lm_loss: 3.4672, ppl: 32.0479, loss: 10.0335, bow_loss: 6.5598, entropy_loss: -2.0969
	step 2300:lm_loss: 3.4694, ppl: 32.1184, loss: 10.0300, bow_loss: 6.5575, entropy_loss: -2.1028
	step 2400:lm_loss: 3.4714, ppl: 32.1818, loss: 10.0282, bow_loss: 6.5559, entropy_loss: -2.1075
	step 2500:lm_loss: 3.4728, ppl: 32.2275, loss: 10.0237, bow_loss: 6.5529, entropy_loss: -2.1127
	step 2600:lm_loss: 3.4675, ppl: 32.0570, loss: 10.0200, bow_loss: 6.5549, entropy_loss: -2.1147
	step 2700:lm_loss: 3.4589, ppl: 31.7823, loss: 10.0156, bow_loss: 6.5580, entropy_loss: -2.1154
	step 2800:lm_loss: 3.4507, ppl: 31.5233, loss: 10.0115, bow_loss: 6.5614, entropy_loss: -2.1165
	step 2900:lm_loss: 3.4426, ppl: 31.2673, loss: 10.0077, bow_loss: 6.5644, entropy_loss: -2.1168
	step 3000:lm_loss: 3.4350, ppl: 31.0307, loss: 10.0044, bow_loss: 6.5677, entropy_loss: -2.1176
	step 3100:lm_loss: 3.4268, ppl: 30.7767, loss: 9.9997, bow_loss: 6.5705, entropy_loss: -2.1183
	step 3200:lm_loss: 3.4185, ppl: 30.5241, loss: 9.9957, bow_loss: 6.5733, entropy_loss: -2.1182
	step 3300:lm_loss: 3.4101, ppl: 30.2689, loss: 9.9912, bow_loss: 6.5760, entropy_loss: -2.1183
	step 3400:lm_loss: 3.4048, ppl: 30.1073, loss: 9.9888, bow_loss: 6.5787, entropy_loss: -2.1193
	step 3500:lm_loss: 3.3993, ppl: 29.9421, loss: 9.9861, bow_loss: 6.5813, entropy_loss: -2.1203
	step 3600:lm_loss: 3.3927, ppl: 29.7451, loss: 9.9835, bow_loss: 6.5847, entropy_loss: -2.1216
	step 3700:lm_loss: 3.3879, ppl: 29.6050, loss: 9.9809, bow_loss: 6.5868, entropy_loss: -2.1225
	step 3800:lm_loss: 3.3841, ppl: 29.4905, loss: 9.9789, bow_loss: 6.5889, entropy_loss: -2.1234
	step 3900:lm_loss: 3.3804, ppl: 29.3819, loss: 9.9777, bow_loss: 6.5911, entropy_loss: -2.1244
[Evaluation][7000]lm_loss: 3.3780, ppl: 29.3115, loss: 9.9764, bow_loss: 6.5921, entropy_loss: -2.1249
	time cost: 1592.378
================================================================================
[train][1] progress: 1/1 step: 7100, time: 97.663, speed: 1.024 steps/s
	current lr: 0.0000038
	lm_loss: 3.5868, ppl: 36.1192, loss: 10.0868, bow_loss: 6.5000, entropy_loss: -2.1939
[train][1] progress: 1/1 step: 7200, time: 97.973, speed: 1.021 steps/s
	current lr: 0.0000037
	lm_loss: 3.5268, ppl: 34.0159, loss: 10.0882, bow_loss: 6.5613, entropy_loss: -1.8921
[train][1] progress: 1/1 step: 7300, time: 98.005, speed: 1.020 steps/s
	current lr: 0.0000037
	lm_loss: 3.4961, ppl: 32.9857, loss: 10.0353, bow_loss: 6.5392, entropy_loss: -2.0429
[train][1] progress: 1/1 step: 7400, time: 97.916, speed: 1.021 steps/s
	current lr: 0.0000037
	lm_loss: 3.5694, ppl: 35.4941, loss: 10.1844, bow_loss: 6.6150, entropy_loss: -1.9219
[train][1] progress: 1/1 step: 7500, time: 98.160, speed: 1.019 steps/s
	current lr: 0.0000037
	lm_loss: 3.5485, ppl: 34.7602, loss: 10.0999, bow_loss: 6.5515, entropy_loss: -1.7822
[train][1] progress: 1/1 step: 7600, time: 97.988, speed: 1.021 steps/s
	current lr: 0.0000036
	lm_loss: 3.4988, ppl: 33.0746, loss: 10.0978, bow_loss: 6.5990, entropy_loss: -1.5664
[train][1] progress: 1/1 step: 7700, time: 97.284, speed: 1.028 steps/s
	current lr: 0.0000036
	lm_loss: 3.5854, ppl: 36.0663, loss: 10.2095, bow_loss: 6.6241, entropy_loss: -1.8981
[train][1] progress: 1/1 step: 7800, time: 98.118, speed: 1.019 steps/s
	current lr: 0.0000036
	lm_loss: 3.4921, ppl: 32.8559, loss: 10.0646, bow_loss: 6.5725, entropy_loss: -1.6893
[train][1] progress: 1/1 step: 7900, time: 97.829, speed: 1.022 steps/s
	current lr: 0.0000036
	lm_loss: 3.5203, ppl: 33.7950, loss: 10.1102, bow_loss: 6.5899, entropy_loss: -1.7054
[train][1] progress: 1/1 step: 8000, time: 98.053, speed: 1.020 steps/s
	current lr: 0.0000035
	lm_loss: 3.4695, ppl: 32.1195, loss: 10.0404, bow_loss: 6.5710, entropy_loss: -1.5653
================================================================================
Evaluation:
	step 100:lm_loss: 3.4697, ppl: 32.1260, loss: 10.0305, bow_loss: 6.5307, entropy_loss: -1.8275
	step 200:lm_loss: 3.4675, ppl: 32.0577, loss: 10.0154, bow_loss: 6.5236, entropy_loss: -1.8281
	step 300:lm_loss: 3.4564, ppl: 31.7015, loss: 9.9999, bow_loss: 6.5187, entropy_loss: -1.8234
	step 400:lm_loss: 3.4520, ppl: 31.5636, loss: 9.9931, bow_loss: 6.5175, entropy_loss: -1.8300
	step 500:lm_loss: 3.4491, ppl: 31.4735, loss: 9.9903, bow_loss: 6.5185, entropy_loss: -1.8255
	step 600:lm_loss: 3.4453, ppl: 31.3536, loss: 9.9865, bow_loss: 6.5204, entropy_loss: -1.8162
	step 700:lm_loss: 3.4420, ppl: 31.2486, loss: 9.9837, bow_loss: 6.5201, entropy_loss: -1.8156
	step 800:lm_loss: 3.4447, ppl: 31.3329, loss: 9.9871, bow_loss: 6.5205, entropy_loss: -1.8193
	step 900:lm_loss: 3.4431, ppl: 31.2837, loss: 9.9863, bow_loss: 6.5207, entropy_loss: -1.8183
	step 1000:lm_loss: 3.4405, ppl: 31.2017, loss: 9.9822, bow_loss: 6.5199, entropy_loss: -1.8168
	step 1100:lm_loss: 3.4411, ppl: 31.2218, loss: 9.9829, bow_loss: 6.5196, entropy_loss: -1.8189
	step 1200:lm_loss: 3.4419, ppl: 31.2458, loss: 9.9832, bow_loss: 6.5202, entropy_loss: -1.8186
	step 1300:lm_loss: 3.4413, ppl: 31.2289, loss: 9.9831, bow_loss: 6.5200, entropy_loss: -1.8196
	step 1400:lm_loss: 3.4396, ppl: 31.1758, loss: 9.9812, bow_loss: 6.5197, entropy_loss: -1.8193
	step 1500:lm_loss: 3.4388, ppl: 31.1481, loss: 9.9801, bow_loss: 6.5192, entropy_loss: -1.8183
	step 1600:lm_loss: 3.4386, ppl: 31.1444, loss: 9.9798, bow_loss: 6.5196, entropy_loss: -1.8177
	step 1700:lm_loss: 3.4405, ppl: 31.2017, loss: 9.9832, bow_loss: 6.5213, entropy_loss: -1.8197
	step 1800:lm_loss: 3.4457, ppl: 31.3659, loss: 9.9924, bow_loss: 6.5281, entropy_loss: -1.8216
	step 1900:lm_loss: 3.4520, ppl: 31.5648, loss: 10.0016, bow_loss: 6.5355, entropy_loss: -1.8221
	step 2000:lm_loss: 3.4574, ppl: 31.7346, loss: 10.0103, bow_loss: 6.5437, entropy_loss: -1.8226
	step 2100:lm_loss: 3.4628, ppl: 31.9052, loss: 10.0201, bow_loss: 6.5502, entropy_loss: -1.8240
	step 2200:lm_loss: 3.4662, ppl: 32.0155, loss: 10.0193, bow_loss: 6.5502, entropy_loss: -1.8310
	step 2300:lm_loss: 3.4682, ppl: 32.0783, loss: 10.0168, bow_loss: 6.5484, entropy_loss: -1.8371
	step 2400:lm_loss: 3.4700, ppl: 32.1366, loss: 10.0126, bow_loss: 6.5458, entropy_loss: -1.8442
	step 2500:lm_loss: 3.4717, ppl: 32.1908, loss: 10.0079, bow_loss: 6.5425, entropy_loss: -1.8527
	step 2600:lm_loss: 3.4654, ppl: 31.9903, loss: 10.0032, bow_loss: 6.5438, entropy_loss: -1.8544
	step 2700:lm_loss: 3.4563, ppl: 31.7009, loss: 9.9983, bow_loss: 6.5466, entropy_loss: -1.8547
	step 2800:lm_loss: 3.4479, ppl: 31.4329, loss: 9.9946, bow_loss: 6.5503, entropy_loss: -1.8543
	step 2900:lm_loss: 3.4386, ppl: 31.1444, loss: 9.9902, bow_loss: 6.5536, entropy_loss: -1.8544
	step 3000:lm_loss: 3.4312, ppl: 30.9148, loss: 9.9869, bow_loss: 6.5569, entropy_loss: -1.8550
	step 3100:lm_loss: 3.4237, ppl: 30.6816, loss: 9.9827, bow_loss: 6.5595, entropy_loss: -1.8557
	step 3200:lm_loss: 3.4157, ppl: 30.4385, loss: 9.9784, bow_loss: 6.5625, entropy_loss: -1.8560
	step 3300:lm_loss: 3.4094, ppl: 30.2485, loss: 9.9754, bow_loss: 6.5648, entropy_loss: -1.8569
	step 3400:lm_loss: 3.4034, ppl: 30.0666, loss: 9.9717, bow_loss: 6.5670, entropy_loss: -1.8578
	step 3500:lm_loss: 3.3986, ppl: 29.9228, loss: 9.9693, bow_loss: 6.5694, entropy_loss: -1.8588
	step 3600:lm_loss: 3.3943, ppl: 29.7927, loss: 9.9670, bow_loss: 6.5716, entropy_loss: -1.8596
	step 3700:lm_loss: 3.3894, ppl: 29.6474, loss: 9.9647, bow_loss: 6.5740, entropy_loss: -1.8604
	step 3800:lm_loss: 3.3848, ppl: 29.5128, loss: 9.9627, bow_loss: 6.5764, entropy_loss: -1.8608
	step 3900:lm_loss: 3.3794, ppl: 29.3520, loss: 9.9607, bow_loss: 6.5792, entropy_loss: -1.8626
[Evaluation][8000]lm_loss: 3.3776, ppl: 29.3011, loss: 9.9600, bow_loss: 6.5804, entropy_loss: -1.8641
	time cost: 1591.020
================================================================================
[train][1] progress: 1/1 step: 8100, time: 97.464, speed: 1.026 steps/s
	current lr: 0.0000035
	lm_loss: 3.5444, ppl: 34.6203, loss: 10.0591, bow_loss: 6.5147, entropy_loss: -1.4719
[train][1] progress: 1/1 step: 8200, time: 97.875, speed: 1.022 steps/s
	current lr: 0.0000035
	lm_loss: 3.4127, ppl: 30.3478, loss: 9.9037, bow_loss: 6.4910, entropy_loss: -1.5184
[train][1] progress: 1/1 step: 8300, time: 97.159, speed: 1.029 steps/s
	current lr: 0.0000035
	lm_loss: 3.6889, ppl: 40.0017, loss: 10.1958, bow_loss: 6.5068, entropy_loss: -2.1002
[train][1] progress: 1/1 step: 8400, time: 98.014, speed: 1.020 steps/s
	current lr: 0.0000035
	lm_loss: 3.5643, ppl: 35.3141, loss: 10.1235, bow_loss: 6.5592, entropy_loss: -1.7759
[train][1] progress: 1/1 step: 8500, time: 97.983, speed: 1.021 steps/s
	current lr: 0.0000034
	lm_loss: 3.5296, ppl: 34.1117, loss: 10.1651, bow_loss: 6.6354, entropy_loss: -1.3086
[train][1] progress: 1/1 step: 8600, time: 97.315, speed: 1.028 steps/s
	current lr: 0.0000034
	lm_loss: 3.2768, ppl: 26.4910, loss: 9.7683, bow_loss: 6.4914, entropy_loss: -0.9363
[train][1] progress: 1/1 step: 8700, time: 97.943, speed: 1.021 steps/s
	current lr: 0.0000034
	lm_loss: 3.3553, ppl: 28.6545, loss: 9.9411, bow_loss: 6.5858, entropy_loss: -1.0099
[train][1] progress: 1/1 step: 8800, time: 97.902, speed: 1.021 steps/s
	current lr: 0.0000034
	lm_loss: 3.5317, ppl: 34.1825, loss: 10.0461, bow_loss: 6.5144, entropy_loss: -1.4797
[train][1] progress: 1/1 step: 8900, time: 97.130, speed: 1.030 steps/s
	current lr: 0.0000034
	lm_loss: 3.4732, ppl: 32.2384, loss: 9.9763, bow_loss: 6.5031, entropy_loss: -1.6270
[train][1] progress: 1/1 step: 9000, time: 97.860, speed: 1.022 steps/s
	current lr: 0.0000033
	lm_loss: 3.6743, ppl: 39.4191, loss: 10.1907, bow_loss: 6.5165, entropy_loss: -1.8215
================================================================================
Evaluation:
	step 100:lm_loss: 3.4634, ppl: 31.9239, loss: 10.0146, bow_loss: 6.5346, entropy_loss: -1.4710
	step 200:lm_loss: 3.4620, ppl: 31.8807, loss: 10.0059, bow_loss: 6.5224, entropy_loss: -1.4788
	step 300:lm_loss: 3.4552, ppl: 31.6634, loss: 9.9923, bow_loss: 6.5136, entropy_loss: -1.4817
	step 400:lm_loss: 3.4536, ppl: 31.6152, loss: 9.9899, bow_loss: 6.5158, entropy_loss: -1.4827
	step 500:lm_loss: 3.4521, ppl: 31.5678, loss: 9.9890, bow_loss: 6.5163, entropy_loss: -1.4833
	step 600:lm_loss: 3.4476, ppl: 31.4234, loss: 9.9814, bow_loss: 6.5151, entropy_loss: -1.4785
	step 700:lm_loss: 3.4448, ppl: 31.3380, loss: 9.9791, bow_loss: 6.5144, entropy_loss: -1.4792
	step 800:lm_loss: 3.4432, ppl: 31.2858, loss: 9.9785, bow_loss: 6.5161, entropy_loss: -1.4748
	step 900:lm_loss: 3.4431, ppl: 31.2831, loss: 9.9789, bow_loss: 6.5154, entropy_loss: -1.4779
	step 1000:lm_loss: 3.4426, ppl: 31.2692, loss: 9.9796, bow_loss: 6.5158, entropy_loss: -1.4786
	step 1100:lm_loss: 3.4407, ppl: 31.2091, loss: 9.9764, bow_loss: 6.5149, entropy_loss: -1.4791
	step 1200:lm_loss: 3.4397, ppl: 31.1780, loss: 9.9762, bow_loss: 6.5151, entropy_loss: -1.4766
	step 1300:lm_loss: 3.4398, ppl: 31.1808, loss: 9.9757, bow_loss: 6.5148, entropy_loss: -1.4796
	step 1400:lm_loss: 3.4388, ppl: 31.1483, loss: 9.9743, bow_loss: 6.5150, entropy_loss: -1.4782
	step 1500:lm_loss: 3.4381, ppl: 31.1274, loss: 9.9730, bow_loss: 6.5150, entropy_loss: -1.4758
	step 1600:lm_loss: 3.4383, ppl: 31.1353, loss: 9.9733, bow_loss: 6.5141, entropy_loss: -1.4792
	step 1700:lm_loss: 3.4398, ppl: 31.1805, loss: 9.9758, bow_loss: 6.5165, entropy_loss: -1.4793
	step 1800:lm_loss: 3.4466, ppl: 31.3935, loss: 9.9870, bow_loss: 6.5250, entropy_loss: -1.4803
	step 1900:lm_loss: 3.4520, ppl: 31.5636, loss: 9.9961, bow_loss: 6.5324, entropy_loss: -1.4823
	step 2000:lm_loss: 3.4577, ppl: 31.7425, loss: 10.0058, bow_loss: 6.5405, entropy_loss: -1.4830
	step 2100:lm_loss: 3.4624, ppl: 31.8936, loss: 10.0139, bow_loss: 6.5462, entropy_loss: -1.4835
	step 2200:lm_loss: 3.4650, ppl: 31.9761, loss: 10.0116, bow_loss: 6.5447, entropy_loss: -1.4901
	step 2300:lm_loss: 3.4668, ppl: 32.0345, loss: 10.0070, bow_loss: 6.5419, entropy_loss: -1.4971
	step 2400:lm_loss: 3.4695, ppl: 32.1215, loss: 10.0041, bow_loss: 6.5393, entropy_loss: -1.5055
	step 2500:lm_loss: 3.4714, ppl: 32.1830, loss: 10.0024, bow_loss: 6.5382, entropy_loss: -1.5102
	step 2600:lm_loss: 3.4650, ppl: 31.9764, loss: 9.9976, bow_loss: 6.5397, entropy_loss: -1.5127
	step 2700:lm_loss: 3.4555, ppl: 31.6730, loss: 9.9924, bow_loss: 6.5427, entropy_loss: -1.5122
	step 2800:lm_loss: 3.4478, ppl: 31.4318, loss: 9.9887, bow_loss: 6.5456, entropy_loss: -1.5133
	step 2900:lm_loss: 3.4396, ppl: 31.1743, loss: 9.9846, bow_loss: 6.5485, entropy_loss: -1.5134
	step 3000:lm_loss: 3.4328, ppl: 30.9627, loss: 9.9813, bow_loss: 6.5514, entropy_loss: -1.5143
	step 3100:lm_loss: 3.4236, ppl: 30.6808, loss: 9.9764, bow_loss: 6.5545, entropy_loss: -1.5138
	step 3200:lm_loss: 3.4161, ppl: 30.4516, loss: 9.9722, bow_loss: 6.5570, entropy_loss: -1.5140
	step 3300:lm_loss: 3.4092, ppl: 30.2403, loss: 9.9688, bow_loss: 6.5598, entropy_loss: -1.5139
	step 3400:lm_loss: 3.4044, ppl: 30.0970, loss: 9.9659, bow_loss: 6.5618, entropy_loss: -1.5154
	step 3500:lm_loss: 3.4009, ppl: 29.9916, loss: 9.9638, bow_loss: 6.5637, entropy_loss: -1.5161
	step 3600:lm_loss: 3.3960, ppl: 29.8457, loss: 9.9619, bow_loss: 6.5664, entropy_loss: -1.5165
	step 3700:lm_loss: 3.3907, ppl: 29.6873, loss: 9.9593, bow_loss: 6.5691, entropy_loss: -1.5173
	step 3800:lm_loss: 3.3860, ppl: 29.5479, loss: 9.9579, bow_loss: 6.5718, entropy_loss: -1.5167
	step 3900:lm_loss: 3.3811, ppl: 29.4043, loss: 9.9556, bow_loss: 6.5738, entropy_loss: -1.5183
[Evaluation][9000]lm_loss: 3.3785, ppl: 29.3267, loss: 9.9539, bow_loss: 6.5748, entropy_loss: -1.5191
	time cost: 1600.005
================================================================================
[train][1] progress: 1/1 step: 9100, time: 98.050, speed: 1.020 steps/s
	current lr: 0.0000033
	lm_loss: 3.4808, ppl: 32.4847, loss: 10.0797, bow_loss: 6.5989, entropy_loss: -1.2425
[train][1] progress: 1/1 step: 9200, time: 98.224, speed: 1.018 steps/s
	current lr: 0.0000033
	lm_loss: 3.5573, ppl: 35.0671, loss: 10.0107, bow_loss: 6.4535, entropy_loss: -1.2197
[train][1] progress: 1/1 step: 9300, time: 98.012, speed: 1.020 steps/s
	current lr: 0.0000033
	lm_loss: 3.6409, ppl: 38.1244, loss: 10.2237, bow_loss: 6.5828, entropy_loss: -1.4550
[train][1] progress: 1/1 step: 9400, time: 96.934, speed: 1.032 steps/s
	current lr: 0.0000033
	lm_loss: 3.4870, ppl: 32.6877, loss: 10.0698, bow_loss: 6.5828, entropy_loss: -0.8725
[train][1] progress: 1/1 step: 9500, time: 98.135, speed: 1.019 steps/s
	current lr: 0.0000032
	lm_loss: 3.5649, ppl: 35.3346, loss: 10.1757, bow_loss: 6.6109, entropy_loss: -1.1187
[train][1] progress: 1/1 step: 9600, time: 97.843, speed: 1.022 steps/s
	current lr: 0.0000032
	lm_loss: 3.6517, ppl: 38.5383, loss: 10.1797, bow_loss: 6.5280, entropy_loss: -1.3117
[train][1] progress: 1/1 step: 9700, time: 97.176, speed: 1.029 steps/s
	current lr: 0.0000032
	lm_loss: 3.4985, ppl: 33.0666, loss: 10.1289, bow_loss: 6.6303, entropy_loss: -0.5530
[train][1] progress: 1/1 step: 9800, time: 97.253, speed: 1.028 steps/s
	current lr: 0.0000032
	lm_loss: 3.5584, ppl: 35.1065, loss: 10.1556, bow_loss: 6.5972, entropy_loss: -0.6491
[train][1] progress: 1/1 step: 9900, time: 97.902, speed: 1.021 steps/s
	current lr: 0.0000032
	lm_loss: 3.5352, ppl: 34.3026, loss: 10.1265, bow_loss: 6.5913, entropy_loss: -0.4633
[train][1] progress: 1/1 step: 10000, time: 98.169, speed: 1.019 steps/s
	current lr: 0.0000032
	lm_loss: 3.6176, ppl: 37.2471, loss: 10.2638, bow_loss: 6.6462, entropy_loss: -0.9572
================================================================================
Evaluation:
	step 100:lm_loss: 3.4360, ppl: 31.0610, loss: 9.9662, bow_loss: 6.4988, entropy_loss: -0.7302
	step 200:lm_loss: 3.4564, ppl: 31.7039, loss: 9.9878, bow_loss: 6.5081, entropy_loss: -0.7331
	step 300:lm_loss: 3.4462, ppl: 31.3817, loss: 9.9766, bow_loss: 6.5059, entropy_loss: -0.7347
	step 400:lm_loss: 3.4402, ppl: 31.1939, loss: 9.9685, bow_loss: 6.5065, entropy_loss: -0.7310
	step 500:lm_loss: 3.4411, ppl: 31.2222, loss: 9.9675, bow_loss: 6.5066, entropy_loss: -0.7330
	step 600:lm_loss: 3.4464, ppl: 31.3874, loss: 9.9715, bow_loss: 6.5070, entropy_loss: -0.7337
	step 700:lm_loss: 3.4447, ppl: 31.3337, loss: 9.9723, bow_loss: 6.5089, entropy_loss: -0.7336
	step 800:lm_loss: 3.4425, ppl: 31.2664, loss: 9.9690, bow_loss: 6.5076, entropy_loss: -0.7339
	step 900:lm_loss: 3.4401, ppl: 31.1912, loss: 9.9665, bow_loss: 6.5067, entropy_loss: -0.7344
	step 1000:lm_loss: 3.4394, ppl: 31.1683, loss: 9.9659, bow_loss: 6.5061, entropy_loss: -0.7329
	step 1100:lm_loss: 3.4391, ppl: 31.1574, loss: 9.9646, bow_loss: 6.5062, entropy_loss: -0.7330
	step 1200:lm_loss: 3.4378, ppl: 31.1183, loss: 9.9633, bow_loss: 6.5063, entropy_loss: -0.7333
	step 1300:lm_loss: 3.4362, ppl: 31.0694, loss: 9.9620, bow_loss: 6.5060, entropy_loss: -0.7335
	step 1400:lm_loss: 3.4369, ppl: 31.0913, loss: 9.9626, bow_loss: 6.5063, entropy_loss: -0.7334
	step 1500:lm_loss: 3.4365, ppl: 31.0774, loss: 9.9617, bow_loss: 6.5063, entropy_loss: -0.7332
	step 1600:lm_loss: 3.4349, ppl: 31.0282, loss: 9.9604, bow_loss: 6.5057, entropy_loss: -0.7331
	step 1700:lm_loss: 3.4372, ppl: 31.0989, loss: 9.9634, bow_loss: 6.5079, entropy_loss: -0.7333
	step 1800:lm_loss: 3.4429, ppl: 31.2783, loss: 9.9738, bow_loss: 6.5158, entropy_loss: -0.7346
	step 1900:lm_loss: 3.4487, ppl: 31.4591, loss: 9.9831, bow_loss: 6.5224, entropy_loss: -0.7359
	step 2000:lm_loss: 3.4534, ppl: 31.6090, loss: 9.9907, bow_loss: 6.5291, entropy_loss: -0.7372
	step 2100:lm_loss: 3.4606, ppl: 31.8370, loss: 10.0025, bow_loss: 6.5374, entropy_loss: -0.7388
	step 2200:lm_loss: 3.4639, ppl: 31.9406, loss: 10.0020, bow_loss: 6.5372, entropy_loss: -0.7425
	step 2300:lm_loss: 3.4660, ppl: 32.0100, loss: 9.9977, bow_loss: 6.5344, entropy_loss: -0.7465
	step 2400:lm_loss: 3.4684, ppl: 32.0844, loss: 9.9926, bow_loss: 6.5306, entropy_loss: -0.7506
	step 2500:lm_loss: 3.4701, ppl: 32.1411, loss: 9.9903, bow_loss: 6.5290, entropy_loss: -0.7536
	step 2600:lm_loss: 3.4647, ppl: 31.9680, loss: 9.9866, bow_loss: 6.5306, entropy_loss: -0.7561
	step 2700:lm_loss: 3.4564, ppl: 31.7033, loss: 9.9825, bow_loss: 6.5338, entropy_loss: -0.7570
	step 2800:lm_loss: 3.4490, ppl: 31.4676, loss: 9.9792, bow_loss: 6.5368, entropy_loss: -0.7583
	step 2900:lm_loss: 3.4413, ppl: 31.2278, loss: 9.9757, bow_loss: 6.5399, entropy_loss: -0.7597
	step 3000:lm_loss: 3.4329, ppl: 30.9660, loss: 9.9712, bow_loss: 6.5429, entropy_loss: -0.7607
	step 3100:lm_loss: 3.4244, ppl: 30.7034, loss: 9.9660, bow_loss: 6.5452, entropy_loss: -0.7623
	step 3200:lm_loss: 3.4176, ppl: 30.4975, loss: 9.9632, bow_loss: 6.5481, entropy_loss: -0.7636
	step 3300:lm_loss: 3.4092, ppl: 30.2423, loss: 9.9583, bow_loss: 6.5506, entropy_loss: -0.7650
	step 3400:lm_loss: 3.4042, ppl: 30.0900, loss: 9.9563, bow_loss: 6.5532, entropy_loss: -0.7663
	step 3500:lm_loss: 3.4005, ppl: 29.9776, loss: 9.9545, bow_loss: 6.5555, entropy_loss: -0.7675
	step 3600:lm_loss: 3.3944, ppl: 29.7959, loss: 9.9503, bow_loss: 6.5574, entropy_loss: -0.7688
	step 3700:lm_loss: 3.3897, ppl: 29.6581, loss: 9.9476, bow_loss: 6.5594, entropy_loss: -0.7702
	step 3800:lm_loss: 3.3845, ppl: 29.5036, loss: 9.9463, bow_loss: 6.5626, entropy_loss: -0.7709
	step 3900:lm_loss: 3.3803, ppl: 29.3804, loss: 9.9438, bow_loss: 6.5644, entropy_loss: -0.7722
[Evaluation][10000]lm_loss: 3.3781, ppl: 29.3152, loss: 9.9429, bow_loss: 6.5657, entropy_loss: -0.7727
	time cost: 1591.859
================================================================================
[train][1] progress: 1/1 step: 10100, time: 97.671, speed: 1.024 steps/s
	current lr: 0.0000031
	lm_loss: 3.6671, ppl: 39.1394, loss: 10.2574, bow_loss: 6.5903, entropy_loss: -0.8864
[train][1] progress: 1/1 step: 10200, time: 97.881, speed: 1.022 steps/s
	current lr: 0.0000031
	lm_loss: 3.4724, ppl: 32.2137, loss: 10.0757, bow_loss: 6.6033, entropy_loss: -0.3535
[train][1] progress: 1/1 step: 10300, time: 97.182, speed: 1.029 steps/s
	current lr: 0.0000031
	lm_loss: 3.4672, ppl: 32.0454, loss: 10.0095, bow_loss: 6.5424, entropy_loss: -0.4958
[train][1] progress: 1/1 step: 10400, time: 97.978, speed: 1.021 steps/s
	current lr: 0.0000031
	lm_loss: 3.4940, ppl: 32.9163, loss: 10.0242, bow_loss: 6.5303, entropy_loss: -0.4660
[train][1] progress: 1/1 step: 10500, time: 97.999, speed: 1.020 steps/s
	current lr: 0.0000031
	lm_loss: 3.5066, ppl: 33.3359, loss: 10.1062, bow_loss: 6.5996, entropy_loss: -0.3648
[train][1] progress: 1/1 step: 10600, time: 97.505, speed: 1.026 steps/s
	current lr: 0.0000031
	lm_loss: 3.5617, ppl: 35.2220, loss: 10.1689, bow_loss: 6.6072, entropy_loss: -0.2204
[train][1] progress: 1/1 step: 10700, time: 97.418, speed: 1.027 steps/s
	current lr: 0.0000031
	lm_loss: 3.7004, ppl: 40.4615, loss: 10.2635, bow_loss: 6.5632, entropy_loss: -0.3049
[train][1] progress: 1/1 step: 10800, time: 97.917, speed: 1.021 steps/s
	current lr: 0.0000030
	lm_loss: 3.5341, ppl: 34.2651, loss: 9.9600, bow_loss: 6.4259, entropy_loss: -0.2629
[train][1] progress: 1/1 step: 10900, time: 98.122, speed: 1.019 steps/s
	current lr: 0.0000030
	lm_loss: 3.4008, ppl: 29.9874, loss: 9.9685, bow_loss: 6.5677, entropy_loss: -0.2218
[train][1] progress: 1/1 step: 11000, time: 97.945, speed: 1.021 steps/s
	current lr: 0.0000030
	lm_loss: 3.5022, ppl: 33.1882, loss: 9.9556, bow_loss: 6.4534, entropy_loss: -0.2065
================================================================================
Evaluation:
	step 100:lm_loss: 3.4476, ppl: 31.4240, loss: 9.9749, bow_loss: 6.5042, entropy_loss: -0.1461
	step 200:lm_loss: 3.4433, ppl: 31.2885, loss: 9.9685, bow_loss: 6.5034, entropy_loss: -0.1464
	step 300:lm_loss: 3.4399, ppl: 31.1827, loss: 9.9639, bow_loss: 6.4999, entropy_loss: -0.1471
	step 400:lm_loss: 3.4384, ppl: 31.1366, loss: 9.9601, bow_loss: 6.5009, entropy_loss: -0.1473
	step 500:lm_loss: 3.4371, ppl: 31.0977, loss: 9.9573, bow_loss: 6.4996, entropy_loss: -0.1461
	step 600:lm_loss: 3.4306, ppl: 30.8955, loss: 9.9505, bow_loss: 6.4987, entropy_loss: -0.1468
	step 700:lm_loss: 3.4295, ppl: 30.8618, loss: 9.9505, bow_loss: 6.4989, entropy_loss: -0.1463
	step 800:lm_loss: 3.4289, ppl: 30.8426, loss: 9.9510, bow_loss: 6.5000, entropy_loss: -0.1467
	step 900:lm_loss: 3.4300, ppl: 30.8762, loss: 9.9513, bow_loss: 6.5003, entropy_loss: -0.1471
	step 1000:lm_loss: 3.4277, ppl: 30.8067, loss: 9.9494, bow_loss: 6.5008, entropy_loss: -0.1477
	step 1100:lm_loss: 3.4263, ppl: 30.7641, loss: 9.9473, bow_loss: 6.4997, entropy_loss: -0.1470
	step 1200:lm_loss: 3.4249, ppl: 30.7199, loss: 9.9462, bow_loss: 6.4998, entropy_loss: -0.1468
	step 1300:lm_loss: 3.4250, ppl: 30.7230, loss: 9.9463, bow_loss: 6.4997, entropy_loss: -0.1469
	step 1400:lm_loss: 3.4254, ppl: 30.7338, loss: 9.9465, bow_loss: 6.4998, entropy_loss: -0.1466
	step 1500:lm_loss: 3.4238, ppl: 30.6860, loss: 9.9448, bow_loss: 6.4995, entropy_loss: -0.1468
	step 1600:lm_loss: 3.4249, ppl: 30.7199, loss: 9.9455, bow_loss: 6.4999, entropy_loss: -0.1467
	step 1700:lm_loss: 3.4258, ppl: 30.7479, loss: 9.9472, bow_loss: 6.5009, entropy_loss: -0.1465
	step 1800:lm_loss: 3.4317, ppl: 30.9304, loss: 9.9572, bow_loss: 6.5088, entropy_loss: -0.1467
	step 1900:lm_loss: 3.4383, ppl: 31.1354, loss: 9.9679, bow_loss: 6.5169, entropy_loss: -0.1472
	step 2000:lm_loss: 3.4440, ppl: 31.3129, loss: 9.9769, bow_loss: 6.5238, entropy_loss: -0.1477
	step 2100:lm_loss: 3.4506, ppl: 31.5200, loss: 9.9872, bow_loss: 6.5312, entropy_loss: -0.1479
	step 2200:lm_loss: 3.4542, ppl: 31.6321, loss: 9.9856, bow_loss: 6.5302, entropy_loss: -0.1476
	step 2300:lm_loss: 3.4566, ppl: 31.7104, loss: 9.9825, bow_loss: 6.5271, entropy_loss: -0.1472
	step 2400:lm_loss: 3.4600, ppl: 31.8159, loss: 9.9799, bow_loss: 6.5245, entropy_loss: -0.1467
	step 2500:lm_loss: 3.4624, ppl: 31.8936, loss: 9.9783, bow_loss: 6.5228, entropy_loss: -0.1463
	step 2600:lm_loss: 3.4567, ppl: 31.7110, loss: 9.9750, bow_loss: 6.5250, entropy_loss: -0.1472
	step 2700:lm_loss: 3.4491, ppl: 31.4732, loss: 9.9713, bow_loss: 6.5280, entropy_loss: -0.1483
	step 2800:lm_loss: 3.4408, ppl: 31.2111, loss: 9.9671, bow_loss: 6.5309, entropy_loss: -0.1492
	step 2900:lm_loss: 3.4325, ppl: 30.9547, loss: 9.9631, bow_loss: 6.5340, entropy_loss: -0.1500
	step 3000:lm_loss: 3.4245, ppl: 30.7068, loss: 9.9585, bow_loss: 6.5364, entropy_loss: -0.1509
	step 3100:lm_loss: 3.4166, ppl: 30.4668, loss: 9.9544, bow_loss: 6.5390, entropy_loss: -0.1518
	step 3200:lm_loss: 3.4096, ppl: 30.2545, loss: 9.9510, bow_loss: 6.5417, entropy_loss: -0.1527
	step 3300:lm_loss: 3.4027, ppl: 30.0451, loss: 9.9470, bow_loss: 6.5440, entropy_loss: -0.1534
	step 3400:lm_loss: 3.3972, ppl: 29.8814, loss: 9.9444, bow_loss: 6.5463, entropy_loss: -0.1541
	step 3500:lm_loss: 3.3923, ppl: 29.7338, loss: 9.9422, bow_loss: 6.5491, entropy_loss: -0.1546
	step 3600:lm_loss: 3.3873, ppl: 29.5849, loss: 9.9397, bow_loss: 6.5514, entropy_loss: -0.1550
	step 3700:lm_loss: 3.3824, ppl: 29.4426, loss: 9.9369, bow_loss: 6.5533, entropy_loss: -0.1554
	step 3800:lm_loss: 3.3793, ppl: 29.3504, loss: 9.9352, bow_loss: 6.5550, entropy_loss: -0.1559
	step 3900:lm_loss: 3.3757, ppl: 29.2447, loss: 9.9332, bow_loss: 6.5568, entropy_loss: -0.1563
[Evaluation][11000]lm_loss: 3.3730, ppl: 29.1670, loss: 9.9322, bow_loss: 6.5583, entropy_loss: -0.1566
	time cost: 1592.203
================================================================================
[train][1] progress: 1/1 step: 11100, time: 98.123, speed: 1.019 steps/s
	current lr: 0.0000030
	lm_loss: 3.5396, ppl: 34.4528, loss: 10.0513, bow_loss: 6.5117, entropy_loss: -0.2147
[train][1] progress: 1/1 step: 11200, time: 97.965, speed: 1.021 steps/s
	current lr: 0.0000030
	lm_loss: 3.5374, ppl: 34.3779, loss: 10.1138, bow_loss: 6.5764, entropy_loss: -0.1846
[train][1] progress: 1/1 step: 11300, time: 97.605, speed: 1.025 steps/s
	current lr: 0.0000030
	lm_loss: 3.3211, ppl: 27.6911, loss: 9.8775, bow_loss: 6.5564, entropy_loss: -0.1239
[train][1] progress: 1/1 step: 11400, time: 98.109, speed: 1.019 steps/s
	current lr: 0.0000030
	lm_loss: 3.5494, ppl: 34.7916, loss: 10.0815, bow_loss: 6.5322, entropy_loss: -0.1575
[train][1] progress: 1/1 step: 11500, time: 98.020, speed: 1.020 steps/s
	current lr: 0.0000029
	lm_loss: 3.7423, ppl: 42.1947, loss: 10.3215, bow_loss: 6.5792, entropy_loss: -0.1700
[train][1] progress: 1/1 step: 11600, time: 97.278, speed: 1.028 steps/s
	current lr: 0.0000029
	lm_loss: 3.3645, ppl: 28.9177, loss: 9.9251, bow_loss: 6.5607, entropy_loss: -0.1443
[train][1] progress: 1/1 step: 11700, time: 97.924, speed: 1.021 steps/s
	current lr: 0.0000029
	lm_loss: 3.5040, ppl: 33.2466, loss: 10.0329, bow_loss: 6.5289, entropy_loss: -0.1716
[train][1] progress: 1/1 step: 11800, time: 97.110, speed: 1.030 steps/s
	current lr: 0.0000029
	lm_loss: 3.3361, ppl: 28.1092, loss: 9.8484, bow_loss: 6.5123, entropy_loss: -0.1149
[train][1] progress: 1/1 step: 11900, time: 97.847, speed: 1.022 steps/s
	current lr: 0.0000029
	lm_loss: 3.4209, ppl: 30.5968, loss: 9.9021, bow_loss: 6.4812, entropy_loss: -0.1665
[train][1] progress: 1/1 step: 12000, time: 98.213, speed: 1.018 steps/s
	current lr: 0.0000029
	lm_loss: 3.7783, ppl: 43.7410, loss: 10.3726, bow_loss: 6.5943, entropy_loss: -0.1703
================================================================================
Evaluation:
	step 100:lm_loss: 3.4055, ppl: 30.1284, loss: 9.9409, bow_loss: 6.4951, entropy_loss: -0.1094
	step 200:lm_loss: 3.4306, ppl: 30.8945, loss: 9.9546, bow_loss: 6.4960, entropy_loss: -0.1103
	step 300:lm_loss: 3.4371, ppl: 31.0963, loss: 9.9539, bow_loss: 6.4939, entropy_loss: -0.1078
	step 400:lm_loss: 3.4325, ppl: 30.9543, loss: 9.9505, bow_loss: 6.4956, entropy_loss: -0.1072
	step 500:lm_loss: 3.4297, ppl: 30.8684, loss: 9.9438, bow_loss: 6.4928, entropy_loss: -0.1072
	step 600:lm_loss: 3.4288, ppl: 30.8386, loss: 9.9424, bow_loss: 6.4944, entropy_loss: -0.1086
	step 700:lm_loss: 3.4274, ppl: 30.7956, loss: 9.9414, bow_loss: 6.4944, entropy_loss: -0.1085
	step 800:lm_loss: 3.4259, ppl: 30.7505, loss: 9.9403, bow_loss: 6.4946, entropy_loss: -0.1088
	step 900:lm_loss: 3.4260, ppl: 30.7529, loss: 9.9416, bow_loss: 6.4950, entropy_loss: -0.1077
	step 1000:lm_loss: 3.4243, ppl: 30.7016, loss: 9.9385, bow_loss: 6.4945, entropy_loss: -0.1074
	step 1100:lm_loss: 3.4240, ppl: 30.6912, loss: 9.9377, bow_loss: 6.4937, entropy_loss: -0.1076
	step 1200:lm_loss: 3.4221, ppl: 30.6335, loss: 9.9360, bow_loss: 6.4936, entropy_loss: -0.1079
	step 1300:lm_loss: 3.4218, ppl: 30.6250, loss: 9.9356, bow_loss: 6.4941, entropy_loss: -0.1082
	step 1400:lm_loss: 3.4224, ppl: 30.6434, loss: 9.9370, bow_loss: 6.4943, entropy_loss: -0.1075
	step 1500:lm_loss: 3.4211, ppl: 30.6044, loss: 9.9365, bow_loss: 6.4940, entropy_loss: -0.1073
	step 1600:lm_loss: 3.4212, ppl: 30.6055, loss: 9.9358, bow_loss: 6.4936, entropy_loss: -0.1072
	step 1700:lm_loss: 3.4226, ppl: 30.6487, loss: 9.9374, bow_loss: 6.4950, entropy_loss: -0.1077
	step 1800:lm_loss: 3.4298, ppl: 30.8697, loss: 9.9499, bow_loss: 6.5042, entropy_loss: -0.1078
	step 1900:lm_loss: 3.4369, ppl: 31.0908, loss: 9.9595, bow_loss: 6.5119, entropy_loss: -0.1078
	step 2000:lm_loss: 3.4443, ppl: 31.3200, loss: 9.9706, bow_loss: 6.5203, entropy_loss: -0.1079
	step 2100:lm_loss: 3.4503, ppl: 31.5083, loss: 9.9801, bow_loss: 6.5268, entropy_loss: -0.1080
	step 2200:lm_loss: 3.4535, ppl: 31.6104, loss: 9.9794, bow_loss: 6.5260, entropy_loss: -0.1075
	step 2300:lm_loss: 3.4569, ppl: 31.7179, loss: 9.9784, bow_loss: 6.5244, entropy_loss: -0.1069
	step 2400:lm_loss: 3.4598, ppl: 31.8108, loss: 9.9738, bow_loss: 6.5209, entropy_loss: -0.1058
	step 2500:lm_loss: 3.4626, ppl: 31.9004, loss: 9.9721, bow_loss: 6.5188, entropy_loss: -0.1052
	step 2600:lm_loss: 3.4576, ppl: 31.7405, loss: 9.9683, bow_loss: 6.5199, entropy_loss: -0.1054
	step 2700:lm_loss: 3.4492, ppl: 31.4765, loss: 9.9636, bow_loss: 6.5224, entropy_loss: -0.1060
	step 2800:lm_loss: 3.4414, ppl: 31.2298, loss: 9.9600, bow_loss: 6.5254, entropy_loss: -0.1066
	step 2900:lm_loss: 3.4336, ppl: 30.9892, loss: 9.9557, bow_loss: 6.5280, entropy_loss: -0.1070
	step 3000:lm_loss: 3.4253, ppl: 30.7317, loss: 9.9514, bow_loss: 6.5308, entropy_loss: -0.1078
	step 3100:lm_loss: 3.4179, ppl: 30.5053, loss: 9.9480, bow_loss: 6.5338, entropy_loss: -0.1084
	step 3200:lm_loss: 3.4109, ppl: 30.2936, loss: 9.9443, bow_loss: 6.5364, entropy_loss: -0.1089
	step 3300:lm_loss: 3.4040, ppl: 30.0835, loss: 9.9405, bow_loss: 6.5387, entropy_loss: -0.1094
	step 3400:lm_loss: 3.3986, ppl: 29.9209, loss: 9.9372, bow_loss: 6.5405, entropy_loss: -0.1097
	step 3500:lm_loss: 3.3937, ppl: 29.7745, loss: 9.9355, bow_loss: 6.5434, entropy_loss: -0.1101
	step 3600:lm_loss: 3.3897, ppl: 29.6582, loss: 9.9338, bow_loss: 6.5454, entropy_loss: -0.1102
	step 3700:lm_loss: 3.3845, ppl: 29.5037, loss: 9.9312, bow_loss: 6.5480, entropy_loss: -0.1106
	step 3800:lm_loss: 3.3805, ppl: 29.3868, loss: 9.9280, bow_loss: 6.5495, entropy_loss: -0.1107
	step 3900:lm_loss: 3.3777, ppl: 29.3042, loss: 9.9265, bow_loss: 6.5510, entropy_loss: -0.1109
[Evaluation][12000]lm_loss: 3.3750, ppl: 29.2252, loss: 9.9254, bow_loss: 6.5524, entropy_loss: -0.1110
	time cost: 1593.543
================================================================================
[train][1] progress: 1/1 step: 12100, time: 97.791, speed: 1.023 steps/s
	current lr: 0.0000029
	lm_loss: 3.5956, ppl: 36.4368, loss: 10.1607, bow_loss: 6.5651, entropy_loss: -0.0591
[train][1] progress: 1/1 step: 12200, time: 97.066, speed: 1.030 steps/s
	current lr: 0.0000029
	lm_loss: 3.5683, ppl: 35.4553, loss: 10.1016, bow_loss: 6.5333, entropy_loss: -0.1313
[train][1] progress: 1/1 step: 12300, time: 98.220, speed: 1.018 steps/s
	current lr: 0.0000029
	lm_loss: 3.4770, ppl: 32.3634, loss: 9.9660, bow_loss: 6.4890, entropy_loss: -0.1134
[train][1] progress: 1/1 step: 12400, time: 97.919, speed: 1.021 steps/s
	current lr: 0.0000028
	lm_loss: 3.5222, ppl: 33.8580, loss: 10.1414, bow_loss: 6.6192, entropy_loss: -0.1343
[train][1] progress: 1/1 step: 12500, time: 97.907, speed: 1.021 steps/s
	current lr: 0.0000028
	lm_loss: 3.5717, ppl: 35.5771, loss: 10.1563, bow_loss: 6.5846, entropy_loss: -0.1424
[train][1] progress: 1/1 step: 12600, time: 97.259, speed: 1.028 steps/s
	current lr: 0.0000028
	lm_loss: 3.5249, ppl: 33.9496, loss: 10.0753, bow_loss: 6.5504, entropy_loss: -0.1993
[train][1] progress: 1/1 step: 12700, time: 97.921, speed: 1.021 steps/s
	current lr: 0.0000028
	lm_loss: 3.6333, ppl: 37.8378, loss: 10.1246, bow_loss: 6.4913, entropy_loss: -0.1541
[train][1] progress: 1/1 step: 12800, time: 97.918, speed: 1.021 steps/s
	current lr: 0.0000028
	lm_loss: 3.4524, ppl: 31.5760, loss: 10.0424, bow_loss: 6.5900, entropy_loss: -0.1307
[train][1] progress: 1/1 step: 12900, time: 98.136, speed: 1.019 steps/s
	current lr: 0.0000028
	lm_loss: 3.4705, ppl: 32.1540, loss: 9.9795, bow_loss: 6.5090, entropy_loss: -0.1754
[train][1] progress: 1/1 step: 13000, time: 97.854, speed: 1.022 steps/s
	current lr: 0.0000028
	lm_loss: 3.6062, ppl: 36.8252, loss: 10.1179, bow_loss: 6.5117, entropy_loss: -0.1081
================================================================================
Evaluation:
	step 100:lm_loss: 3.4336, ppl: 30.9893, loss: 9.9440, bow_loss: 6.4912, entropy_loss: -0.0942
	step 200:lm_loss: 3.4304, ppl: 30.8877, loss: 9.9475, bow_loss: 6.4889, entropy_loss: -0.0942
	step 300:lm_loss: 3.4300, ppl: 30.8752, loss: 9.9431, bow_loss: 6.4875, entropy_loss: -0.0945
	step 400:lm_loss: 3.4289, ppl: 30.8433, loss: 9.9399, bow_loss: 6.4881, entropy_loss: -0.0940
	step 500:lm_loss: 3.4252, ppl: 30.7301, loss: 9.9368, bow_loss: 6.4879, entropy_loss: -0.0941
	step 600:lm_loss: 3.4240, ppl: 30.6934, loss: 9.9334, bow_loss: 6.4874, entropy_loss: -0.0942
	step 700:lm_loss: 3.4261, ppl: 30.7553, loss: 9.9364, bow_loss: 6.4884, entropy_loss: -0.0940
	step 800:lm_loss: 3.4213, ppl: 30.6081, loss: 9.9332, bow_loss: 6.4889, entropy_loss: -0.0948
	step 900:lm_loss: 3.4213, ppl: 30.6102, loss: 9.9336, bow_loss: 6.4898, entropy_loss: -0.0944
	step 1000:lm_loss: 3.4198, ppl: 30.5629, loss: 9.9310, bow_loss: 6.4893, entropy_loss: -0.0944
	step 1100:lm_loss: 3.4184, ppl: 30.5212, loss: 9.9293, bow_loss: 6.4881, entropy_loss: -0.0944
	step 1200:lm_loss: 3.4176, ppl: 30.4949, loss: 9.9279, bow_loss: 6.4882, entropy_loss: -0.0947
	step 1300:lm_loss: 3.4173, ppl: 30.4885, loss: 9.9281, bow_loss: 6.4882, entropy_loss: -0.0943
	step 1400:lm_loss: 3.4166, ppl: 30.4648, loss: 9.9275, bow_loss: 6.4881, entropy_loss: -0.0947
	step 1500:lm_loss: 3.4173, ppl: 30.4863, loss: 9.9282, bow_loss: 6.4881, entropy_loss: -0.0943
	step 1600:lm_loss: 3.4156, ppl: 30.4356, loss: 9.9269, bow_loss: 6.4877, entropy_loss: -0.0943
	step 1700:lm_loss: 3.4178, ppl: 30.5011, loss: 9.9296, bow_loss: 6.4896, entropy_loss: -0.0943
	step 1800:lm_loss: 3.4255, ppl: 30.7365, loss: 9.9413, bow_loss: 6.4978, entropy_loss: -0.0943
	step 1900:lm_loss: 3.4321, ppl: 30.9423, loss: 9.9505, bow_loss: 6.5056, entropy_loss: -0.0945
	step 2000:lm_loss: 3.4393, ppl: 31.1656, loss: 9.9613, bow_loss: 6.5136, entropy_loss: -0.0944
	step 2100:lm_loss: 3.4456, ppl: 31.3635, loss: 9.9712, bow_loss: 6.5208, entropy_loss: -0.0947
	step 2200:lm_loss: 3.4490, ppl: 31.4696, loss: 9.9699, bow_loss: 6.5193, entropy_loss: -0.0940
	step 2300:lm_loss: 3.4523, ppl: 31.5717, loss: 9.9670, bow_loss: 6.5158, entropy_loss: -0.0932
	step 2400:lm_loss: 3.4569, ppl: 31.7189, loss: 9.9667, bow_loss: 6.5137, entropy_loss: -0.0921
	step 2500:lm_loss: 3.4598, ppl: 31.8116, loss: 9.9649, bow_loss: 6.5121, entropy_loss: -0.0915
	step 2600:lm_loss: 3.4545, ppl: 31.6421, loss: 9.9621, bow_loss: 6.5141, entropy_loss: -0.0918
	step 2700:lm_loss: 3.4459, ppl: 31.3708, loss: 9.9571, bow_loss: 6.5168, entropy_loss: -0.0923
	step 2800:lm_loss: 3.4391, ppl: 31.1594, loss: 9.9532, bow_loss: 6.5189, entropy_loss: -0.0926
	step 2900:lm_loss: 3.4334, ppl: 30.9816, loss: 9.9506, bow_loss: 6.5214, entropy_loss: -0.0928
	step 3000:lm_loss: 3.4259, ppl: 30.7499, loss: 9.9466, bow_loss: 6.5242, entropy_loss: -0.0933
	step 3100:lm_loss: 3.4177, ppl: 30.4978, loss: 9.9432, bow_loss: 6.5274, entropy_loss: -0.0939
	step 3200:lm_loss: 3.4113, ppl: 30.3045, loss: 9.9400, bow_loss: 6.5298, entropy_loss: -0.0944
	step 3300:lm_loss: 3.4033, ppl: 30.0618, loss: 9.9358, bow_loss: 6.5323, entropy_loss: -0.0950
	step 3400:lm_loss: 3.3972, ppl: 29.8794, loss: 9.9325, bow_loss: 6.5348, entropy_loss: -0.0955
	step 3500:lm_loss: 3.3927, ppl: 29.7447, loss: 9.9305, bow_loss: 6.5372, entropy_loss: -0.0958
	step 3600:lm_loss: 3.3874, ppl: 29.5894, loss: 9.9280, bow_loss: 6.5395, entropy_loss: -0.0959
	step 3700:lm_loss: 3.3845, ppl: 29.5027, loss: 9.9262, bow_loss: 6.5410, entropy_loss: -0.0960
	step 3800:lm_loss: 3.3806, ppl: 29.3895, loss: 9.9241, bow_loss: 6.5429, entropy_loss: -0.0960
	step 3900:lm_loss: 3.3763, ppl: 29.2623, loss: 9.9220, bow_loss: 6.5451, entropy_loss: -0.0962
[Evaluation][13000]lm_loss: 3.3744, ppl: 29.2059, loss: 9.9209, bow_loss: 6.5460, entropy_loss: -0.0963
	time cost: 1610.956
================================================================================
[train][1] progress: 1/1 step: 13100, time: 97.017, speed: 1.031 steps/s
	current lr: 0.0000028
	lm_loss: 3.5660, ppl: 35.3747, loss: 10.1055, bow_loss: 6.5395, entropy_loss: -0.1551
[train][1] progress: 1/1 step: 13200, time: 97.050, speed: 1.030 steps/s
	current lr: 0.0000028
	lm_loss: 4.0285, ppl: 56.1784, loss: 11.1202, bow_loss: 7.0917, entropy_loss: -0.1250
[train][1] progress: 1/1 step: 13300, time: 97.759, speed: 1.023 steps/s
	current lr: 0.0000027
	lm_loss: 4.0531, ppl: 57.5762, loss: 11.0346, bow_loss: 6.9815, entropy_loss: -0.1256
[train][1] progress: 1/1 step: 13400, time: 97.542, speed: 1.025 steps/s
	current lr: 0.0000027
	lm_loss: 4.0220, ppl: 55.8104, loss: 11.3004, bow_loss: 7.2785, entropy_loss: -0.2526
{
  "is_distributed": true,
  "save_path": "./output",
  "infer_file": "data/test.txt",
  "output_name": "response",
  "log_steps": 1,
  "Model": {
    "model": "UnifiedTransformer",
    "config_path": "./config/12L.json",
    "init_checkpoint": "",
    "init_pretraining_params": "12L",
    "learning_rate": 1e-05,
    "warmup_steps": 0,
    "weight_decay": 0.0,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": false,
    "amp_loss_scaling": 12800,
    "max_seq_len": 256,
    "weight_sharing": true,
    "mem_efficient": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "latent_type_size": 20,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": 20,
    "topk": 5,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "DialogGeneration",
    "do_generation": "true",
    "is_cn": true,
    "nsp_inference_model_path": null,
    "nsp_attention_style": "bidirectional",
    "ranking_score": "decode_score"
  },
  "Reader": {
    "max_src_len": 128,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": false,
    "batch_size": 4,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  },
  "run_infer": true
}
W1025 20:34:58.731557   571 device_context.cc:252] Please NOTE: device: 0, CUDA Capability: 70, Driver API Version: 10.1, Runtime API Version: 9.0
W1025 20:34:58.735945   571 device_context.cc:260] device: 0, cuDNN Version: 7.6.
Load pretraining parameters from 12L.
Traceback (most recent call last):
  File "./infer.py", line 136, in <module>
    infer(args)
  File "./infer.py", line 83, in infer
    predictions = task.infer_step(model, data)
  File "/home/aistudio/ldk/Knover/tasks/task_base.py", line 43, in infer_step
    predictions = model.infer_step(inputs)
  File "/home/aistudio/ldk/Knover/models/unified_transformer.py", line 440, in infer_step
    predictions = self._run_generation(inputs)
  File "/home/aistudio/ldk/Knover/models/unified_transformer.py", line 397, in _run_generation
    return_numpy=False)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 266, in _execute
    fetch_vars = self.exe.run(program, feed, fetch_list, **kwargs)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1066, in run
    return_merged=return_merged)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1154, in _run_impl
    use_program_cache=use_program_cache)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1229, in _run_program
    fetch_var_name)
KeyboardInterrupt
{
  "is_distributed": true,
  "save_path": "./output",
  "infer_file": "data/test.txt",
  "output_name": "response",
  "log_steps": 1,
  "Model": {
    "model": "UnifiedTransformer",
    "config_path": "./config/12L.json",
    "init_checkpoint": "",
    "init_pretraining_params": "12L",
    "learning_rate": 1e-05,
    "warmup_steps": 0,
    "weight_decay": 0.0,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": false,
    "amp_loss_scaling": 12800,
    "max_seq_len": 256,
    "weight_sharing": true,
    "mem_efficient": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "latent_type_size": 20,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": 20,
    "topk": 5,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "DialogGeneration",
    "do_generation": "true",
    "is_cn": true,
    "nsp_inference_model_path": null,
    "nsp_attention_style": "bidirectional",
    "ranking_score": "decode_score"
  },
  "Reader": {
    "max_src_len": 128,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": false,
    "batch_size": 4,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  },
  "run_infer": true
}
W1025 20:36:56.561107   730 device_context.cc:252] Please NOTE: device: 0, CUDA Capability: 70, Driver API Version: 10.1, Runtime API Version: 9.0
W1025 20:36:56.565618   730 device_context.cc:260] device: 0, cuDNN Version: 7.6.
Load pretraining parameters from 12L.
	step: 1, time: 0.913, speed: 1.096 steps/s
	step: 2, time: 1.811, speed: 1.104 steps/s
	step: 3, time: 2.500, speed: 1.200 steps/s
	step: 4, time: 3.365, speed: 1.189 steps/s
	step: 5, time: 4.246, speed: 1.178 steps/s
	step: 6, time: 5.144, speed: 1.166 steps/s
	step: 7, time: 6.053, speed: 1.156 steps/s
	step: 8, time: 6.994, speed: 1.144 steps/s
	step: 9, time: 7.866, speed: 1.144 steps/s
	step: 10, time: 8.560, speed: 1.168 steps/s
	step: 11, time: 9.520, speed: 1.156 steps/s
	step: 12, time: 10.402, speed: 1.154 steps/s
	step: 13, time: 11.315, speed: 1.149 steps/s
	step: 14, time: 12.231, speed: 1.145 steps/s
	step: 15, time: 13.077, speed: 1.147 steps/s
	step: 16, time: 13.835, speed: 1.156 steps/s
	step: 17, time: 14.389, speed: 1.181 steps/s
	step: 18, time: 15.326, speed: 1.174 steps/s
	step: 19, time: 16.267, speed: 1.168 steps/s
	step: 20, time: 17.080, speed: 1.171 steps/s
	step: 21, time: 17.926, speed: 1.171 steps/s
	step: 22, time: 18.751, speed: 1.173 steps/s
	step: 23, time: 19.507, speed: 1.179 steps/s
	step: 24, time: 20.114, speed: 1.193 steps/s
	step: 25, time: 21.149, speed: 1.182 steps/s
	step: 26, time: 21.921, speed: 1.186 steps/s
	step: 27, time: 22.825, speed: 1.183 steps/s
	step: 28, time: 23.595, speed: 1.187 steps/s
	step: 29, time: 24.522, speed: 1.183 steps/s
	step: 30, time: 25.369, speed: 1.183 steps/s
	step: 31, time: 26.160, speed: 1.185 steps/s
	step: 32, time: 26.926, speed: 1.188 steps/s
	step: 33, time: 27.740, speed: 1.190 steps/s
	step: 34, time: 28.687, speed: 1.185 steps/s
	step: 35, time: 29.603, speed: 1.182 steps/s
	step: 36, time: 30.336, speed: 1.187 steps/s
	step: 37, time: 31.040, speed: 1.192 steps/s
	step: 38, time: 31.769, speed: 1.196 steps/s
	step: 39, time: 32.608, speed: 1.196 steps/s
	step: 40, time: 33.489, speed: 1.194 steps/s
	step: 41, time: 34.375, speed: 1.193 steps/s
	step: 42, time: 35.274, speed: 1.191 steps/s
	step: 43, time: 36.296, speed: 1.185 steps/s
	step: 44, time: 37.205, speed: 1.183 steps/s
	step: 45, time: 37.915, speed: 1.187 steps/s
	step: 46, time: 38.574, speed: 1.193 steps/s
	step: 47, time: 39.280, speed: 1.197 steps/s
	step: 48, time: 40.152, speed: 1.195 steps/s
	step: 49, time: 41.043, speed: 1.194 steps/s
	step: 50, time: 41.799, speed: 1.196 steps/s
	step: 51, time: 42.465, speed: 1.201 steps/s
	step: 52, time: 42.989, speed: 1.210 steps/s
	step: 53, time: 43.894, speed: 1.207 steps/s
	step: 54, time: 44.418, speed: 1.216 steps/s
	step: 55, time: 45.299, speed: 1.214 steps/s
	step: 56, time: 46.168, speed: 1.213 steps/s
	step: 57, time: 46.966, speed: 1.214 steps/s
	step: 58, time: 47.852, speed: 1.212 steps/s
	step: 59, time: 48.763, speed: 1.210 steps/s
	step: 60, time: 49.771, speed: 1.206 steps/s
	step: 61, time: 50.683, speed: 1.204 steps/s
	step: 62, time: 51.446, speed: 1.205 steps/s
	step: 63, time: 52.337, speed: 1.204 steps/s
	step: 64, time: 52.996, speed: 1.208 steps/s
	step: 65, time: 53.717, speed: 1.210 steps/s
	step: 66, time: 54.528, speed: 1.210 steps/s
	step: 67, time: 55.394, speed: 1.210 steps/s
	step: 68, time: 56.274, speed: 1.208 steps/s
	step: 69, time: 57.206, speed: 1.206 steps/s
	step: 70, time: 58.122, speed: 1.204 steps/s
	step: 71, time: 59.013, speed: 1.203 steps/s
	step: 72, time: 59.894, speed: 1.202 steps/s
	step: 73, time: 60.777, speed: 1.201 steps/s
	step: 74, time: 61.501, speed: 1.203 steps/s
	step: 75, time: 62.357, speed: 1.203 steps/s
	step: 76, time: 63.035, speed: 1.206 steps/s
	step: 77, time: 64.088, speed: 1.201 steps/s
	step: 78, time: 65.021, speed: 1.200 steps/s
	step: 79, time: 65.929, speed: 1.198 steps/s
	step: 80, time: 66.446, speed: 1.204 steps/s
	step: 81, time: 67.366, speed: 1.202 steps/s
	step: 82, time: 68.068, speed: 1.205 steps/s
	step: 83, time: 68.981, speed: 1.203 steps/s
	step: 84, time: 69.856, speed: 1.202 steps/s
	step: 85, time: 70.744, speed: 1.202 steps/s
	step: 86, time: 71.642, speed: 1.200 steps/s
	step: 87, time: 72.532, speed: 1.199 steps/s
	step: 88, time: 73.342, speed: 1.200 steps/s
	step: 89, time: 74.245, speed: 1.199 steps/s
	step: 90, time: 74.949, speed: 1.201 steps/s
	step: 91, time: 75.845, speed: 1.200 steps/s
	step: 92, time: 76.575, speed: 1.201 steps/s
	step: 93, time: 77.476, speed: 1.200 steps/s
	step: 94, time: 78.405, speed: 1.199 steps/s
	step: 95, time: 79.222, speed: 1.199 steps/s
	step: 96, time: 80.086, speed: 1.199 steps/s
	step: 97, time: 80.986, speed: 1.198 steps/s
	step: 98, time: 81.735, speed: 1.199 steps/s
	step: 99, time: 82.635, speed: 1.198 steps/s
	step: 100, time: 83.532, speed: 1.197 steps/s
	step: 101, time: 84.104, speed: 1.201 steps/s
	step: 102, time: 85.022, speed: 1.200 steps/s
	step: 103, time: 85.868, speed: 1.200 steps/s
	step: 104, time: 86.778, speed: 1.198 steps/s
	step: 105, time: 87.676, speed: 1.198 steps/s
	step: 106, time: 88.570, speed: 1.197 steps/s
	step: 107, time: 89.433, speed: 1.196 steps/s
	step: 108, time: 90.190, speed: 1.197 steps/s
	step: 109, time: 91.109, speed: 1.196 steps/s
	step: 110, time: 92.000, speed: 1.196 steps/s
	step: 111, time: 93.027, speed: 1.193 steps/s
	step: 112, time: 93.755, speed: 1.195 steps/s
	step: 113, time: 94.422, speed: 1.197 steps/s
	step: 114, time: 95.342, speed: 1.196 steps/s
	step: 115, time: 96.207, speed: 1.195 steps/s
	step: 116, time: 97.052, speed: 1.195 steps/s
	step: 117, time: 97.933, speed: 1.195 steps/s
	step: 118, time: 98.814, speed: 1.194 steps/s
	step: 119, time: 99.700, speed: 1.194 steps/s
	step: 120, time: 100.623, speed: 1.193 steps/s
	step: 121, time: 101.392, speed: 1.193 steps/s
	step: 122, time: 102.126, speed: 1.195 steps/s
	step: 123, time: 103.036, speed: 1.194 steps/s
	step: 124, time: 103.639, speed: 1.196 steps/s
	step: 125, time: 104.447, speed: 1.197 steps/s
	step: 126, time: 105.049, speed: 1.199 steps/s
	step: 127, time: 105.950, speed: 1.199 steps/s
	step: 128, time: 106.881, speed: 1.198 steps/s
	step: 129, time: 107.551, speed: 1.199 steps/s
	step: 130, time: 108.405, speed: 1.199 steps/s
	step: 131, time: 109.314, speed: 1.198 steps/s
	step: 132, time: 110.221, speed: 1.198 steps/s
	step: 133, time: 111.089, speed: 1.197 steps/s
	step: 134, time: 111.992, speed: 1.197 steps/s
	step: 135, time: 112.909, speed: 1.196 steps/s
	step: 136, time: 113.782, speed: 1.195 steps/s
	step: 137, time: 114.451, speed: 1.197 steps/s
	step: 138, time: 115.021, speed: 1.200 steps/s
	step: 139, time: 115.870, speed: 1.200 steps/s
	step: 140, time: 116.581, speed: 1.201 steps/s
	step: 141, time: 117.473, speed: 1.200 steps/s
	step: 142, time: 118.383, speed: 1.199 steps/s
	step: 143, time: 119.286, speed: 1.199 steps/s
	step: 144, time: 119.959, speed: 1.200 steps/s
	step: 145, time: 121.010, speed: 1.198 steps/s
	step: 146, time: 121.748, speed: 1.199 steps/s
	step: 147, time: 122.454, speed: 1.200 steps/s
	step: 148, time: 123.372, speed: 1.200 steps/s
	step: 149, time: 124.267, speed: 1.199 steps/s
	step: 150, time: 125.185, speed: 1.198 steps/s
	step: 151, time: 126.061, speed: 1.198 steps/s
	step: 152, time: 126.870, speed: 1.198 steps/s
	step: 153, time: 127.555, speed: 1.199 steps/s
	step: 154, time: 128.448, speed: 1.199 steps/s
	step: 155, time: 129.341, speed: 1.198 steps/s
	step: 156, time: 129.980, speed: 1.200 steps/s
	step: 157, time: 130.938, speed: 1.199 steps/s
	step: 158, time: 131.775, speed: 1.199 steps/s
	step: 159, time: 132.684, speed: 1.198 steps/s
	step: 160, time: 133.559, speed: 1.198 steps/s
	step: 161, time: 134.434, speed: 1.198 steps/s
	step: 162, time: 135.502, speed: 1.196 steps/s
	step: 163, time: 136.219, speed: 1.197 steps/s
	step: 164, time: 137.151, speed: 1.196 steps/s
	step: 165, time: 138.060, speed: 1.195 steps/s
	step: 166, time: 138.968, speed: 1.195 steps/s
	step: 167, time: 139.883, speed: 1.194 steps/s
	step: 168, time: 140.675, speed: 1.194 steps/s
	step: 169, time: 141.561, speed: 1.194 steps/s
	step: 170, time: 142.449, speed: 1.193 steps/s
	step: 171, time: 143.316, speed: 1.193 steps/s
	step: 172, time: 144.202, speed: 1.193 steps/s
	step: 173, time: 144.892, speed: 1.194 steps/s
	step: 174, time: 145.704, speed: 1.194 steps/s
	step: 175, time: 146.560, speed: 1.194 steps/s
	step: 176, time: 147.278, speed: 1.195 steps/s
	step: 177, time: 148.207, speed: 1.194 steps/s
	step: 178, time: 148.953, speed: 1.195 steps/s
	step: 179, time: 149.910, speed: 1.194 steps/s
	step: 180, time: 150.766, speed: 1.194 steps/s
	step: 181, time: 151.695, speed: 1.193 steps/s
	step: 182, time: 152.445, speed: 1.194 steps/s
	step: 183, time: 153.339, speed: 1.193 steps/s
	step: 184, time: 154.218, speed: 1.193 steps/s
	step: 185, time: 155.007, speed: 1.193 steps/s
	step: 186, time: 155.847, speed: 1.193 steps/s
	step: 187, time: 156.612, speed: 1.194 steps/s
	step: 188, time: 157.286, speed: 1.195 steps/s
	step: 189, time: 157.973, speed: 1.196 steps/s
	step: 190, time: 158.734, speed: 1.197 steps/s
	step: 191, time: 159.643, speed: 1.196 steps/s
	step: 192, time: 160.524, speed: 1.196 steps/s
	step: 193, time: 161.430, speed: 1.196 steps/s
	step: 194, time: 162.319, speed: 1.195 steps/s
	step: 195, time: 163.234, speed: 1.195 steps/s
	step: 196, time: 164.152, speed: 1.194 steps/s
	step: 197, time: 165.041, speed: 1.194 steps/s
	step: 198, time: 165.917, speed: 1.193 steps/s
	step: 199, time: 166.803, speed: 1.193 steps/s
	step: 200, time: 167.715, speed: 1.192 steps/s
	step: 201, time: 168.416, speed: 1.193 steps/s
	step: 202, time: 168.995, speed: 1.195 steps/s
	step: 203, time: 169.479, speed: 1.198 steps/s
	step: 204, time: 170.342, speed: 1.198 steps/s
	step: 205, time: 171.118, speed: 1.198 steps/s
	step: 206, time: 171.876, speed: 1.199 steps/s
	step: 207, time: 172.600, speed: 1.199 steps/s
	step: 208, time: 173.524, speed: 1.199 steps/s
	step: 209, time: 174.406, speed: 1.198 steps/s
	step: 210, time: 175.295, speed: 1.198 steps/s
	step: 211, time: 176.073, speed: 1.198 steps/s
	step: 212, time: 176.795, speed: 1.199 steps/s
	step: 213, time: 177.594, speed: 1.199 steps/s
	step: 214, time: 178.216, speed: 1.201 steps/s
	step: 215, time: 179.019, speed: 1.201 steps/s
	step: 216, time: 179.909, speed: 1.201 steps/s
	step: 217, time: 180.821, speed: 1.200 steps/s
	step: 218, time: 181.764, speed: 1.199 steps/s
	step: 219, time: 182.667, speed: 1.199 steps/s
	step: 220, time: 183.540, speed: 1.199 steps/s
	step: 221, time: 184.401, speed: 1.198 steps/s
	step: 222, time: 185.309, speed: 1.198 steps/s
	step: 223, time: 186.023, speed: 1.199 steps/s
	step: 224, time: 186.917, speed: 1.198 steps/s
	step: 225, time: 187.825, speed: 1.198 steps/s
	step: 226, time: 188.723, speed: 1.198 steps/s
	step: 227, time: 189.587, speed: 1.197 steps/s
	step: 228, time: 190.485, speed: 1.197 steps/s
	step: 229, time: 191.164, speed: 1.198 steps/s
	step: 230, time: 191.997, speed: 1.198 steps/s
	step: 231, time: 192.840, speed: 1.198 steps/s
	step: 232, time: 193.685, speed: 1.198 steps/s
	step: 233, time: 194.567, speed: 1.198 steps/s
	step: 234, time: 195.268, speed: 1.198 steps/s
	step: 235, time: 196.148, speed: 1.198 steps/s
	step: 236, time: 196.897, speed: 1.199 steps/s
	step: 237, time: 197.829, speed: 1.198 steps/s
	step: 238, time: 198.719, speed: 1.198 steps/s
	step: 239, time: 199.624, speed: 1.197 steps/s
	step: 240, time: 200.535, speed: 1.197 steps/s
	step: 241, time: 201.123, speed: 1.198 steps/s
	step: 242, time: 201.994, speed: 1.198 steps/s
	step: 243, time: 202.624, speed: 1.199 steps/s
	step: 244, time: 203.517, speed: 1.199 steps/s
	step: 245, time: 204.422, speed: 1.199 steps/s
	step: 246, time: 205.313, speed: 1.198 steps/s
	step: 247, time: 206.203, speed: 1.198 steps/s
	step: 248, time: 207.242, speed: 1.197 steps/s
	step: 249, time: 208.003, speed: 1.197 steps/s
	step: 250, time: 208.890, speed: 1.197 steps/s
	step: 251, time: 209.573, speed: 1.198 steps/s
	step: 252, time: 210.443, speed: 1.197 steps/s
	step: 253, time: 211.148, speed: 1.198 steps/s
	step: 254, time: 212.039, speed: 1.198 steps/s
	step: 255, time: 212.666, speed: 1.199 steps/s
	step: 256, time: 213.559, speed: 1.199 steps/s
	step: 257, time: 214.416, speed: 1.199 steps/s
	step: 258, time: 215.305, speed: 1.198 steps/s
	step: 259, time: 216.153, speed: 1.198 steps/s
	step: 260, time: 217.005, speed: 1.198 steps/s
	step: 261, time: 217.664, speed: 1.199 steps/s
	step: 262, time: 218.572, speed: 1.199 steps/s
	step: 263, time: 219.415, speed: 1.199 steps/s
	step: 264, time: 220.302, speed: 1.198 steps/s
	step: 265, time: 221.349, speed: 1.197 steps/s
	step: 266, time: 222.264, speed: 1.197 steps/s
	step: 267, time: 223.162, speed: 1.196 steps/s
	step: 268, time: 223.785, speed: 1.198 steps/s
	step: 269, time: 224.633, speed: 1.198 steps/s
	step: 270, time: 225.305, speed: 1.198 steps/s
	step: 271, time: 226.180, speed: 1.198 steps/s
	step: 272, time: 227.071, speed: 1.198 steps/s
	step: 273, time: 227.738, speed: 1.199 steps/s
	step: 274, time: 228.609, speed: 1.199 steps/s
	step: 275, time: 229.455, speed: 1.198 steps/s
	step: 276, time: 230.250, speed: 1.199 steps/s
	step: 277, time: 231.005, speed: 1.199 steps/s
	step: 278, time: 231.705, speed: 1.200 steps/s
	step: 279, time: 232.597, speed: 1.199 steps/s
	step: 280, time: 233.451, speed: 1.199 steps/s
	step: 281, time: 234.235, speed: 1.200 steps/s
	step: 282, time: 235.279, speed: 1.199 steps/s
	step: 283, time: 235.908, speed: 1.200 steps/s
	step: 284, time: 236.788, speed: 1.199 steps/s
	step: 285, time: 237.663, speed: 1.199 steps/s
	step: 286, time: 238.531, speed: 1.199 steps/s
	step: 287, time: 239.430, speed: 1.199 steps/s
	step: 288, time: 240.387, speed: 1.198 steps/s
	step: 289, time: 241.316, speed: 1.198 steps/s
	step: 290, time: 242.049, speed: 1.198 steps/s
	step: 291, time: 242.630, speed: 1.199 steps/s
	step: 292, time: 243.574, speed: 1.199 steps/s
	step: 293, time: 244.563, speed: 1.198 steps/s
	step: 294, time: 245.510, speed: 1.198 steps/s
	step: 295, time: 246.361, speed: 1.197 steps/s
	step: 296, time: 247.251, speed: 1.197 steps/s
	step: 297, time: 247.887, speed: 1.198 steps/s
	step: 298, time: 248.581, speed: 1.199 steps/s
	step: 299, time: 249.568, speed: 1.198 steps/s
	step: 300, time: 250.361, speed: 1.198 steps/s
	step: 301, time: 251.285, speed: 1.198 steps/s
	step: 302, time: 252.185, speed: 1.198 steps/s
	step: 303, time: 253.093, speed: 1.197 steps/s
	step: 304, time: 253.975, speed: 1.197 steps/s
	step: 305, time: 254.858, speed: 1.197 steps/s
	step: 306, time: 255.537, speed: 1.197 steps/s
	step: 307, time: 256.167, speed: 1.198 steps/s
	step: 308, time: 256.939, speed: 1.199 steps/s
	step: 309, time: 257.572, speed: 1.200 steps/s
	step: 310, time: 258.490, speed: 1.199 steps/s
	step: 311, time: 259.066, speed: 1.200 steps/s
	step: 312, time: 259.745, speed: 1.201 steps/s
	step: 313, time: 260.492, speed: 1.202 steps/s
	step: 314, time: 261.390, speed: 1.201 steps/s
	step: 315, time: 262.300, speed: 1.201 steps/s
	step: 316, time: 263.376, speed: 1.200 steps/s
	step: 317, time: 264.239, speed: 1.200 steps/s
	step: 318, time: 265.141, speed: 1.199 steps/s
	step: 319, time: 266.047, speed: 1.199 steps/s
	step: 320, time: 266.666, speed: 1.200 steps/s
	step: 321, time: 267.575, speed: 1.200 steps/s
	step: 322, time: 268.450, speed: 1.199 steps/s
	step: 323, time: 269.334, speed: 1.199 steps/s
	step: 324, time: 270.207, speed: 1.199 steps/s
	step: 325, time: 271.117, speed: 1.199 steps/s
	step: 326, time: 271.695, speed: 1.200 steps/s
	step: 327, time: 272.590, speed: 1.200 steps/s
	step: 328, time: 273.239, speed: 1.200 steps/s
	step: 329, time: 274.150, speed: 1.200 steps/s
	step: 330, time: 274.876, speed: 1.201 steps/s
	step: 331, time: 275.750, speed: 1.200 steps/s
	step: 332, time: 276.305, speed: 1.202 steps/s
	step: 333, time: 277.369, speed: 1.201 steps/s
	step: 334, time: 278.203, speed: 1.201 steps/s
	step: 335, time: 279.119, speed: 1.200 steps/s
	step: 336, time: 280.000, speed: 1.200 steps/s
	step: 337, time: 280.728, speed: 1.200 steps/s
	step: 338, time: 281.332, speed: 1.201 steps/s
	step: 339, time: 282.079, speed: 1.202 steps/s
	step: 340, time: 282.967, speed: 1.202 steps/s
	step: 341, time: 283.801, speed: 1.202 steps/s
	step: 342, time: 284.692, speed: 1.201 steps/s
	step: 343, time: 285.293, speed: 1.202 steps/s
	step: 344, time: 286.213, speed: 1.202 steps/s
	step: 345, time: 287.077, speed: 1.202 steps/s
	step: 346, time: 287.800, speed: 1.202 steps/s
	step: 347, time: 288.372, speed: 1.203 steps/s
	step: 348, time: 289.305, speed: 1.203 steps/s
	step: 349, time: 290.167, speed: 1.203 steps/s
	step: 350, time: 291.103, speed: 1.202 steps/s
	step: 351, time: 291.860, speed: 1.203 steps/s
	step: 352, time: 292.783, speed: 1.202 steps/s
	step: 353, time: 293.658, speed: 1.202 steps/s
	step: 354, time: 294.354, speed: 1.203 steps/s
	step: 355, time: 295.109, speed: 1.203 steps/s
	step: 356, time: 295.751, speed: 1.204 steps/s
	step: 357, time: 296.647, speed: 1.203 steps/s
	step: 358, time: 297.529, speed: 1.203 steps/s
	step: 359, time: 298.433, speed: 1.203 steps/s
	step: 360, time: 299.182, speed: 1.203 steps/s
	step: 361, time: 299.791, speed: 1.204 steps/s
	step: 362, time: 300.675, speed: 1.204 steps/s
	step: 363, time: 301.505, speed: 1.204 steps/s
	step: 364, time: 302.145, speed: 1.205 steps/s
	step: 365, time: 302.866, speed: 1.205 steps/s
	step: 366, time: 303.738, speed: 1.205 steps/s
	step: 367, time: 304.698, speed: 1.204 steps/s
	step: 368, time: 305.572, speed: 1.204 steps/s
	step: 369, time: 306.202, speed: 1.205 steps/s
	step: 370, time: 306.868, speed: 1.206 steps/s
	step: 371, time: 307.727, speed: 1.206 steps/s
	step: 372, time: 308.660, speed: 1.205 steps/s
	step: 373, time: 309.546, speed: 1.205 steps/s
	step: 374, time: 310.430, speed: 1.205 steps/s
	step: 375, time: 311.382, speed: 1.204 steps/s
	step: 376, time: 312.262, speed: 1.204 steps/s
	step: 377, time: 313.170, speed: 1.204 steps/s
	step: 378, time: 313.825, speed: 1.204 steps/s
	step: 379, time: 314.724, speed: 1.204 steps/s
	step: 380, time: 315.587, speed: 1.204 steps/s
	step: 381, time: 316.369, speed: 1.204 steps/s
	step: 382, time: 317.256, speed: 1.204 steps/s
	step: 383, time: 318.163, speed: 1.204 steps/s
	step: 384, time: 319.211, speed: 1.203 steps/s
	step: 385, time: 320.123, speed: 1.203 steps/s
	step: 386, time: 320.724, speed: 1.204 steps/s
	step: 387, time: 321.636, speed: 1.203 steps/s
	step: 388, time: 322.343, speed: 1.204 steps/s
	step: 389, time: 323.248, speed: 1.203 steps/s
	step: 390, time: 323.974, speed: 1.204 steps/s
	step: 391, time: 324.840, speed: 1.204 steps/s
	step: 392, time: 325.405, speed: 1.205 steps/s
	step: 393, time: 326.008, speed: 1.205 steps/s
	step: 394, time: 326.940, speed: 1.205 steps/s
	step: 395, time: 327.821, speed: 1.205 steps/s
	step: 396, time: 328.646, speed: 1.205 steps/s
	step: 397, time: 329.567, speed: 1.205 steps/s
	step: 398, time: 330.108, speed: 1.206 steps/s
	step: 399, time: 330.991, speed: 1.205 steps/s
	step: 400, time: 331.882, speed: 1.205 steps/s
	step: 401, time: 332.923, speed: 1.204 steps/s
	step: 402, time: 333.599, speed: 1.205 steps/s
	step: 403, time: 334.137, speed: 1.206 steps/s
	step: 404, time: 334.955, speed: 1.206 steps/s
	step: 405, time: 335.605, speed: 1.207 steps/s
	step: 406, time: 336.417, speed: 1.207 steps/s
	step: 407, time: 337.113, speed: 1.207 steps/s
	step: 408, time: 338.018, speed: 1.207 steps/s
	step: 409, time: 338.660, speed: 1.208 steps/s
	step: 410, time: 339.411, speed: 1.208 steps/s
	step: 411, time: 340.074, speed: 1.209 steps/s
	step: 412, time: 340.983, speed: 1.208 steps/s
	step: 413, time: 341.864, speed: 1.208 steps/s
	step: 414, time: 342.747, speed: 1.208 steps/s
	step: 415, time: 343.661, speed: 1.208 steps/s
	step: 416, time: 344.560, speed: 1.207 steps/s
	step: 417, time: 345.270, speed: 1.208 steps/s
	step: 418, time: 346.151, speed: 1.208 steps/s
	step: 419, time: 347.197, speed: 1.207 steps/s
	step: 420, time: 347.879, speed: 1.207 steps/s
	step: 421, time: 348.767, speed: 1.207 steps/s
	step: 422, time: 349.657, speed: 1.207 steps/s
	step: 423, time: 350.536, speed: 1.207 steps/s
	step: 424, time: 351.024, speed: 1.208 steps/s
	step: 425, time: 351.566, speed: 1.209 steps/s
	step: 426, time: 352.403, speed: 1.209 steps/s
	step: 427, time: 353.278, speed: 1.209 steps/s
	step: 428, time: 353.980, speed: 1.209 steps/s
	step: 429, time: 354.697, speed: 1.209 steps/s
	step: 430, time: 355.583, speed: 1.209 steps/s
	step: 431, time: 356.509, speed: 1.209 steps/s
	step: 432, time: 357.164, speed: 1.210 steps/s
	step: 433, time: 358.042, speed: 1.209 steps/s
	step: 434, time: 358.975, speed: 1.209 steps/s
	step: 435, time: 359.629, speed: 1.210 steps/s
	step: 436, time: 360.462, speed: 1.210 steps/s
	step: 437, time: 361.362, speed: 1.209 steps/s
	step: 438, time: 362.254, speed: 1.209 steps/s
	step: 439, time: 363.135, speed: 1.209 steps/s
	step: 440, time: 364.017, speed: 1.209 steps/s
	step: 441, time: 364.779, speed: 1.209 steps/s
	step: 442, time: 365.331, speed: 1.210 steps/s
	step: 443, time: 365.919, speed: 1.211 steps/s
	step: 444, time: 366.597, speed: 1.211 steps/s
	step: 445, time: 367.468, speed: 1.211 steps/s
	step: 446, time: 368.361, speed: 1.211 steps/s
	step: 447, time: 369.268, speed: 1.211 steps/s
	step: 448, time: 370.179, speed: 1.210 steps/s
	step: 449, time: 370.973, speed: 1.210 steps/s
	step: 450, time: 371.884, speed: 1.210 steps/s
	step: 451, time: 372.714, speed: 1.210 steps/s
	step: 452, time: 373.258, speed: 1.211 steps/s
	step: 453, time: 374.156, speed: 1.211 steps/s
	step: 454, time: 375.045, speed: 1.211 steps/s
	step: 455, time: 375.924, speed: 1.210 steps/s
	step: 456, time: 376.860, speed: 1.210 steps/s
	step: 457, time: 377.763, speed: 1.210 steps/s
	step: 458, time: 378.662, speed: 1.210 steps/s
	step: 459, time: 379.573, speed: 1.209 steps/s
	step: 460, time: 380.369, speed: 1.209 steps/s
	step: 461, time: 381.214, speed: 1.209 steps/s
	step: 462, time: 382.074, speed: 1.209 steps/s
	step: 463, time: 382.914, speed: 1.209 steps/s
	step: 464, time: 383.773, speed: 1.209 steps/s
	step: 465, time: 384.396, speed: 1.210 steps/s
	step: 466, time: 385.295, speed: 1.209 steps/s
	step: 467, time: 386.184, speed: 1.209 steps/s
	step: 468, time: 387.075, speed: 1.209 steps/s
	step: 469, time: 387.714, speed: 1.210 steps/s
	step: 470, time: 388.769, speed: 1.209 steps/s
	step: 471, time: 389.698, speed: 1.209 steps/s
	step: 472, time: 390.558, speed: 1.209 steps/s
	step: 473, time: 391.436, speed: 1.208 steps/s
	step: 474, time: 392.219, speed: 1.209 steps/s
	step: 475, time: 393.101, speed: 1.208 steps/s
	step: 476, time: 393.992, speed: 1.208 steps/s
	step: 477, time: 394.745, speed: 1.208 steps/s
	step: 478, time: 395.643, speed: 1.208 steps/s
	step: 479, time: 396.481, speed: 1.208 steps/s
	step: 480, time: 397.350, speed: 1.208 steps/s
	step: 481, time: 398.196, speed: 1.208 steps/s
	step: 482, time: 399.095, speed: 1.208 steps/s
	step: 483, time: 399.962, speed: 1.208 steps/s
	step: 484, time: 400.664, speed: 1.208 steps/s
	step: 485, time: 401.556, speed: 1.208 steps/s
	step: 486, time: 402.311, speed: 1.208 steps/s
	step: 487, time: 403.259, speed: 1.208 steps/s
	step: 488, time: 404.166, speed: 1.207 steps/s
	step: 489, time: 405.044, speed: 1.207 steps/s
	step: 490, time: 405.935, speed: 1.207 steps/s
	step: 491, time: 406.811, speed: 1.207 steps/s
	step: 492, time: 407.705, speed: 1.207 steps/s
	step: 493, time: 408.581, speed: 1.207 steps/s
	step: 494, time: 409.490, speed: 1.206 steps/s
	step: 495, time: 410.214, speed: 1.207 steps/s
	step: 496, time: 411.110, speed: 1.206 steps/s
	step: 497, time: 411.929, speed: 1.207 steps/s
	step: 498, time: 412.458, speed: 1.207 steps/s
	step: 499, time: 413.006, speed: 1.208 steps/s
	step: 500, time: 413.928, speed: 1.208 steps/s
	step: 501, time: 414.818, speed: 1.208 steps/s
	step: 502, time: 415.676, speed: 1.208 steps/s
	step: 503, time: 416.566, speed: 1.207 steps/s
	step: 504, time: 417.611, speed: 1.207 steps/s
	step: 505, time: 418.234, speed: 1.207 steps/s
	step: 506, time: 419.002, speed: 1.208 steps/s
	step: 507, time: 419.875, speed: 1.208 steps/s
	step: 508, time: 420.756, speed: 1.207 steps/s
	step: 509, time: 421.638, speed: 1.207 steps/s
	step: 510, time: 422.218, speed: 1.208 steps/s
	step: 511, time: 422.876, speed: 1.208 steps/s
	step: 512, time: 423.729, speed: 1.208 steps/s
	step: 513, time: 424.420, speed: 1.209 steps/s
	step: 514, time: 425.299, speed: 1.209 steps/s
	step: 515, time: 426.128, speed: 1.209 steps/s
	step: 516, time: 426.992, speed: 1.208 steps/s
	step: 517, time: 427.858, speed: 1.208 steps/s
	step: 518, time: 428.752, speed: 1.208 steps/s
	step: 519, time: 429.640, speed: 1.208 steps/s
	step: 520, time: 430.390, speed: 1.208 steps/s
	step: 521, time: 431.153, speed: 1.208 steps/s
	step: 522, time: 432.070, speed: 1.208 steps/s
	step: 523, time: 432.695, speed: 1.209 steps/s
	step: 524, time: 433.585, speed: 1.209 steps/s
	step: 525, time: 434.482, speed: 1.208 steps/s
	step: 526, time: 435.386, speed: 1.208 steps/s
	step: 527, time: 436.069, speed: 1.209 steps/s
	step: 528, time: 436.809, speed: 1.209 steps/s
	step: 529, time: 437.724, speed: 1.209 steps/s
	step: 530, time: 438.629, speed: 1.208 steps/s
	step: 531, time: 439.355, speed: 1.209 steps/s
	step: 532, time: 440.244, speed: 1.208 steps/s
	step: 533, time: 441.142, speed: 1.208 steps/s
	step: 534, time: 442.054, speed: 1.208 steps/s
	step: 535, time: 442.738, speed: 1.208 steps/s
	step: 536, time: 443.588, speed: 1.208 steps/s
	step: 537, time: 444.422, speed: 1.208 steps/s
	step: 538, time: 445.380, speed: 1.208 steps/s
	step: 539, time: 446.160, speed: 1.208 steps/s
	step: 540, time: 447.098, speed: 1.208 steps/s
	step: 541, time: 448.010, speed: 1.208 steps/s
	step: 542, time: 448.901, speed: 1.207 steps/s
	step: 543, time: 449.808, speed: 1.207 steps/s
	step: 544, time: 450.737, speed: 1.207 steps/s
	step: 545, time: 451.377, speed: 1.207 steps/s
	step: 546, time: 452.271, speed: 1.207 steps/s
	step: 547, time: 452.982, speed: 1.208 steps/s
	step: 548, time: 453.753, speed: 1.208 steps/s
	step: 549, time: 454.290, speed: 1.208 steps/s
	step: 550, time: 455.197, speed: 1.208 steps/s
	step: 551, time: 455.995, speed: 1.208 steps/s
	step: 552, time: 456.925, speed: 1.208 steps/s
	step: 553, time: 457.818, speed: 1.208 steps/s
	step: 554, time: 458.731, speed: 1.208 steps/s
	step: 555, time: 459.686, speed: 1.207 steps/s
	step: 556, time: 460.519, speed: 1.207 steps/s
	step: 557, time: 461.347, speed: 1.207 steps/s
	step: 558, time: 462.255, speed: 1.207 steps/s
	step: 559, time: 462.943, speed: 1.207 steps/s
	step: 560, time: 463.874, speed: 1.207 steps/s
	step: 561, time: 464.528, speed: 1.208 steps/s
	step: 562, time: 465.425, speed: 1.207 steps/s
	step: 563, time: 466.326, speed: 1.207 steps/s
	step: 564, time: 467.027, speed: 1.208 steps/s
	step: 565, time: 467.935, speed: 1.207 steps/s
	step: 566, time: 468.852, speed: 1.207 steps/s
	step: 567, time: 469.456, speed: 1.208 steps/s
	step: 568, time: 470.331, speed: 1.208 steps/s
	step: 569, time: 471.226, speed: 1.207 steps/s
	step: 570, time: 472.122, speed: 1.207 steps/s
	step: 571, time: 472.771, speed: 1.208 steps/s
	step: 572, time: 473.654, speed: 1.208 steps/s
	step: 573, time: 474.530, speed: 1.208 steps/s
	step: 574, time: 475.205, speed: 1.208 steps/s
	step: 575, time: 475.716, speed: 1.209 steps/s
	step: 576, time: 476.634, speed: 1.208 steps/s
	step: 577, time: 477.220, speed: 1.209 steps/s
	step: 578, time: 478.107, speed: 1.209 steps/s
	step: 579, time: 479.010, speed: 1.209 steps/s
	step: 580, time: 479.881, speed: 1.209 steps/s
	step: 581, time: 480.692, speed: 1.209 steps/s
	step: 582, time: 481.591, speed: 1.208 steps/s
	step: 583, time: 482.495, speed: 1.208 steps/s
	step: 584, time: 483.405, speed: 1.208 steps/s
	step: 585, time: 484.319, speed: 1.208 steps/s
	step: 586, time: 485.244, speed: 1.208 steps/s
	step: 587, time: 485.954, speed: 1.208 steps/s
	step: 588, time: 486.853, speed: 1.208 steps/s
	step: 589, time: 487.920, speed: 1.207 steps/s
	step: 590, time: 488.699, speed: 1.207 steps/s
	step: 591, time: 489.589, speed: 1.207 steps/s
	step: 592, time: 490.464, speed: 1.207 steps/s
	step: 593, time: 491.093, speed: 1.208 steps/s
	step: 594, time: 492.008, speed: 1.207 steps/s
	step: 595, time: 492.908, speed: 1.207 steps/s
	step: 596, time: 493.729, speed: 1.207 steps/s
	step: 597, time: 494.610, speed: 1.207 steps/s
	step: 598, time: 495.498, speed: 1.207 steps/s
	step: 599, time: 496.386, speed: 1.207 steps/s
	step: 600, time: 497.282, speed: 1.207 steps/s
	step: 601, time: 498.145, speed: 1.206 steps/s
	step: 602, time: 498.779, speed: 1.207 steps/s
	step: 603, time: 499.634, speed: 1.207 steps/s
	step: 604, time: 500.529, speed: 1.207 steps/s
	step: 605, time: 501.419, speed: 1.207 steps/s
	step: 606, time: 502.342, speed: 1.206 steps/s
	step: 607, time: 503.399, speed: 1.206 steps/s
	step: 608, time: 504.263, speed: 1.206 steps/s
	step: 609, time: 505.116, speed: 1.206 steps/s
	step: 610, time: 505.816, speed: 1.206 steps/s
	step: 611, time: 506.699, speed: 1.206 steps/s
	step: 612, time: 507.578, speed: 1.206 steps/s
	step: 613, time: 508.455, speed: 1.206 steps/s
	step: 614, time: 509.079, speed: 1.206 steps/s
	step: 615, time: 509.997, speed: 1.206 steps/s
	step: 616, time: 510.503, speed: 1.207 steps/s
	step: 617, time: 511.400, speed: 1.206 steps/s
	step: 618, time: 512.313, speed: 1.206 steps/s
	step: 619, time: 513.065, speed: 1.206 steps/s
	step: 620, time: 513.960, speed: 1.206 steps/s
	step: 621, time: 514.867, speed: 1.206 steps/s
	step: 622, time: 515.489, speed: 1.207 steps/s
	step: 623, time: 516.341, speed: 1.207 steps/s
	step: 624, time: 517.399, speed: 1.206 steps/s
	step: 625, time: 518.061, speed: 1.206 steps/s
	step: 626, time: 518.938, speed: 1.206 steps/s
	step: 627, time: 519.818, speed: 1.206 steps/s
	step: 628, time: 520.706, speed: 1.206 steps/s
	step: 629, time: 521.583, speed: 1.206 steps/s
	step: 630, time: 522.486, speed: 1.206 steps/s
	step: 631, time: 523.116, speed: 1.206 steps/s
	step: 632, time: 523.997, speed: 1.206 steps/s
	step: 633, time: 524.921, speed: 1.206 steps/s
	step: 634, time: 525.833, speed: 1.206 steps/s
	step: 635, time: 526.503, speed: 1.206 steps/s
	step: 636, time: 527.417, speed: 1.206 steps/s
	step: 637, time: 528.084, speed: 1.206 steps/s
	step: 638, time: 528.933, speed: 1.206 steps/s
	step: 639, time: 529.846, speed: 1.206 steps/s
	step: 640, time: 530.720, speed: 1.206 steps/s
	step: 641, time: 531.461, speed: 1.206 steps/s
	step: 642, time: 532.366, speed: 1.206 steps/s
	step: 643, time: 533.179, speed: 1.206 steps/s
	step: 644, time: 534.096, speed: 1.206 steps/s
	step: 645, time: 534.978, speed: 1.206 steps/s
	step: 646, time: 535.670, speed: 1.206 steps/s
	step: 647, time: 536.550, speed: 1.206 steps/s
	step: 648, time: 537.445, speed: 1.206 steps/s
	step: 649, time: 538.340, speed: 1.206 steps/s
	step: 650, time: 539.271, speed: 1.205 steps/s
	step: 651, time: 540.022, speed: 1.206 steps/s
	step: 652, time: 540.919, speed: 1.205 steps/s
	step: 653, time: 541.687, speed: 1.205 steps/s
	step: 654, time: 542.360, speed: 1.206 steps/s
	step: 655, time: 543.280, speed: 1.206 steps/s
	step: 656, time: 544.179, speed: 1.205 steps/s
	step: 657, time: 544.879, speed: 1.206 steps/s
	step: 658, time: 545.979, speed: 1.205 steps/s
	step: 659, time: 546.889, speed: 1.205 steps/s
	step: 660, time: 547.741, speed: 1.205 steps/s
	step: 661, time: 548.624, speed: 1.205 steps/s
	step: 662, time: 549.529, speed: 1.205 steps/s
	step: 663, time: 550.418, speed: 1.205 steps/s
	step: 664, time: 551.311, speed: 1.204 steps/s
	step: 665, time: 552.191, speed: 1.204 steps/s
	step: 666, time: 553.085, speed: 1.204 steps/s
	step: 667, time: 553.997, speed: 1.204 steps/s
	step: 668, time: 554.840, speed: 1.204 steps/s
	step: 669, time: 555.644, speed: 1.204 steps/s
	step: 670, time: 556.403, speed: 1.204 steps/s
	step: 671, time: 557.253, speed: 1.204 steps/s
	step: 672, time: 558.126, speed: 1.204 steps/s
	step: 673, time: 558.998, speed: 1.204 steps/s
	step: 674, time: 559.888, speed: 1.204 steps/s
	step: 675, time: 560.911, speed: 1.203 steps/s
	step: 676, time: 561.802, speed: 1.203 steps/s
	step: 677, time: 562.647, speed: 1.203 steps/s
	step: 678, time: 563.490, speed: 1.203 steps/s
	step: 679, time: 564.419, speed: 1.203 steps/s
	step: 680, time: 565.310, speed: 1.203 steps/s
	step: 681, time: 566.089, speed: 1.203 steps/s
	step: 682, time: 567.000, speed: 1.203 steps/s
	step: 683, time: 567.786, speed: 1.203 steps/s
	step: 684, time: 568.673, speed: 1.203 steps/s
	step: 685, time: 569.521, speed: 1.203 steps/s
	step: 686, time: 570.411, speed: 1.203 steps/s
	step: 687, time: 571.288, speed: 1.203 steps/s
	step: 688, time: 572.179, speed: 1.202 steps/s
	step: 689, time: 573.074, speed: 1.202 steps/s
	step: 690, time: 573.944, speed: 1.202 steps/s
	step: 691, time: 574.839, speed: 1.202 steps/s
	step: 692, time: 575.894, speed: 1.202 steps/s
	step: 693, time: 576.772, speed: 1.202 steps/s
	step: 694, time: 577.470, speed: 1.202 steps/s
	step: 695, time: 578.369, speed: 1.202 steps/s
	step: 696, time: 579.216, speed: 1.202 steps/s
	step: 697, time: 580.125, speed: 1.201 steps/s
	step: 698, time: 581.048, speed: 1.201 steps/s
	step: 699, time: 581.950, speed: 1.201 steps/s
	step: 700, time: 582.825, speed: 1.201 steps/s
	step: 701, time: 583.698, speed: 1.201 steps/s
	step: 702, time: 584.594, speed: 1.201 steps/s
	step: 703, time: 585.368, speed: 1.201 steps/s
	step: 704, time: 586.116, speed: 1.201 steps/s
	step: 705, time: 587.034, speed: 1.201 steps/s
	step: 706, time: 587.912, speed: 1.201 steps/s
	step: 707, time: 588.767, speed: 1.201 steps/s
	step: 708, time: 589.658, speed: 1.201 steps/s
	step: 709, time: 590.689, speed: 1.200 steps/s
	step: 710, time: 591.476, speed: 1.200 steps/s
	step: 711, time: 592.383, speed: 1.200 steps/s
	step: 712, time: 593.293, speed: 1.200 steps/s
	step: 713, time: 594.102, speed: 1.200 steps/s
	step: 714, time: 594.991, speed: 1.200 steps/s
	step: 715, time: 595.750, speed: 1.200 steps/s
	step: 716, time: 596.420, speed: 1.200 steps/s
	step: 717, time: 597.310, speed: 1.200 steps/s
	step: 718, time: 598.209, speed: 1.200 steps/s
	step: 719, time: 599.092, speed: 1.200 steps/s
	step: 720, time: 599.963, speed: 1.200 steps/s
	step: 721, time: 600.841, speed: 1.200 steps/s
	step: 722, time: 601.579, speed: 1.200 steps/s
	step: 723, time: 602.180, speed: 1.201 steps/s
	step: 724, time: 603.086, speed: 1.200 steps/s
	step: 725, time: 603.983, speed: 1.200 steps/s
	step: 726, time: 605.072, speed: 1.200 steps/s
	step: 727, time: 605.711, speed: 1.200 steps/s
	step: 728, time: 606.341, speed: 1.201 steps/s
	step: 729, time: 607.225, speed: 1.201 steps/s
	step: 730, time: 607.969, speed: 1.201 steps/s
	step: 731, time: 608.833, speed: 1.201 steps/s
	step: 732, time: 609.722, speed: 1.201 steps/s
	step: 733, time: 610.618, speed: 1.200 steps/s
	step: 734, time: 611.530, speed: 1.200 steps/s
	step: 735, time: 612.420, speed: 1.200 steps/s
	step: 736, time: 613.039, speed: 1.201 steps/s
	step: 737, time: 613.815, speed: 1.201 steps/s
	step: 738, time: 614.716, speed: 1.201 steps/s
	step: 739, time: 615.611, speed: 1.200 steps/s
	step: 740, time: 616.494, speed: 1.200 steps/s
	step: 741, time: 617.344, speed: 1.200 steps/s
	step: 742, time: 618.114, speed: 1.200 steps/s
	step: 743, time: 619.150, speed: 1.200 steps/s
	step: 744, time: 620.069, speed: 1.200 steps/s
	step: 745, time: 620.974, speed: 1.200 steps/s
	step: 746, time: 621.854, speed: 1.200 steps/s
	step: 747, time: 622.707, speed: 1.200 steps/s
	step: 748, time: 623.500, speed: 1.200 steps/s
	step: 749, time: 624.300, speed: 1.200 steps/s
	step: 750, time: 625.211, speed: 1.200 steps/s
	step: 751, time: 625.940, speed: 1.200 steps/s
	step: 752, time: 626.863, speed: 1.200 steps/s
	step: 753, time: 627.781, speed: 1.199 steps/s
	step: 754, time: 628.657, speed: 1.199 steps/s
	step: 755, time: 629.527, speed: 1.199 steps/s
	step: 756, time: 630.483, speed: 1.199 steps/s
	step: 757, time: 631.317, speed: 1.199 steps/s
	step: 758, time: 632.121, speed: 1.199 steps/s
	step: 759, time: 633.072, speed: 1.199 steps/s
	step: 760, time: 633.914, speed: 1.199 steps/s
	step: 761, time: 634.830, speed: 1.199 steps/s
	step: 762, time: 635.578, speed: 1.199 steps/s
	step: 763, time: 636.088, speed: 1.200 steps/s
	step: 764, time: 636.829, speed: 1.200 steps/s
	step: 765, time: 637.606, speed: 1.200 steps/s
	step: 766, time: 638.531, speed: 1.200 steps/s
	step: 767, time: 639.474, speed: 1.199 steps/s
	step: 768, time: 640.341, speed: 1.199 steps/s
	step: 769, time: 641.229, speed: 1.199 steps/s
	step: 770, time: 642.137, speed: 1.199 steps/s
	step: 771, time: 643.028, speed: 1.199 steps/s
	step: 772, time: 644.011, speed: 1.199 steps/s
	step: 773, time: 644.976, speed: 1.198 steps/s
	step: 774, time: 645.605, speed: 1.199 steps/s
	step: 775, time: 646.381, speed: 1.199 steps/s
	step: 776, time: 647.076, speed: 1.199 steps/s
	step: 777, time: 647.863, speed: 1.199 steps/s
	step: 778, time: 648.725, speed: 1.199 steps/s
	step: 779, time: 649.649, speed: 1.199 steps/s
	step: 780, time: 650.417, speed: 1.199 steps/s
	step: 781, time: 651.357, speed: 1.199 steps/s
	step: 782, time: 652.270, speed: 1.199 steps/s
	step: 783, time: 653.072, speed: 1.199 steps/s
	step: 784, time: 653.828, speed: 1.199 steps/s
	step: 785, time: 654.744, speed: 1.199 steps/s
	step: 786, time: 655.569, speed: 1.199 steps/s
	step: 787, time: 656.448, speed: 1.199 steps/s
	step: 788, time: 657.318, speed: 1.199 steps/s
	step: 789, time: 658.191, speed: 1.199 steps/s
	step: 790, time: 659.085, speed: 1.199 steps/s
	step: 791, time: 659.990, speed: 1.199 steps/s
	step: 792, time: 660.896, speed: 1.198 steps/s
	step: 793, time: 661.800, speed: 1.198 steps/s
	step: 794, time: 662.711, speed: 1.198 steps/s
	step: 795, time: 663.749, speed: 1.198 steps/s
	step: 796, time: 664.218, speed: 1.198 steps/s
	step: 797, time: 665.113, speed: 1.198 steps/s
	step: 798, time: 665.903, speed: 1.198 steps/s
	step: 799, time: 666.789, speed: 1.198 steps/s
	step: 800, time: 667.362, speed: 1.199 steps/s
	step: 801, time: 667.976, speed: 1.199 steps/s
	step: 802, time: 668.546, speed: 1.200 steps/s
	step: 803, time: 669.209, speed: 1.200 steps/s
	step: 804, time: 670.106, speed: 1.200 steps/s
	step: 805, time: 671.015, speed: 1.200 steps/s
	step: 806, time: 671.907, speed: 1.200 steps/s
	step: 807, time: 672.798, speed: 1.199 steps/s
	step: 808, time: 673.679, speed: 1.199 steps/s
	step: 809, time: 674.491, speed: 1.199 steps/s
	step: 810, time: 675.393, speed: 1.199 steps/s
	step: 811, time: 675.981, speed: 1.200 steps/s
	step: 812, time: 677.003, speed: 1.199 steps/s
	step: 813, time: 677.737, speed: 1.200 steps/s
	step: 814, time: 678.411, speed: 1.200 steps/s
	step: 815, time: 679.157, speed: 1.200 steps/s
	step: 816, time: 680.035, speed: 1.200 steps/s
	step: 817, time: 680.859, speed: 1.200 steps/s
	step: 818, time: 681.601, speed: 1.200 steps/s
	step: 819, time: 682.481, speed: 1.200 steps/s
	step: 820, time: 683.373, speed: 1.200 steps/s
	step: 821, time: 684.237, speed: 1.200 steps/s
	step: 822, time: 684.845, speed: 1.200 steps/s
	step: 823, time: 685.462, speed: 1.201 steps/s
	step: 824, time: 686.210, speed: 1.201 steps/s
	step: 825, time: 687.083, speed: 1.201 steps/s
	step: 826, time: 687.785, speed: 1.201 steps/s
	step: 827, time: 688.655, speed: 1.201 steps/s
	step: 828, time: 689.522, speed: 1.201 steps/s
	step: 829, time: 690.542, speed: 1.201 steps/s
	step: 830, time: 691.435, speed: 1.200 steps/s
	step: 831, time: 692.112, speed: 1.201 steps/s
	step: 832, time: 692.961, speed: 1.201 steps/s
	step: 833, time: 693.804, speed: 1.201 steps/s
	step: 834, time: 694.672, speed: 1.201 steps/s
	step: 835, time: 695.570, speed: 1.200 steps/s
	step: 836, time: 696.346, speed: 1.201 steps/s
	step: 837, time: 697.244, speed: 1.200 steps/s
	step: 838, time: 698.124, speed: 1.200 steps/s
	step: 839, time: 698.984, speed: 1.200 steps/s
	step: 840, time: 699.884, speed: 1.200 steps/s
	step: 841, time: 700.769, speed: 1.200 steps/s
	step: 842, time: 701.644, speed: 1.200 steps/s
	step: 843, time: 702.537, speed: 1.200 steps/s
	step: 844, time: 703.410, speed: 1.200 steps/s
	step: 845, time: 704.291, speed: 1.200 steps/s
	step: 846, time: 705.154, speed: 1.200 steps/s
	step: 847, time: 705.892, speed: 1.200 steps/s
	step: 848, time: 706.742, speed: 1.200 steps/s
	step: 849, time: 707.565, speed: 1.200 steps/s
	step: 850, time: 708.450, speed: 1.200 steps/s
	step: 851, time: 709.113, speed: 1.200 steps/s
	step: 852, time: 709.892, speed: 1.200 steps/s
	step: 853, time: 710.766, speed: 1.200 steps/s
	step: 854, time: 711.567, speed: 1.200 steps/s
	step: 855, time: 712.451, speed: 1.200 steps/s
	step: 856, time: 713.354, speed: 1.200 steps/s
	step: 857, time: 713.995, speed: 1.200 steps/s
	step: 858, time: 714.879, speed: 1.200 steps/s
	step: 859, time: 715.731, speed: 1.200 steps/s
	step: 860, time: 716.394, speed: 1.200 steps/s
	step: 861, time: 717.274, speed: 1.200 steps/s
	step: 862, time: 717.939, speed: 1.201 steps/s
	step: 863, time: 718.747, speed: 1.201 steps/s
	step: 864, time: 719.443, speed: 1.201 steps/s
	step: 865, time: 720.222, speed: 1.201 steps/s
	step: 866, time: 721.103, speed: 1.201 steps/s
	step: 867, time: 721.964, speed: 1.201 steps/s
	step: 868, time: 722.842, speed: 1.201 steps/s
	step: 869, time: 723.713, speed: 1.201 steps/s
	step: 870, time: 724.564, speed: 1.201 steps/s
	step: 871, time: 725.470, speed: 1.201 steps/s
	step: 872, time: 726.333, speed: 1.201 steps/s
	step: 873, time: 727.019, speed: 1.201 steps/s
	step: 874, time: 727.589, speed: 1.201 steps/s
	step: 875, time: 728.486, speed: 1.201 steps/s
	step: 876, time: 729.345, speed: 1.201 steps/s
	step: 877, time: 730.217, speed: 1.201 steps/s
	step: 878, time: 730.819, speed: 1.201 steps/s
	step: 879, time: 731.720, speed: 1.201 steps/s
	step: 880, time: 732.782, speed: 1.201 steps/s
	step: 881, time: 733.470, speed: 1.201 steps/s
	step: 882, time: 734.372, speed: 1.201 steps/s
	step: 883, time: 735.280, speed: 1.201 steps/s
	step: 884, time: 735.944, speed: 1.201 steps/s
	step: 885, time: 736.806, speed: 1.201 steps/s
	step: 886, time: 737.674, speed: 1.201 steps/s
	step: 887, time: 738.325, speed: 1.201 steps/s
	step: 888, time: 739.210, speed: 1.201 steps/s
	step: 889, time: 739.994, speed: 1.201 steps/s
	step: 890, time: 740.863, speed: 1.201 steps/s
	step: 891, time: 741.446, speed: 1.202 steps/s
	step: 892, time: 741.988, speed: 1.202 steps/s
	step: 893, time: 742.613, speed: 1.203 steps/s
	step: 894, time: 743.337, speed: 1.203 steps/s
	step: 895, time: 744.098, speed: 1.203 steps/s
	step: 896, time: 744.863, speed: 1.203 steps/s
	step: 897, time: 745.877, speed: 1.203 steps/s
	step: 898, time: 746.755, speed: 1.203 steps/s
	step: 899, time: 747.696, speed: 1.202 steps/s
	step: 900, time: 748.544, speed: 1.202 steps/s
	step: 901, time: 749.233, speed: 1.203 steps/s
	step: 902, time: 750.052, speed: 1.203 steps/s
	step: 903, time: 750.709, speed: 1.203 steps/s
	step: 904, time: 751.596, speed: 1.203 steps/s
	step: 905, time: 752.218, speed: 1.203 steps/s
	step: 906, time: 752.836, speed: 1.203 steps/s
	step: 907, time: 753.684, speed: 1.203 steps/s
	step: 908, time: 754.534, speed: 1.203 steps/s
	step: 909, time: 755.155, speed: 1.204 steps/s
	step: 910, time: 756.042, speed: 1.204 steps/s
	step: 911, time: 756.908, speed: 1.204 steps/s
	step: 912, time: 757.605, speed: 1.204 steps/s
	step: 913, time: 758.353, speed: 1.204 steps/s
	step: 914, time: 759.259, speed: 1.204 steps/s
	step: 915, time: 760.122, speed: 1.204 steps/s
	step: 916, time: 760.996, speed: 1.204 steps/s
	step: 917, time: 761.864, speed: 1.204 steps/s
	step: 918, time: 762.727, speed: 1.204 steps/s
	step: 919, time: 763.591, speed: 1.204 steps/s
	step: 920, time: 764.403, speed: 1.204 steps/s
	step: 921, time: 765.071, speed: 1.204 steps/s
	step: 922, time: 765.727, speed: 1.204 steps/s
	step: 923, time: 766.543, speed: 1.204 steps/s
	step: 924, time: 767.302, speed: 1.204 steps/s
	step: 925, time: 768.201, speed: 1.204 steps/s
	step: 926, time: 769.108, speed: 1.204 steps/s
	step: 927, time: 769.867, speed: 1.204 steps/s
	step: 928, time: 770.757, speed: 1.204 steps/s
	step: 929, time: 771.666, speed: 1.204 steps/s
	step: 930, time: 772.449, speed: 1.204 steps/s
	step: 931, time: 773.338, speed: 1.204 steps/s
	step: 932, time: 773.892, speed: 1.204 steps/s
	step: 933, time: 774.490, speed: 1.205 steps/s
	step: 934, time: 775.351, speed: 1.205 steps/s
	step: 935, time: 776.038, speed: 1.205 steps/s
	step: 936, time: 776.783, speed: 1.205 steps/s
	step: 937, time: 777.656, speed: 1.205 steps/s
	step: 938, time: 778.521, speed: 1.205 steps/s
	step: 939, time: 779.403, speed: 1.205 steps/s
	step: 940, time: 779.988, speed: 1.205 steps/s
	step: 941, time: 780.868, speed: 1.205 steps/s
	step: 942, time: 781.741, speed: 1.205 steps/s
	step: 943, time: 782.577, speed: 1.205 steps/s
	step: 944, time: 783.213, speed: 1.205 steps/s
	step: 945, time: 784.099, speed: 1.205 steps/s
	step: 946, time: 784.845, speed: 1.205 steps/s
	step: 947, time: 785.441, speed: 1.206 steps/s
	step: 948, time: 786.494, speed: 1.205 steps/s
	step: 949, time: 787.391, speed: 1.205 steps/s
	step: 950, time: 788.012, speed: 1.206 steps/s
	step: 951, time: 788.865, speed: 1.206 steps/s
	step: 952, time: 789.758, speed: 1.205 steps/s
	step: 953, time: 790.610, speed: 1.205 steps/s
	step: 954, time: 791.137, speed: 1.206 steps/s
	step: 955, time: 792.033, speed: 1.206 steps/s
	step: 956, time: 792.913, speed: 1.206 steps/s
	step: 957, time: 793.797, speed: 1.206 steps/s
	step: 958, time: 794.679, speed: 1.206 steps/s
	step: 959, time: 795.387, speed: 1.206 steps/s
	step: 960, time: 796.243, speed: 1.206 steps/s
	step: 961, time: 796.931, speed: 1.206 steps/s
	step: 962, time: 797.816, speed: 1.206 steps/s
	step: 963, time: 798.429, speed: 1.206 steps/s
	step: 964, time: 799.331, speed: 1.206 steps/s
	step: 965, time: 800.333, speed: 1.206 steps/s
	step: 966, time: 800.963, speed: 1.206 steps/s
	step: 967, time: 801.590, speed: 1.206 steps/s
	step: 968, time: 802.358, speed: 1.206 steps/s
	step: 969, time: 803.019, speed: 1.207 steps/s
	step: 970, time: 803.923, speed: 1.207 steps/s
	step: 971, time: 804.796, speed: 1.207 steps/s
	step: 972, time: 805.487, speed: 1.207 steps/s
	step: 973, time: 806.180, speed: 1.207 steps/s
	step: 974, time: 807.072, speed: 1.207 steps/s
	step: 975, time: 807.803, speed: 1.207 steps/s
	step: 976, time: 808.380, speed: 1.207 steps/s
	step: 977, time: 809.250, speed: 1.207 steps/s
	step: 978, time: 810.121, speed: 1.207 steps/s
	step: 979, time: 810.782, speed: 1.207 steps/s
	step: 980, time: 811.656, speed: 1.207 steps/s
	step: 981, time: 812.559, speed: 1.207 steps/s
	step: 982, time: 813.420, speed: 1.207 steps/s
	step: 983, time: 814.449, speed: 1.207 steps/s
	step: 984, time: 815.345, speed: 1.207 steps/s
	step: 985, time: 816.095, speed: 1.207 steps/s
	step: 986, time: 816.864, speed: 1.207 steps/s
	step: 987, time: 817.723, speed: 1.207 steps/s
	step: 988, time: 818.458, speed: 1.207 steps/s
	step: 989, time: 819.191, speed: 1.207 steps/s
	step: 990, time: 819.946, speed: 1.207 steps/s
	step: 991, time: 820.703, speed: 1.208 steps/s
	step: 992, time: 821.485, speed: 1.208 steps/s
	step: 993, time: 822.310, speed: 1.208 steps/s
	step: 994, time: 822.888, speed: 1.208 steps/s
	step: 995, time: 823.761, speed: 1.208 steps/s
	step: 996, time: 824.599, speed: 1.208 steps/s
	step: 997, time: 825.471, speed: 1.208 steps/s
	step: 998, time: 826.235, speed: 1.208 steps/s
	step: 999, time: 826.940, speed: 1.208 steps/s
	step: 1000, time: 827.743, speed: 1.208 steps/s
	step: 1001, time: 828.469, speed: 1.208 steps/s
	step: 1002, time: 829.342, speed: 1.208 steps/s
	step: 1003, time: 829.881, speed: 1.209 steps/s
	step: 1004, time: 830.665, speed: 1.209 steps/s
	step: 1005, time: 831.152, speed: 1.209 steps/s
	step: 1006, time: 831.823, speed: 1.209 steps/s
	step: 1007, time: 832.718, speed: 1.209 steps/s
	step: 1008, time: 833.460, speed: 1.209 steps/s
	step: 1009, time: 834.083, speed: 1.210 steps/s
	step: 1010, time: 834.719, speed: 1.210 steps/s
	step: 1011, time: 835.517, speed: 1.210 steps/s
	step: 1012, time: 836.391, speed: 1.210 steps/s
	step: 1013, time: 837.285, speed: 1.210 steps/s
	step: 1014, time: 837.962, speed: 1.210 steps/s
	step: 1015, time: 838.841, speed: 1.210 steps/s
	step: 1016, time: 839.467, speed: 1.210 steps/s
	step: 1017, time: 840.529, speed: 1.210 steps/s
	step: 1018, time: 841.420, speed: 1.210 steps/s
	step: 1019, time: 842.312, speed: 1.210 steps/s
	step: 1020, time: 843.014, speed: 1.210 steps/s
	step: 1021, time: 843.872, speed: 1.210 steps/s
	step: 1022, time: 844.714, speed: 1.210 steps/s
	step: 1023, time: 845.509, speed: 1.210 steps/s
	step: 1024, time: 846.180, speed: 1.210 steps/s
	step: 1025, time: 847.051, speed: 1.210 steps/s
	step: 1026, time: 847.961, speed: 1.210 steps/s
	step: 1027, time: 848.430, speed: 1.210 steps/s
	step: 1028, time: 849.244, speed: 1.210 steps/s
	step: 1029, time: 850.122, speed: 1.210 steps/s
	step: 1030, time: 851.032, speed: 1.210 steps/s
	step: 1031, time: 851.708, speed: 1.211 steps/s
	step: 1032, time: 852.602, speed: 1.210 steps/s
	step: 1033, time: 853.403, speed: 1.210 steps/s
	step: 1034, time: 854.443, speed: 1.210 steps/s
	step: 1035, time: 855.317, speed: 1.210 steps/s
	step: 1036, time: 856.149, speed: 1.210 steps/s
	step: 1037, time: 856.882, speed: 1.210 steps/s
	step: 1038, time: 857.747, speed: 1.210 steps/s
	step: 1039, time: 858.406, speed: 1.210 steps/s
	step: 1040, time: 859.296, speed: 1.210 steps/s
	step: 1041, time: 860.165, speed: 1.210 steps/s
	step: 1042, time: 860.745, speed: 1.211 steps/s
	step: 1043, time: 861.636, speed: 1.210 steps/s
	step: 1044, time: 862.531, speed: 1.210 steps/s
	step: 1045, time: 863.396, speed: 1.210 steps/s
	step: 1046, time: 864.225, speed: 1.210 steps/s
	step: 1047, time: 864.758, speed: 1.211 steps/s
	step: 1048, time: 865.647, speed: 1.211 steps/s
	step: 1049, time: 866.187, speed: 1.211 steps/s
	step: 1050, time: 866.719, speed: 1.211 steps/s
	step: 1051, time: 867.707, speed: 1.211 steps/s
	step: 1052, time: 868.607, speed: 1.211 steps/s
	step: 1053, time: 869.519, speed: 1.211 steps/s
	step: 1054, time: 870.422, speed: 1.211 steps/s
	step: 1055, time: 871.308, speed: 1.211 steps/s
	step: 1056, time: 872.186, speed: 1.211 steps/s
	step: 1057, time: 873.095, speed: 1.211 steps/s
	step: 1058, time: 873.985, speed: 1.211 steps/s
	step: 1059, time: 874.671, speed: 1.211 steps/s
	step: 1060, time: 875.583, speed: 1.211 steps/s
	step: 1061, time: 876.416, speed: 1.211 steps/s
	step: 1062, time: 877.167, speed: 1.211 steps/s
	step: 1063, time: 878.005, speed: 1.211 steps/s
	step: 1064, time: 878.860, speed: 1.211 steps/s
	step: 1065, time: 879.736, speed: 1.211 steps/s
	step: 1066, time: 880.537, speed: 1.211 steps/s
	step: 1067, time: 881.391, speed: 1.211 steps/s
	step: 1068, time: 882.441, speed: 1.210 steps/s
	step: 1069, time: 883.098, speed: 1.211 steps/s
	step: 1070, time: 883.991, speed: 1.210 steps/s
	step: 1071, time: 884.852, speed: 1.210 steps/s
	step: 1072, time: 885.678, speed: 1.210 steps/s
	step: 1073, time: 886.431, speed: 1.210 steps/s
	step: 1074, time: 887.257, speed: 1.210 steps/s
	step: 1075, time: 888.178, speed: 1.210 steps/s
	step: 1076, time: 889.050, speed: 1.210 steps/s
	step: 1077, time: 889.822, speed: 1.210 steps/s
	step: 1078, time: 890.620, speed: 1.210 steps/s
	step: 1079, time: 891.217, speed: 1.211 steps/s
	step: 1080, time: 892.071, speed: 1.211 steps/s
	step: 1081, time: 892.831, speed: 1.211 steps/s
	step: 1082, time: 893.469, speed: 1.211 steps/s
	step: 1083, time: 894.359, speed: 1.211 steps/s
	step: 1084, time: 894.865, speed: 1.211 steps/s
	step: 1085, time: 895.588, speed: 1.211 steps/s
	step: 1086, time: 896.487, speed: 1.211 steps/s
	step: 1087, time: 897.390, speed: 1.211 steps/s
	step: 1088, time: 897.937, speed: 1.212 steps/s
	step: 1089, time: 898.812, speed: 1.212 steps/s
	step: 1090, time: 899.666, speed: 1.212 steps/s
	step: 1091, time: 900.567, speed: 1.211 steps/s
	step: 1092, time: 901.409, speed: 1.211 steps/s
	step: 1093, time: 902.311, speed: 1.211 steps/s
	step: 1094, time: 902.941, speed: 1.212 steps/s
	step: 1095, time: 903.808, speed: 1.212 steps/s
	step: 1096, time: 904.466, speed: 1.212 steps/s
	step: 1097, time: 905.176, speed: 1.212 steps/s
	step: 1098, time: 906.056, speed: 1.212 steps/s
	step: 1099, time: 906.924, speed: 1.212 steps/s
	step: 1100, time: 907.657, speed: 1.212 steps/s
	step: 1101, time: 908.568, speed: 1.212 steps/s
	step: 1102, time: 909.251, speed: 1.212 steps/s
	step: 1103, time: 910.099, speed: 1.212 steps/s
	step: 1104, time: 910.980, speed: 1.212 steps/s
	step: 1105, time: 911.851, speed: 1.212 steps/s
	step: 1106, time: 912.738, speed: 1.212 steps/s
	step: 1107, time: 913.612, speed: 1.212 steps/s
	step: 1108, time: 914.466, speed: 1.212 steps/s
	step: 1109, time: 915.361, speed: 1.212 steps/s
	step: 1110, time: 916.233, speed: 1.211 steps/s
	step: 1111, time: 916.916, speed: 1.212 steps/s
	step: 1112, time: 917.523, speed: 1.212 steps/s
	step: 1113, time: 918.397, speed: 1.212 steps/s
	step: 1114, time: 919.285, speed: 1.212 steps/s
	step: 1115, time: 920.154, speed: 1.212 steps/s
	step: 1116, time: 920.835, speed: 1.212 steps/s
	step: 1117, time: 921.670, speed: 1.212 steps/s
	step: 1118, time: 922.447, speed: 1.212 steps/s
	step: 1119, time: 923.435, speed: 1.212 steps/s
	step: 1120, time: 924.262, speed: 1.212 steps/s
	step: 1121, time: 925.177, speed: 1.212 steps/s
	step: 1122, time: 926.047, speed: 1.212 steps/s
	step: 1123, time: 926.862, speed: 1.212 steps/s
	step: 1124, time: 927.745, speed: 1.212 steps/s
	step: 1125, time: 928.465, speed: 1.212 steps/s
	step: 1126, time: 929.238, speed: 1.212 steps/s
	step: 1127, time: 929.897, speed: 1.212 steps/s
	step: 1128, time: 930.801, speed: 1.212 steps/s
	step: 1129, time: 931.501, speed: 1.212 steps/s
	step: 1130, time: 932.388, speed: 1.212 steps/s
	step: 1131, time: 933.191, speed: 1.212 steps/s
	step: 1132, time: 934.071, speed: 1.212 steps/s
	step: 1133, time: 934.930, speed: 1.212 steps/s
	step: 1134, time: 935.692, speed: 1.212 steps/s
	step: 1135, time: 936.596, speed: 1.212 steps/s
	step: 1136, time: 937.496, speed: 1.212 steps/s
	step: 1137, time: 938.392, speed: 1.212 steps/s
	step: 1138, time: 939.291, speed: 1.212 steps/s
	step: 1139, time: 939.944, speed: 1.212 steps/s
	step: 1140, time: 940.503, speed: 1.212 steps/s
	step: 1141, time: 941.076, speed: 1.212 steps/s
	step: 1142, time: 941.744, speed: 1.213 steps/s
	step: 1143, time: 942.575, speed: 1.213 steps/s
	step: 1144, time: 943.446, speed: 1.213 steps/s
	step: 1145, time: 944.256, speed: 1.213 steps/s
	step: 1146, time: 945.129, speed: 1.213 steps/s
	step: 1147, time: 945.981, speed: 1.212 steps/s
	step: 1148, time: 946.762, speed: 1.213 steps/s
	step: 1149, time: 947.533, speed: 1.213 steps/s
	step: 1150, time: 948.403, speed: 1.213 steps/s
	step: 1151, time: 949.200, speed: 1.213 steps/s
	step: 1152, time: 949.841, speed: 1.213 steps/s
	step: 1153, time: 950.821, speed: 1.213 steps/s
	step: 1154, time: 951.680, speed: 1.213 steps/s
	step: 1155, time: 952.572, speed: 1.213 steps/s
	step: 1156, time: 953.388, speed: 1.213 steps/s
	step: 1157, time: 954.156, speed: 1.213 steps/s
	step: 1158, time: 955.012, speed: 1.213 steps/s
	step: 1159, time: 955.631, speed: 1.213 steps/s
	step: 1160, time: 956.499, speed: 1.213 steps/s
	step: 1161, time: 957.028, speed: 1.213 steps/s
	step: 1162, time: 957.694, speed: 1.213 steps/s
	step: 1163, time: 958.591, speed: 1.213 steps/s
	step: 1164, time: 959.440, speed: 1.213 steps/s
	step: 1165, time: 960.239, speed: 1.213 steps/s
	step: 1166, time: 961.114, speed: 1.213 steps/s
	step: 1167, time: 961.916, speed: 1.213 steps/s
	step: 1168, time: 962.467, speed: 1.214 steps/s
	step: 1169, time: 963.345, speed: 1.213 steps/s
	step: 1170, time: 963.955, speed: 1.214 steps/s
	step: 1171, time: 964.983, speed: 1.213 steps/s
	step: 1172, time: 965.901, speed: 1.213 steps/s
	step: 1173, time: 966.571, speed: 1.214 steps/s
	step: 1174, time: 967.435, speed: 1.214 steps/s
	step: 1175, time: 968.311, speed: 1.213 steps/s
	step: 1176, time: 969.205, speed: 1.213 steps/s
	step: 1177, time: 970.091, speed: 1.213 steps/s
	step: 1178, time: 970.744, speed: 1.214 steps/s
	step: 1179, time: 971.602, speed: 1.213 steps/s
	step: 1180, time: 972.374, speed: 1.214 steps/s
	step: 1181, time: 973.053, speed: 1.214 steps/s
	step: 1182, time: 973.938, speed: 1.214 steps/s
	step: 1183, time: 974.826, speed: 1.214 steps/s
	step: 1184, time: 975.655, speed: 1.214 steps/s
	step: 1185, time: 976.539, speed: 1.213 steps/s
	step: 1186, time: 977.349, speed: 1.213 steps/s
	step: 1187, time: 978.032, speed: 1.214 steps/s
	step: 1188, time: 979.099, speed: 1.213 steps/s
	step: 1189, time: 979.633, speed: 1.214 steps/s
	step: 1190, time: 980.265, speed: 1.214 steps/s
	step: 1191, time: 981.167, speed: 1.214 steps/s
	step: 1192, time: 982.030, speed: 1.214 steps/s
	step: 1193, time: 982.699, speed: 1.214 steps/s
	step: 1194, time: 983.554, speed: 1.214 steps/s
	step: 1195, time: 984.462, speed: 1.214 steps/s
	step: 1196, time: 985.354, speed: 1.214 steps/s
	step: 1197, time: 986.230, speed: 1.214 steps/s
	step: 1198, time: 986.908, speed: 1.214 steps/s
	step: 1199, time: 987.771, speed: 1.214 steps/s
	step: 1200, time: 988.522, speed: 1.214 steps/s
	step: 1201, time: 989.147, speed: 1.214 steps/s
	step: 1202, time: 990.011, speed: 1.214 steps/s
	step: 1203, time: 990.697, speed: 1.214 steps/s
	step: 1204, time: 991.570, speed: 1.214 steps/s
	step: 1205, time: 992.351, speed: 1.214 steps/s
	step: 1206, time: 993.233, speed: 1.214 steps/s
	step: 1207, time: 994.112, speed: 1.214 steps/s
	step: 1208, time: 994.990, speed: 1.214 steps/s
	step: 1209, time: 995.563, speed: 1.214 steps/s
	step: 1210, time: 996.427, speed: 1.214 steps/s
	step: 1211, time: 997.331, speed: 1.214 steps/s
	step: 1212, time: 998.234, speed: 1.214 steps/s
	step: 1213, time: 999.154, speed: 1.214 steps/s
	step: 1214, time: 1000.015, speed: 1.214 steps/s
	step: 1215, time: 1000.835, speed: 1.214 steps/s
	step: 1216, time: 1001.483, speed: 1.214 steps/s
	step: 1217, time: 1002.182, speed: 1.214 steps/s
	step: 1218, time: 1002.843, speed: 1.215 steps/s
	step: 1219, time: 1003.685, speed: 1.215 steps/s
	step: 1220, time: 1004.479, speed: 1.215 steps/s
	step: 1221, time: 1005.338, speed: 1.215 steps/s
	step: 1222, time: 1006.340, speed: 1.214 steps/s
	step: 1223, time: 1007.219, speed: 1.214 steps/s
	step: 1224, time: 1008.080, speed: 1.214 steps/s
	step: 1225, time: 1008.976, speed: 1.214 steps/s
	step: 1226, time: 1009.887, speed: 1.214 steps/s
	step: 1227, time: 1010.667, speed: 1.214 steps/s
	step: 1228, time: 1011.264, speed: 1.214 steps/s
	step: 1229, time: 1012.003, speed: 1.214 steps/s
	step: 1230, time: 1012.759, speed: 1.215 steps/s
	step: 1231, time: 1013.633, speed: 1.214 steps/s
	step: 1232, time: 1014.275, speed: 1.215 steps/s
	step: 1233, time: 1015.114, speed: 1.215 steps/s
	step: 1234, time: 1015.844, speed: 1.215 steps/s
	step: 1235, time: 1016.445, speed: 1.215 steps/s
	step: 1236, time: 1017.256, speed: 1.215 steps/s
	step: 1237, time: 1018.015, speed: 1.215 steps/s
	step: 1238, time: 1018.872, speed: 1.215 steps/s
	step: 1239, time: 1019.803, speed: 1.215 steps/s
	step: 1240, time: 1020.661, speed: 1.215 steps/s
	step: 1241, time: 1021.242, speed: 1.215 steps/s
	step: 1242, time: 1021.965, speed: 1.215 steps/s
	step: 1243, time: 1022.841, speed: 1.215 steps/s
	step: 1244, time: 1023.729, speed: 1.215 steps/s
	step: 1245, time: 1024.628, speed: 1.215 steps/s
	step: 1246, time: 1025.228, speed: 1.215 steps/s
	step: 1247, time: 1026.092, speed: 1.215 steps/s
	step: 1248, time: 1026.788, speed: 1.215 steps/s
	step: 1249, time: 1027.693, speed: 1.215 steps/s
	step: 1250, time: 1028.537, speed: 1.215 steps/s
	step: 1251, time: 1029.325, speed: 1.215 steps/s
	step: 1252, time: 1030.194, speed: 1.215 steps/s
	step: 1253, time: 1031.072, speed: 1.215 steps/s
	step: 1254, time: 1031.947, speed: 1.215 steps/s
	step: 1255, time: 1032.816, speed: 1.215 steps/s
	step: 1256, time: 1033.630, speed: 1.215 steps/s
	step: 1257, time: 1034.500, speed: 1.215 steps/s
	step: 1258, time: 1035.408, speed: 1.215 steps/s
	step: 1259, time: 1036.257, speed: 1.215 steps/s
	step: 1260, time: 1037.086, speed: 1.215 steps/s
	step: 1261, time: 1037.968, speed: 1.215 steps/s
	step: 1262, time: 1038.548, speed: 1.215 steps/s
	step: 1263, time: 1039.459, speed: 1.215 steps/s
	step: 1264, time: 1040.343, speed: 1.215 steps/s
	step: 1265, time: 1041.222, speed: 1.215 steps/s
	step: 1266, time: 1042.095, speed: 1.215 steps/s
	step: 1267, time: 1042.835, speed: 1.215 steps/s
	step: 1268, time: 1043.701, speed: 1.215 steps/s
	step: 1269, time: 1044.518, speed: 1.215 steps/s
	step: 1270, time: 1045.368, speed: 1.215 steps/s
	step: 1271, time: 1046.076, speed: 1.215 steps/s
	step: 1272, time: 1046.976, speed: 1.215 steps/s
	step: 1273, time: 1047.762, speed: 1.215 steps/s
	step: 1274, time: 1048.651, speed: 1.215 steps/s
	step: 1275, time: 1049.571, speed: 1.215 steps/s
	step: 1276, time: 1050.219, speed: 1.215 steps/s
	step: 1277, time: 1050.934, speed: 1.215 steps/s
	step: 1278, time: 1051.822, speed: 1.215 steps/s
	step: 1279, time: 1052.552, speed: 1.215 steps/s
	step: 1280, time: 1053.440, speed: 1.215 steps/s
	step: 1281, time: 1054.319, speed: 1.215 steps/s
	step: 1282, time: 1055.146, speed: 1.215 steps/s
	step: 1283, time: 1055.892, speed: 1.215 steps/s
	step: 1284, time: 1056.797, speed: 1.215 steps/s
	step: 1285, time: 1057.509, speed: 1.215 steps/s
	step: 1286, time: 1058.337, speed: 1.215 steps/s
	step: 1287, time: 1059.000, speed: 1.215 steps/s
	step: 1288, time: 1059.659, speed: 1.215 steps/s
	step: 1289, time: 1060.562, speed: 1.215 steps/s
	step: 1290, time: 1061.517, speed: 1.215 steps/s
	step: 1291, time: 1062.322, speed: 1.215 steps/s
	step: 1292, time: 1063.254, speed: 1.215 steps/s
	step: 1293, time: 1063.833, speed: 1.215 steps/s
	step: 1294, time: 1064.708, speed: 1.215 steps/s
	step: 1295, time: 1065.325, speed: 1.216 steps/s
	step: 1296, time: 1066.149, speed: 1.216 steps/s
	step: 1297, time: 1066.916, speed: 1.216 steps/s
	step: 1298, time: 1067.781, speed: 1.216 steps/s
	step: 1299, time: 1068.400, speed: 1.216 steps/s
	step: 1300, time: 1069.278, speed: 1.216 steps/s
	step: 1301, time: 1070.158, speed: 1.216 steps/s
	step: 1302, time: 1070.838, speed: 1.216 steps/s
	step: 1303, time: 1071.591, speed: 1.216 steps/s
	step: 1304, time: 1072.176, speed: 1.216 steps/s
	step: 1305, time: 1072.984, speed: 1.216 steps/s
	step: 1306, time: 1073.902, speed: 1.216 steps/s
	step: 1307, time: 1074.911, speed: 1.216 steps/s
	step: 1308, time: 1075.740, speed: 1.216 steps/s
	step: 1309, time: 1076.373, speed: 1.216 steps/s
	step: 1310, time: 1077.074, speed: 1.216 steps/s
	step: 1311, time: 1077.731, speed: 1.216 steps/s
	step: 1312, time: 1078.588, speed: 1.216 steps/s
	step: 1313, time: 1079.473, speed: 1.216 steps/s
	step: 1314, time: 1080.370, speed: 1.216 steps/s
	step: 1315, time: 1081.237, speed: 1.216 steps/s
	step: 1316, time: 1082.031, speed: 1.216 steps/s
	step: 1317, time: 1082.893, speed: 1.216 steps/s
	step: 1318, time: 1083.804, speed: 1.216 steps/s
	step: 1319, time: 1084.685, speed: 1.216 steps/s
	step: 1320, time: 1085.378, speed: 1.216 steps/s
	step: 1321, time: 1086.033, speed: 1.216 steps/s
	step: 1322, time: 1086.908, speed: 1.216 steps/s
	step: 1323, time: 1087.550, speed: 1.216 steps/s
	step: 1324, time: 1088.599, speed: 1.216 steps/s
	step: 1325, time: 1089.389, speed: 1.216 steps/s
	step: 1326, time: 1090.295, speed: 1.216 steps/s
	step: 1327, time: 1091.069, speed: 1.216 steps/s
	step: 1328, time: 1091.717, speed: 1.216 steps/s
	step: 1329, time: 1092.296, speed: 1.217 steps/s
	step: 1330, time: 1093.208, speed: 1.217 steps/s
	step: 1331, time: 1094.119, speed: 1.217 steps/s
	step: 1332, time: 1095.042, speed: 1.216 steps/s
	step: 1333, time: 1095.812, speed: 1.216 steps/s
	step: 1334, time: 1096.688, speed: 1.216 steps/s
	step: 1335, time: 1097.490, speed: 1.216 steps/s
	step: 1336, time: 1098.193, speed: 1.217 steps/s
	step: 1337, time: 1099.058, speed: 1.216 steps/s
	step: 1338, time: 1099.868, speed: 1.217 steps/s
	step: 1339, time: 1100.672, speed: 1.217 steps/s
	step: 1340, time: 1101.496, speed: 1.217 steps/s
	step: 1341, time: 1102.518, speed: 1.216 steps/s
	step: 1342, time: 1103.400, speed: 1.216 steps/s
	step: 1343, time: 1103.962, speed: 1.217 steps/s
	step: 1344, time: 1104.849, speed: 1.216 steps/s
	step: 1345, time: 1105.589, speed: 1.217 steps/s
	step: 1346, time: 1106.482, speed: 1.216 steps/s
	step: 1347, time: 1107.213, speed: 1.217 steps/s
	step: 1348, time: 1108.139, speed: 1.216 steps/s
	step: 1349, time: 1108.797, speed: 1.217 steps/s
	step: 1350, time: 1109.655, speed: 1.217 steps/s
	step: 1351, time: 1110.520, speed: 1.217 steps/s
	step: 1352, time: 1111.398, speed: 1.216 steps/s
	step: 1353, time: 1112.210, speed: 1.216 steps/s
	step: 1354, time: 1113.092, speed: 1.216 steps/s
	step: 1355, time: 1113.954, speed: 1.216 steps/s
	step: 1356, time: 1114.797, speed: 1.216 steps/s
	step: 1357, time: 1115.539, speed: 1.216 steps/s
	step: 1358, time: 1116.379, speed: 1.216 steps/s
	step: 1359, time: 1117.208, speed: 1.216 steps/s
	step: 1360, time: 1118.097, speed: 1.216 steps/s
	step: 1361, time: 1118.988, speed: 1.216 steps/s
	step: 1362, time: 1119.694, speed: 1.216 steps/s
	step: 1363, time: 1120.566, speed: 1.216 steps/s
	step: 1364, time: 1121.397, speed: 1.216 steps/s
	step: 1365, time: 1122.238, speed: 1.216 steps/s
	step: 1366, time: 1122.883, speed: 1.217 steps/s
	step: 1367, time: 1123.737, speed: 1.216 steps/s
	step: 1368, time: 1124.649, speed: 1.216 steps/s
	step: 1369, time: 1125.339, speed: 1.217 steps/s
	step: 1370, time: 1126.186, speed: 1.216 steps/s
	step: 1371, time: 1127.089, speed: 1.216 steps/s
	step: 1372, time: 1127.933, speed: 1.216 steps/s
	step: 1373, time: 1128.605, speed: 1.217 steps/s
	step: 1374, time: 1129.427, speed: 1.217 steps/s
	step: 1375, time: 1130.255, speed: 1.217 steps/s
	step: 1376, time: 1131.270, speed: 1.216 steps/s
	step: 1377, time: 1132.115, speed: 1.216 steps/s
	step: 1378, time: 1133.010, speed: 1.216 steps/s
	step: 1379, time: 1133.894, speed: 1.216 steps/s
	step: 1380, time: 1134.815, speed: 1.216 steps/s
	step: 1381, time: 1135.700, speed: 1.216 steps/s
	step: 1382, time: 1136.616, speed: 1.216 steps/s
	step: 1383, time: 1137.497, speed: 1.216 steps/s
	step: 1384, time: 1138.377, speed: 1.216 steps/s
	step: 1385, time: 1139.268, speed: 1.216 steps/s
	step: 1386, time: 1140.148, speed: 1.216 steps/s
	step: 1387, time: 1141.045, speed: 1.216 steps/s
	step: 1388, time: 1141.926, speed: 1.215 steps/s
	step: 1389, time: 1142.822, speed: 1.215 steps/s
	step: 1390, time: 1143.687, speed: 1.215 steps/s
	step: 1391, time: 1144.593, speed: 1.215 steps/s
	step: 1392, time: 1145.212, speed: 1.215 steps/s
	step: 1393, time: 1146.256, speed: 1.215 steps/s
	step: 1394, time: 1147.129, speed: 1.215 steps/s
	step: 1395, time: 1147.810, speed: 1.215 steps/s
	step: 1396, time: 1148.449, speed: 1.216 steps/s
	step: 1397, time: 1149.158, speed: 1.216 steps/s
	step: 1398, time: 1149.719, speed: 1.216 steps/s
	step: 1399, time: 1150.614, speed: 1.216 steps/s
	step: 1400, time: 1151.315, speed: 1.216 steps/s
	step: 1401, time: 1151.847, speed: 1.216 steps/s
	step: 1402, time: 1152.710, speed: 1.216 steps/s
	step: 1403, time: 1153.591, speed: 1.216 steps/s
	step: 1404, time: 1154.473, speed: 1.216 steps/s
	step: 1405, time: 1155.255, speed: 1.216 steps/s
	step: 1406, time: 1156.144, speed: 1.216 steps/s
	step: 1407, time: 1157.000, speed: 1.216 steps/s
	step: 1408, time: 1157.892, speed: 1.216 steps/s
	step: 1409, time: 1158.749, speed: 1.216 steps/s
	step: 1410, time: 1159.785, speed: 1.216 steps/s
	step: 1411, time: 1160.392, speed: 1.216 steps/s
	step: 1412, time: 1161.254, speed: 1.216 steps/s
	step: 1413, time: 1162.120, speed: 1.216 steps/s
	step: 1414, time: 1162.992, speed: 1.216 steps/s
	step: 1415, time: 1163.795, speed: 1.216 steps/s
	step: 1416, time: 1164.483, speed: 1.216 steps/s
	step: 1417, time: 1165.259, speed: 1.216 steps/s
	step: 1418, time: 1166.154, speed: 1.216 steps/s
	step: 1419, time: 1166.822, speed: 1.216 steps/s
	step: 1420, time: 1167.675, speed: 1.216 steps/s
	step: 1421, time: 1168.592, speed: 1.216 steps/s
	step: 1422, time: 1169.466, speed: 1.216 steps/s
	step: 1423, time: 1170.326, speed: 1.216 steps/s
	step: 1424, time: 1171.193, speed: 1.216 steps/s
	step: 1425, time: 1171.869, speed: 1.216 steps/s
	step: 1426, time: 1172.556, speed: 1.216 steps/s
	step: 1427, time: 1173.533, speed: 1.216 steps/s
	step: 1428, time: 1174.403, speed: 1.216 steps/s
	step: 1429, time: 1175.036, speed: 1.216 steps/s
	step: 1430, time: 1175.748, speed: 1.216 steps/s
	step: 1431, time: 1176.622, speed: 1.216 steps/s
	step: 1432, time: 1177.503, speed: 1.216 steps/s
	step: 1433, time: 1178.431, speed: 1.216 steps/s
	step: 1434, time: 1179.324, speed: 1.216 steps/s
	step: 1435, time: 1180.128, speed: 1.216 steps/s
	step: 1436, time: 1180.880, speed: 1.216 steps/s
	step: 1437, time: 1181.585, speed: 1.216 steps/s
	step: 1438, time: 1182.458, speed: 1.216 steps/s
	step: 1439, time: 1183.312, speed: 1.216 steps/s
	step: 1440, time: 1184.187, speed: 1.216 steps/s
	step: 1441, time: 1184.989, speed: 1.216 steps/s
	step: 1442, time: 1185.653, speed: 1.216 steps/s
	step: 1443, time: 1186.437, speed: 1.216 steps/s
	step: 1444, time: 1187.483, speed: 1.216 steps/s
	step: 1445, time: 1188.361, speed: 1.216 steps/s
	step: 1446, time: 1189.131, speed: 1.216 steps/s
	step: 1447, time: 1190.042, speed: 1.216 steps/s
	step: 1448, time: 1190.765, speed: 1.216 steps/s
	step: 1449, time: 1191.522, speed: 1.216 steps/s
	step: 1450, time: 1192.379, speed: 1.216 steps/s
	step: 1451, time: 1193.087, speed: 1.216 steps/s
	step: 1452, time: 1193.959, speed: 1.216 steps/s
	step: 1453, time: 1194.731, speed: 1.216 steps/s
	step: 1454, time: 1195.267, speed: 1.216 steps/s
	step: 1455, time: 1196.151, speed: 1.216 steps/s
	step: 1456, time: 1197.062, speed: 1.216 steps/s
	step: 1457, time: 1197.735, speed: 1.216 steps/s
	step: 1458, time: 1198.600, speed: 1.216 steps/s
	step: 1459, time: 1199.481, speed: 1.216 steps/s
	step: 1460, time: 1200.029, speed: 1.217 steps/s
	step: 1461, time: 1201.069, speed: 1.216 steps/s
	step: 1462, time: 1201.949, speed: 1.216 steps/s
	step: 1463, time: 1202.767, speed: 1.216 steps/s
	step: 1464, time: 1203.668, speed: 1.216 steps/s
	step: 1465, time: 1204.544, speed: 1.216 steps/s
	step: 1466, time: 1205.450, speed: 1.216 steps/s
	step: 1467, time: 1206.228, speed: 1.216 steps/s
	step: 1468, time: 1206.961, speed: 1.216 steps/s
	step: 1469, time: 1207.824, speed: 1.216 steps/s
	step: 1470, time: 1208.668, speed: 1.216 steps/s
	step: 1471, time: 1209.407, speed: 1.216 steps/s
	step: 1472, time: 1210.311, speed: 1.216 steps/s
	step: 1473, time: 1211.216, speed: 1.216 steps/s
	step: 1474, time: 1211.853, speed: 1.216 steps/s
	step: 1475, time: 1212.532, speed: 1.216 steps/s
	step: 1476, time: 1213.313, speed: 1.217 steps/s
	step: 1477, time: 1214.209, speed: 1.216 steps/s
	step: 1478, time: 1215.261, speed: 1.216 steps/s
	step: 1479, time: 1216.126, speed: 1.216 steps/s
	step: 1480, time: 1216.990, speed: 1.216 steps/s
	step: 1481, time: 1217.560, speed: 1.216 steps/s
	step: 1482, time: 1218.291, speed: 1.216 steps/s
	step: 1483, time: 1219.196, speed: 1.216 steps/s
	step: 1484, time: 1220.087, speed: 1.216 steps/s
	step: 1485, time: 1220.963, speed: 1.216 steps/s
	step: 1486, time: 1221.857, speed: 1.216 steps/s
	step: 1487, time: 1222.753, speed: 1.216 steps/s
	step: 1488, time: 1223.579, speed: 1.216 steps/s
	step: 1489, time: 1224.457, speed: 1.216 steps/s
	step: 1490, time: 1225.315, speed: 1.216 steps/s
	step: 1491, time: 1226.000, speed: 1.216 steps/s
	step: 1492, time: 1226.894, speed: 1.216 steps/s
	step: 1493, time: 1227.788, speed: 1.216 steps/s
	step: 1494, time: 1228.542, speed: 1.216 steps/s
	step: 1495, time: 1229.372, speed: 1.216 steps/s
	step: 1496, time: 1230.140, speed: 1.216 steps/s
	step: 1497, time: 1231.001, speed: 1.216 steps/s
	step: 1498, time: 1231.669, speed: 1.216 steps/s
	step: 1499, time: 1232.504, speed: 1.216 steps/s
	step: 1500, time: 1233.256, speed: 1.216 steps/s
	step: 1501, time: 1233.914, speed: 1.216 steps/s
	step: 1502, time: 1234.813, speed: 1.216 steps/s
	step: 1503, time: 1235.700, speed: 1.216 steps/s
	step: 1504, time: 1236.394, speed: 1.216 steps/s
	step: 1505, time: 1237.279, speed: 1.216 steps/s
	step: 1506, time: 1237.863, speed: 1.217 steps/s
	step: 1507, time: 1238.705, speed: 1.217 steps/s
	step: 1508, time: 1239.324, speed: 1.217 steps/s
	step: 1509, time: 1240.199, speed: 1.217 steps/s
	step: 1510, time: 1241.070, speed: 1.217 steps/s
	step: 1511, time: 1241.856, speed: 1.217 steps/s
	step: 1512, time: 1242.706, speed: 1.217 steps/s
	step: 1513, time: 1243.573, speed: 1.217 steps/s
	step: 1514, time: 1244.445, speed: 1.217 steps/s
	step: 1515, time: 1245.240, speed: 1.217 steps/s
	step: 1516, time: 1246.126, speed: 1.217 steps/s
	step: 1517, time: 1246.814, speed: 1.217 steps/s
	step: 1518, time: 1247.681, speed: 1.217 steps/s
	step: 1519, time: 1248.571, speed: 1.217 steps/s
	step: 1520, time: 1249.450, speed: 1.217 steps/s
	step: 1521, time: 1250.347, speed: 1.216 steps/s
	step: 1522, time: 1250.965, speed: 1.217 steps/s
	step: 1523, time: 1251.678, speed: 1.217 steps/s
	step: 1524, time: 1252.540, speed: 1.217 steps/s
	step: 1525, time: 1253.149, speed: 1.217 steps/s
	step: 1526, time: 1253.918, speed: 1.217 steps/s
	step: 1527, time: 1254.800, speed: 1.217 steps/s
	step: 1528, time: 1255.698, speed: 1.217 steps/s
	step: 1529, time: 1256.527, speed: 1.217 steps/s
	step: 1530, time: 1257.360, speed: 1.217 steps/s
	step: 1531, time: 1257.963, speed: 1.217 steps/s
	step: 1532, time: 1258.844, speed: 1.217 steps/s
	step: 1533, time: 1259.735, speed: 1.217 steps/s
	step: 1534, time: 1260.436, speed: 1.217 steps/s
	step: 1535, time: 1261.169, speed: 1.217 steps/s
	step: 1536, time: 1261.884, speed: 1.217 steps/s
	step: 1537, time: 1262.778, speed: 1.217 steps/s
	step: 1538, time: 1263.665, speed: 1.217 steps/s
	step: 1539, time: 1264.178, speed: 1.217 steps/s
	step: 1540, time: 1265.066, speed: 1.217 steps/s
	step: 1541, time: 1265.887, speed: 1.217 steps/s
	step: 1542, time: 1266.770, speed: 1.217 steps/s
	step: 1543, time: 1267.388, speed: 1.217 steps/s
	step: 1544, time: 1268.129, speed: 1.218 steps/s
	step: 1545, time: 1268.899, speed: 1.218 steps/s
	step: 1546, time: 1269.779, speed: 1.218 steps/s
	step: 1547, time: 1270.790, speed: 1.217 steps/s
	step: 1548, time: 1271.655, speed: 1.217 steps/s
	step: 1549, time: 1272.550, speed: 1.217 steps/s
	step: 1550, time: 1273.424, speed: 1.217 steps/s
	step: 1551, time: 1274.170, speed: 1.217 steps/s
	step: 1552, time: 1275.068, speed: 1.217 steps/s
	step: 1553, time: 1275.986, speed: 1.217 steps/s
	step: 1554, time: 1276.639, speed: 1.217 steps/s
	step: 1555, time: 1277.306, speed: 1.217 steps/s
	step: 1556, time: 1278.178, speed: 1.217 steps/s
	step: 1557, time: 1279.012, speed: 1.217 steps/s
	step: 1558, time: 1279.880, speed: 1.217 steps/s
	step: 1559, time: 1280.781, speed: 1.217 steps/s
	step: 1560, time: 1281.643, speed: 1.217 steps/s
	step: 1561, time: 1282.523, speed: 1.217 steps/s
	step: 1562, time: 1283.280, speed: 1.217 steps/s
	step: 1563, time: 1284.190, speed: 1.217 steps/s
	step: 1564, time: 1285.161, speed: 1.217 steps/s
	step: 1565, time: 1286.037, speed: 1.217 steps/s
	step: 1566, time: 1286.954, speed: 1.217 steps/s
	step: 1567, time: 1287.829, speed: 1.217 steps/s
	step: 1568, time: 1288.579, speed: 1.217 steps/s
	step: 1569, time: 1289.452, speed: 1.217 steps/s
	step: 1570, time: 1290.340, speed: 1.217 steps/s
	step: 1571, time: 1291.088, speed: 1.217 steps/s
	step: 1572, time: 1291.576, speed: 1.217 steps/s
	step: 1573, time: 1292.292, speed: 1.217 steps/s
	step: 1574, time: 1292.842, speed: 1.217 steps/s
	step: 1575, time: 1293.731, speed: 1.217 steps/s
	step: 1576, time: 1294.637, speed: 1.217 steps/s
	step: 1577, time: 1295.362, speed: 1.217 steps/s
	step: 1578, time: 1296.136, speed: 1.217 steps/s
	step: 1579, time: 1297.014, speed: 1.217 steps/s
	step: 1580, time: 1297.902, speed: 1.217 steps/s
	step: 1581, time: 1298.929, speed: 1.217 steps/s
	step: 1582, time: 1299.803, speed: 1.217 steps/s
	step: 1583, time: 1300.444, speed: 1.217 steps/s
	step: 1584, time: 1301.338, speed: 1.217 steps/s
	step: 1585, time: 1302.211, speed: 1.217 steps/s
	step: 1586, time: 1303.087, speed: 1.217 steps/s
	step: 1587, time: 1303.958, speed: 1.217 steps/s
	step: 1588, time: 1304.720, speed: 1.217 steps/s
	step: 1589, time: 1305.529, speed: 1.217 steps/s
	step: 1590, time: 1306.280, speed: 1.217 steps/s
	step: 1591, time: 1307.173, speed: 1.217 steps/s
	step: 1592, time: 1307.784, speed: 1.217 steps/s
	step: 1593, time: 1308.447, speed: 1.217 steps/s
	step: 1594, time: 1309.266, speed: 1.217 steps/s
	step: 1595, time: 1310.144, speed: 1.217 steps/s
	step: 1596, time: 1311.014, speed: 1.217 steps/s
	step: 1597, time: 1311.870, speed: 1.217 steps/s
	step: 1598, time: 1312.914, speed: 1.217 steps/s
	step: 1599, time: 1313.661, speed: 1.217 steps/s
	step: 1600, time: 1314.527, speed: 1.217 steps/s
	step: 1601, time: 1315.237, speed: 1.217 steps/s
	step: 1602, time: 1315.975, speed: 1.217 steps/s
	step: 1603, time: 1316.825, speed: 1.217 steps/s
	step: 1604, time: 1317.609, speed: 1.217 steps/s
	step: 1605, time: 1318.495, speed: 1.217 steps/s
	step: 1606, time: 1319.374, speed: 1.217 steps/s
	step: 1607, time: 1320.268, speed: 1.217 steps/s
	step: 1608, time: 1321.018, speed: 1.217 steps/s
	step: 1609, time: 1321.937, speed: 1.217 steps/s
	step: 1610, time: 1322.812, speed: 1.217 steps/s
	step: 1611, time: 1323.525, speed: 1.217 steps/s
	step: 1612, time: 1324.372, speed: 1.217 steps/s
	step: 1613, time: 1325.267, speed: 1.217 steps/s
	step: 1614, time: 1326.141, speed: 1.217 steps/s
	step: 1615, time: 1327.182, speed: 1.217 steps/s
	step: 1616, time: 1328.037, speed: 1.217 steps/s
	step: 1617, time: 1328.923, speed: 1.217 steps/s
	step: 1618, time: 1329.670, speed: 1.217 steps/s
	step: 1619, time: 1330.543, speed: 1.217 steps/s
	step: 1620, time: 1331.447, speed: 1.217 steps/s
	step: 1621, time: 1332.099, speed: 1.217 steps/s
	step: 1622, time: 1332.948, speed: 1.217 steps/s
	step: 1623, time: 1333.847, speed: 1.217 steps/s
	step: 1624, time: 1334.734, speed: 1.217 steps/s
	step: 1625, time: 1335.425, speed: 1.217 steps/s
	step: 1626, time: 1336.323, speed: 1.217 steps/s
	step: 1627, time: 1337.179, speed: 1.217 steps/s
	step: 1628, time: 1337.701, speed: 1.217 steps/s
	step: 1629, time: 1338.554, speed: 1.217 steps/s
	step: 1630, time: 1339.434, speed: 1.217 steps/s
	step: 1631, time: 1340.168, speed: 1.217 steps/s
	step: 1632, time: 1341.037, speed: 1.217 steps/s
	step: 1633, time: 1341.883, speed: 1.217 steps/s
	step: 1634, time: 1342.753, speed: 1.217 steps/s
	step: 1635, time: 1343.653, speed: 1.217 steps/s
	step: 1636, time: 1344.377, speed: 1.217 steps/s
	step: 1637, time: 1345.111, speed: 1.217 steps/s
	step: 1638, time: 1345.995, speed: 1.217 steps/s
	step: 1639, time: 1346.506, speed: 1.217 steps/s
	step: 1640, time: 1347.408, speed: 1.217 steps/s
	step: 1641, time: 1348.220, speed: 1.217 steps/s
	step: 1642, time: 1349.123, speed: 1.217 steps/s
	step: 1643, time: 1350.017, speed: 1.217 steps/s
	step: 1644, time: 1350.929, speed: 1.217 steps/s
	step: 1645, time: 1351.829, speed: 1.217 steps/s
	step: 1646, time: 1352.726, speed: 1.217 steps/s
	step: 1647, time: 1353.397, speed: 1.217 steps/s
	step: 1648, time: 1354.272, speed: 1.217 steps/s
	step: 1649, time: 1355.300, speed: 1.217 steps/s
	step: 1650, time: 1356.101, speed: 1.217 steps/s
	step: 1651, time: 1356.933, speed: 1.217 steps/s
	step: 1652, time: 1357.814, speed: 1.217 steps/s
	step: 1653, time: 1358.489, speed: 1.217 steps/s
	step: 1654, time: 1359.221, speed: 1.217 steps/s
	step: 1655, time: 1360.119, speed: 1.217 steps/s
	step: 1656, time: 1360.975, speed: 1.217 steps/s
	step: 1657, time: 1361.874, speed: 1.217 steps/s
	step: 1658, time: 1362.737, speed: 1.217 steps/s
	step: 1659, time: 1363.648, speed: 1.217 steps/s
	step: 1660, time: 1364.551, speed: 1.217 steps/s
	step: 1661, time: 1365.367, speed: 1.217 steps/s
	step: 1662, time: 1366.261, speed: 1.216 steps/s
	step: 1663, time: 1367.148, speed: 1.216 steps/s
	step: 1664, time: 1367.694, speed: 1.217 steps/s
	step: 1665, time: 1368.571, speed: 1.217 steps/s
	step: 1666, time: 1369.623, speed: 1.216 steps/s
	step: 1667, time: 1370.505, speed: 1.216 steps/s
	step: 1668, time: 1371.211, speed: 1.216 steps/s
	step: 1669, time: 1372.107, speed: 1.216 steps/s
	step: 1670, time: 1372.880, speed: 1.216 steps/s
	step: 1671, time: 1373.654, speed: 1.216 steps/s
	step: 1672, time: 1374.296, speed: 1.217 steps/s
	step: 1673, time: 1375.155, speed: 1.217 steps/s
	step: 1674, time: 1376.048, speed: 1.217 steps/s
	step: 1675, time: 1376.885, speed: 1.217 steps/s
	step: 1676, time: 1377.774, speed: 1.216 steps/s
	step: 1677, time: 1378.529, speed: 1.217 steps/s
	step: 1678, time: 1379.422, speed: 1.216 steps/s
	step: 1679, time: 1380.136, speed: 1.217 steps/s
	step: 1680, time: 1381.055, speed: 1.216 steps/s
	step: 1681, time: 1381.916, speed: 1.216 steps/s
	step: 1682, time: 1382.579, speed: 1.217 steps/s
	step: 1683, time: 1383.615, speed: 1.216 steps/s
	step: 1684, time: 1384.394, speed: 1.216 steps/s
	step: 1685, time: 1385.173, speed: 1.216 steps/s
	step: 1686, time: 1385.879, speed: 1.217 steps/s
	step: 1687, time: 1386.662, speed: 1.217 steps/s
	step: 1688, time: 1387.356, speed: 1.217 steps/s
	step: 1689, time: 1388.217, speed: 1.217 steps/s
	step: 1690, time: 1389.097, speed: 1.217 steps/s
	step: 1691, time: 1389.984, speed: 1.217 steps/s
	step: 1692, time: 1390.858, speed: 1.217 steps/s
	step: 1693, time: 1391.718, speed: 1.216 steps/s
	step: 1694, time: 1392.335, speed: 1.217 steps/s
	step: 1695, time: 1393.149, speed: 1.217 steps/s
	step: 1696, time: 1394.013, speed: 1.217 steps/s
	step: 1697, time: 1394.882, speed: 1.217 steps/s
	step: 1698, time: 1395.738, speed: 1.217 steps/s
	step: 1699, time: 1396.611, speed: 1.217 steps/s
	step: 1700, time: 1397.389, speed: 1.217 steps/s
	step: 1701, time: 1398.259, speed: 1.217 steps/s
	step: 1702, time: 1399.123, speed: 1.216 steps/s
	step: 1703, time: 1399.812, speed: 1.217 steps/s
	step: 1704, time: 1400.743, speed: 1.216 steps/s
	step: 1705, time: 1401.651, speed: 1.216 steps/s
	step: 1706, time: 1402.519, speed: 1.216 steps/s
	step: 1707, time: 1403.408, speed: 1.216 steps/s
	step: 1708, time: 1404.192, speed: 1.216 steps/s
	step: 1709, time: 1405.058, speed: 1.216 steps/s
	step: 1710, time: 1405.956, speed: 1.216 steps/s
	step: 1711, time: 1406.839, speed: 1.216 steps/s
	step: 1712, time: 1407.722, speed: 1.216 steps/s
	step: 1713, time: 1408.586, speed: 1.216 steps/s
	step: 1714, time: 1409.469, speed: 1.216 steps/s
	step: 1715, time: 1410.009, speed: 1.216 steps/s
	step: 1716, time: 1410.563, speed: 1.217 steps/s
	step: 1717, time: 1411.581, speed: 1.216 steps/s
	step: 1718, time: 1412.463, speed: 1.216 steps/s
	step: 1719, time: 1413.334, speed: 1.216 steps/s
	step: 1720, time: 1414.221, speed: 1.216 steps/s
	step: 1721, time: 1414.702, speed: 1.217 steps/s
	step: 1722, time: 1415.569, speed: 1.216 steps/s
	step: 1723, time: 1416.235, speed: 1.217 steps/s
	step: 1724, time: 1417.108, speed: 1.217 steps/s
	step: 1725, time: 1417.832, speed: 1.217 steps/s
	step: 1726, time: 1418.738, speed: 1.217 steps/s
	step: 1727, time: 1419.627, speed: 1.217 steps/s
	step: 1728, time: 1420.446, speed: 1.217 steps/s
	step: 1729, time: 1421.030, speed: 1.217 steps/s
	step: 1730, time: 1421.877, speed: 1.217 steps/s
	step: 1731, time: 1422.375, speed: 1.217 steps/s
	step: 1732, time: 1423.083, speed: 1.217 steps/s
	step: 1733, time: 1423.977, speed: 1.217 steps/s
	step: 1734, time: 1424.843, speed: 1.217 steps/s
	step: 1735, time: 1425.849, speed: 1.217 steps/s
	step: 1736, time: 1426.738, speed: 1.217 steps/s
	step: 1737, time: 1427.403, speed: 1.217 steps/s
	step: 1738, time: 1428.280, speed: 1.217 steps/s
	step: 1739, time: 1429.144, speed: 1.217 steps/s
	step: 1740, time: 1430.009, speed: 1.217 steps/s
	step: 1741, time: 1430.914, speed: 1.217 steps/s
	step: 1742, time: 1431.812, speed: 1.217 steps/s
	step: 1743, time: 1432.392, speed: 1.217 steps/s
	step: 1744, time: 1433.280, speed: 1.217 steps/s
	step: 1745, time: 1434.162, speed: 1.217 steps/s
	step: 1746, time: 1435.078, speed: 1.217 steps/s
	step: 1747, time: 1435.987, speed: 1.217 steps/s
	step: 1748, time: 1436.784, speed: 1.217 steps/s
	step: 1749, time: 1437.659, speed: 1.217 steps/s
	step: 1750, time: 1438.505, speed: 1.217 steps/s
	step: 1751, time: 1439.380, speed: 1.216 steps/s
	step: 1752, time: 1440.413, speed: 1.216 steps/s
	step: 1753, time: 1441.190, speed: 1.216 steps/s
	step: 1754, time: 1442.090, speed: 1.216 steps/s
	step: 1755, time: 1442.969, speed: 1.216 steps/s
	step: 1756, time: 1443.524, speed: 1.216 steps/s
	step: 1757, time: 1444.266, speed: 1.217 steps/s
	step: 1758, time: 1444.973, speed: 1.217 steps/s
	step: 1759, time: 1445.860, speed: 1.217 steps/s
	step: 1760, time: 1446.642, speed: 1.217 steps/s
	step: 1761, time: 1447.354, speed: 1.217 steps/s
	step: 1762, time: 1448.210, speed: 1.217 steps/s
	step: 1763, time: 1449.099, speed: 1.217 steps/s
	step: 1764, time: 1449.976, speed: 1.217 steps/s
	step: 1765, time: 1450.845, speed: 1.217 steps/s
	step: 1766, time: 1451.580, speed: 1.217 steps/s
	step: 1767, time: 1452.334, speed: 1.217 steps/s
	step: 1768, time: 1453.186, speed: 1.217 steps/s
	step: 1769, time: 1454.217, speed: 1.216 steps/s
	step: 1770, time: 1455.109, speed: 1.216 steps/s
	step: 1771, time: 1455.600, speed: 1.217 steps/s
	step: 1772, time: 1456.482, speed: 1.217 steps/s
	step: 1773, time: 1457.347, speed: 1.217 steps/s
	step: 1774, time: 1457.862, speed: 1.217 steps/s
	step: 1775, time: 1458.749, speed: 1.217 steps/s
	step: 1776, time: 1459.639, speed: 1.217 steps/s
	step: 1777, time: 1460.432, speed: 1.217 steps/s
	step: 1778, time: 1461.320, speed: 1.217 steps/s
	step: 1779, time: 1462.182, speed: 1.217 steps/s
	step: 1780, time: 1463.050, speed: 1.217 steps/s
	step: 1781, time: 1463.859, speed: 1.217 steps/s
	step: 1782, time: 1464.491, speed: 1.217 steps/s
	step: 1783, time: 1465.244, speed: 1.217 steps/s
	step: 1784, time: 1466.119, speed: 1.217 steps/s
	step: 1785, time: 1466.969, speed: 1.217 steps/s
	step: 1786, time: 1467.723, speed: 1.217 steps/s
	step: 1787, time: 1468.593, speed: 1.217 steps/s
	step: 1788, time: 1469.500, speed: 1.217 steps/s
	step: 1789, time: 1470.386, speed: 1.217 steps/s
	step: 1790, time: 1471.243, speed: 1.217 steps/s
	step: 1791, time: 1472.121, speed: 1.217 steps/s
	step: 1792, time: 1472.747, speed: 1.217 steps/s
	step: 1793, time: 1473.607, speed: 1.217 steps/s
	step: 1794, time: 1474.478, speed: 1.217 steps/s
	step: 1795, time: 1475.395, speed: 1.217 steps/s
	step: 1796, time: 1476.283, speed: 1.217 steps/s
	step: 1797, time: 1477.184, speed: 1.217 steps/s
	step: 1798, time: 1478.082, speed: 1.216 steps/s
	step: 1799, time: 1478.958, speed: 1.216 steps/s
	step: 1800, time: 1479.871, speed: 1.216 steps/s
	step: 1801, time: 1480.763, speed: 1.216 steps/s
	step: 1802, time: 1481.628, speed: 1.216 steps/s
	step: 1803, time: 1482.691, speed: 1.216 steps/s
	step: 1804, time: 1483.591, speed: 1.216 steps/s
	step: 1805, time: 1484.211, speed: 1.216 steps/s
	step: 1806, time: 1485.105, speed: 1.216 steps/s
	step: 1807, time: 1485.967, speed: 1.216 steps/s
	step: 1808, time: 1486.755, speed: 1.216 steps/s
	step: 1809, time: 1487.594, speed: 1.216 steps/s
	step: 1810, time: 1488.430, speed: 1.216 steps/s
	step: 1811, time: 1489.302, speed: 1.216 steps/s
	step: 1812, time: 1490.104, speed: 1.216 steps/s
	step: 1813, time: 1490.771, speed: 1.216 steps/s
	step: 1814, time: 1491.447, speed: 1.216 steps/s
	step: 1815, time: 1492.215, speed: 1.216 steps/s
	step: 1816, time: 1493.096, speed: 1.216 steps/s
	step: 1817, time: 1493.818, speed: 1.216 steps/s
	step: 1818, time: 1494.685, speed: 1.216 steps/s
	step: 1819, time: 1495.357, speed: 1.216 steps/s
	step: 1820, time: 1496.382, speed: 1.216 steps/s
	step: 1821, time: 1497.097, speed: 1.216 steps/s
	step: 1822, time: 1498.009, speed: 1.216 steps/s
	step: 1823, time: 1498.742, speed: 1.216 steps/s
	step: 1824, time: 1499.639, speed: 1.216 steps/s
	step: 1825, time: 1500.513, speed: 1.216 steps/s
	step: 1826, time: 1501.156, speed: 1.216 steps/s
	step: 1827, time: 1501.645, speed: 1.217 steps/s
	step: 1828, time: 1502.542, speed: 1.217 steps/s
	step: 1829, time: 1503.404, speed: 1.217 steps/s
	step: 1830, time: 1504.277, speed: 1.217 steps/s
	step: 1831, time: 1504.870, speed: 1.217 steps/s
	step: 1832, time: 1505.367, speed: 1.217 steps/s
	step: 1833, time: 1506.123, speed: 1.217 steps/s
	step: 1834, time: 1507.004, speed: 1.217 steps/s
	step: 1835, time: 1507.853, speed: 1.217 steps/s
	step: 1836, time: 1508.734, speed: 1.217 steps/s
	step: 1837, time: 1509.761, speed: 1.217 steps/s
	step: 1838, time: 1510.457, speed: 1.217 steps/s
	step: 1839, time: 1511.334, speed: 1.217 steps/s
	step: 1840, time: 1512.206, speed: 1.217 steps/s
	step: 1841, time: 1513.080, speed: 1.217 steps/s
	step: 1842, time: 1513.951, speed: 1.217 steps/s
	step: 1843, time: 1514.613, speed: 1.217 steps/s
	step: 1844, time: 1515.486, speed: 1.217 steps/s
	step: 1845, time: 1516.179, speed: 1.217 steps/s
	step: 1846, time: 1517.071, speed: 1.217 steps/s
	step: 1847, time: 1517.949, speed: 1.217 steps/s
	step: 1848, time: 1518.787, speed: 1.217 steps/s
	step: 1849, time: 1519.700, speed: 1.217 steps/s
	step: 1850, time: 1520.522, speed: 1.217 steps/s
	step: 1851, time: 1521.402, speed: 1.217 steps/s
	step: 1852, time: 1522.221, speed: 1.217 steps/s
	step: 1853, time: 1522.994, speed: 1.217 steps/s
	step: 1854, time: 1524.049, speed: 1.216 steps/s
	step: 1855, time: 1524.774, speed: 1.217 steps/s
	step: 1856, time: 1525.357, speed: 1.217 steps/s
	step: 1857, time: 1525.916, speed: 1.217 steps/s
	step: 1858, time: 1526.725, speed: 1.217 steps/s
	step: 1859, time: 1527.437, speed: 1.217 steps/s
	step: 1860, time: 1528.336, speed: 1.217 steps/s
	step: 1861, time: 1529.185, speed: 1.217 steps/s
	step: 1862, time: 1529.989, speed: 1.217 steps/s
	step: 1863, time: 1530.825, speed: 1.217 steps/s
	step: 1864, time: 1531.485, speed: 1.217 steps/s
	step: 1865, time: 1532.372, speed: 1.217 steps/s
	step: 1866, time: 1533.083, speed: 1.217 steps/s
	step: 1867, time: 1533.939, speed: 1.217 steps/s
	step: 1868, time: 1534.841, speed: 1.217 steps/s
	step: 1869, time: 1535.711, speed: 1.217 steps/s
	step: 1870, time: 1536.334, speed: 1.217 steps/s
	step: 1871, time: 1537.356, speed: 1.217 steps/s
	step: 1872, time: 1538.216, speed: 1.217 steps/s
	step: 1873, time: 1539.064, speed: 1.217 steps/s
	step: 1874, time: 1539.961, speed: 1.217 steps/s
	step: 1875, time: 1540.869, speed: 1.217 steps/s
	step: 1876, time: 1541.616, speed: 1.217 steps/s
	step: 1877, time: 1542.482, speed: 1.217 steps/s
	step: 1878, time: 1543.346, speed: 1.217 steps/s
	step: 1879, time: 1544.036, speed: 1.217 steps/s
	step: 1880, time: 1544.879, speed: 1.217 steps/s
	step: 1881, time: 1545.743, speed: 1.217 steps/s
	step: 1882, time: 1546.544, speed: 1.217 steps/s
	step: 1883, time: 1547.184, speed: 1.217 steps/s
	step: 1884, time: 1548.074, speed: 1.217 steps/s
	step: 1885, time: 1548.810, speed: 1.217 steps/s
	step: 1886, time: 1549.711, speed: 1.217 steps/s
	step: 1887, time: 1550.433, speed: 1.217 steps/s
	step: 1888, time: 1551.472, speed: 1.217 steps/s
	step: 1889, time: 1552.334, speed: 1.217 steps/s
	step: 1890, time: 1553.195, speed: 1.217 steps/s
	step: 1891, time: 1554.057, speed: 1.217 steps/s
	step: 1892, time: 1554.869, speed: 1.217 steps/s
	step: 1893, time: 1555.606, speed: 1.217 steps/s
	step: 1894, time: 1556.489, speed: 1.217 steps/s
	step: 1895, time: 1557.370, speed: 1.217 steps/s
	step: 1896, time: 1558.254, speed: 1.217 steps/s
	step: 1897, time: 1559.109, speed: 1.217 steps/s
	step: 1898, time: 1559.941, speed: 1.217 steps/s
	step: 1899, time: 1560.678, speed: 1.217 steps/s
	step: 1900, time: 1561.545, speed: 1.217 steps/s
	step: 1901, time: 1562.418, speed: 1.217 steps/s
	step: 1902, time: 1563.298, speed: 1.217 steps/s
	step: 1903, time: 1564.226, speed: 1.217 steps/s
	step: 1904, time: 1565.002, speed: 1.217 steps/s
	step: 1905, time: 1566.018, speed: 1.216 steps/s
	step: 1906, time: 1566.938, speed: 1.216 steps/s
	step: 1907, time: 1567.529, speed: 1.217 steps/s
	step: 1908, time: 1568.149, speed: 1.217 steps/s
	step: 1909, time: 1568.930, speed: 1.217 steps/s
	step: 1910, time: 1569.695, speed: 1.217 steps/s
	step: 1911, time: 1570.540, speed: 1.217 steps/s
	step: 1912, time: 1571.368, speed: 1.217 steps/s
	step: 1913, time: 1572.252, speed: 1.217 steps/s
	step: 1914, time: 1572.897, speed: 1.217 steps/s
	step: 1915, time: 1573.756, speed: 1.217 steps/s
	step: 1916, time: 1574.415, speed: 1.217 steps/s
	step: 1917, time: 1575.325, speed: 1.217 steps/s
	step: 1918, time: 1576.191, speed: 1.217 steps/s
	step: 1919, time: 1577.082, speed: 1.217 steps/s
	step: 1920, time: 1577.633, speed: 1.217 steps/s
	step: 1921, time: 1578.358, speed: 1.217 steps/s
	step: 1922, time: 1579.022, speed: 1.217 steps/s
	step: 1923, time: 1580.071, speed: 1.217 steps/s
	step: 1924, time: 1580.733, speed: 1.217 steps/s
	step: 1925, time: 1581.626, speed: 1.217 steps/s
	step: 1926, time: 1582.486, speed: 1.217 steps/s
	step: 1927, time: 1583.378, speed: 1.217 steps/s
	step: 1928, time: 1584.285, speed: 1.217 steps/s
	step: 1929, time: 1585.165, speed: 1.217 steps/s
	step: 1930, time: 1586.108, speed: 1.217 steps/s
	step: 1931, time: 1586.728, speed: 1.217 steps/s
	step: 1932, time: 1587.615, speed: 1.217 steps/s
	step: 1933, time: 1588.366, speed: 1.217 steps/s
	step: 1934, time: 1589.126, speed: 1.217 steps/s
	step: 1935, time: 1590.008, speed: 1.217 steps/s
	step: 1936, time: 1590.893, speed: 1.217 steps/s
	step: 1937, time: 1591.761, speed: 1.217 steps/s
	step: 1938, time: 1592.391, speed: 1.217 steps/s
	step: 1939, time: 1593.272, speed: 1.217 steps/s
	step: 1940, time: 1594.232, speed: 1.217 steps/s
	step: 1941, time: 1595.102, speed: 1.217 steps/s
	step: 1942, time: 1595.972, speed: 1.217 steps/s
	step: 1943, time: 1596.747, speed: 1.217 steps/s
	step: 1944, time: 1597.426, speed: 1.217 steps/s
	step: 1945, time: 1598.080, speed: 1.217 steps/s
	step: 1946, time: 1598.970, speed: 1.217 steps/s
	step: 1947, time: 1599.864, speed: 1.217 steps/s
	step: 1948, time: 1600.697, speed: 1.217 steps/s
	step: 1949, time: 1601.583, speed: 1.217 steps/s
	step: 1950, time: 1602.361, speed: 1.217 steps/s
	step: 1951, time: 1603.237, speed: 1.217 steps/s
	step: 1952, time: 1604.146, speed: 1.217 steps/s
	step: 1953, time: 1605.008, speed: 1.217 steps/s
	step: 1954, time: 1605.843, speed: 1.217 steps/s
	step: 1955, time: 1606.745, speed: 1.217 steps/s
	step: 1956, time: 1607.537, speed: 1.217 steps/s
	step: 1957, time: 1608.582, speed: 1.217 steps/s
	step: 1958, time: 1609.442, speed: 1.217 steps/s
	step: 1959, time: 1610.324, speed: 1.217 steps/s
	step: 1960, time: 1611.107, speed: 1.217 steps/s
	step: 1961, time: 1611.812, speed: 1.217 steps/s
	step: 1962, time: 1612.425, speed: 1.217 steps/s
	step: 1963, time: 1613.286, speed: 1.217 steps/s
	step: 1964, time: 1613.925, speed: 1.217 steps/s
	step: 1965, time: 1614.611, speed: 1.217 steps/s
	step: 1966, time: 1615.491, speed: 1.217 steps/s
	step: 1967, time: 1616.411, speed: 1.217 steps/s
	step: 1968, time: 1617.185, speed: 1.217 steps/s
	step: 1969, time: 1617.837, speed: 1.217 steps/s
	step: 1970, time: 1618.729, speed: 1.217 steps/s
	step: 1971, time: 1619.492, speed: 1.217 steps/s
	step: 1972, time: 1620.377, speed: 1.217 steps/s
	step: 1973, time: 1621.291, speed: 1.217 steps/s
	step: 1974, time: 1622.247, speed: 1.217 steps/s
	step: 1975, time: 1623.150, speed: 1.217 steps/s
	step: 1976, time: 1624.020, speed: 1.217 steps/s
	step: 1977, time: 1624.911, speed: 1.217 steps/s
	step: 1978, time: 1625.739, speed: 1.217 steps/s
	step: 1979, time: 1626.630, speed: 1.217 steps/s
	step: 1980, time: 1627.521, speed: 1.217 steps/s
	step: 1981, time: 1628.199, speed: 1.217 steps/s
	step: 1982, time: 1629.103, speed: 1.217 steps/s
	step: 1983, time: 1629.852, speed: 1.217 steps/s
	step: 1984, time: 1630.721, speed: 1.217 steps/s
	step: 1985, time: 1631.626, speed: 1.217 steps/s
	step: 1986, time: 1632.203, speed: 1.217 steps/s
	step: 1987, time: 1633.080, speed: 1.217 steps/s
	step: 1988, time: 1633.811, speed: 1.217 steps/s
	step: 1989, time: 1634.631, speed: 1.217 steps/s
	step: 1990, time: 1635.317, speed: 1.217 steps/s
	step: 1991, time: 1636.329, speed: 1.217 steps/s
	step: 1992, time: 1636.974, speed: 1.217 steps/s
	step: 1993, time: 1637.573, speed: 1.217 steps/s
	step: 1994, time: 1638.444, speed: 1.217 steps/s
	step: 1995, time: 1639.281, speed: 1.217 steps/s
	step: 1996, time: 1640.138, speed: 1.217 steps/s
	step: 1997, time: 1641.040, speed: 1.217 steps/s
	step: 1998, time: 1641.698, speed: 1.217 steps/s
	step: 1999, time: 1642.588, speed: 1.217 steps/s
	step: 2000, time: 1643.474, speed: 1.217 steps/s
	step: 2001, time: 1644.189, speed: 1.217 steps/s
	step: 2002, time: 1645.066, speed: 1.217 steps/s
	step: 2003, time: 1645.654, speed: 1.217 steps/s
	step: 2004, time: 1646.479, speed: 1.217 steps/s
	step: 2005, time: 1647.376, speed: 1.217 steps/s
	step: 2006, time: 1647.964, speed: 1.217 steps/s
	step: 2007, time: 1648.800, speed: 1.217 steps/s
	step: 2008, time: 1649.743, speed: 1.217 steps/s
	step: 2009, time: 1650.626, speed: 1.217 steps/s
	step: 2010, time: 1651.502, speed: 1.217 steps/s
	step: 2011, time: 1652.381, speed: 1.217 steps/s
	step: 2012, time: 1653.236, speed: 1.217 steps/s
	step: 2013, time: 1654.137, speed: 1.217 steps/s
	step: 2014, time: 1654.803, speed: 1.217 steps/s
	step: 2015, time: 1655.673, speed: 1.217 steps/s
	step: 2016, time: 1656.500, speed: 1.217 steps/s
	step: 2017, time: 1657.367, speed: 1.217 steps/s
	step: 2018, time: 1657.943, speed: 1.217 steps/s
Traceback (most recent call last):
  File "./infer.py", line 136, in <module>
    infer(args)
  File "./infer.py", line 83, in infer
    predictions = task.infer_step(model, data)
  File "/home/aistudio/ldk/Knover/tasks/task_base.py", line 43, in infer_step
    predictions = model.infer_step(inputs)
  File "/home/aistudio/ldk/Knover/models/unified_transformer.py", line 440, in infer_step
    predictions = self._run_generation(inputs)
  File "/home/aistudio/ldk/Knover/models/unified_transformer.py", line 397, in _run_generation
    return_numpy=False)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 266, in _execute
    fetch_vars = self.exe.run(program, feed, fetch_list, **kwargs)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1066, in run
    return_merged=return_merged)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1154, in _run_impl
    use_program_cache=use_program_cache)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1229, in _run_program
    fetch_var_name)
KeyboardInterrupt
{
  "is_distributed": true,
  "save_path": "./result",
  "infer_file": "data/test.txt",
  "output_name": "response",
  "log_steps": 1,
  "Model": {
    "model": "Plato",
    "config_path": "./config/12L.json",
    "init_checkpoint": "",
    "init_pretraining_params": "./output/step_13000",
    "learning_rate": 1e-05,
    "warmup_steps": 0,
    "weight_decay": 0.0,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": false,
    "amp_loss_scaling": 12800,
    "max_seq_len": 256,
    "weight_sharing": true,
    "mem_efficient": false,
    "use_bow": true,
    "use_entropy": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "latent_type_size": 20,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": 20,
    "topk": 5,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "DialogGeneration",
    "do_generation": "true",
    "is_cn": true,
    "nsp_inference_model_path": null,
    "nsp_attention_style": "bidirectional",
    "ranking_score": "decode_score"
  },
  "Reader": {
    "max_src_len": 128,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": false,
    "batch_size": 4,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  },
  "run_infer": true
}
{'is_distributed': True, 'save_path': './result', 'infer_file': 'data/test.txt', 'output_name': 'response', 'log_steps': 1, 'Model': {'model': 'Plato', 'config_path': './config/12L.json', 'init_checkpoint': '', 'init_pretraining_params': './output/step_13000', 'learning_rate': 1e-05, 'warmup_steps': 0, 'weight_decay': 0.0, 'max_grad_norm': 0.1, 'use_recompute': False, 'use_amp': False, 'amp_loss_scaling': 12800, 'max_seq_len': 256, 'weight_sharing': True, 'mem_efficient': False, 'use_bow': True, 'use_entropy': False, 'pre_encoder_cmd': 'd', 'preprocess_cmd': 'n', 'postprocess_cmd': 'da', 'post_cls_cmd': 'n', 'cls_bias': True, 'attention_probs_dropout_prob': 0.1, 'hidden_act': 'gelu', 'hidden_dropout_prob': 0.1, 'hidden_size': 768, 'initializer_range': 0.02, 'max_position_embeddings': 512, 'latent_type_size': 20, 'num_attention_heads': 12, 'num_hidden_layers': 12, 'type_vocab_size': 2, 'role_type_size': 32, 'vocab_size': 30004}, 'Generator': {'min_dec_len': 1, 'max_dec_len': 64, 'decoding_strategy': 'topk_sampling', 'temperature': 1.0, 'ignore_unk': True, 'num_samples': 20, 'topk': 5, 'topp': 0.9, 'beam_size': 10, 'length_average': True, 'length_penalty': 0.0}, 'Task': {'task': 'DialogGeneration', 'do_generation': 'true', 'is_cn': True, 'nsp_inference_model_path': None, 'nsp_attention_style': 'bidirectional', 'ranking_score': 'decode_score'}, 'Reader': {'max_src_len': 128, 'max_tgt_len': 128, 'truncate_first_turn': False, 'file_format': 'file', 'data_format': 'numerical', 'in_tokens': False, 'batch_size': 4, 'continuous_position': True, 'random_seed': 11, 'sort_pool_size': 65536}, 'Tokenizer': {'tokenizer': 'SentencePieceTokenizer', 'vocab_path': './config/vocab.txt', 'do_lower_case': False, 'spm_model_file': './config/spm.model'}, 'run_infer': True, 'pad_id': 0, 'bos_id': 1, 'eos_id': 2, 'unk_id': 0, 'mask_id': 30000}
W1025 21:06:55.715844  1889 device_context.cc:252] Please NOTE: device: 0, CUDA Capability: 70, Driver API Version: 10.1, Runtime API Version: 9.0
W1025 21:06:55.720161  1889 device_context.cc:260] device: 0, cuDNN Version: 7.6.
Load pretraining parameters from ./output/step_13000.
	step: 1, time: 7.607, speed: 0.131 steps/s
	step: 2, time: 15.125, speed: 0.132 steps/s
	step: 3, time: 22.267, speed: 0.135 steps/s
	step: 4, time: 29.574, speed: 0.135 steps/s
	step: 5, time: 36.546, speed: 0.137 steps/s
	step: 6, time: 44.255, speed: 0.136 steps/s
	step: 7, time: 51.451, speed: 0.136 steps/s
	step: 8, time: 58.991, speed: 0.136 steps/s
	step: 9, time: 66.484, speed: 0.135 steps/s
	step: 10, time: 74.117, speed: 0.135 steps/s
	step: 11, time: 82.388, speed: 0.134 steps/s
	step: 12, time: 89.139, speed: 0.135 steps/s
	step: 13, time: 96.621, speed: 0.135 steps/s
	step: 14, time: 104.146, speed: 0.134 steps/s
	step: 15, time: 111.639, speed: 0.134 steps/s
	step: 16, time: 119.755, speed: 0.134 steps/s
	step: 17, time: 126.734, speed: 0.134 steps/s
	step: 18, time: 134.179, speed: 0.134 steps/s
	step: 19, time: 141.758, speed: 0.134 steps/s
	step: 20, time: 149.916, speed: 0.133 steps/s
	step: 21, time: 156.695, speed: 0.134 steps/s
	step: 22, time: 164.175, speed: 0.134 steps/s
	step: 23, time: 171.461, speed: 0.134 steps/s
	step: 24, time: 178.810, speed: 0.134 steps/s
	step: 25, time: 186.208, speed: 0.134 steps/s
	step: 26, time: 193.378, speed: 0.134 steps/s
	step: 27, time: 200.554, speed: 0.135 steps/s
	step: 28, time: 208.146, speed: 0.135 steps/s
	step: 29, time: 215.567, speed: 0.135 steps/s
	step: 30, time: 224.003, speed: 0.134 steps/s
	step: 31, time: 231.569, speed: 0.134 steps/s
	step: 32, time: 239.217, speed: 0.134 steps/s
	step: 33, time: 247.149, speed: 0.134 steps/s
	step: 34, time: 254.894, speed: 0.133 steps/s
	step: 35, time: 262.011, speed: 0.134 steps/s
	step: 36, time: 269.147, speed: 0.134 steps/s
	step: 37, time: 276.497, speed: 0.134 steps/s
	step: 38, time: 283.499, speed: 0.134 steps/s
	step: 39, time: 291.276, speed: 0.134 steps/s
	step: 40, time: 298.756, speed: 0.134 steps/s
	step: 41, time: 306.166, speed: 0.134 steps/s
	step: 42, time: 313.915, speed: 0.134 steps/s
	step: 43, time: 321.781, speed: 0.134 steps/s
	step: 44, time: 329.501, speed: 0.134 steps/s
	step: 45, time: 337.110, speed: 0.133 steps/s
	step: 46, time: 344.543, speed: 0.134 steps/s
	step: 47, time: 352.154, speed: 0.133 steps/s
	step: 48, time: 359.947, speed: 0.133 steps/s
	step: 49, time: 367.751, speed: 0.133 steps/s
	step: 50, time: 375.227, speed: 0.133 steps/s
	step: 51, time: 382.312, speed: 0.133 steps/s
	step: 52, time: 389.976, speed: 0.133 steps/s
	step: 53, time: 397.954, speed: 0.133 steps/s
	step: 54, time: 405.458, speed: 0.133 steps/s
	step: 55, time: 413.143, speed: 0.133 steps/s
	step: 56, time: 420.586, speed: 0.133 steps/s
	step: 57, time: 428.863, speed: 0.133 steps/s
	step: 58, time: 436.411, speed: 0.133 steps/s
	step: 59, time: 444.201, speed: 0.133 steps/s
	step: 60, time: 451.953, speed: 0.133 steps/s
	step: 61, time: 459.479, speed: 0.133 steps/s
	step: 62, time: 466.938, speed: 0.133 steps/s
	step: 63, time: 474.470, speed: 0.133 steps/s
	step: 64, time: 481.860, speed: 0.133 steps/s
	step: 65, time: 488.784, speed: 0.133 steps/s
	step: 66, time: 496.523, speed: 0.133 steps/s
	step: 67, time: 503.613, speed: 0.133 steps/s
	step: 68, time: 511.122, speed: 0.133 steps/s
	step: 69, time: 519.010, speed: 0.133 steps/s
	step: 70, time: 527.148, speed: 0.133 steps/s
	step: 71, time: 534.944, speed: 0.133 steps/s
	step: 72, time: 541.893, speed: 0.133 steps/s
	step: 73, time: 548.901, speed: 0.133 steps/s
	step: 74, time: 556.166, speed: 0.133 steps/s
	step: 75, time: 563.514, speed: 0.133 steps/s
	step: 76, time: 571.128, speed: 0.133 steps/s
	step: 77, time: 578.622, speed: 0.133 steps/s
	step: 78, time: 586.208, speed: 0.133 steps/s
	step: 79, time: 594.211, speed: 0.133 steps/s
	step: 80, time: 601.370, speed: 0.133 steps/s
	step: 81, time: 609.233, speed: 0.133 steps/s
	step: 82, time: 616.812, speed: 0.133 steps/s
	step: 83, time: 624.324, speed: 0.133 steps/s
	step: 84, time: 631.621, speed: 0.133 steps/s
	step: 85, time: 639.444, speed: 0.133 steps/s
	step: 86, time: 647.067, speed: 0.133 steps/s
	step: 87, time: 654.518, speed: 0.133 steps/s
	step: 88, time: 662.867, speed: 0.133 steps/s
	step: 89, time: 670.314, speed: 0.133 steps/s
	step: 90, time: 678.160, speed: 0.133 steps/s
	step: 91, time: 685.909, speed: 0.133 steps/s
	step: 92, time: 693.336, speed: 0.133 steps/s
	step: 93, time: 700.889, speed: 0.133 steps/s
	step: 94, time: 708.325, speed: 0.133 steps/s
	step: 95, time: 715.836, speed: 0.133 steps/s
	step: 96, time: 722.569, speed: 0.133 steps/s
	step: 97, time: 730.278, speed: 0.133 steps/s
	step: 98, time: 738.126, speed: 0.133 steps/s
	step: 99, time: 745.673, speed: 0.133 steps/s
	step: 100, time: 753.103, speed: 0.133 steps/s
	step: 101, time: 760.150, speed: 0.133 steps/s
	step: 102, time: 767.935, speed: 0.133 steps/s
	step: 103, time: 775.465, speed: 0.133 steps/s
	step: 104, time: 783.323, speed: 0.133 steps/s
	step: 105, time: 791.198, speed: 0.133 steps/s
	step: 106, time: 798.727, speed: 0.133 steps/s
	step: 107, time: 806.761, speed: 0.133 steps/s
	step: 108, time: 814.678, speed: 0.133 steps/s
	step: 109, time: 822.821, speed: 0.132 steps/s
	step: 110, time: 830.597, speed: 0.132 steps/s
	step: 111, time: 837.875, speed: 0.132 steps/s
	step: 112, time: 844.884, speed: 0.133 steps/s
	step: 113, time: 852.348, speed: 0.133 steps/s
	step: 114, time: 860.396, speed: 0.132 steps/s
	step: 115, time: 868.795, speed: 0.132 steps/s
	step: 116, time: 876.030, speed: 0.132 steps/s
	step: 117, time: 883.469, speed: 0.132 steps/s
	step: 118, time: 890.584, speed: 0.132 steps/s
	step: 119, time: 897.881, speed: 0.133 steps/s
	step: 120, time: 905.725, speed: 0.132 steps/s
	step: 121, time: 913.687, speed: 0.132 steps/s
	step: 122, time: 921.136, speed: 0.132 steps/s
	step: 123, time: 928.865, speed: 0.132 steps/s
	step: 124, time: 935.894, speed: 0.132 steps/s
	step: 125, time: 943.505, speed: 0.132 steps/s
	step: 126, time: 950.374, speed: 0.133 steps/s
	step: 127, time: 957.967, speed: 0.133 steps/s
	step: 128, time: 965.992, speed: 0.133 steps/s
	step: 129, time: 973.237, speed: 0.133 steps/s
	step: 130, time: 980.794, speed: 0.133 steps/s
	step: 131, time: 988.342, speed: 0.133 steps/s
	step: 132, time: 995.795, speed: 0.133 steps/s
	step: 133, time: 1003.233, speed: 0.133 steps/s
	step: 134, time: 1010.822, speed: 0.133 steps/s
	step: 135, time: 1018.993, speed: 0.132 steps/s
	step: 136, time: 1026.619, speed: 0.132 steps/s
	step: 137, time: 1034.543, speed: 0.132 steps/s
	step: 138, time: 1041.982, speed: 0.132 steps/s
	step: 139, time: 1049.253, speed: 0.132 steps/s
	step: 140, time: 1057.169, speed: 0.132 steps/s
	step: 141, time: 1065.197, speed: 0.132 steps/s
	step: 142, time: 1073.023, speed: 0.132 steps/s
	step: 143, time: 1080.362, speed: 0.132 steps/s
	step: 144, time: 1088.052, speed: 0.132 steps/s
	step: 145, time: 1095.920, speed: 0.132 steps/s
	step: 146, time: 1103.453, speed: 0.132 steps/s
	step: 147, time: 1111.184, speed: 0.132 steps/s
	step: 148, time: 1118.624, speed: 0.132 steps/s
	step: 149, time: 1126.329, speed: 0.132 steps/s
	step: 150, time: 1133.926, speed: 0.132 steps/s
	step: 151, time: 1141.277, speed: 0.132 steps/s
	step: 152, time: 1148.784, speed: 0.132 steps/s
	step: 153, time: 1156.266, speed: 0.132 steps/s
	step: 154, time: 1163.679, speed: 0.132 steps/s
	step: 155, time: 1171.018, speed: 0.132 steps/s
	step: 156, time: 1178.918, speed: 0.132 steps/s
	step: 157, time: 1187.557, speed: 0.132 steps/s
	step: 158, time: 1195.038, speed: 0.132 steps/s
	step: 159, time: 1202.846, speed: 0.132 steps/s
	step: 160, time: 1210.406, speed: 0.132 steps/s
	step: 161, time: 1217.705, speed: 0.132 steps/s
	step: 162, time: 1225.383, speed: 0.132 steps/s
	step: 163, time: 1232.215, speed: 0.132 steps/s
	step: 164, time: 1239.838, speed: 0.132 steps/s
	step: 165, time: 1247.779, speed: 0.132 steps/s
	step: 166, time: 1255.384, speed: 0.132 steps/s
	step: 167, time: 1262.952, speed: 0.132 steps/s
	step: 168, time: 1270.736, speed: 0.132 steps/s
	step: 169, time: 1278.864, speed: 0.132 steps/s
	step: 170, time: 1286.176, speed: 0.132 steps/s
	step: 171, time: 1293.804, speed: 0.132 steps/s
	step: 172, time: 1301.511, speed: 0.132 steps/s
	step: 173, time: 1308.835, speed: 0.132 steps/s
	step: 174, time: 1315.837, speed: 0.132 steps/s
	step: 175, time: 1322.967, speed: 0.132 steps/s
	step: 176, time: 1330.633, speed: 0.132 steps/s
	step: 177, time: 1338.294, speed: 0.132 steps/s
	step: 178, time: 1346.176, speed: 0.132 steps/s
	step: 179, time: 1353.878, speed: 0.132 steps/s
	step: 180, time: 1361.476, speed: 0.132 steps/s
	step: 181, time: 1369.119, speed: 0.132 steps/s
	step: 182, time: 1376.450, speed: 0.132 steps/s
	step: 183, time: 1384.242, speed: 0.132 steps/s
	step: 184, time: 1391.423, speed: 0.132 steps/s
	step: 185, time: 1399.228, speed: 0.132 steps/s
	step: 186, time: 1406.260, speed: 0.132 steps/s
	step: 187, time: 1414.102, speed: 0.132 steps/s
	step: 188, time: 1421.769, speed: 0.132 steps/s
	step: 189, time: 1429.370, speed: 0.132 steps/s
	step: 190, time: 1437.758, speed: 0.132 steps/s
	step: 191, time: 1445.571, speed: 0.132 steps/s
	step: 192, time: 1452.801, speed: 0.132 steps/s
	step: 193, time: 1460.793, speed: 0.132 steps/s
	step: 194, time: 1468.373, speed: 0.132 steps/s
	step: 195, time: 1476.069, speed: 0.132 steps/s
	step: 196, time: 1483.524, speed: 0.132 steps/s
	step: 197, time: 1490.790, speed: 0.132 steps/s
	step: 198, time: 1498.370, speed: 0.132 steps/s
	step: 199, time: 1505.400, speed: 0.132 steps/s
	step: 200, time: 1512.897, speed: 0.132 steps/s
	step: 201, time: 1520.350, speed: 0.132 steps/s
	step: 202, time: 1527.646, speed: 0.132 steps/s
	step: 203, time: 1534.902, speed: 0.132 steps/s
	step: 204, time: 1542.977, speed: 0.132 steps/s
	step: 205, time: 1550.101, speed: 0.132 steps/s
	step: 206, time: 1557.230, speed: 0.132 steps/s
	step: 207, time: 1564.355, speed: 0.132 steps/s
	step: 208, time: 1572.809, speed: 0.132 steps/s
	step: 209, time: 1580.384, speed: 0.132 steps/s
	step: 210, time: 1587.850, speed: 0.132 steps/s
	step: 211, time: 1595.083, speed: 0.132 steps/s
	step: 212, time: 1602.094, speed: 0.132 steps/s
	step: 213, time: 1609.288, speed: 0.132 steps/s
	step: 214, time: 1616.795, speed: 0.132 steps/s
	step: 215, time: 1624.126, speed: 0.132 steps/s
	step: 216, time: 1631.109, speed: 0.132 steps/s
	step: 217, time: 1639.048, speed: 0.132 steps/s
	step: 218, time: 1646.852, speed: 0.132 steps/s
	step: 219, time: 1654.676, speed: 0.132 steps/s
	step: 220, time: 1662.127, speed: 0.132 steps/s
	step: 221, time: 1670.056, speed: 0.132 steps/s
	step: 222, time: 1677.459, speed: 0.132 steps/s
	step: 223, time: 1684.693, speed: 0.132 steps/s
	step: 224, time: 1692.324, speed: 0.132 steps/s
	step: 225, time: 1700.076, speed: 0.132 steps/s
	step: 226, time: 1707.647, speed: 0.132 steps/s
	step: 227, time: 1714.927, speed: 0.132 steps/s
	step: 228, time: 1722.432, speed: 0.132 steps/s
	step: 229, time: 1729.789, speed: 0.132 steps/s
	step: 230, time: 1737.389, speed: 0.132 steps/s
	step: 231, time: 1744.534, speed: 0.132 steps/s
	step: 232, time: 1751.546, speed: 0.132 steps/s
	step: 233, time: 1759.010, speed: 0.132 steps/s
	step: 234, time: 1766.062, speed: 0.132 steps/s
	step: 235, time: 1773.173, speed: 0.133 steps/s
	step: 236, time: 1780.919, speed: 0.133 steps/s
	step: 237, time: 1788.761, speed: 0.132 steps/s
	step: 238, time: 1796.633, speed: 0.132 steps/s
	step: 239, time: 1804.223, speed: 0.132 steps/s
	step: 240, time: 1811.552, speed: 0.132 steps/s
	step: 241, time: 1819.218, speed: 0.132 steps/s
	step: 242, time: 1826.931, speed: 0.132 steps/s
	step: 243, time: 1834.859, speed: 0.132 steps/s
	step: 244, time: 1842.520, speed: 0.132 steps/s
	step: 245, time: 1850.339, speed: 0.132 steps/s
	step: 246, time: 1858.040, speed: 0.132 steps/s
	step: 247, time: 1865.619, speed: 0.132 steps/s
	step: 248, time: 1873.009, speed: 0.132 steps/s
	step: 249, time: 1880.545, speed: 0.132 steps/s
	step: 250, time: 1888.428, speed: 0.132 steps/s
	step: 251, time: 1895.858, speed: 0.132 steps/s
	step: 252, time: 1903.390, speed: 0.132 steps/s
	step: 253, time: 1910.982, speed: 0.132 steps/s
	step: 254, time: 1918.363, speed: 0.132 steps/s
	step: 255, time: 1925.499, speed: 0.132 steps/s
	step: 256, time: 1932.911, speed: 0.132 steps/s
	step: 257, time: 1940.421, speed: 0.132 steps/s
	step: 258, time: 1947.990, speed: 0.132 steps/s
	step: 259, time: 1955.523, speed: 0.132 steps/s
	step: 260, time: 1962.606, speed: 0.132 steps/s
	step: 261, time: 1969.860, speed: 0.132 steps/s
	step: 262, time: 1977.505, speed: 0.132 steps/s
	step: 263, time: 1985.072, speed: 0.132 steps/s
	step: 264, time: 1993.192, speed: 0.132 steps/s
	step: 265, time: 2000.830, speed: 0.132 steps/s
	step: 266, time: 2008.789, speed: 0.132 steps/s
	step: 267, time: 2016.186, speed: 0.132 steps/s
	step: 268, time: 2023.059, speed: 0.132 steps/s
	step: 269, time: 2030.566, speed: 0.132 steps/s
	step: 270, time: 2037.882, speed: 0.132 steps/s
	step: 271, time: 2045.219, speed: 0.133 steps/s
	step: 272, time: 2053.144, speed: 0.132 steps/s
	step: 273, time: 2060.368, speed: 0.133 steps/s
	step: 274, time: 2067.851, speed: 0.133 steps/s
	step: 275, time: 2075.002, speed: 0.133 steps/s
	step: 276, time: 2082.476, speed: 0.133 steps/s
	step: 277, time: 2089.916, speed: 0.133 steps/s
	step: 278, time: 2097.546, speed: 0.133 steps/s
	step: 279, time: 2104.942, speed: 0.133 steps/s
	step: 280, time: 2112.587, speed: 0.133 steps/s
	step: 281, time: 2119.729, speed: 0.133 steps/s
	step: 282, time: 2127.131, speed: 0.133 steps/s
	step: 283, time: 2134.631, speed: 0.133 steps/s
	step: 284, time: 2141.872, speed: 0.133 steps/s
	step: 285, time: 2149.629, speed: 0.133 steps/s
	step: 286, time: 2157.136, speed: 0.133 steps/s
	step: 287, time: 2164.784, speed: 0.133 steps/s
	step: 288, time: 2172.094, speed: 0.133 steps/s
	step: 289, time: 2179.232, speed: 0.133 steps/s
	step: 290, time: 2186.462, speed: 0.133 steps/s
	step: 291, time: 2193.353, speed: 0.133 steps/s
	step: 292, time: 2201.142, speed: 0.133 steps/s
	step: 293, time: 2209.454, speed: 0.133 steps/s
	step: 294, time: 2217.566, speed: 0.133 steps/s
	step: 295, time: 2225.590, speed: 0.133 steps/s
	step: 296, time: 2232.731, speed: 0.133 steps/s
	step: 297, time: 2240.010, speed: 0.133 steps/s
	step: 298, time: 2247.164, speed: 0.133 steps/s
	step: 299, time: 2255.060, speed: 0.133 steps/s
	step: 300, time: 2262.083, speed: 0.133 steps/s
	step: 301, time: 2269.748, speed: 0.133 steps/s
	step: 302, time: 2277.626, speed: 0.133 steps/s
	step: 303, time: 2285.381, speed: 0.133 steps/s
	step: 304, time: 2292.398, speed: 0.133 steps/s
	step: 305, time: 2299.381, speed: 0.133 steps/s
	step: 306, time: 2306.867, speed: 0.133 steps/s
	step: 307, time: 2314.432, speed: 0.133 steps/s
	step: 308, time: 2322.240, speed: 0.133 steps/s
	step: 309, time: 2329.493, speed: 0.133 steps/s
	step: 310, time: 2336.932, speed: 0.133 steps/s
	step: 311, time: 2344.434, speed: 0.133 steps/s
	step: 312, time: 2352.209, speed: 0.133 steps/s
	step: 313, time: 2359.786, speed: 0.133 steps/s
	step: 314, time: 2367.315, speed: 0.133 steps/s
	step: 315, time: 2374.536, speed: 0.133 steps/s
	step: 316, time: 2382.253, speed: 0.133 steps/s
	step: 317, time: 2389.896, speed: 0.133 steps/s
	step: 318, time: 2397.850, speed: 0.133 steps/s
	step: 319, time: 2405.647, speed: 0.133 steps/s
	step: 320, time: 2412.746, speed: 0.133 steps/s
	step: 321, time: 2420.159, speed: 0.133 steps/s
	step: 322, time: 2427.314, speed: 0.133 steps/s
	step: 323, time: 2434.642, speed: 0.133 steps/s
	step: 324, time: 2442.195, speed: 0.133 steps/s
	step: 325, time: 2449.838, speed: 0.133 steps/s
	step: 326, time: 2457.200, speed: 0.133 steps/s
	step: 327, time: 2464.750, speed: 0.133 steps/s
	step: 328, time: 2472.401, speed: 0.133 steps/s
	step: 329, time: 2480.684, speed: 0.133 steps/s
	step: 330, time: 2488.141, speed: 0.133 steps/s
	step: 331, time: 2495.472, speed: 0.133 steps/s
	step: 332, time: 2502.530, speed: 0.133 steps/s
	step: 333, time: 2510.179, speed: 0.133 steps/s
	step: 334, time: 2518.047, speed: 0.133 steps/s
	step: 335, time: 2526.006, speed: 0.133 steps/s
	step: 336, time: 2533.470, speed: 0.133 steps/s
	step: 337, time: 2541.210, speed: 0.133 steps/s
	step: 338, time: 2548.465, speed: 0.133 steps/s
	step: 339, time: 2556.086, speed: 0.133 steps/s
	step: 340, time: 2563.365, speed: 0.133 steps/s
	step: 341, time: 2570.819, speed: 0.133 steps/s
	step: 342, time: 2577.955, speed: 0.133 steps/s
	step: 343, time: 2584.910, speed: 0.133 steps/s
	step: 344, time: 2592.605, speed: 0.133 steps/s
	step: 345, time: 2600.141, speed: 0.133 steps/s
	step: 346, time: 2607.505, speed: 0.133 steps/s
	step: 347, time: 2615.076, speed: 0.133 steps/s
	step: 348, time: 2622.955, speed: 0.133 steps/s
	step: 349, time: 2630.084, speed: 0.133 steps/s
	step: 350, time: 2637.393, speed: 0.133 steps/s
	step: 351, time: 2644.833, speed: 0.133 steps/s
	step: 352, time: 2652.530, speed: 0.133 steps/s
	step: 353, time: 2660.006, speed: 0.133 steps/s
	step: 354, time: 2667.652, speed: 0.133 steps/s
	step: 355, time: 2675.189, speed: 0.133 steps/s
	step: 356, time: 2682.341, speed: 0.133 steps/s
	step: 357, time: 2689.596, speed: 0.133 steps/s
	step: 358, time: 2698.166, speed: 0.133 steps/s
	step: 359, time: 2706.135, speed: 0.133 steps/s
	step: 360, time: 2713.244, speed: 0.133 steps/s
	step: 361, time: 2720.561, speed: 0.133 steps/s
	step: 362, time: 2728.145, speed: 0.133 steps/s
	step: 363, time: 2735.289, speed: 0.133 steps/s
	step: 364, time: 2743.035, speed: 0.133 steps/s
	step: 365, time: 2751.071, speed: 0.133 steps/s
	step: 366, time: 2758.653, speed: 0.133 steps/s
	step: 367, time: 2766.313, speed: 0.133 steps/s
	step: 368, time: 2773.444, speed: 0.133 steps/s
	step: 369, time: 2780.391, speed: 0.133 steps/s
	step: 370, time: 2787.919, speed: 0.133 steps/s
	step: 371, time: 2795.048, speed: 0.133 steps/s
	step: 372, time: 2803.209, speed: 0.133 steps/s
	step: 373, time: 2810.400, speed: 0.133 steps/s
	step: 374, time: 2817.912, speed: 0.133 steps/s
	step: 375, time: 2826.557, speed: 0.133 steps/s
	step: 376, time: 2834.034, speed: 0.133 steps/s
	step: 377, time: 2841.753, speed: 0.133 steps/s
	step: 378, time: 2848.914, speed: 0.133 steps/s
	step: 379, time: 2856.470, speed: 0.133 steps/s
	step: 380, time: 2863.884, speed: 0.133 steps/s
	step: 381, time: 2871.454, speed: 0.133 steps/s
	step: 382, time: 2878.699, speed: 0.133 steps/s
	step: 383, time: 2886.256, speed: 0.133 steps/s
	step: 384, time: 2893.835, speed: 0.133 steps/s
	step: 385, time: 2901.119, speed: 0.133 steps/s
	step: 386, time: 2908.324, speed: 0.133 steps/s
	step: 387, time: 2915.821, speed: 0.133 steps/s
	step: 388, time: 2923.668, speed: 0.133 steps/s
	step: 389, time: 2930.915, speed: 0.133 steps/s
	step: 390, time: 2938.472, speed: 0.133 steps/s
	step: 391, time: 2946.136, speed: 0.133 steps/s
	step: 392, time: 2953.108, speed: 0.133 steps/s
	step: 393, time: 2960.645, speed: 0.133 steps/s
	step: 394, time: 2968.183, speed: 0.133 steps/s
	step: 395, time: 2975.784, speed: 0.133 steps/s
	step: 396, time: 2983.071, speed: 0.133 steps/s
	step: 397, time: 2990.874, speed: 0.133 steps/s
	step: 398, time: 2998.514, speed: 0.133 steps/s
	step: 399, time: 3005.597, speed: 0.133 steps/s
	step: 400, time: 3013.245, speed: 0.133 steps/s
	step: 401, time: 3020.677, speed: 0.133 steps/s
	step: 402, time: 3027.918, speed: 0.133 steps/s
	step: 403, time: 3035.214, speed: 0.133 steps/s
	step: 404, time: 3042.255, speed: 0.133 steps/s
	step: 405, time: 3049.392, speed: 0.133 steps/s
	step: 406, time: 3057.527, speed: 0.133 steps/s
	step: 407, time: 3064.719, speed: 0.133 steps/s
	step: 408, time: 3072.314, speed: 0.133 steps/s
	step: 409, time: 3080.048, speed: 0.133 steps/s
	step: 410, time: 3086.913, speed: 0.133 steps/s
	step: 411, time: 3094.374, speed: 0.133 steps/s
	step: 412, time: 3102.230, speed: 0.133 steps/s
	step: 413, time: 3109.591, speed: 0.133 steps/s
	step: 414, time: 3117.120, speed: 0.133 steps/s
	step: 415, time: 3125.441, speed: 0.133 steps/s
	step: 416, time: 3132.938, speed: 0.133 steps/s
	step: 417, time: 3140.992, speed: 0.133 steps/s
	step: 418, time: 3148.087, speed: 0.133 steps/s
	step: 419, time: 3155.408, speed: 0.133 steps/s
	step: 420, time: 3162.940, speed: 0.133 steps/s
	step: 421, time: 3170.547, speed: 0.133 steps/s
	step: 422, time: 3178.189, speed: 0.133 steps/s
	step: 423, time: 3185.643, speed: 0.133 steps/s
	step: 424, time: 3192.867, speed: 0.133 steps/s
	step: 425, time: 3200.164, speed: 0.133 steps/s
	step: 426, time: 3207.621, speed: 0.133 steps/s
	step: 427, time: 3215.140, speed: 0.133 steps/s
	step: 428, time: 3222.669, speed: 0.133 steps/s
	step: 429, time: 3230.617, speed: 0.133 steps/s
	step: 430, time: 3238.266, speed: 0.133 steps/s
	step: 431, time: 3246.192, speed: 0.133 steps/s
	step: 432, time: 3253.850, speed: 0.133 steps/s
	step: 433, time: 3261.335, speed: 0.133 steps/s
	step: 434, time: 3268.891, speed: 0.133 steps/s
	step: 435, time: 3276.332, speed: 0.133 steps/s
	step: 436, time: 3283.635, speed: 0.133 steps/s
	step: 437, time: 3291.062, speed: 0.133 steps/s
	step: 438, time: 3298.256, speed: 0.133 steps/s
	step: 439, time: 3305.864, speed: 0.133 steps/s
	step: 440, time: 3313.603, speed: 0.133 steps/s
	step: 441, time: 3320.872, speed: 0.133 steps/s
	step: 442, time: 3328.838, speed: 0.133 steps/s
	step: 443, time: 3336.128, speed: 0.133 steps/s
	step: 444, time: 3343.690, speed: 0.133 steps/s
	step: 445, time: 3350.588, speed: 0.133 steps/s
	step: 446, time: 3358.577, speed: 0.133 steps/s
	step: 447, time: 3366.069, speed: 0.133 steps/s
	step: 448, time: 3373.472, speed: 0.133 steps/s
	step: 449, time: 3382.196, speed: 0.133 steps/s
	step: 450, time: 3390.219, speed: 0.133 steps/s
	step: 451, time: 3397.564, speed: 0.133 steps/s
	step: 452, time: 3405.122, speed: 0.133 steps/s
	step: 453, time: 3412.233, speed: 0.133 steps/s
	step: 454, time: 3419.721, speed: 0.133 steps/s
	step: 455, time: 3426.700, speed: 0.133 steps/s
	step: 456, time: 3434.640, speed: 0.133 steps/s
	step: 457, time: 3442.441, speed: 0.133 steps/s
	step: 458, time: 3451.128, speed: 0.133 steps/s
	step: 459, time: 3458.541, speed: 0.133 steps/s
	step: 460, time: 3466.288, speed: 0.133 steps/s
	step: 461, time: 3473.689, speed: 0.133 steps/s
	step: 462, time: 3481.159, speed: 0.133 steps/s
	step: 463, time: 3488.133, speed: 0.133 steps/s
	step: 464, time: 3495.476, speed: 0.133 steps/s
	step: 465, time: 3502.548, speed: 0.133 steps/s
	step: 466, time: 3509.819, speed: 0.133 steps/s
	step: 467, time: 3517.015, speed: 0.133 steps/s
	step: 468, time: 3524.358, speed: 0.133 steps/s
	step: 469, time: 3531.620, speed: 0.133 steps/s
	step: 470, time: 3540.009, speed: 0.133 steps/s
Traceback (most recent call last):
  File "./infer.py", line 136, in <module>
    infer(args)
  File "./infer.py", line 83, in infer
    predictions = task.infer_step(model, data)
  File "/home/aistudio/ldk/Knover/tasks/task_base.py", line 43, in infer_step
    predictions = model.infer_step(inputs)
  File "/home/aistudio/ldk/Knover/models/plato.py", line 282, in infer_step
    return super(Plato, self).infer_step(inputs)
  File "/home/aistudio/ldk/Knover/models/unified_transformer.py", line 440, in infer_step
    predictions = self._run_generation(inputs)
  File "/home/aistudio/ldk/Knover/models/unified_transformer.py", line 397, in _run_generation
    return_numpy=False)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 266, in _execute
    fetch_vars = self.exe.run(program, feed, fetch_list, **kwargs)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1066, in run
    return_merged=return_merged)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1154, in _run_impl
    use_program_cache=use_program_cache)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1229, in _run_program
    fetch_var_name)
KeyboardInterrupt
{
  "is_distributed": true,
  "save_path": "./result",
  "infer_file": "data/test.txt",
  "output_name": "response",
  "log_steps": 1,
  "Model": {
    "model": "UnifiedTransformer",
    "config_path": "./config/12L.json",
    "init_checkpoint": "",
    "init_pretraining_params": "12L",
    "learning_rate": 1e-05,
    "warmup_steps": 0,
    "weight_decay": 0.0,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": false,
    "amp_loss_scaling": 12800,
    "max_seq_len": 256,
    "weight_sharing": true,
    "mem_efficient": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "latent_type_size": 20,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": 20,
    "topk": 5,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "DialogGeneration",
    "do_generation": "true",
    "is_cn": true,
    "nsp_inference_model_path": null,
    "nsp_attention_style": "bidirectional",
    "ranking_score": "decode_score"
  },
  "Reader": {
    "max_src_len": 128,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": false,
    "batch_size": 4,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  },
  "run_infer": true
}
W1026 08:51:37.879573   446 device_context.cc:252] Please NOTE: device: 0, CUDA Capability: 70, Driver API Version: 10.1, Runtime API Version: 9.0
W1026 08:51:37.884054   446 device_context.cc:260] device: 0, cuDNN Version: 7.6.
Load pretraining parameters from 12L.
	step: 1, time: 0.587, speed: 1.703 steps/s
	step: 2, time: 1.481, speed: 1.350 steps/s
	step: 3, time: 2.358, speed: 1.272 steps/s
	step: 4, time: 2.931, speed: 1.365 steps/s
	step: 5, time: 3.576, speed: 1.398 steps/s
	step: 6, time: 4.429, speed: 1.355 steps/s
	step: 7, time: 5.304, speed: 1.320 steps/s
	step: 8, time: 6.292, speed: 1.272 steps/s
	step: 9, time: 7.190, speed: 1.252 steps/s
	step: 10, time: 8.035, speed: 1.244 steps/s
	step: 11, time: 8.960, speed: 1.228 steps/s
	step: 12, time: 9.724, speed: 1.234 steps/s
	step: 13, time: 10.679, speed: 1.217 steps/s
	step: 14, time: 11.585, speed: 1.208 steps/s
	step: 15, time: 12.318, speed: 1.218 steps/s
	step: 16, time: 13.211, speed: 1.211 steps/s
	step: 17, time: 14.083, speed: 1.207 steps/s
	step: 18, time: 14.624, speed: 1.231 steps/s
	step: 19, time: 15.509, speed: 1.225 steps/s
	step: 20, time: 16.414, speed: 1.218 steps/s
	step: 21, time: 17.232, speed: 1.219 steps/s
	step: 22, time: 18.105, speed: 1.215 steps/s
	step: 23, time: 18.977, speed: 1.212 steps/s
	step: 24, time: 19.837, speed: 1.210 steps/s
	step: 25, time: 20.847, speed: 1.199 steps/s
	step: 26, time: 21.715, speed: 1.197 steps/s
	step: 27, time: 22.610, speed: 1.194 steps/s
	step: 28, time: 23.424, speed: 1.195 steps/s
	step: 29, time: 24.272, speed: 1.195 steps/s
	step: 30, time: 25.257, speed: 1.188 steps/s
	step: 31, time: 26.112, speed: 1.187 steps/s
	step: 32, time: 26.847, speed: 1.192 steps/s
	step: 33, time: 27.769, speed: 1.188 steps/s
	step: 34, time: 28.671, speed: 1.186 steps/s
	step: 35, time: 29.556, speed: 1.184 steps/s
	step: 36, time: 30.430, speed: 1.183 steps/s
	step: 37, time: 31.295, speed: 1.182 steps/s
	step: 38, time: 32.168, speed: 1.181 steps/s
	step: 39, time: 33.068, speed: 1.179 steps/s
	step: 40, time: 33.694, speed: 1.187 steps/s
	step: 41, time: 34.356, speed: 1.193 steps/s
	step: 42, time: 35.251, speed: 1.191 steps/s
	step: 43, time: 36.065, speed: 1.192 steps/s
	step: 44, time: 36.959, speed: 1.191 steps/s
	step: 45, time: 37.605, speed: 1.197 steps/s
	step: 46, time: 38.512, speed: 1.194 steps/s
	step: 47, time: 39.409, speed: 1.193 steps/s
	step: 48, time: 40.291, speed: 1.191 steps/s
	step: 49, time: 41.002, speed: 1.195 steps/s
	step: 50, time: 41.851, speed: 1.195 steps/s
	step: 51, time: 42.733, speed: 1.193 steps/s
	step: 52, time: 43.620, speed: 1.192 steps/s
	step: 53, time: 44.508, speed: 1.191 steps/s
	step: 54, time: 45.366, speed: 1.190 steps/s
	step: 55, time: 46.306, speed: 1.188 steps/s
	step: 56, time: 47.199, speed: 1.186 steps/s
	step: 57, time: 48.114, speed: 1.185 steps/s
	step: 58, time: 48.799, speed: 1.189 steps/s
	step: 59, time: 49.695, speed: 1.187 steps/s
	step: 60, time: 50.637, speed: 1.185 steps/s
	step: 61, time: 51.365, speed: 1.188 steps/s
	step: 62, time: 52.028, speed: 1.192 steps/s
	step: 63, time: 52.938, speed: 1.190 steps/s
	step: 64, time: 53.517, speed: 1.196 steps/s
	step: 65, time: 54.204, speed: 1.199 steps/s
	step: 66, time: 55.019, speed: 1.200 steps/s
	step: 67, time: 55.722, speed: 1.202 steps/s
	step: 68, time: 56.584, speed: 1.202 steps/s
	step: 69, time: 57.507, speed: 1.200 steps/s
	step: 70, time: 58.283, speed: 1.201 steps/s
	step: 71, time: 58.958, speed: 1.204 steps/s
	step: 72, time: 59.838, speed: 1.203 steps/s
	step: 73, time: 60.699, speed: 1.203 steps/s
	step: 74, time: 61.595, speed: 1.201 steps/s
	step: 75, time: 62.477, speed: 1.200 steps/s
	step: 76, time: 63.388, speed: 1.199 steps/s
	step: 77, time: 64.389, speed: 1.196 steps/s
	step: 78, time: 65.177, speed: 1.197 steps/s
	step: 79, time: 66.052, speed: 1.196 steps/s
	step: 80, time: 66.690, speed: 1.200 steps/s
	step: 81, time: 67.364, speed: 1.202 steps/s
	step: 82, time: 68.186, speed: 1.203 steps/s
	step: 83, time: 69.080, speed: 1.201 steps/s
	step: 84, time: 69.973, speed: 1.200 steps/s
	step: 85, time: 70.598, speed: 1.204 steps/s
	step: 86, time: 71.439, speed: 1.204 steps/s
	step: 87, time: 72.078, speed: 1.207 steps/s
	step: 88, time: 72.982, speed: 1.206 steps/s
	step: 89, time: 73.864, speed: 1.205 steps/s
	step: 90, time: 74.746, speed: 1.204 steps/s
	step: 91, time: 75.625, speed: 1.203 steps/s
	step: 92, time: 76.253, speed: 1.207 steps/s
	step: 93, time: 76.972, speed: 1.208 steps/s
	step: 94, time: 78.030, speed: 1.205 steps/s
	step: 95, time: 78.913, speed: 1.204 steps/s
	step: 96, time: 79.476, speed: 1.208 steps/s
	step: 97, time: 80.375, speed: 1.207 steps/s
	step: 98, time: 81.068, speed: 1.209 steps/s
	step: 99, time: 81.933, speed: 1.208 steps/s
	step: 100, time: 82.520, speed: 1.212 steps/s
	step: 101, time: 83.400, speed: 1.211 steps/s
	step: 102, time: 84.183, speed: 1.212 steps/s
	step: 103, time: 85.071, speed: 1.211 steps/s
	step: 104, time: 85.944, speed: 1.210 steps/s
	step: 105, time: 86.832, speed: 1.209 steps/s
	step: 106, time: 87.690, speed: 1.209 steps/s
	step: 107, time: 88.345, speed: 1.211 steps/s
	step: 108, time: 89.241, speed: 1.210 steps/s
	step: 109, time: 90.146, speed: 1.209 steps/s
	step: 110, time: 91.045, speed: 1.208 steps/s
	step: 111, time: 92.044, speed: 1.206 steps/s
	step: 112, time: 92.911, speed: 1.205 steps/s
	step: 113, time: 93.435, speed: 1.209 steps/s
	step: 114, time: 94.320, speed: 1.209 steps/s
	step: 115, time: 95.092, speed: 1.209 steps/s
	step: 116, time: 95.927, speed: 1.209 steps/s
	step: 117, time: 96.828, speed: 1.208 steps/s
	step: 118, time: 97.509, speed: 1.210 steps/s
	step: 119, time: 98.384, speed: 1.210 steps/s
	step: 120, time: 99.295, speed: 1.209 steps/s
	step: 121, time: 100.190, speed: 1.208 steps/s
	step: 122, time: 101.039, speed: 1.207 steps/s
	step: 123, time: 101.942, speed: 1.207 steps/s
	step: 124, time: 102.799, speed: 1.206 steps/s
	step: 125, time: 103.691, speed: 1.206 steps/s
	step: 126, time: 104.549, speed: 1.205 steps/s
	step: 127, time: 105.360, speed: 1.205 steps/s
	step: 128, time: 106.388, speed: 1.203 steps/s
	step: 129, time: 106.985, speed: 1.206 steps/s
	step: 130, time: 107.836, speed: 1.206 steps/s
	step: 131, time: 108.756, speed: 1.205 steps/s
	step: 132, time: 109.661, speed: 1.204 steps/s
	step: 133, time: 110.532, speed: 1.203 steps/s
	step: 134, time: 111.404, speed: 1.203 steps/s
	step: 135, time: 112.299, speed: 1.202 steps/s
	step: 136, time: 113.214, speed: 1.201 steps/s
	step: 137, time: 114.039, speed: 1.201 steps/s
	step: 138, time: 114.907, speed: 1.201 steps/s
	step: 139, time: 115.597, speed: 1.202 steps/s
	step: 140, time: 116.414, speed: 1.203 steps/s
	step: 141, time: 117.183, speed: 1.203 steps/s
	step: 142, time: 118.076, speed: 1.203 steps/s
	step: 143, time: 118.958, speed: 1.202 steps/s
	step: 144, time: 119.732, speed: 1.203 steps/s
	step: 145, time: 120.758, speed: 1.201 steps/s
	step: 146, time: 121.390, speed: 1.203 steps/s
	step: 147, time: 122.322, speed: 1.202 steps/s
	step: 148, time: 123.095, speed: 1.202 steps/s
	step: 149, time: 123.989, speed: 1.202 steps/s
	step: 150, time: 124.883, speed: 1.201 steps/s
	step: 151, time: 125.672, speed: 1.202 steps/s
	step: 152, time: 126.364, speed: 1.203 steps/s
	step: 153, time: 127.259, speed: 1.202 steps/s
	step: 154, time: 127.774, speed: 1.205 steps/s
	step: 155, time: 128.669, speed: 1.205 steps/s
	step: 156, time: 129.593, speed: 1.204 steps/s
	step: 157, time: 130.538, speed: 1.203 steps/s
	step: 158, time: 131.392, speed: 1.203 steps/s
	step: 159, time: 132.257, speed: 1.202 steps/s
	step: 160, time: 132.983, speed: 1.203 steps/s
	step: 161, time: 133.631, speed: 1.205 steps/s
	step: 162, time: 134.706, speed: 1.203 steps/s
	step: 163, time: 135.570, speed: 1.202 steps/s
	step: 164, time: 136.472, speed: 1.202 steps/s
	step: 165, time: 137.352, speed: 1.201 steps/s
	step: 166, time: 138.231, speed: 1.201 steps/s
	step: 167, time: 139.020, speed: 1.201 steps/s
	step: 168, time: 139.922, speed: 1.201 steps/s
	step: 169, time: 140.672, speed: 1.201 steps/s
	step: 170, time: 141.303, speed: 1.203 steps/s
	step: 171, time: 141.897, speed: 1.205 steps/s
	step: 172, time: 142.767, speed: 1.205 steps/s
	step: 173, time: 143.529, speed: 1.205 steps/s
	step: 174, time: 144.339, speed: 1.205 steps/s
	step: 175, time: 145.177, speed: 1.205 steps/s
	step: 176, time: 145.867, speed: 1.207 steps/s
	step: 177, time: 146.759, speed: 1.206 steps/s
	step: 178, time: 147.571, speed: 1.206 steps/s
	step: 179, time: 148.596, speed: 1.205 steps/s
	step: 180, time: 149.313, speed: 1.206 steps/s
	step: 181, time: 150.239, speed: 1.205 steps/s
	step: 182, time: 151.125, speed: 1.204 steps/s
	step: 183, time: 151.986, speed: 1.204 steps/s
	step: 184, time: 152.777, speed: 1.204 steps/s
	step: 185, time: 153.408, speed: 1.206 steps/s
	step: 186, time: 154.201, speed: 1.206 steps/s
	step: 187, time: 155.069, speed: 1.206 steps/s
	step: 188, time: 155.888, speed: 1.206 steps/s
	step: 189, time: 156.762, speed: 1.206 steps/s
	step: 190, time: 157.465, speed: 1.207 steps/s
	step: 191, time: 158.365, speed: 1.206 steps/s
	step: 192, time: 159.195, speed: 1.206 steps/s
	step: 193, time: 160.094, speed: 1.206 steps/s
	step: 194, time: 160.982, speed: 1.205 steps/s
	step: 195, time: 161.887, speed: 1.205 steps/s
	step: 196, time: 162.961, speed: 1.203 steps/s
	step: 197, time: 163.732, speed: 1.203 steps/s
	step: 198, time: 164.615, speed: 1.203 steps/s
	step: 199, time: 165.351, speed: 1.203 steps/s
	step: 200, time: 166.241, speed: 1.203 steps/s
	step: 201, time: 167.118, speed: 1.203 steps/s
	step: 202, time: 167.975, speed: 1.203 steps/s
	step: 203, time: 168.851, speed: 1.202 steps/s
	step: 204, time: 169.736, speed: 1.202 steps/s
	step: 205, time: 170.515, speed: 1.202 steps/s
	step: 206, time: 171.391, speed: 1.202 steps/s
	step: 207, time: 172.248, speed: 1.202 steps/s
	step: 208, time: 173.207, speed: 1.201 steps/s
	step: 209, time: 174.134, speed: 1.200 steps/s
	step: 210, time: 175.030, speed: 1.200 steps/s
	step: 211, time: 175.814, speed: 1.200 steps/s
	step: 212, time: 176.400, speed: 1.202 steps/s
	step: 213, time: 177.402, speed: 1.201 steps/s
	step: 214, time: 178.079, speed: 1.202 steps/s
	step: 215, time: 178.666, speed: 1.203 steps/s
	step: 216, time: 179.443, speed: 1.204 steps/s
	step: 217, time: 180.343, speed: 1.203 steps/s
	step: 218, time: 181.121, speed: 1.204 steps/s
	step: 219, time: 182.012, speed: 1.203 steps/s
	step: 220, time: 182.759, speed: 1.204 steps/s
	step: 221, time: 183.518, speed: 1.204 steps/s
	step: 222, time: 184.379, speed: 1.204 steps/s
	step: 223, time: 185.284, speed: 1.204 steps/s
	step: 224, time: 186.189, speed: 1.203 steps/s
	step: 225, time: 187.027, speed: 1.203 steps/s
	step: 226, time: 187.621, speed: 1.205 steps/s
	step: 227, time: 188.497, speed: 1.204 steps/s
	step: 228, time: 189.383, speed: 1.204 steps/s
	step: 229, time: 190.248, speed: 1.204 steps/s
	step: 230, time: 191.130, speed: 1.203 steps/s
	step: 231, time: 192.028, speed: 1.203 steps/s
	step: 232, time: 192.711, speed: 1.204 steps/s
	step: 233, time: 193.547, speed: 1.204 steps/s
	step: 234, time: 194.408, speed: 1.204 steps/s
	step: 235, time: 195.196, speed: 1.204 steps/s
	step: 236, time: 195.903, speed: 1.205 steps/s
	step: 237, time: 196.759, speed: 1.205 steps/s
	step: 238, time: 197.644, speed: 1.204 steps/s
	step: 239, time: 198.503, speed: 1.204 steps/s
	step: 240, time: 199.370, speed: 1.204 steps/s
	step: 241, time: 200.261, speed: 1.203 steps/s
	step: 242, time: 201.117, speed: 1.203 steps/s
	step: 243, time: 201.993, speed: 1.203 steps/s
	step: 244, time: 202.877, speed: 1.203 steps/s
	step: 245, time: 203.748, speed: 1.202 steps/s
	step: 246, time: 204.423, speed: 1.203 steps/s
	step: 247, time: 205.323, speed: 1.203 steps/s
	step: 248, time: 206.359, speed: 1.202 steps/s
	step: 249, time: 207.260, speed: 1.201 steps/s
	step: 250, time: 208.140, speed: 1.201 steps/s
	step: 251, time: 209.019, speed: 1.201 steps/s
	step: 252, time: 209.750, speed: 1.201 steps/s
	step: 253, time: 210.567, speed: 1.202 steps/s
	step: 254, time: 211.428, speed: 1.201 steps/s
	step: 255, time: 212.286, speed: 1.201 steps/s
	step: 256, time: 212.904, speed: 1.202 steps/s
	step: 257, time: 213.700, speed: 1.203 steps/s
	step: 258, time: 214.572, speed: 1.202 steps/s
	step: 259, time: 215.459, speed: 1.202 steps/s
	step: 260, time: 216.318, speed: 1.202 steps/s
	step: 261, time: 217.208, speed: 1.202 steps/s
	step: 262, time: 218.108, speed: 1.201 steps/s
	step: 263, time: 218.705, speed: 1.203 steps/s
	step: 264, time: 219.443, speed: 1.203 steps/s
	step: 265, time: 220.463, speed: 1.202 steps/s
	step: 266, time: 221.344, speed: 1.202 steps/s
	step: 267, time: 222.032, speed: 1.203 steps/s
	step: 268, time: 222.886, speed: 1.202 steps/s
	step: 269, time: 223.826, speed: 1.202 steps/s
	step: 270, time: 224.706, speed: 1.202 steps/s
	step: 271, time: 225.591, speed: 1.201 steps/s
	step: 272, time: 226.498, speed: 1.201 steps/s
	step: 273, time: 227.322, speed: 1.201 steps/s
	step: 274, time: 227.989, speed: 1.202 steps/s
	step: 275, time: 228.865, speed: 1.202 steps/s
	step: 276, time: 229.760, speed: 1.201 steps/s
	step: 277, time: 230.630, speed: 1.201 steps/s
	step: 278, time: 231.423, speed: 1.201 steps/s
	step: 279, time: 231.959, speed: 1.203 steps/s
	step: 280, time: 232.700, speed: 1.203 steps/s
	step: 281, time: 233.201, speed: 1.205 steps/s
	step: 282, time: 234.111, speed: 1.205 steps/s
	step: 283, time: 234.988, speed: 1.204 steps/s
	step: 284, time: 235.875, speed: 1.204 steps/s
	step: 285, time: 236.648, speed: 1.204 steps/s
	step: 286, time: 237.359, speed: 1.205 steps/s
	step: 287, time: 238.074, speed: 1.206 steps/s
	step: 288, time: 238.948, speed: 1.205 steps/s
	step: 289, time: 239.753, speed: 1.205 steps/s
	step: 290, time: 240.629, speed: 1.205 steps/s
	step: 291, time: 241.388, speed: 1.206 steps/s
	step: 292, time: 242.257, speed: 1.205 steps/s
	step: 293, time: 243.179, speed: 1.205 steps/s
	step: 294, time: 244.103, speed: 1.204 steps/s
	step: 295, time: 245.000, speed: 1.204 steps/s
	step: 296, time: 245.762, speed: 1.204 steps/s
	step: 297, time: 246.636, speed: 1.204 steps/s
	step: 298, time: 247.496, speed: 1.204 steps/s
	step: 299, time: 248.424, speed: 1.204 steps/s
	step: 300, time: 249.085, speed: 1.204 steps/s
	step: 301, time: 249.956, speed: 1.204 steps/s
	step: 302, time: 250.854, speed: 1.204 steps/s
	step: 303, time: 251.735, speed: 1.204 steps/s
	step: 304, time: 252.210, speed: 1.205 steps/s
	step: 305, time: 253.091, speed: 1.205 steps/s
	step: 306, time: 253.843, speed: 1.205 steps/s
	step: 307, time: 254.575, speed: 1.206 steps/s
	step: 308, time: 255.486, speed: 1.206 steps/s
	step: 309, time: 256.304, speed: 1.206 steps/s
	step: 310, time: 257.001, speed: 1.206 steps/s
	step: 311, time: 257.892, speed: 1.206 steps/s
	step: 312, time: 258.739, speed: 1.206 steps/s
	step: 313, time: 259.595, speed: 1.206 steps/s
	step: 314, time: 260.440, speed: 1.206 steps/s
	step: 315, time: 261.067, speed: 1.207 steps/s
	step: 316, time: 262.109, speed: 1.206 steps/s
	step: 317, time: 262.949, speed: 1.206 steps/s
	step: 318, time: 263.768, speed: 1.206 steps/s
	step: 319, time: 264.671, speed: 1.205 steps/s
	step: 320, time: 265.522, speed: 1.205 steps/s
	step: 321, time: 266.428, speed: 1.205 steps/s
	step: 322, time: 267.274, speed: 1.205 steps/s
	step: 323, time: 268.114, speed: 1.205 steps/s
	step: 324, time: 268.949, speed: 1.205 steps/s
	step: 325, time: 269.695, speed: 1.205 steps/s
	step: 326, time: 270.583, speed: 1.205 steps/s
	step: 327, time: 271.468, speed: 1.205 steps/s
	step: 328, time: 272.332, speed: 1.204 steps/s
	step: 329, time: 273.205, speed: 1.204 steps/s
	step: 330, time: 274.102, speed: 1.204 steps/s
	step: 331, time: 274.803, speed: 1.204 steps/s
	step: 332, time: 275.620, speed: 1.205 steps/s
	step: 333, time: 276.637, speed: 1.204 steps/s
	step: 334, time: 277.194, speed: 1.205 steps/s
	step: 335, time: 278.140, speed: 1.204 steps/s
	step: 336, time: 279.087, speed: 1.204 steps/s
	step: 337, time: 279.861, speed: 1.204 steps/s
	step: 338, time: 280.731, speed: 1.204 steps/s
	step: 339, time: 281.573, speed: 1.204 steps/s
	step: 340, time: 282.507, speed: 1.204 steps/s
	step: 341, time: 283.404, speed: 1.203 steps/s
	step: 342, time: 284.159, speed: 1.204 steps/s
	step: 343, time: 285.011, speed: 1.203 steps/s
	step: 344, time: 285.809, speed: 1.204 steps/s
	step: 345, time: 286.545, speed: 1.204 steps/s
	step: 346, time: 287.203, speed: 1.205 steps/s
	step: 347, time: 287.998, speed: 1.205 steps/s
	step: 348, time: 288.637, speed: 1.206 steps/s
	step: 349, time: 289.501, speed: 1.206 steps/s
	step: 350, time: 290.408, speed: 1.205 steps/s
	step: 351, time: 291.227, speed: 1.205 steps/s
	step: 352, time: 292.143, speed: 1.205 steps/s
	step: 353, time: 292.760, speed: 1.206 steps/s
	step: 354, time: 293.671, speed: 1.205 steps/s
	step: 355, time: 294.588, speed: 1.205 steps/s
	step: 356, time: 295.274, speed: 1.206 steps/s
	step: 357, time: 295.963, speed: 1.206 steps/s
	step: 358, time: 296.957, speed: 1.206 steps/s
	step: 359, time: 297.858, speed: 1.205 steps/s
	step: 360, time: 298.522, speed: 1.206 steps/s
	step: 361, time: 299.258, speed: 1.206 steps/s
	step: 362, time: 299.928, speed: 1.207 steps/s
	step: 363, time: 300.822, speed: 1.207 steps/s
	step: 364, time: 301.503, speed: 1.207 steps/s
	step: 365, time: 302.398, speed: 1.207 steps/s
	step: 366, time: 303.290, speed: 1.207 steps/s
	step: 367, time: 304.321, speed: 1.206 steps/s
	step: 368, time: 305.127, speed: 1.206 steps/s
	step: 369, time: 305.659, speed: 1.207 steps/s
	step: 370, time: 306.536, speed: 1.207 steps/s
	step: 371, time: 307.281, speed: 1.207 steps/s
	step: 372, time: 308.198, speed: 1.207 steps/s
	step: 373, time: 308.790, speed: 1.208 steps/s
	step: 374, time: 309.674, speed: 1.208 steps/s
	step: 375, time: 310.609, speed: 1.207 steps/s
	step: 376, time: 311.430, speed: 1.207 steps/s
	step: 377, time: 312.256, speed: 1.207 steps/s
	step: 378, time: 313.031, speed: 1.208 steps/s
	step: 379, time: 313.666, speed: 1.208 steps/s
	step: 380, time: 314.242, speed: 1.209 steps/s
	step: 381, time: 315.136, speed: 1.209 steps/s
	step: 382, time: 316.008, speed: 1.209 steps/s
	step: 383, time: 316.842, speed: 1.209 steps/s
	step: 384, time: 317.870, speed: 1.208 steps/s
	step: 385, time: 318.756, speed: 1.208 steps/s
	step: 386, time: 319.650, speed: 1.208 steps/s
	step: 387, time: 320.539, speed: 1.207 steps/s
	step: 388, time: 321.423, speed: 1.207 steps/s
	step: 389, time: 322.287, speed: 1.207 steps/s
	step: 390, time: 322.926, speed: 1.208 steps/s
	step: 391, time: 323.785, speed: 1.208 steps/s
	step: 392, time: 324.277, speed: 1.209 steps/s
	step: 393, time: 324.964, speed: 1.209 steps/s
	step: 394, time: 325.711, speed: 1.210 steps/s
	step: 395, time: 326.564, speed: 1.210 steps/s
	step: 396, time: 327.216, speed: 1.210 steps/s
	step: 397, time: 328.111, speed: 1.210 steps/s
	step: 398, time: 328.895, speed: 1.210 steps/s
	step: 399, time: 329.579, speed: 1.211 steps/s
	step: 400, time: 330.483, speed: 1.210 steps/s
	step: 401, time: 331.512, speed: 1.210 steps/s
	step: 402, time: 332.389, speed: 1.209 steps/s
	step: 403, time: 333.147, speed: 1.210 steps/s
	step: 404, time: 333.701, speed: 1.211 steps/s
	step: 405, time: 334.560, speed: 1.211 steps/s
	step: 406, time: 335.381, speed: 1.211 steps/s
	step: 407, time: 335.983, speed: 1.211 steps/s
	step: 408, time: 336.895, speed: 1.211 steps/s
	step: 409, time: 337.772, speed: 1.211 steps/s
	step: 410, time: 338.579, speed: 1.211 steps/s
	step: 411, time: 339.459, speed: 1.211 steps/s
	step: 412, time: 340.360, speed: 1.210 steps/s
	step: 413, time: 341.231, speed: 1.210 steps/s
	step: 414, time: 342.092, speed: 1.210 steps/s
	step: 415, time: 342.991, speed: 1.210 steps/s
	step: 416, time: 343.647, speed: 1.211 steps/s
	step: 417, time: 344.544, speed: 1.210 steps/s
	step: 418, time: 345.444, speed: 1.210 steps/s
	step: 419, time: 346.496, speed: 1.209 steps/s
	step: 420, time: 347.376, speed: 1.209 steps/s
	step: 421, time: 348.181, speed: 1.209 steps/s
	step: 422, time: 348.887, speed: 1.210 steps/s
	step: 423, time: 349.624, speed: 1.210 steps/s
	step: 424, time: 350.291, speed: 1.210 steps/s
	step: 425, time: 351.160, speed: 1.210 steps/s
	step: 426, time: 352.029, speed: 1.210 steps/s
	step: 427, time: 352.784, speed: 1.210 steps/s
	step: 428, time: 353.665, speed: 1.210 steps/s
	step: 429, time: 354.402, speed: 1.210 steps/s
	step: 430, time: 355.312, speed: 1.210 steps/s
	step: 431, time: 356.213, speed: 1.210 steps/s
	step: 432, time: 357.041, speed: 1.210 steps/s
	step: 433, time: 357.890, speed: 1.210 steps/s
	step: 434, time: 358.725, speed: 1.210 steps/s
	step: 435, time: 359.513, speed: 1.210 steps/s
	step: 436, time: 360.637, speed: 1.209 steps/s
	step: 437, time: 361.394, speed: 1.209 steps/s
	step: 438, time: 362.154, speed: 1.209 steps/s
	step: 439, time: 363.056, speed: 1.209 steps/s
	step: 440, time: 363.926, speed: 1.209 steps/s
	step: 441, time: 364.505, speed: 1.210 steps/s
	step: 442, time: 365.114, speed: 1.211 steps/s
	step: 443, time: 365.883, speed: 1.211 steps/s
	step: 444, time: 366.662, speed: 1.211 steps/s
	step: 445, time: 367.348, speed: 1.211 steps/s
	step: 446, time: 368.244, speed: 1.211 steps/s
	step: 447, time: 369.131, speed: 1.211 steps/s
	step: 448, time: 370.016, speed: 1.211 steps/s
	step: 449, time: 370.816, speed: 1.211 steps/s
	step: 450, time: 371.692, speed: 1.211 steps/s
	step: 451, time: 372.211, speed: 1.212 steps/s
	step: 452, time: 373.098, speed: 1.211 steps/s
	step: 453, time: 374.073, speed: 1.211 steps/s
	step: 454, time: 374.953, speed: 1.211 steps/s
	step: 455, time: 375.837, speed: 1.211 steps/s
	step: 456, time: 376.742, speed: 1.210 steps/s
	step: 457, time: 377.653, speed: 1.210 steps/s
	step: 458, time: 378.583, speed: 1.210 steps/s
	step: 459, time: 379.393, speed: 1.210 steps/s
	step: 460, time: 380.169, speed: 1.210 steps/s
	step: 461, time: 381.040, speed: 1.210 steps/s
	step: 462, time: 381.927, speed: 1.210 steps/s
	step: 463, time: 382.796, speed: 1.210 steps/s
	step: 464, time: 383.668, speed: 1.209 steps/s
	step: 465, time: 384.468, speed: 1.209 steps/s
	step: 466, time: 385.167, speed: 1.210 steps/s
	step: 467, time: 386.006, speed: 1.210 steps/s
	step: 468, time: 386.897, speed: 1.210 steps/s
	step: 469, time: 387.750, speed: 1.210 steps/s
	step: 470, time: 388.554, speed: 1.210 steps/s
	step: 471, time: 389.439, speed: 1.209 steps/s
	step: 472, time: 390.305, speed: 1.209 steps/s
	step: 473, time: 391.004, speed: 1.210 steps/s
	step: 474, time: 391.879, speed: 1.210 steps/s
	step: 475, time: 392.763, speed: 1.209 steps/s
	step: 476, time: 393.635, speed: 1.209 steps/s
	step: 477, time: 394.550, speed: 1.209 steps/s
	step: 478, time: 395.432, speed: 1.209 steps/s
	step: 479, time: 396.153, speed: 1.209 steps/s
	step: 480, time: 397.043, speed: 1.209 steps/s
	step: 481, time: 397.756, speed: 1.209 steps/s
	step: 482, time: 398.639, speed: 1.209 steps/s
	step: 483, time: 399.516, speed: 1.209 steps/s
	step: 484, time: 400.371, speed: 1.209 steps/s
	step: 485, time: 401.251, speed: 1.209 steps/s
	step: 486, time: 401.923, speed: 1.209 steps/s
	step: 487, time: 402.685, speed: 1.209 steps/s
	step: 488, time: 403.578, speed: 1.209 steps/s
	step: 489, time: 404.232, speed: 1.210 steps/s
	step: 490, time: 404.954, speed: 1.210 steps/s
	step: 491, time: 405.844, speed: 1.210 steps/s
	step: 492, time: 406.742, speed: 1.210 steps/s
	step: 493, time: 407.401, speed: 1.210 steps/s
	step: 494, time: 408.246, speed: 1.210 steps/s
	step: 495, time: 408.836, speed: 1.211 steps/s
	step: 496, time: 409.461, speed: 1.211 steps/s
	step: 497, time: 410.350, speed: 1.211 steps/s
	step: 498, time: 410.979, speed: 1.212 steps/s
	step: 499, time: 411.616, speed: 1.212 steps/s
	step: 500, time: 412.529, speed: 1.212 steps/s
	step: 501, time: 413.361, speed: 1.212 steps/s
	step: 502, time: 414.242, speed: 1.212 steps/s
	step: 503, time: 415.165, speed: 1.212 steps/s
	step: 504, time: 415.958, speed: 1.212 steps/s
	step: 505, time: 416.834, speed: 1.212 steps/s
	step: 506, time: 417.335, speed: 1.212 steps/s
	step: 507, time: 418.259, speed: 1.212 steps/s
	step: 508, time: 419.131, speed: 1.212 steps/s
	step: 509, time: 420.032, speed: 1.212 steps/s
	step: 510, time: 420.987, speed: 1.211 steps/s
	step: 511, time: 421.671, speed: 1.212 steps/s
	step: 512, time: 422.702, speed: 1.211 steps/s
	step: 513, time: 423.688, speed: 1.211 steps/s
	step: 514, time: 424.264, speed: 1.212 steps/s
	step: 515, time: 425.002, speed: 1.212 steps/s
	step: 516, time: 425.691, speed: 1.212 steps/s
	step: 517, time: 426.208, speed: 1.213 steps/s
	step: 518, time: 427.080, speed: 1.213 steps/s
	step: 519, time: 427.951, speed: 1.213 steps/s
	step: 520, time: 428.727, speed: 1.213 steps/s
	step: 521, time: 429.503, speed: 1.213 steps/s
	step: 522, time: 430.232, speed: 1.213 steps/s
	step: 523, time: 431.155, speed: 1.213 steps/s
	step: 524, time: 432.007, speed: 1.213 steps/s
	step: 525, time: 432.905, speed: 1.213 steps/s
	step: 526, time: 433.638, speed: 1.213 steps/s
	step: 527, time: 434.514, speed: 1.213 steps/s
	step: 528, time: 435.401, speed: 1.213 steps/s
	step: 529, time: 436.331, speed: 1.212 steps/s
	step: 530, time: 437.210, speed: 1.212 steps/s
	step: 531, time: 438.102, speed: 1.212 steps/s
	step: 532, time: 438.730, speed: 1.213 steps/s
	step: 533, time: 439.585, speed: 1.213 steps/s
	step: 534, time: 440.262, speed: 1.213 steps/s
	step: 535, time: 441.143, speed: 1.213 steps/s
	step: 536, time: 441.977, speed: 1.213 steps/s
	step: 537, time: 442.862, speed: 1.213 steps/s
	step: 538, time: 443.919, speed: 1.212 steps/s
	step: 539, time: 444.661, speed: 1.212 steps/s
	step: 540, time: 445.580, speed: 1.212 steps/s
	step: 541, time: 446.480, speed: 1.212 steps/s
	step: 542, time: 447.346, speed: 1.212 steps/s
	step: 543, time: 448.256, speed: 1.211 steps/s
	step: 544, time: 449.173, speed: 1.211 steps/s
	step: 545, time: 450.016, speed: 1.211 steps/s
	step: 546, time: 450.933, speed: 1.211 steps/s
	step: 547, time: 451.832, speed: 1.211 steps/s
	step: 548, time: 452.588, speed: 1.211 steps/s
	step: 549, time: 453.353, speed: 1.211 steps/s
	step: 550, time: 454.236, speed: 1.211 steps/s
	step: 551, time: 455.123, speed: 1.211 steps/s
	step: 552, time: 455.847, speed: 1.211 steps/s
	step: 553, time: 456.737, speed: 1.211 steps/s
	step: 554, time: 457.533, speed: 1.211 steps/s
	step: 555, time: 458.563, speed: 1.210 steps/s
	step: 556, time: 459.228, speed: 1.211 steps/s
	step: 557, time: 460.032, speed: 1.211 steps/s
	step: 558, time: 460.868, speed: 1.211 steps/s
	step: 559, time: 461.660, speed: 1.211 steps/s
	step: 560, time: 462.566, speed: 1.211 steps/s
	step: 561, time: 463.434, speed: 1.211 steps/s
	step: 562, time: 464.308, speed: 1.210 steps/s
	step: 563, time: 465.204, speed: 1.210 steps/s
	step: 564, time: 466.076, speed: 1.210 steps/s
	step: 565, time: 466.949, speed: 1.210 steps/s
	step: 566, time: 467.666, speed: 1.210 steps/s
	step: 567, time: 468.412, speed: 1.210 steps/s
	step: 568, time: 469.301, speed: 1.210 steps/s
	step: 569, time: 470.200, speed: 1.210 steps/s
	step: 570, time: 471.075, speed: 1.210 steps/s
	step: 571, time: 471.810, speed: 1.210 steps/s
	step: 572, time: 472.830, speed: 1.210 steps/s
	step: 573, time: 473.725, speed: 1.210 steps/s
	step: 574, time: 474.593, speed: 1.209 steps/s
	step: 575, time: 475.478, speed: 1.209 steps/s
	step: 576, time: 476.338, speed: 1.209 steps/s
	step: 577, time: 477.107, speed: 1.209 steps/s
	step: 578, time: 477.977, speed: 1.209 steps/s
	step: 579, time: 478.851, speed: 1.209 steps/s
	step: 580, time: 479.735, speed: 1.209 steps/s
	step: 581, time: 480.504, speed: 1.209 steps/s
	step: 582, time: 481.270, speed: 1.209 steps/s
	step: 583, time: 481.931, speed: 1.210 steps/s
	step: 584, time: 482.798, speed: 1.210 steps/s
	step: 585, time: 483.594, speed: 1.210 steps/s
	step: 586, time: 484.497, speed: 1.210 steps/s
	step: 587, time: 485.390, speed: 1.209 steps/s
	step: 588, time: 486.266, speed: 1.209 steps/s
	step: 589, time: 487.318, speed: 1.209 steps/s
	step: 590, time: 488.062, speed: 1.209 steps/s
	step: 591, time: 488.817, speed: 1.209 steps/s
	step: 592, time: 489.708, speed: 1.209 steps/s
	step: 593, time: 490.450, speed: 1.209 steps/s
	step: 594, time: 491.151, speed: 1.209 steps/s
	step: 595, time: 492.041, speed: 1.209 steps/s
	step: 596, time: 492.677, speed: 1.210 steps/s
	step: 597, time: 493.436, speed: 1.210 steps/s
	step: 598, time: 494.280, speed: 1.210 steps/s
	step: 599, time: 495.200, speed: 1.210 steps/s
	step: 600, time: 496.073, speed: 1.209 steps/s
	step: 601, time: 496.667, speed: 1.210 steps/s
	step: 602, time: 497.534, speed: 1.210 steps/s
	step: 603, time: 498.396, speed: 1.210 steps/s
	step: 604, time: 499.056, speed: 1.210 steps/s
	step: 605, time: 499.575, speed: 1.211 steps/s
	step: 606, time: 500.457, speed: 1.211 steps/s
	step: 607, time: 501.219, speed: 1.211 steps/s
	step: 608, time: 502.104, speed: 1.211 steps/s
	step: 609, time: 503.002, speed: 1.211 steps/s
	step: 610, time: 503.650, speed: 1.211 steps/s
	step: 611, time: 504.522, speed: 1.211 steps/s
	step: 612, time: 505.223, speed: 1.211 steps/s
	step: 613, time: 506.129, speed: 1.211 steps/s
	step: 614, time: 507.009, speed: 1.211 steps/s
	step: 615, time: 507.926, speed: 1.211 steps/s
	step: 616, time: 508.533, speed: 1.211 steps/s
	step: 617, time: 509.444, speed: 1.211 steps/s
	step: 618, time: 510.075, speed: 1.212 steps/s
	step: 619, time: 510.965, speed: 1.211 steps/s
	step: 620, time: 511.870, speed: 1.211 steps/s
	step: 621, time: 512.775, speed: 1.211 steps/s
	step: 622, time: 513.649, speed: 1.211 steps/s
	step: 623, time: 514.542, speed: 1.211 steps/s
	step: 624, time: 515.577, speed: 1.210 steps/s
	step: 625, time: 516.394, speed: 1.210 steps/s
	step: 626, time: 517.270, speed: 1.210 steps/s
	step: 627, time: 517.974, speed: 1.210 steps/s
	step: 628, time: 518.729, speed: 1.211 steps/s
	step: 629, time: 519.642, speed: 1.210 steps/s
	step: 630, time: 520.286, speed: 1.211 steps/s
	step: 631, time: 520.838, speed: 1.212 steps/s
	step: 632, time: 521.737, speed: 1.211 steps/s
	step: 633, time: 522.434, speed: 1.212 steps/s
	step: 634, time: 523.324, speed: 1.211 steps/s
	step: 635, time: 524.125, speed: 1.212 steps/s
	step: 636, time: 525.017, speed: 1.211 steps/s
	step: 637, time: 525.885, speed: 1.211 steps/s
	step: 638, time: 526.805, speed: 1.211 steps/s
	step: 639, time: 527.688, speed: 1.211 steps/s
	step: 640, time: 528.577, speed: 1.211 steps/s
	step: 641, time: 529.610, speed: 1.210 steps/s
	step: 642, time: 530.400, speed: 1.210 steps/s
	step: 643, time: 531.086, speed: 1.211 steps/s
	step: 644, time: 531.874, speed: 1.211 steps/s
	step: 645, time: 532.697, speed: 1.211 steps/s
	step: 646, time: 533.424, speed: 1.211 steps/s
	step: 647, time: 534.303, speed: 1.211 steps/s
	step: 648, time: 535.177, speed: 1.211 steps/s
	step: 649, time: 536.066, speed: 1.211 steps/s
	step: 650, time: 536.984, speed: 1.210 steps/s
	step: 651, time: 537.576, speed: 1.211 steps/s
	step: 652, time: 538.440, speed: 1.211 steps/s
	step: 653, time: 539.190, speed: 1.211 steps/s
	step: 654, time: 540.094, speed: 1.211 steps/s
	step: 655, time: 540.788, speed: 1.211 steps/s
	step: 656, time: 541.681, speed: 1.211 steps/s
	step: 657, time: 542.576, speed: 1.211 steps/s
	step: 658, time: 543.631, speed: 1.210 steps/s
	step: 659, time: 544.504, speed: 1.210 steps/s
	step: 660, time: 545.388, speed: 1.210 steps/s
	step: 661, time: 546.268, speed: 1.210 steps/s
	step: 662, time: 547.034, speed: 1.210 steps/s
	step: 663, time: 547.817, speed: 1.210 steps/s
	step: 664, time: 548.664, speed: 1.210 steps/s
Traceback (most recent call last):
  File "./infer.py", line 136, in <module>
    infer(args)
  File "./infer.py", line 83, in infer
    predictions = task.infer_step(model, data)
  File "/home/aistudio/ldk/Knover/tasks/task_base.py", line 43, in infer_step
    predictions = model.infer_step(inputs)
  File "/home/aistudio/ldk/Knover/models/unified_transformer.py", line 440, in infer_step
    predictions = self._run_generation(inputs)
  File "/home/aistudio/ldk/Knover/models/unified_transformer.py", line 397, in _run_generation
    return_numpy=False)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 266, in _execute
    fetch_vars = self.exe.run(program, feed, fetch_list, **kwargs)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1066, in run
    return_merged=return_merged)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1154, in _run_impl
    use_program_cache=use_program_cache)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1229, in _run_program
    fetch_var_name)
KeyboardInterrupt
{
  "is_distributed": true,
  "save_path": "./result",
  "infer_file": "data/test.txt",
  "output_name": "response",
  "log_steps": 1,
  "Model": {
    "model": "UnifiedTransformer",
    "config_path": "./config/12L.json",
    "init_checkpoint": "",
    "init_pretraining_params": "12L",
    "learning_rate": 1e-05,
    "warmup_steps": 0,
    "weight_decay": 0.0,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": false,
    "amp_loss_scaling": 12800,
    "max_seq_len": 256,
    "weight_sharing": true,
    "mem_efficient": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "latent_type_size": 20,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": 20,
    "topk": 5,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "DialogGeneration",
    "do_generation": "true",
    "is_cn": true,
    "nsp_inference_model_path": null,
    "nsp_attention_style": "bidirectional",
    "ranking_score": "decode_score"
  },
  "Reader": {
    "max_src_len": 128,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": false,
    "batch_size": 40,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  },
  "run_infer": true
}
W1026 09:01:15.733470   995 device_context.cc:252] Please NOTE: device: 0, CUDA Capability: 70, Driver API Version: 10.1, Runtime API Version: 9.0
W1026 09:01:15.737912   995 device_context.cc:260] device: 0, cuDNN Version: 7.6.
Load pretraining parameters from 12L.
	step: 1, time: 3.578, speed: 0.280 steps/s
	step: 2, time: 7.408, speed: 0.270 steps/s
	step: 3, time: 11.207, speed: 0.268 steps/s
	step: 4, time: 14.781, speed: 0.271 steps/s
	step: 5, time: 18.589, speed: 0.269 steps/s
	step: 6, time: 22.310, speed: 0.269 steps/s
	step: 7, time: 25.916, speed: 0.270 steps/s
	step: 8, time: 29.436, speed: 0.272 steps/s
	step: 9, time: 33.185, speed: 0.271 steps/s
	step: 10, time: 36.778, speed: 0.272 steps/s
	step: 11, time: 40.402, speed: 0.272 steps/s
	step: 12, time: 43.950, speed: 0.273 steps/s
	step: 13, time: 47.388, speed: 0.274 steps/s
	step: 14, time: 50.915, speed: 0.275 steps/s
	step: 15, time: 54.658, speed: 0.274 steps/s
	step: 16, time: 58.262, speed: 0.275 steps/s
	step: 17, time: 62.036, speed: 0.274 steps/s
	step: 18, time: 65.680, speed: 0.274 steps/s
	step: 19, time: 69.397, speed: 0.274 steps/s
	step: 20, time: 72.982, speed: 0.274 steps/s
	step: 21, time: 76.477, speed: 0.275 steps/s
	step: 22, time: 80.011, speed: 0.275 steps/s
	step: 23, time: 83.657, speed: 0.275 steps/s
	step: 24, time: 87.024, speed: 0.276 steps/s
	step: 25, time: 90.622, speed: 0.276 steps/s
	step: 26, time: 94.013, speed: 0.277 steps/s
	step: 27, time: 97.502, speed: 0.277 steps/s
	step: 28, time: 101.090, speed: 0.277 steps/s
	step: 29, time: 104.742, speed: 0.277 steps/s
	step: 30, time: 108.569, speed: 0.276 steps/s
	step: 31, time: 112.137, speed: 0.276 steps/s
	step: 32, time: 115.589, speed: 0.277 steps/s
	step: 33, time: 119.160, speed: 0.277 steps/s
	step: 34, time: 122.806, speed: 0.277 steps/s
	step: 35, time: 126.180, speed: 0.277 steps/s
	step: 36, time: 130.163, speed: 0.277 steps/s
	step: 37, time: 133.754, speed: 0.277 steps/s
	step: 38, time: 137.473, speed: 0.276 steps/s
	step: 39, time: 141.064, speed: 0.276 steps/s
	step: 40, time: 144.447, speed: 0.277 steps/s
	step: 41, time: 147.869, speed: 0.277 steps/s
	step: 42, time: 151.410, speed: 0.277 steps/s
	step: 43, time: 155.060, speed: 0.277 steps/s
	step: 44, time: 158.720, speed: 0.277 steps/s
	step: 45, time: 162.737, speed: 0.277 steps/s
	step: 46, time: 166.497, speed: 0.276 steps/s
	step: 47, time: 170.196, speed: 0.276 steps/s
	step: 48, time: 173.926, speed: 0.276 steps/s
	step: 49, time: 177.305, speed: 0.276 steps/s
	step: 50, time: 180.786, speed: 0.277 steps/s
	step: 51, time: 184.198, speed: 0.277 steps/s
	step: 52, time: 187.550, speed: 0.277 steps/s
	step: 53, time: 191.126, speed: 0.277 steps/s
	step: 54, time: 194.563, speed: 0.278 steps/s
	step: 55, time: 198.368, speed: 0.277 steps/s
	step: 56, time: 201.876, speed: 0.277 steps/s
	step: 57, time: 205.270, speed: 0.278 steps/s
	step: 58, time: 208.601, speed: 0.278 steps/s
	step: 59, time: 212.533, speed: 0.278 steps/s
	step: 60, time: 215.939, speed: 0.278 steps/s
	step: 61, time: 219.377, speed: 0.278 steps/s
	step: 62, time: 223.045, speed: 0.278 steps/s
	step: 63, time: 226.454, speed: 0.278 steps/s
	step: 64, time: 230.143, speed: 0.278 steps/s
	step: 65, time: 233.865, speed: 0.278 steps/s
	step: 66, time: 237.538, speed: 0.278 steps/s
	step: 67, time: 241.221, speed: 0.278 steps/s
	step: 68, time: 244.690, speed: 0.278 steps/s
	step: 69, time: 248.173, speed: 0.278 steps/s
	step: 70, time: 251.808, speed: 0.278 steps/s
	step: 71, time: 255.323, speed: 0.278 steps/s
	step: 72, time: 258.814, speed: 0.278 steps/s
	step: 73, time: 262.249, speed: 0.278 steps/s
	step: 74, time: 265.624, speed: 0.279 steps/s
	step: 75, time: 269.331, speed: 0.278 steps/s
	step: 76, time: 272.765, speed: 0.279 steps/s
	step: 77, time: 276.357, speed: 0.279 steps/s
	step: 78, time: 279.809, speed: 0.279 steps/s
	step: 79, time: 283.247, speed: 0.279 steps/s
	step: 80, time: 286.857, speed: 0.279 steps/s
	step: 81, time: 290.839, speed: 0.279 steps/s
	step: 82, time: 294.407, speed: 0.279 steps/s
	step: 83, time: 297.684, speed: 0.279 steps/s
	step: 84, time: 301.259, speed: 0.279 steps/s
	step: 85, time: 305.019, speed: 0.279 steps/s
	step: 86, time: 308.621, speed: 0.279 steps/s
	step: 87, time: 312.077, speed: 0.279 steps/s
	step: 88, time: 315.578, speed: 0.279 steps/s
	step: 89, time: 319.170, speed: 0.279 steps/s
	step: 90, time: 322.742, speed: 0.279 steps/s
	step: 91, time: 326.617, speed: 0.279 steps/s
	step: 92, time: 329.940, speed: 0.279 steps/s
	step: 93, time: 333.397, speed: 0.279 steps/s
	step: 94, time: 336.957, speed: 0.279 steps/s
	step: 95, time: 340.364, speed: 0.279 steps/s
	step: 96, time: 343.975, speed: 0.279 steps/s
	step: 97, time: 347.442, speed: 0.279 steps/s
	step: 98, time: 351.163, speed: 0.279 steps/s
	step: 99, time: 354.731, speed: 0.279 steps/s
	step: 100, time: 358.181, speed: 0.279 steps/s
	step: 101, time: 361.624, speed: 0.279 steps/s
	step: 102, time: 365.253, speed: 0.279 steps/s
	step: 103, time: 369.041, speed: 0.279 steps/s
	step: 104, time: 372.527, speed: 0.279 steps/s
	step: 105, time: 375.970, speed: 0.279 steps/s
	step: 106, time: 379.590, speed: 0.279 steps/s
	step: 107, time: 383.143, speed: 0.279 steps/s
	step: 108, time: 386.697, speed: 0.279 steps/s
	step: 109, time: 390.147, speed: 0.279 steps/s
	step: 110, time: 393.664, speed: 0.279 steps/s
	step: 111, time: 397.075, speed: 0.280 steps/s
	step: 112, time: 400.427, speed: 0.280 steps/s
	step: 113, time: 404.093, speed: 0.280 steps/s
	step: 114, time: 407.452, speed: 0.280 steps/s
	step: 115, time: 410.716, speed: 0.280 steps/s
	step: 116, time: 414.173, speed: 0.280 steps/s
	step: 117, time: 417.790, speed: 0.280 steps/s
	step: 118, time: 421.172, speed: 0.280 steps/s
	step: 119, time: 424.727, speed: 0.280 steps/s
	step: 120, time: 428.283, speed: 0.280 steps/s
	step: 121, time: 431.634, speed: 0.280 steps/s
	step: 122, time: 435.303, speed: 0.280 steps/s
	step: 123, time: 438.695, speed: 0.280 steps/s
	step: 124, time: 442.052, speed: 0.281 steps/s
	step: 125, time: 445.539, speed: 0.281 steps/s
	step: 126, time: 448.994, speed: 0.281 steps/s
	step: 127, time: 452.573, speed: 0.281 steps/s
	step: 128, time: 456.307, speed: 0.281 steps/s
	step: 129, time: 459.758, speed: 0.281 steps/s
	step: 130, time: 463.392, speed: 0.281 steps/s
	step: 131, time: 467.016, speed: 0.281 steps/s
	step: 132, time: 470.481, speed: 0.281 steps/s
	step: 133, time: 474.309, speed: 0.280 steps/s
	step: 134, time: 478.009, speed: 0.280 steps/s
	step: 135, time: 481.535, speed: 0.280 steps/s
	step: 136, time: 484.771, speed: 0.281 steps/s
	step: 137, time: 488.254, speed: 0.281 steps/s
	step: 138, time: 491.931, speed: 0.281 steps/s
	step: 139, time: 495.578, speed: 0.280 steps/s
	step: 140, time: 499.259, speed: 0.280 steps/s
	step: 141, time: 502.723, speed: 0.280 steps/s
	step: 142, time: 506.229, speed: 0.281 steps/s
	step: 143, time: 509.517, speed: 0.281 steps/s
	step: 144, time: 513.316, speed: 0.281 steps/s
	step: 145, time: 516.761, speed: 0.281 steps/s
	step: 146, time: 520.500, speed: 0.280 steps/s
	step: 147, time: 524.017, speed: 0.281 steps/s
	step: 148, time: 527.544, speed: 0.281 steps/s
	step: 149, time: 531.033, speed: 0.281 steps/s
	step: 150, time: 534.621, speed: 0.281 steps/s
	step: 151, time: 537.980, speed: 0.281 steps/s
	step: 152, time: 541.585, speed: 0.281 steps/s
	step: 153, time: 544.900, speed: 0.281 steps/s
	step: 154, time: 548.464, speed: 0.281 steps/s
	step: 155, time: 551.963, speed: 0.281 steps/s
	step: 156, time: 555.443, speed: 0.281 steps/s
	step: 157, time: 559.098, speed: 0.281 steps/s
	step: 158, time: 562.571, speed: 0.281 steps/s
	step: 159, time: 566.143, speed: 0.281 steps/s
	step: 160, time: 569.614, speed: 0.281 steps/s
	step: 161, time: 572.983, speed: 0.281 steps/s
	step: 162, time: 576.309, speed: 0.281 steps/s
	step: 163, time: 579.594, speed: 0.281 steps/s
	step: 164, time: 583.216, speed: 0.281 steps/s
	step: 165, time: 586.926, speed: 0.281 steps/s
	step: 166, time: 590.319, speed: 0.281 steps/s
	step: 167, time: 593.766, speed: 0.281 steps/s
	step: 168, time: 597.238, speed: 0.281 steps/s
	step: 169, time: 600.645, speed: 0.281 steps/s
	step: 170, time: 603.962, speed: 0.281 steps/s
	step: 171, time: 607.648, speed: 0.281 steps/s
	step: 172, time: 611.070, speed: 0.281 steps/s
	step: 173, time: 614.481, speed: 0.282 steps/s
	step: 174, time: 617.964, speed: 0.282 steps/s
	step: 175, time: 621.555, speed: 0.282 steps/s
	step: 176, time: 625.162, speed: 0.282 steps/s
	step: 177, time: 628.512, speed: 0.282 steps/s
	step: 178, time: 631.837, speed: 0.282 steps/s
	step: 179, time: 635.589, speed: 0.282 steps/s
	step: 180, time: 639.720, speed: 0.281 steps/s
	step: 181, time: 643.127, speed: 0.281 steps/s
	step: 182, time: 646.570, speed: 0.281 steps/s
	step: 183, time: 650.315, speed: 0.281 steps/s
	step: 184, time: 653.714, speed: 0.281 steps/s
	step: 185, time: 657.294, speed: 0.281 steps/s
	step: 186, time: 660.728, speed: 0.282 steps/s
	step: 187, time: 663.965, speed: 0.282 steps/s
	step: 188, time: 667.229, speed: 0.282 steps/s
	step: 189, time: 670.870, speed: 0.282 steps/s
	step: 190, time: 674.139, speed: 0.282 steps/s
	step: 191, time: 677.530, speed: 0.282 steps/s
	step: 192, time: 680.940, speed: 0.282 steps/s
	step: 193, time: 684.758, speed: 0.282 steps/s
	step: 194, time: 688.187, speed: 0.282 steps/s
	step: 195, time: 691.738, speed: 0.282 steps/s
	step: 196, time: 695.427, speed: 0.282 steps/s
	step: 197, time: 698.797, speed: 0.282 steps/s
	step: 198, time: 702.471, speed: 0.282 steps/s
	step: 199, time: 705.914, speed: 0.282 steps/s
	step: 200, time: 709.452, speed: 0.282 steps/s
	step: 201, time: 713.037, speed: 0.282 steps/s
	step: 202, time: 716.758, speed: 0.282 steps/s
	step: 203, time: 720.220, speed: 0.282 steps/s
	step: 204, time: 723.755, speed: 0.282 steps/s
	step: 205, time: 727.293, speed: 0.282 steps/s
	step: 206, time: 730.915, speed: 0.282 steps/s
	step: 207, time: 734.304, speed: 0.282 steps/s
	step: 208, time: 737.756, speed: 0.282 steps/s
	step: 209, time: 741.162, speed: 0.282 steps/s
	step: 210, time: 744.638, speed: 0.282 steps/s
	step: 211, time: 748.230, speed: 0.282 steps/s
	step: 212, time: 751.875, speed: 0.282 steps/s
	step: 213, time: 755.926, speed: 0.282 steps/s
	step: 214, time: 759.447, speed: 0.282 steps/s
	step: 215, time: 763.059, speed: 0.282 steps/s
	step: 216, time: 766.603, speed: 0.282 steps/s
	step: 217, time: 770.212, speed: 0.282 steps/s
	step: 218, time: 773.630, speed: 0.282 steps/s
	step: 219, time: 776.859, speed: 0.282 steps/s
	step: 220, time: 780.333, speed: 0.282 steps/s
	step: 221, time: 784.226, speed: 0.282 steps/s
	step: 222, time: 787.513, speed: 0.282 steps/s
	step: 223, time: 790.972, speed: 0.282 steps/s
	step: 224, time: 794.603, speed: 0.282 steps/s
	step: 225, time: 798.384, speed: 0.282 steps/s
	step: 226, time: 801.909, speed: 0.282 steps/s
	step: 227, time: 805.152, speed: 0.282 steps/s
	step: 228, time: 808.942, speed: 0.282 steps/s
	step: 229, time: 812.599, speed: 0.282 steps/s
	step: 230, time: 816.295, speed: 0.282 steps/s
	step: 231, time: 819.654, speed: 0.282 steps/s
	step: 232, time: 823.089, speed: 0.282 steps/s
	step: 233, time: 826.829, speed: 0.282 steps/s
	step: 234, time: 830.327, speed: 0.282 steps/s
	step: 235, time: 834.007, speed: 0.282 steps/s
	step: 236, time: 837.452, speed: 0.282 steps/s
	step: 237, time: 841.631, speed: 0.282 steps/s
	step: 238, time: 845.021, speed: 0.282 steps/s
	step: 239, time: 848.923, speed: 0.282 steps/s
	step: 240, time: 852.472, speed: 0.282 steps/s
	step: 241, time: 856.038, speed: 0.282 steps/s
	step: 242, time: 859.463, speed: 0.282 steps/s
	step: 243, time: 862.824, speed: 0.282 steps/s
	step: 244, time: 866.418, speed: 0.282 steps/s
	step: 245, time: 869.805, speed: 0.282 steps/s
	step: 246, time: 873.337, speed: 0.282 steps/s
	step: 247, time: 876.875, speed: 0.282 steps/s
	step: 248, time: 880.181, speed: 0.282 steps/s
	step: 249, time: 883.848, speed: 0.282 steps/s
	step: 250, time: 887.360, speed: 0.282 steps/s
W1026 09:16:20.563202   995 operator.cc:187] elementwise_add raises an exception paddle::memory::allocation::BadAlloc, 

--------------------------------------------
C++ Call Stacks (More useful to developers):
--------------------------------------------
0   std::string paddle::platform::GetTraceBackString<std::string>(std::string&&, char const*, int)
1   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
2   paddle::memory::allocation::AlignedAllocator::AllocateImpl(unsigned long)
3   paddle::memory::allocation::AutoGrowthBestFitAllocator::AllocateImpl(unsigned long)
4   paddle::memory::allocation::Allocator::Allocate(unsigned long)
5   paddle::memory::allocation::RetryAllocator::AllocateImpl(unsigned long)
6   paddle::memory::allocation::AllocatorFacade::Alloc(paddle::platform::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::AllocShared(paddle::platform::Place const&, unsigned long)
8   paddle::memory::AllocShared(paddle::platform::Place const&, unsigned long)
9   paddle::framework::Tensor::mutable_data(paddle::platform::Place const&, paddle::framework::proto::VarType_Type, unsigned long)
10  paddle::operators::ElementwiseAddKernel<paddle::platform::CUDADeviceContext, float>::Compute(paddle::framework::ExecutionContext const&) const
11  std::_Function_handler<void (paddle::framework::ExecutionContext const&), paddle::framework::OpKernelRegistrarFunctor<paddle::platform::CUDAPlace, false, 0ul, paddle::operators::ElementwiseAddKernel<paddle::platform::CUDADeviceContext, float>, paddle::operators::ElementwiseAddKernel<paddle::platform::CUDADeviceContext, double>, paddle::operators::ElementwiseAddKernel<paddle::platform::CUDADeviceContext, int>, paddle::operators::ElementwiseAddKernel<paddle::platform::CUDADeviceContext, long>, paddle::operators::ElementwiseAddKernel<paddle::platform::CUDADeviceContext, paddle::platform::float16> >::operator()(char const*, char const*, int) const::{lambda(paddle::framework::ExecutionContext const&)#1}>::_M_invoke(std::_Any_data const&, paddle::framework::ExecutionContext const&)
12  paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&, paddle::framework::RuntimeContext*) const
13  paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&) const
14  paddle::framework::OperatorBase::Run(paddle::framework::Scope const&, paddle::platform::Place const&)
15  paddle::framework::Executor::RunPartialPreparedContext(paddle::framework::ExecutorPrepareContext*, paddle::framework::Scope*, long, long, bool, bool, bool)
16  paddle::framework::Executor::RunPreparedContext(paddle::framework::ExecutorPrepareContext*, paddle::framework::Scope*, bool, bool, bool)
17  paddle::framework::Executor::Run(paddle::framework::ProgramDesc const&, paddle::framework::Scope*, int, bool, bool, std::vector<std::string, std::allocator<std::string> > const&, bool, bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 5.300939GB memory on GPU 0, available memory is only 4.233276GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 

 at (/paddle/paddle/fluid/memory/allocation/cuda_allocator.cc:69)
/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py:1070: UserWarning: The following exception is not an EOF exception.
  "The following exception is not an EOF exception.")
Traceback (most recent call last):
  File "./infer.py", line 136, in <module>
    infer(args)
  File "./infer.py", line 83, in infer
    predictions = task.infer_step(model, data)
  File "/home/aistudio/ldk/Knover/tasks/task_base.py", line 43, in infer_step
    predictions = model.infer_step(inputs)
  File "/home/aistudio/ldk/Knover/models/unified_transformer.py", line 440, in infer_step
    predictions = self._run_generation(inputs)
  File "/home/aistudio/ldk/Knover/models/unified_transformer.py", line 397, in _run_generation
    return_numpy=False)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 266, in _execute
    fetch_vars = self.exe.run(program, feed, fetch_list, **kwargs)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1071, in run
    six.reraise(*sys.exc_info())
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/six.py", line 703, in reraise
    raise value
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1066, in run
    return_merged=return_merged)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1154, in _run_impl
    use_program_cache=use_program_cache)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1229, in _run_program
    fetch_var_name)
RuntimeError: 

--------------------------------------------
C++ Call Stacks (More useful to developers):
--------------------------------------------
0   std::string paddle::platform::GetTraceBackString<std::string>(std::string&&, char const*, int)
1   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
2   paddle::memory::allocation::AlignedAllocator::AllocateImpl(unsigned long)
3   paddle::memory::allocation::AutoGrowthBestFitAllocator::AllocateImpl(unsigned long)
4   paddle::memory::allocation::Allocator::Allocate(unsigned long)
5   paddle::memory::allocation::RetryAllocator::AllocateImpl(unsigned long)
6   paddle::memory::allocation::AllocatorFacade::Alloc(paddle::platform::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::AllocShared(paddle::platform::Place const&, unsigned long)
8   paddle::memory::AllocShared(paddle::platform::Place const&, unsigned long)
9   paddle::framework::Tensor::mutable_data(paddle::platform::Place const&, paddle::framework::proto::VarType_Type, unsigned long)
10  paddle::operators::ElementwiseAddKernel<paddle::platform::CUDADeviceContext, float>::Compute(paddle::framework::ExecutionContext const&) const
11  std::_Function_handler<void (paddle::framework::ExecutionContext const&), paddle::framework::OpKernelRegistrarFunctor<paddle::platform::CUDAPlace, false, 0ul, paddle::operators::ElementwiseAddKernel<paddle::platform::CUDADeviceContext, float>, paddle::operators::ElementwiseAddKernel<paddle::platform::CUDADeviceContext, double>, paddle::operators::ElementwiseAddKernel<paddle::platform::CUDADeviceContext, int>, paddle::operators::ElementwiseAddKernel<paddle::platform::CUDADeviceContext, long>, paddle::operators::ElementwiseAddKernel<paddle::platform::CUDADeviceContext, paddle::platform::float16> >::operator()(char const*, char const*, int) const::{lambda(paddle::framework::ExecutionContext const&)#1}>::_M_invoke(std::_Any_data const&, paddle::framework::ExecutionContext const&)
12  paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&, paddle::framework::RuntimeContext*) const
13  paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&) const
14  paddle::framework::OperatorBase::Run(paddle::framework::Scope const&, paddle::platform::Place const&)
15  paddle::framework::Executor::RunPartialPreparedContext(paddle::framework::ExecutorPrepareContext*, paddle::framework::Scope*, long, long, bool, bool, bool)
16  paddle::framework::Executor::RunPreparedContext(paddle::framework::ExecutorPrepareContext*, paddle::framework::Scope*, bool, bool, bool)
17  paddle::framework::Executor::Run(paddle::framework::ProgramDesc const&, paddle::framework::Scope*, int, bool, bool, std::vector<std::string, std::allocator<std::string> > const&, bool, bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 5.300939GB memory on GPU 0, available memory is only 4.233276GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 

 at (/paddle/paddle/fluid/memory/allocation/cuda_allocator.cc:69)

{
  "is_distributed": true,
  "save_path": "./result",
  "infer_file": "data/test.txt",
  "output_name": "response",
  "log_steps": 1,
  "Model": {
    "model": "UnifiedTransformer",
    "config_path": "./config/12L.json",
    "init_checkpoint": "",
    "init_pretraining_params": "12L",
    "learning_rate": 1e-05,
    "warmup_steps": 0,
    "weight_decay": 0.0,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": false,
    "amp_loss_scaling": 12800,
    "max_seq_len": 256,
    "weight_sharing": true,
    "mem_efficient": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "latent_type_size": 20,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": 20,
    "topk": 5,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "DialogGeneration",
    "do_generation": "true",
    "is_cn": true,
    "nsp_inference_model_path": null,
    "nsp_attention_style": "bidirectional",
    "ranking_score": "decode_score"
  },
  "Reader": {
    "max_src_len": 128,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": false,
    "batch_size": 20,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  },
  "run_infer": true
}
W1026 09:29:52.623944  2914 device_context.cc:252] Please NOTE: device: 0, CUDA Capability: 70, Driver API Version: 10.1, Runtime API Version: 9.0
W1026 09:29:52.628582  2914 device_context.cc:260] device: 0, cuDNN Version: 7.6.
Load pretraining parameters from 12L.
	step: 1, time: 1.984, speed: 0.504 steps/s
	step: 2, time: 3.961, speed: 0.505 steps/s
	step: 3, time: 5.976, speed: 0.502 steps/s
	step: 4, time: 8.139, speed: 0.491 steps/s
	step: 5, time: 10.153, speed: 0.492 steps/s
	step: 6, time: 12.409, speed: 0.484 steps/s
	step: 7, time: 14.543, speed: 0.481 steps/s
	step: 8, time: 16.555, speed: 0.483 steps/s
	step: 9, time: 18.714, speed: 0.481 steps/s
	step: 10, time: 20.757, speed: 0.482 steps/s
	step: 11, time: 22.763, speed: 0.483 steps/s
	step: 12, time: 24.830, speed: 0.483 steps/s
	step: 13, time: 26.784, speed: 0.485 steps/s
	step: 14, time: 28.991, speed: 0.483 steps/s
	step: 15, time: 30.949, speed: 0.485 steps/s
	step: 16, time: 33.089, speed: 0.484 steps/s
	step: 17, time: 35.095, speed: 0.484 steps/s
	step: 18, time: 37.212, speed: 0.484 steps/s
	step: 19, time: 39.251, speed: 0.484 steps/s
	step: 20, time: 41.260, speed: 0.485 steps/s
	step: 21, time: 43.278, speed: 0.485 steps/s
	step: 22, time: 45.437, speed: 0.484 steps/s
	step: 23, time: 47.487, speed: 0.484 steps/s
	step: 24, time: 49.521, speed: 0.485 steps/s
	step: 25, time: 51.433, speed: 0.486 steps/s
	step: 26, time: 53.413, speed: 0.487 steps/s
	step: 27, time: 55.408, speed: 0.487 steps/s
	step: 28, time: 57.459, speed: 0.487 steps/s
	step: 29, time: 59.523, speed: 0.487 steps/s
	step: 30, time: 61.532, speed: 0.488 steps/s
	step: 31, time: 63.489, speed: 0.488 steps/s
	step: 32, time: 65.526, speed: 0.488 steps/s
	step: 33, time: 67.591, speed: 0.488 steps/s
	step: 34, time: 69.571, speed: 0.489 steps/s
	step: 35, time: 71.318, speed: 0.491 steps/s
Traceback (most recent call last):
  File "./infer.py", line 138, in <module>
    infer(args)
  File "./infer.py", line 83, in infer
    predictions = task.infer_step(model, data)
  File "/home/aistudio/ldk/Knover/tasks/task_base.py", line 43, in infer_step
    predictions = model.infer_step(inputs)
  File "/home/aistudio/ldk/Knover/models/unified_transformer.py", line 440, in infer_step
    predictions = self._run_generation(inputs)
  File "/home/aistudio/ldk/Knover/models/unified_transformer.py", line 397, in _run_generation
    return_numpy=False)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 266, in _execute
    fetch_vars = self.exe.run(program, feed, fetch_list, **kwargs)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1066, in run
    return_merged=return_merged)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1154, in _run_impl
    use_program_cache=use_program_cache)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1229, in _run_program
    fetch_var_name)
KeyboardInterrupt
{
  "is_distributed": true,
  "save_path": "./result",
  "infer_file": "data/test.txt",
  "output_name": "response",
  "log_steps": 1,
  "Model": {
    "model": "UnifiedTransformer",
    "config_path": "./config/12L.json",
    "init_checkpoint": "",
    "init_pretraining_params": "12L",
    "learning_rate": 1e-05,
    "warmup_steps": 0,
    "weight_decay": 0.0,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": false,
    "amp_loss_scaling": 12800,
    "max_seq_len": 256,
    "weight_sharing": true,
    "mem_efficient": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "latent_type_size": 20,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": 20,
    "topk": 5,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "DialogGeneration",
    "do_generation": "true",
    "is_cn": true,
    "nsp_inference_model_path": null,
    "nsp_attention_style": "bidirectional",
    "ranking_score": "decode_score"
  },
  "Reader": {
    "max_src_len": 128,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": false,
    "batch_size": 20,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  },
  "run_infer": true
}
W1026 09:32:19.397678  3099 device_context.cc:252] Please NOTE: device: 0, CUDA Capability: 70, Driver API Version: 10.1, Runtime API Version: 9.0
W1026 09:32:19.402148  3099 device_context.cc:260] device: 0, cuDNN Version: 7.6.
Load pretraining parameters from 12L.
	step: 1, time: 2.040, speed: 0.490 steps/s
	step: 2, time: 4.053, speed: 0.493 steps/s
	step: 3, time: 6.135, speed: 0.489 steps/s
	step: 4, time: 8.359, speed: 0.479 steps/s
	step: 5, time: 10.392, speed: 0.481 steps/s
	step: 6, time: 12.608, speed: 0.476 steps/s
	step: 7, time: 14.669, speed: 0.477 steps/s
	step: 8, time: 16.676, speed: 0.480 steps/s
	step: 9, time: 18.789, speed: 0.479 steps/s
	step: 10, time: 20.899, speed: 0.478 steps/s
[infer] steps: 10 time cost: 20.89888095855713, speed: 0.47849451938743454 steps/s
save inference result into: ./result/inference_output.txt
{
  "is_distributed": true,
  "save_path": "./result",
  "infer_file": "data/test.txt",
  "output_name": "response",
  "log_steps": 1,
  "Model": {
    "model": "UnifiedTransformer",
    "config_path": "./config/12L.json",
    "init_checkpoint": "",
    "init_pretraining_params": "12L",
    "learning_rate": 1e-05,
    "warmup_steps": 0,
    "weight_decay": 0.0,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": false,
    "amp_loss_scaling": 12800,
    "max_seq_len": 256,
    "weight_sharing": true,
    "mem_efficient": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "latent_type_size": 20,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": 20,
    "topk": 5,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "DialogGeneration",
    "do_generation": "true",
    "is_cn": true,
    "nsp_inference_model_path": null,
    "nsp_attention_style": "bidirectional",
    "ranking_score": "decode_score"
  },
  "Reader": {
    "max_src_len": 128,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": false,
    "batch_size": 4,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  },
  "run_infer": true
}
W1026 09:41:05.066141  3463 device_context.cc:252] Please NOTE: device: 0, CUDA Capability: 70, Driver API Version: 10.1, Runtime API Version: 9.0
W1026 09:41:05.070601  3463 device_context.cc:260] device: 0, cuDNN Version: 7.6.
Load pretraining parameters from 12L.
	step: 1, time: 0.887, speed: 1.127 steps/s
	step: 2, time: 1.768, speed: 1.131 steps/s
	step: 3, time: 2.638, speed: 1.137 steps/s
	step: 4, time: 3.513, speed: 1.139 steps/s
	step: 5, time: 4.315, speed: 1.159 steps/s
	step: 6, time: 5.072, speed: 1.183 steps/s
	step: 7, time: 5.766, speed: 1.214 steps/s
	step: 8, time: 6.759, speed: 1.184 steps/s
	step: 9, time: 7.531, speed: 1.195 steps/s
	step: 10, time: 8.240, speed: 1.214 steps/s
[infer] steps: 10 time cost: 8.24044942855835, speed: 1.2135260445071963 steps/s
save inference result into: ./result/inference_output.txt
{
  "is_distributed": true,
  "save_path": "./result",
  "infer_file": "data/test.txt",
  "output_name": "response",
  "log_steps": 1,
  "Model": {
    "model": "UnifiedTransformer",
    "config_path": "./config/12L.json",
    "init_checkpoint": "",
    "init_pretraining_params": "12L",
    "learning_rate": 1e-05,
    "warmup_steps": 0,
    "weight_decay": 0.0,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": false,
    "amp_loss_scaling": 12800,
    "max_seq_len": 256,
    "weight_sharing": true,
    "mem_efficient": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "latent_type_size": 20,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": 20,
    "topk": 5,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "DialogGeneration",
    "do_generation": "true",
    "is_cn": true,
    "nsp_inference_model_path": null,
    "nsp_attention_style": "bidirectional",
    "ranking_score": "decode_score"
  },
  "Reader": {
    "max_src_len": 128,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": false,
    "batch_size": 4,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  },
  "run_infer": true
}
W1026 09:42:41.264159  3863 device_context.cc:252] Please NOTE: device: 0, CUDA Capability: 70, Driver API Version: 10.1, Runtime API Version: 9.0
W1026 09:42:41.268586  3863 device_context.cc:260] device: 0, cuDNN Version: 7.6.
Load pretraining parameters from 12L.
	step: 1, time: 0.783, speed: 1.278 steps/s
	step: 2, time: 1.320, speed: 1.515 steps/s
	step: 3, time: 2.200, speed: 1.363 steps/s
	step: 4, time: 2.925, speed: 1.368 steps/s
	step: 5, time: 3.747, speed: 1.334 steps/s
	step: 6, time: 4.547, speed: 1.320 steps/s
	step: 7, time: 5.268, speed: 1.329 steps/s
	step: 8, time: 6.184, speed: 1.294 steps/s
	step: 9, time: 7.087, speed: 1.270 steps/s
	step: 10, time: 7.978, speed: 1.253 steps/s
	step: 11, time: 8.886, speed: 1.238 steps/s
	step: 12, time: 9.425, speed: 1.273 steps/s
	step: 13, time: 10.344, speed: 1.257 steps/s
	step: 14, time: 11.210, speed: 1.249 steps/s
	step: 15, time: 12.078, speed: 1.242 steps/s
	step: 16, time: 12.962, speed: 1.234 steps/s
	step: 17, time: 13.614, speed: 1.249 steps/s
	step: 18, time: 14.305, speed: 1.258 steps/s
	step: 19, time: 15.161, speed: 1.253 steps/s
	step: 20, time: 16.057, speed: 1.246 steps/s
	step: 21, time: 16.840, speed: 1.247 steps/s
	step: 22, time: 17.517, speed: 1.256 steps/s
	step: 23, time: 18.372, speed: 1.252 steps/s
	step: 24, time: 19.248, speed: 1.247 steps/s
	step: 25, time: 20.053, speed: 1.247 steps/s
	step: 26, time: 20.934, speed: 1.242 steps/s
	step: 27, time: 21.766, speed: 1.240 steps/s
	step: 28, time: 22.668, speed: 1.235 steps/s
	step: 29, time: 23.551, speed: 1.231 steps/s
	step: 30, time: 24.491, speed: 1.225 steps/s
	step: 31, time: 25.362, speed: 1.222 steps/s
	step: 32, time: 26.039, speed: 1.229 steps/s
	step: 33, time: 26.916, speed: 1.226 steps/s
	step: 34, time: 27.815, speed: 1.222 steps/s
	step: 35, time: 28.501, speed: 1.228 steps/s
	step: 36, time: 29.380, speed: 1.225 steps/s
	step: 37, time: 30.250, speed: 1.223 steps/s
	step: 38, time: 31.099, speed: 1.222 steps/s
	step: 39, time: 31.986, speed: 1.219 steps/s
	step: 40, time: 32.866, speed: 1.217 steps/s
	step: 41, time: 33.745, speed: 1.215 steps/s
	step: 42, time: 34.622, speed: 1.213 steps/s
	step: 43, time: 35.623, speed: 1.207 steps/s
	step: 44, time: 36.555, speed: 1.204 steps/s
	step: 45, time: 37.248, speed: 1.208 steps/s
	step: 46, time: 37.902, speed: 1.214 steps/s
	step: 47, time: 38.796, speed: 1.211 steps/s
	step: 48, time: 39.654, speed: 1.210 steps/s
	step: 49, time: 40.568, speed: 1.208 steps/s
	step: 50, time: 41.384, speed: 1.208 steps/s
	step: 51, time: 42.109, speed: 1.211 steps/s
	step: 52, time: 42.708, speed: 1.218 steps/s
	step: 53, time: 43.316, speed: 1.224 steps/s
	step: 54, time: 44.114, speed: 1.224 steps/s
	step: 55, time: 45.023, speed: 1.222 steps/s
	step: 56, time: 45.584, speed: 1.228 steps/s
	step: 57, time: 46.484, speed: 1.226 steps/s
	step: 58, time: 47.359, speed: 1.225 steps/s
	step: 59, time: 48.247, speed: 1.223 steps/s
	step: 60, time: 49.280, speed: 1.218 steps/s
	step: 61, time: 50.108, speed: 1.217 steps/s
	step: 62, time: 50.974, speed: 1.216 steps/s
	step: 63, time: 51.851, speed: 1.215 steps/s
	step: 64, time: 52.713, speed: 1.214 steps/s
	step: 65, time: 53.538, speed: 1.214 steps/s
	step: 66, time: 54.315, speed: 1.215 steps/s
	step: 67, time: 55.199, speed: 1.214 steps/s
	step: 68, time: 56.065, speed: 1.213 steps/s
	step: 69, time: 56.977, speed: 1.211 steps/s
	step: 70, time: 57.836, speed: 1.210 steps/s
	step: 71, time: 58.617, speed: 1.211 steps/s
	step: 72, time: 59.153, speed: 1.217 steps/s
	step: 73, time: 59.934, speed: 1.218 steps/s
	step: 74, time: 60.672, speed: 1.220 steps/s
	step: 75, time: 61.553, speed: 1.218 steps/s
	step: 76, time: 62.453, speed: 1.217 steps/s
	step: 77, time: 63.441, speed: 1.214 steps/s
	step: 78, time: 64.170, speed: 1.216 steps/s
	step: 79, time: 65.041, speed: 1.215 steps/s
	step: 80, time: 65.886, speed: 1.214 steps/s
	step: 81, time: 66.749, speed: 1.214 steps/s
	step: 82, time: 67.643, speed: 1.212 steps/s
	step: 83, time: 68.525, speed: 1.211 steps/s
	step: 84, time: 69.411, speed: 1.210 steps/s
	step: 85, time: 70.171, speed: 1.211 steps/s
	step: 86, time: 71.054, speed: 1.210 steps/s
	step: 87, time: 71.809, speed: 1.212 steps/s
	step: 88, time: 72.652, speed: 1.211 steps/s
	step: 89, time: 73.554, speed: 1.210 steps/s
	step: 90, time: 74.414, speed: 1.209 steps/s
	step: 91, time: 75.295, speed: 1.209 steps/s
	step: 92, time: 76.171, speed: 1.208 steps/s
	step: 93, time: 77.052, speed: 1.207 steps/s
	step: 94, time: 78.076, speed: 1.204 steps/s
	step: 95, time: 78.957, speed: 1.203 steps/s
	step: 96, time: 79.607, speed: 1.206 steps/s
	step: 97, time: 80.359, speed: 1.207 steps/s
	step: 98, time: 80.981, speed: 1.210 steps/s
	step: 99, time: 81.867, speed: 1.209 steps/s
	step: 100, time: 82.559, speed: 1.211 steps/s
[infer] steps: 100 time cost: 82.55881786346436, speed: 1.211257653487479 steps/s
save inference result into: ./result/inference_output.txt
{
  "is_distributed": true,
  "save_path": "./result",
  "infer_file": "data/test.txt",
  "output_name": "response",
  "log_steps": 1,
  "Model": {
    "model": "Plato",
    "config_path": "./config/12L.json",
    "init_checkpoint": "",
    "init_pretraining_params": "./output/step_13000",
    "learning_rate": 1e-05,
    "warmup_steps": 0,
    "weight_decay": 0.0,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": false,
    "amp_loss_scaling": 12800,
    "max_seq_len": 256,
    "weight_sharing": true,
    "mem_efficient": false,
    "use_bow": true,
    "use_entropy": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "latent_type_size": 20,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": 20,
    "topk": 5,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "DialogGeneration",
    "do_generation": "true",
    "is_cn": true,
    "nsp_inference_model_path": null,
    "nsp_attention_style": "bidirectional",
    "ranking_score": "decode_score"
  },
  "Reader": {
    "max_src_len": 128,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": false,
    "batch_size": 4,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  },
  "run_infer": true
}
{'is_distributed': True, 'save_path': './result', 'infer_file': 'data/test.txt', 'output_name': 'response', 'log_steps': 1, 'Model': {'model': 'Plato', 'config_path': './config/12L.json', 'init_checkpoint': '', 'init_pretraining_params': './output/step_13000', 'learning_rate': 1e-05, 'warmup_steps': 0, 'weight_decay': 0.0, 'max_grad_norm': 0.1, 'use_recompute': False, 'use_amp': False, 'amp_loss_scaling': 12800, 'max_seq_len': 256, 'weight_sharing': True, 'mem_efficient': False, 'use_bow': True, 'use_entropy': False, 'pre_encoder_cmd': 'd', 'preprocess_cmd': 'n', 'postprocess_cmd': 'da', 'post_cls_cmd': 'n', 'cls_bias': True, 'attention_probs_dropout_prob': 0.1, 'hidden_act': 'gelu', 'hidden_dropout_prob': 0.1, 'hidden_size': 768, 'initializer_range': 0.02, 'max_position_embeddings': 512, 'latent_type_size': 20, 'num_attention_heads': 12, 'num_hidden_layers': 12, 'type_vocab_size': 2, 'role_type_size': 32, 'vocab_size': 30004}, 'Generator': {'min_dec_len': 1, 'max_dec_len': 64, 'decoding_strategy': 'topk_sampling', 'temperature': 1.0, 'ignore_unk': True, 'num_samples': 20, 'topk': 5, 'topp': 0.9, 'beam_size': 10, 'length_average': True, 'length_penalty': 0.0}, 'Task': {'task': 'DialogGeneration', 'do_generation': 'true', 'is_cn': True, 'nsp_inference_model_path': None, 'nsp_attention_style': 'bidirectional', 'ranking_score': 'decode_score'}, 'Reader': {'max_src_len': 128, 'max_tgt_len': 128, 'truncate_first_turn': False, 'file_format': 'file', 'data_format': 'numerical', 'in_tokens': False, 'batch_size': 4, 'continuous_position': True, 'random_seed': 11, 'sort_pool_size': 65536}, 'Tokenizer': {'tokenizer': 'SentencePieceTokenizer', 'vocab_path': './config/vocab.txt', 'do_lower_case': False, 'spm_model_file': './config/spm.model'}, 'run_infer': True, 'pad_id': 0, 'bos_id': 1, 'eos_id': 2, 'unk_id': 0, 'mask_id': 30000}
W1026 09:45:05.859622  4227 device_context.cc:252] Please NOTE: device: 0, CUDA Capability: 70, Driver API Version: 10.1, Runtime API Version: 9.0
W1026 09:45:05.864130  4227 device_context.cc:260] device: 0, cuDNN Version: 7.6.
Load pretraining parameters from ./output/step_13000.
	step: 1, time: 7.043, speed: 0.142 steps/s
	step: 2, time: 13.969, speed: 0.143 steps/s
	step: 3, time: 20.516, speed: 0.146 steps/s
	step: 4, time: 27.212, speed: 0.147 steps/s
	step: 5, time: 33.673, speed: 0.148 steps/s
	step: 6, time: 40.713, speed: 0.147 steps/s
	step: 7, time: 47.279, speed: 0.148 steps/s
	step: 8, time: 54.293, speed: 0.147 steps/s
	step: 9, time: 61.228, speed: 0.147 steps/s
	step: 10, time: 68.308, speed: 0.146 steps/s
	step: 11, time: 76.022, speed: 0.145 steps/s
	step: 12, time: 82.267, speed: 0.146 steps/s
	step: 13, time: 89.217, speed: 0.146 steps/s
	step: 14, time: 96.223, speed: 0.145 steps/s
	step: 15, time: 103.183, speed: 0.145 steps/s
	step: 16, time: 110.874, speed: 0.144 steps/s
	step: 17, time: 117.495, speed: 0.145 steps/s
	step: 18, time: 124.245, speed: 0.145 steps/s
	step: 19, time: 131.396, speed: 0.145 steps/s
	step: 20, time: 138.891, speed: 0.144 steps/s
	step: 21, time: 145.225, speed: 0.145 steps/s
	step: 22, time: 152.363, speed: 0.144 steps/s
	step: 23, time: 159.129, speed: 0.145 steps/s
	step: 24, time: 166.119, speed: 0.144 steps/s
	step: 25, time: 173.144, speed: 0.144 steps/s
	step: 26, time: 179.873, speed: 0.145 steps/s
	step: 27, time: 186.560, speed: 0.145 steps/s
	step: 28, time: 193.560, speed: 0.145 steps/s
	step: 29, time: 200.471, speed: 0.145 steps/s
	step: 30, time: 208.384, speed: 0.144 steps/s
	step: 31, time: 215.367, speed: 0.144 steps/s
	step: 32, time: 222.260, speed: 0.144 steps/s
	step: 33, time: 229.579, speed: 0.144 steps/s
	step: 34, time: 236.638, speed: 0.144 steps/s
	step: 35, time: 243.266, speed: 0.144 steps/s
	step: 36, time: 249.787, speed: 0.144 steps/s
	step: 37, time: 256.494, speed: 0.144 steps/s
	step: 38, time: 262.872, speed: 0.145 steps/s
	step: 39, time: 270.197, speed: 0.144 steps/s
	step: 40, time: 277.213, speed: 0.144 steps/s
	step: 41, time: 284.143, speed: 0.144 steps/s
	step: 42, time: 291.278, speed: 0.144 steps/s
	step: 43, time: 298.606, speed: 0.144 steps/s
	step: 44, time: 305.667, speed: 0.144 steps/s
	step: 45, time: 312.672, speed: 0.144 steps/s
	step: 46, time: 319.492, speed: 0.144 steps/s
	step: 47, time: 326.504, speed: 0.144 steps/s
	step: 48, time: 333.749, speed: 0.144 steps/s
	step: 49, time: 340.949, speed: 0.144 steps/s
	step: 50, time: 347.802, speed: 0.144 steps/s
	step: 51, time: 354.168, speed: 0.144 steps/s
	step: 52, time: 361.116, speed: 0.144 steps/s
	step: 53, time: 368.328, speed: 0.144 steps/s
	step: 54, time: 375.277, speed: 0.144 steps/s
	step: 55, time: 382.201, speed: 0.144 steps/s
	step: 56, time: 388.959, speed: 0.144 steps/s
	step: 57, time: 396.616, speed: 0.144 steps/s
	step: 58, time: 403.559, speed: 0.144 steps/s
	step: 59, time: 410.589, speed: 0.144 steps/s
	step: 60, time: 417.863, speed: 0.144 steps/s
	step: 61, time: 424.844, speed: 0.144 steps/s
	step: 62, time: 431.641, speed: 0.144 steps/s
	step: 63, time: 438.463, speed: 0.144 steps/s
	step: 64, time: 445.189, speed: 0.144 steps/s
	step: 65, time: 451.723, speed: 0.144 steps/s
	step: 66, time: 458.852, speed: 0.144 steps/s
	step: 67, time: 465.388, speed: 0.144 steps/s
	step: 68, time: 472.276, speed: 0.144 steps/s
	step: 69, time: 479.659, speed: 0.144 steps/s
	step: 70, time: 487.293, speed: 0.144 steps/s
	step: 71, time: 494.792, speed: 0.143 steps/s
	step: 72, time: 501.272, speed: 0.144 steps/s
	step: 73, time: 507.822, speed: 0.144 steps/s
	step: 74, time: 514.624, speed: 0.144 steps/s
	step: 75, time: 521.430, speed: 0.144 steps/s
	step: 76, time: 528.699, speed: 0.144 steps/s
	step: 77, time: 535.629, speed: 0.144 steps/s
	step: 78, time: 542.701, speed: 0.144 steps/s
	step: 79, time: 550.014, speed: 0.144 steps/s
	step: 80, time: 556.500, speed: 0.144 steps/s
	step: 81, time: 563.660, speed: 0.144 steps/s
	step: 82, time: 570.520, speed: 0.144 steps/s
	step: 83, time: 577.385, speed: 0.144 steps/s
	step: 84, time: 584.206, speed: 0.144 steps/s
	step: 85, time: 591.423, speed: 0.144 steps/s
	step: 86, time: 598.427, speed: 0.144 steps/s
	step: 87, time: 605.225, speed: 0.144 steps/s
	step: 88, time: 612.991, speed: 0.144 steps/s
	step: 89, time: 619.702, speed: 0.144 steps/s
	step: 90, time: 626.999, speed: 0.144 steps/s
	step: 91, time: 634.133, speed: 0.144 steps/s
	step: 92, time: 640.974, speed: 0.144 steps/s
	step: 93, time: 647.978, speed: 0.144 steps/s
	step: 94, time: 654.890, speed: 0.144 steps/s
	step: 95, time: 661.859, speed: 0.144 steps/s
	step: 96, time: 668.053, speed: 0.144 steps/s
	step: 97, time: 675.140, speed: 0.144 steps/s
	step: 98, time: 682.375, speed: 0.144 steps/s
	step: 99, time: 689.487, speed: 0.144 steps/s
	step: 100, time: 696.272, speed: 0.144 steps/s
[infer] steps: 100 time cost: 696.272458076477, speed: 0.1436219382801096 steps/s
save inference result into: ./result/inference_output.txt
{
  "is_distributed": false,
  "save_path": "./result",
  "infer_file": "data/test.txt",
  "output_name": "response",
  "log_steps": 1,
  "Model": {
    "model": "UnifiedTransformer",
    "config_path": "./config/12L.json",
    "init_checkpoint": "",
    "init_pretraining_params": "12L",
    "learning_rate": 1e-05,
    "warmup_steps": 0,
    "weight_decay": 0.0,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": false,
    "amp_loss_scaling": 12800,
    "max_seq_len": 256,
    "weight_sharing": true,
    "mem_efficient": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "latent_type_size": 20,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": 20,
    "topk": 5,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "DialogGeneration",
    "do_generation": "true",
    "is_cn": true,
    "nsp_inference_model_path": null,
    "nsp_attention_style": "bidirectional",
    "ranking_score": "decode_score"
  },
  "Reader": {
    "max_src_len": 128,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": false,
    "batch_size": 4,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  },
  "run_infer": true
}
W1026 10:23:20.544695  7144 device_context.cc:252] Please NOTE: device: 0, CUDA Capability: 70, Driver API Version: 10.1, Runtime API Version: 9.0
W1026 10:23:20.549624  7144 device_context.cc:260] device: 0, cuDNN Version: 7.6.
Traceback (most recent call last):
  File "./infer.py", line 138, in <module>
    infer(args)
  File "./infer.py", line 69, in infer
    model = models.create_model(args, place)
  File "/home/aistudio/ldk/Knover/models/__init__.py", line 49, in create_model
    return MODEL_REGISTRY[args.model](args, place)
  File "/home/aistudio/ldk/Knover/models/unified_transformer.py", line 93, in __init__
    super(UnifiedTransformer, self).__init__(args, place)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 74, in __init__
    self._build_programs()
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 138, in _build_programs
    self.exe.run(self.startup_program)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1066, in run
    return_merged=return_merged)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1154, in _run_impl
    use_program_cache=use_program_cache)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py", line 1229, in _run_program
    fetch_var_name)
KeyboardInterrupt
{
  "is_distributed": false,
  "save_path": "./result",
  "infer_file": "data/test.txt",
  "output_name": "response",
  "log_steps": 1,
  "Model": {
    "model": "UnifiedTransformer",
    "config_path": "./config/12L.json",
    "init_checkpoint": "",
    "init_pretraining_params": "12L",
    "learning_rate": 1e-05,
    "warmup_steps": 0,
    "weight_decay": 0.0,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": false,
    "amp_loss_scaling": 12800,
    "max_seq_len": 256,
    "weight_sharing": true,
    "mem_efficient": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "latent_type_size": 20,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": 20,
    "topk": 5,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "DialogGeneration",
    "do_generation": "true",
    "is_cn": true,
    "nsp_inference_model_path": null,
    "nsp_attention_style": "bidirectional",
    "ranking_score": "decode_score"
  },
  "Reader": {
    "max_src_len": 128,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": false,
    "batch_size": 4,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  },
  "run_infer": true
}
W1026 10:23:55.310873  7223 device_context.cc:252] Please NOTE: device: 0, CUDA Capability: 70, Driver API Version: 10.1, Runtime API Version: 9.0
W1026 10:23:55.315819  7223 device_context.cc:260] device: 0, cuDNN Version: 7.6.
Load pretraining parameters from 12L.
	step: 1, time: 0.935, speed: 1.070 steps/s
	step: 2, time: 1.966, speed: 1.017 steps/s
	step: 3, time: 2.688, speed: 1.116 steps/s
	step: 4, time: 3.571, speed: 1.120 steps/s
	step: 5, time: 4.477, speed: 1.117 steps/s
	step: 6, time: 5.341, speed: 1.123 steps/s
	step: 7, time: 6.094, speed: 1.149 steps/s
	step: 8, time: 6.938, speed: 1.153 steps/s
	step: 9, time: 7.822, speed: 1.151 steps/s
	step: 10, time: 8.512, speed: 1.175 steps/s
	step: 11, time: 9.434, speed: 1.166 steps/s
	step: 12, time: 10.118, speed: 1.186 steps/s
	step: 13, time: 11.022, speed: 1.179 steps/s
	step: 14, time: 12.015, speed: 1.165 steps/s
	step: 15, time: 12.588, speed: 1.192 steps/s
	step: 16, time: 13.618, speed: 1.175 steps/s
	step: 17, time: 14.542, speed: 1.169 steps/s
	step: 18, time: 15.369, speed: 1.171 steps/s
	step: 19, time: 16.252, speed: 1.169 steps/s
	step: 20, time: 17.193, speed: 1.163 steps/s
	step: 21, time: 17.878, speed: 1.175 steps/s
	step: 22, time: 18.826, speed: 1.169 steps/s
	step: 23, time: 19.593, speed: 1.174 steps/s
	step: 24, time: 20.498, speed: 1.171 steps/s
	step: 25, time: 21.404, speed: 1.168 steps/s
	step: 26, time: 22.302, speed: 1.166 steps/s
	step: 27, time: 23.237, speed: 1.162 steps/s
	step: 28, time: 24.568, speed: 1.140 steps/s
	step: 29, time: 25.439, speed: 1.140 steps/s
	step: 30, time: 26.409, speed: 1.136 steps/s
	step: 31, time: 27.067, speed: 1.145 steps/s
	step: 32, time: 27.664, speed: 1.157 steps/s
	step: 33, time: 28.333, speed: 1.165 steps/s
	step: 34, time: 29.233, speed: 1.163 steps/s
	step: 35, time: 30.145, speed: 1.161 steps/s
	step: 36, time: 31.028, speed: 1.160 steps/s
	step: 37, time: 31.610, speed: 1.170 steps/s
	step: 38, time: 32.257, speed: 1.178 steps/s
	step: 39, time: 33.174, speed: 1.176 steps/s
	step: 40, time: 34.064, speed: 1.174 steps/s
	step: 41, time: 34.932, speed: 1.174 steps/s
	step: 42, time: 35.768, speed: 1.174 steps/s
	step: 43, time: 36.810, speed: 1.168 steps/s
	step: 44, time: 37.708, speed: 1.167 steps/s
	step: 45, time: 38.356, speed: 1.173 steps/s
	step: 46, time: 39.236, speed: 1.172 steps/s
	step: 47, time: 39.982, speed: 1.176 steps/s
	step: 48, time: 40.887, speed: 1.174 steps/s
	step: 49, time: 41.545, speed: 1.179 steps/s
	step: 50, time: 42.382, speed: 1.180 steps/s
	step: 51, time: 43.056, speed: 1.185 steps/s
	step: 52, time: 44.023, speed: 1.181 steps/s
	step: 53, time: 44.846, speed: 1.182 steps/s
	step: 54, time: 45.564, speed: 1.185 steps/s
	step: 55, time: 46.460, speed: 1.184 steps/s
	step: 56, time: 47.194, speed: 1.187 steps/s
	step: 57, time: 48.006, speed: 1.187 steps/s
	step: 58, time: 48.933, speed: 1.185 steps/s
	step: 59, time: 49.831, speed: 1.184 steps/s
	step: 60, time: 50.947, speed: 1.178 steps/s
	step: 61, time: 51.827, speed: 1.177 steps/s
	step: 62, time: 52.715, speed: 1.176 steps/s
	step: 63, time: 53.566, speed: 1.176 steps/s
	step: 64, time: 54.409, speed: 1.176 steps/s
	step: 65, time: 55.310, speed: 1.175 steps/s
	step: 66, time: 56.223, speed: 1.174 steps/s
	step: 67, time: 57.143, speed: 1.172 steps/s
	step: 68, time: 58.037, speed: 1.172 steps/s
	step: 69, time: 59.007, speed: 1.169 steps/s
	step: 70, time: 59.947, speed: 1.168 steps/s
	step: 71, time: 60.784, speed: 1.168 steps/s
	step: 72, time: 61.630, speed: 1.168 steps/s
	step: 73, time: 62.518, speed: 1.168 steps/s
	step: 74, time: 63.398, speed: 1.167 steps/s
	step: 75, time: 64.355, speed: 1.165 steps/s
	step: 76, time: 65.315, speed: 1.164 steps/s
	step: 77, time: 66.496, speed: 1.158 steps/s
	step: 78, time: 67.253, speed: 1.160 steps/s
	step: 79, time: 68.052, speed: 1.161 steps/s
	step: 80, time: 68.967, speed: 1.160 steps/s
	step: 81, time: 69.574, speed: 1.164 steps/s
	step: 82, time: 70.145, speed: 1.169 steps/s
	step: 83, time: 71.071, speed: 1.168 steps/s
	step: 84, time: 71.988, speed: 1.167 steps/s
	step: 85, time: 72.883, speed: 1.166 steps/s
	step: 86, time: 73.819, speed: 1.165 steps/s
	step: 87, time: 74.858, speed: 1.162 steps/s
	step: 88, time: 75.970, speed: 1.158 steps/s
	step: 89, time: 76.680, speed: 1.161 steps/s
	step: 90, time: 77.624, speed: 1.159 steps/s
	step: 91, time: 78.335, speed: 1.162 steps/s
	step: 92, time: 79.023, speed: 1.164 steps/s
	step: 93, time: 79.599, speed: 1.168 steps/s
	step: 94, time: 80.668, speed: 1.165 steps/s
	step: 95, time: 81.576, speed: 1.165 steps/s
	step: 96, time: 82.510, speed: 1.163 steps/s
	step: 97, time: 83.150, speed: 1.167 steps/s
	step: 98, time: 84.055, speed: 1.166 steps/s
	step: 99, time: 84.978, speed: 1.165 steps/s
	step: 100, time: 85.706, speed: 1.167 steps/s
[infer] steps: 100 time cost: 85.70693945884705, speed: 1.1667666659362617 steps/s
save inference result into: ./result/inference_output.txt
{
  "is_distributed": false,
  "save_path": "./result",
  "infer_file": "data/test.txt",
  "output_name": "response",
  "log_steps": 1,
  "Model": {
    "model": "Plato",
    "config_path": "./config/12L.json",
    "init_checkpoint": "",
    "init_pretraining_params": "./output/step_13000",
    "learning_rate": 1e-05,
    "warmup_steps": 0,
    "weight_decay": 0.0,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": false,
    "amp_loss_scaling": 12800,
    "max_seq_len": 256,
    "weight_sharing": true,
    "mem_efficient": false,
    "use_bow": true,
    "use_entropy": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "latent_type_size": 20,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": 20,
    "topk": 5,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "DialogGeneration",
    "do_generation": "true",
    "is_cn": true,
    "nsp_inference_model_path": null,
    "nsp_attention_style": "bidirectional",
    "ranking_score": "decode_score"
  },
  "Reader": {
    "max_src_len": 128,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": false,
    "batch_size": 4,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  },
  "run_infer": true
}
{'is_distributed': False, 'save_path': './result', 'infer_file': 'data/test.txt', 'output_name': 'response', 'log_steps': 1, 'Model': {'model': 'Plato', 'config_path': './config/12L.json', 'init_checkpoint': '', 'init_pretraining_params': './output/step_13000', 'learning_rate': 1e-05, 'warmup_steps': 0, 'weight_decay': 0.0, 'max_grad_norm': 0.1, 'use_recompute': False, 'use_amp': False, 'amp_loss_scaling': 12800, 'max_seq_len': 256, 'weight_sharing': True, 'mem_efficient': False, 'use_bow': True, 'use_entropy': False, 'pre_encoder_cmd': 'd', 'preprocess_cmd': 'n', 'postprocess_cmd': 'da', 'post_cls_cmd': 'n', 'cls_bias': True, 'attention_probs_dropout_prob': 0.1, 'hidden_act': 'gelu', 'hidden_dropout_prob': 0.1, 'hidden_size': 768, 'initializer_range': 0.02, 'max_position_embeddings': 512, 'latent_type_size': 20, 'num_attention_heads': 12, 'num_hidden_layers': 12, 'type_vocab_size': 2, 'role_type_size': 32, 'vocab_size': 30004}, 'Generator': {'min_dec_len': 1, 'max_dec_len': 64, 'decoding_strategy': 'topk_sampling', 'temperature': 1.0, 'ignore_unk': True, 'num_samples': 20, 'topk': 5, 'topp': 0.9, 'beam_size': 10, 'length_average': True, 'length_penalty': 0.0}, 'Task': {'task': 'DialogGeneration', 'do_generation': 'true', 'is_cn': True, 'nsp_inference_model_path': None, 'nsp_attention_style': 'bidirectional', 'ranking_score': 'decode_score'}, 'Reader': {'max_src_len': 128, 'max_tgt_len': 128, 'truncate_first_turn': False, 'file_format': 'file', 'data_format': 'numerical', 'in_tokens': False, 'batch_size': 4, 'continuous_position': True, 'random_seed': 11, 'sort_pool_size': 65536}, 'Tokenizer': {'tokenizer': 'SentencePieceTokenizer', 'vocab_path': './config/vocab.txt', 'do_lower_case': False, 'spm_model_file': './config/spm.model'}, 'run_infer': True, 'pad_id': 0, 'bos_id': 1, 'eos_id': 2, 'unk_id': 0, 'mask_id': 30000}
W1026 10:30:35.488700  7546 device_context.cc:252] Please NOTE: device: 0, CUDA Capability: 70, Driver API Version: 10.1, Runtime API Version: 9.0
W1026 10:30:35.493561  7546 device_context.cc:260] device: 0, cuDNN Version: 7.6.
Load pretraining parameters from ./output/step_13000.
	step: 1, time: 7.131, speed: 0.140 steps/s
	step: 2, time: 14.141, speed: 0.141 steps/s
	step: 3, time: 20.757, speed: 0.145 steps/s
	step: 4, time: 27.713, speed: 0.144 steps/s
	step: 5, time: 34.205, speed: 0.146 steps/s
	step: 6, time: 41.255, speed: 0.145 steps/s
	step: 7, time: 47.890, speed: 0.146 steps/s
	step: 8, time: 54.950, speed: 0.146 steps/s
	step: 9, time: 62.154, speed: 0.145 steps/s
	step: 10, time: 69.378, speed: 0.144 steps/s
	step: 11, time: 77.194, speed: 0.142 steps/s
	step: 12, time: 83.535, speed: 0.144 steps/s
	step: 13, time: 90.532, speed: 0.144 steps/s
	step: 14, time: 97.552, speed: 0.144 steps/s
	step: 15, time: 104.585, speed: 0.143 steps/s
	step: 16, time: 112.458, speed: 0.142 steps/s
	step: 17, time: 119.092, speed: 0.143 steps/s
	step: 18, time: 125.968, speed: 0.143 steps/s
	step: 19, time: 133.137, speed: 0.143 steps/s
	step: 20, time: 140.815, speed: 0.142 steps/s
	step: 21, time: 147.273, speed: 0.143 steps/s
	step: 22, time: 154.406, speed: 0.142 steps/s
	step: 23, time: 161.317, speed: 0.143 steps/s
	step: 24, time: 168.347, speed: 0.143 steps/s
	step: 25, time: 175.366, speed: 0.143 steps/s
	step: 26, time: 182.026, speed: 0.143 steps/s
	step: 27, time: 188.771, speed: 0.143 steps/s
	step: 28, time: 195.728, speed: 0.143 steps/s
	step: 29, time: 202.651, speed: 0.143 steps/s
	step: 30, time: 210.576, speed: 0.142 steps/s
	step: 31, time: 217.683, speed: 0.142 steps/s
	step: 32, time: 224.729, speed: 0.142 steps/s
	step: 33, time: 232.238, speed: 0.142 steps/s
	step: 34, time: 239.425, speed: 0.142 steps/s
	step: 35, time: 246.085, speed: 0.142 steps/s
	step: 36, time: 252.756, speed: 0.142 steps/s
	step: 37, time: 259.553, speed: 0.143 steps/s
	step: 38, time: 266.113, speed: 0.143 steps/s
	step: 39, time: 273.508, speed: 0.143 steps/s
	step: 40, time: 280.615, speed: 0.143 steps/s
	step: 41, time: 287.635, speed: 0.143 steps/s
	step: 42, time: 294.963, speed: 0.142 steps/s
	step: 43, time: 302.306, speed: 0.142 steps/s
	step: 44, time: 309.456, speed: 0.142 steps/s
	step: 45, time: 316.626, speed: 0.142 steps/s
	step: 46, time: 323.541, speed: 0.142 steps/s
	step: 47, time: 330.592, speed: 0.142 steps/s
	step: 48, time: 337.903, speed: 0.142 steps/s
	step: 49, time: 345.329, speed: 0.142 steps/s
	step: 50, time: 352.347, speed: 0.142 steps/s
	step: 51, time: 358.848, speed: 0.142 steps/s
	step: 52, time: 365.994, speed: 0.142 steps/s
	step: 53, time: 373.334, speed: 0.142 steps/s
	step: 54, time: 380.308, speed: 0.142 steps/s
	step: 55, time: 387.252, speed: 0.142 steps/s
	step: 56, time: 394.160, speed: 0.142 steps/s
	step: 57, time: 401.835, speed: 0.142 steps/s
	step: 58, time: 408.796, speed: 0.142 steps/s
	step: 59, time: 415.979, speed: 0.142 steps/s
	step: 60, time: 423.383, speed: 0.142 steps/s
	step: 61, time: 430.369, speed: 0.142 steps/s
	step: 62, time: 437.291, speed: 0.142 steps/s
	step: 63, time: 444.211, speed: 0.142 steps/s
	step: 64, time: 450.966, speed: 0.142 steps/s
	step: 65, time: 457.550, speed: 0.142 steps/s
	step: 66, time: 464.776, speed: 0.142 steps/s
	step: 67, time: 471.375, speed: 0.142 steps/s
	step: 68, time: 478.371, speed: 0.142 steps/s
	step: 69, time: 485.867, speed: 0.142 steps/s
	step: 70, time: 493.496, speed: 0.142 steps/s
	step: 71, time: 500.850, speed: 0.142 steps/s
	step: 72, time: 507.248, speed: 0.142 steps/s
	step: 73, time: 513.747, speed: 0.142 steps/s
	step: 74, time: 520.685, speed: 0.142 steps/s
	step: 75, time: 527.571, speed: 0.142 steps/s
	step: 76, time: 534.890, speed: 0.142 steps/s
	step: 77, time: 541.917, speed: 0.142 steps/s
	step: 78, time: 548.996, speed: 0.142 steps/s
	step: 79, time: 556.406, speed: 0.142 steps/s
	step: 80, time: 562.988, speed: 0.142 steps/s
	step: 81, time: 570.271, speed: 0.142 steps/s
	step: 82, time: 577.242, speed: 0.142 steps/s
	step: 83, time: 584.230, speed: 0.142 steps/s
	step: 84, time: 591.142, speed: 0.142 steps/s
	step: 85, time: 598.385, speed: 0.142 steps/s
	step: 86, time: 605.522, speed: 0.142 steps/s
	step: 87, time: 612.316, speed: 0.142 steps/s
	step: 88, time: 620.190, speed: 0.142 steps/s
	step: 89, time: 627.111, speed: 0.142 steps/s
	step: 90, time: 634.481, speed: 0.142 steps/s
	step: 91, time: 641.839, speed: 0.142 steps/s
	step: 92, time: 648.946, speed: 0.142 steps/s
	step: 93, time: 656.460, speed: 0.142 steps/s
	step: 94, time: 663.604, speed: 0.142 steps/s
	step: 95, time: 670.727, speed: 0.142 steps/s
	step: 96, time: 677.091, speed: 0.142 steps/s
	step: 97, time: 684.576, speed: 0.142 steps/s
	step: 98, time: 691.977, speed: 0.142 steps/s
	step: 99, time: 699.530, speed: 0.142 steps/s
	step: 100, time: 706.561, speed: 0.142 steps/s
[infer] steps: 100 time cost: 706.5610251426697, speed: 0.1415305917557622 steps/s
save inference result into: ./result/inference_output.txt
{
  "is_distributed": false,
  "save_path": "./nsp_model",
  "train_file": "data/train.txt",
  "valid_file": "data/valid.txt",
  "start_step": 0,
  "num_epochs": 20,
  "log_steps": 100,
  "validation_steps": 1000,
  "save_steps": 500,
  "Model": {
    "model": "NSPModel",
    "config_path": "./config/12L.json",
    "init_checkpoint": "",
    "init_pretraining_params": "./output/step_13000",
    "learning_rate": 1e-05,
    "warmup_steps": 1000,
    "weight_decay": 0.01,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": true,
    "amp_loss_scaling": 12800,
    "max_seq_len": 512,
    "weight_sharing": true,
    "mem_efficient": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "latent_type_size": 20,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": null,
    "topk": 10,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "NextSentencePrediction"
  },
  "Reader": {
    "max_src_len": 384,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": true,
    "batch_size": 8192,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "attention_style": "bidirectional",
    "mix_negative_sample": false,
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  }
}
Traceback (most recent call last):
  File "./train.py", line 175, in <module>
    train(args)
  File "./train.py", line 78, in train
    model = models.create_model(args, place)
  File "/home/aistudio/ldk/Knover/models/__init__.py", line 49, in create_model
    return MODEL_REGISTRY[args.model](args, place)
  File "/home/aistudio/ldk/Knover/models/unified_transformer.py", line 93, in __init__
    super(UnifiedTransformer, self).__init__(args, place)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 74, in __init__
    self._build_programs()
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 124, in _build_programs
    metrics, statistics = self.get_metrics_and_statistics(inputs, outputs)
  File "/home/aistudio/ldk/Knover/models/unified_transformer.py", line 374, in get_metrics_and_statistics
    metrics = self._get_metrics(inputs, outputs)
  File "/home/aistudio/ldk/Knover/models/nsp_model.py", line 74, in _get_metrics
    fc_out = self._calc_logits(outputs["enc_out"], inputs["tgt_pos"])
  File "/home/aistudio/ldk/Knover/models/unified_transformer.py", line 322, in _calc_logits
    checkpoints.append(seq_trans_feat)
AttributeError: 'Variable' object has no attribute 'append'
{
  "is_distributed": false,
  "save_path": "./nsp_model",
  "train_file": "data/train.txt",
  "valid_file": "data/valid.txt",
  "start_step": 0,
  "num_epochs": 20,
  "log_steps": 100,
  "validation_steps": 1000,
  "save_steps": 500,
  "Model": {
    "model": "NSPModel",
    "config_path": "./config/12L.json",
    "init_checkpoint": "",
    "init_pretraining_params": "./output/step_13000",
    "learning_rate": 1e-05,
    "warmup_steps": 1000,
    "weight_decay": 0.01,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": true,
    "amp_loss_scaling": 12800,
    "max_seq_len": 512,
    "weight_sharing": true,
    "mem_efficient": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "latent_type_size": 20,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": null,
    "topk": 10,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "NextSentencePrediction"
  },
  "Reader": {
    "max_src_len": 384,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": true,
    "batch_size": 8192,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "attention_style": "bidirectional",
    "mix_negative_sample": false,
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  }
}
Traceback (most recent call last):
  File "./train.py", line 175, in <module>
    train(args)
  File "./train.py", line 78, in train
    model = models.create_model(args, place)
  File "/home/aistudio/ldk/Knover/models/__init__.py", line 49, in create_model
    return MODEL_REGISTRY[args.model](args, place)
  File "/home/aistudio/ldk/Knover/models/unified_transformer.py", line 93, in __init__
    super(UnifiedTransformer, self).__init__(args, place)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 74, in __init__
    self._build_programs()
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 124, in _build_programs
    metrics, statistics = self.get_metrics_and_statistics(inputs, outputs)
  File "/home/aistudio/ldk/Knover/models/unified_transformer.py", line 374, in get_metrics_and_statistics
    metrics = self._get_metrics(inputs, outputs)
  File "/home/aistudio/ldk/Knover/models/nsp_model.py", line 74, in _get_metrics
    fc_out = self._calc_logits(outputs["enc_out"], inputs["tgt_pos"])
  File "/home/aistudio/ldk/Knover/models/unified_transformer.py", line 322, in _calc_logits
    checkpoints.append(seq_trans_feat)
AttributeError: 'Variable' object has no attribute 'append'
{
  "is_distributed": false,
  "save_path": "./nsp_model",
  "train_file": "data/train.txt",
  "valid_file": "data/valid.txt",
  "start_step": 0,
  "num_epochs": 20,
  "log_steps": 100,
  "validation_steps": 1000,
  "save_steps": 500,
  "Model": {
    "model": "NSPModel",
    "config_path": "./config/12L.json",
    "init_checkpoint": "./output/step_13000",
    "init_pretraining_params": "./output/step_13000",
    "learning_rate": 1e-05,
    "warmup_steps": 1000,
    "weight_decay": 0.01,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": true,
    "amp_loss_scaling": 12800,
    "max_seq_len": 512,
    "weight_sharing": true,
    "mem_efficient": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "latent_type_size": 20,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": null,
    "topk": 10,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "NextSentencePrediction"
  },
  "Reader": {
    "max_src_len": 384,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": true,
    "batch_size": 8192,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "attention_style": "bidirectional",
    "mix_negative_sample": false,
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  }
}
Traceback (most recent call last):
  File "./train.py", line 175, in <module>
    train(args)
  File "./train.py", line 78, in train
    model = models.create_model(args, place)
  File "/home/aistudio/ldk/Knover/models/__init__.py", line 49, in create_model
    return MODEL_REGISTRY[args.model](args, place)
  File "/home/aistudio/ldk/Knover/models/unified_transformer.py", line 93, in __init__
    super(UnifiedTransformer, self).__init__(args, place)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 74, in __init__
    self._build_programs()
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 124, in _build_programs
    metrics, statistics = self.get_metrics_and_statistics(inputs, outputs)
  File "/home/aistudio/ldk/Knover/models/unified_transformer.py", line 374, in get_metrics_and_statistics
    metrics = self._get_metrics(inputs, outputs)
  File "/home/aistudio/ldk/Knover/models/nsp_model.py", line 74, in _get_metrics
    fc_out = self._calc_logits(outputs["enc_out"], inputs["tgt_pos"])
  File "/home/aistudio/ldk/Knover/models/unified_transformer.py", line 322, in _calc_logits
    checkpoints.append(seq_trans_feat)
AttributeError: 'Variable' object has no attribute 'append'
{
  "is_distributed": false,
  "save_path": "./nsp_model",
  "train_file": "data/train.txt",
  "valid_file": "data/valid.txt",
  "start_step": 0,
  "num_epochs": 20,
  "log_steps": 100,
  "validation_steps": 1000,
  "save_steps": 500,
  "Model": {
    "model": "NSPModel",
    "config_path": "./config/12L.json",
    "init_checkpoint": "",
    "init_pretraining_params": "./output/step_13000",
    "learning_rate": 1e-05,
    "warmup_steps": 1000,
    "weight_decay": 0.01,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": true,
    "amp_loss_scaling": 12800,
    "max_seq_len": 512,
    "weight_sharing": true,
    "mem_efficient": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "latent_type_size": 20,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": null,
    "topk": 10,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "NextSentencePrediction"
  },
  "Reader": {
    "max_src_len": 384,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": true,
    "batch_size": 8192,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "attention_style": "bidirectional",
    "mix_negative_sample": false,
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  }
}
Traceback (most recent call last):
  File "./train.py", line 175, in <module>
    train(args)
  File "./train.py", line 78, in train
    model = models.create_model(args, place)
  File "/home/aistudio/ldk/Knover/models/__init__.py", line 49, in create_model
    return MODEL_REGISTRY[args.model](args, place)
  File "/home/aistudio/ldk/Knover/models/unified_transformer.py", line 93, in __init__
    super(UnifiedTransformer, self).__init__(args, place)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 74, in __init__
    self._build_programs()
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 124, in _build_programs
    metrics, statistics = self.get_metrics_and_statistics(inputs, outputs)
  File "/home/aistudio/ldk/Knover/models/unified_transformer.py", line 374, in get_metrics_and_statistics
    metrics = self._get_metrics(inputs, outputs)
  File "/home/aistudio/ldk/Knover/models/nsp_model.py", line 74, in _get_metrics
    fc_out = self._calc_logits(outputs["enc_out"], inputs["tgt_pos"])
  File "/home/aistudio/ldk/Knover/models/unified_transformer.py", line 322, in _calc_logits
    checkpoints.append(seq_trans_feat)
AttributeError: 'Variable' object has no attribute 'append'
{
  "is_distributed": false,
  "save_path": "./nsp_model",
  "train_file": "data/train.txt",
  "valid_file": "data/valid.txt",
  "start_step": 0,
  "num_epochs": 20,
  "log_steps": 100,
  "validation_steps": 1000,
  "save_steps": 500,
  "Model": {
    "model": "NSPModel",
    "config_path": "./config/12L.json",
    "init_checkpoint": "",
    "init_pretraining_params": "./output/step_13000",
    "learning_rate": 1e-05,
    "warmup_steps": 1000,
    "weight_decay": 0.01,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": true,
    "amp_loss_scaling": 12800,
    "max_seq_len": 512,
    "weight_sharing": true,
    "mem_efficient": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "latent_type_size": 20,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": null,
    "topk": 10,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "NextSentencePrediction"
  },
  "Reader": {
    "max_src_len": 384,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": true,
    "batch_size": 8192,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "attention_style": "bidirectional",
    "mix_negative_sample": false,
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  }
}
W1026 14:57:09.663491  1308 device_context.cc:252] Please NOTE: device: 0, CUDA Capability: 70, Driver API Version: 10.1, Runtime API Version: 9.0
W1026 14:57:09.668387  1308 device_context.cc:260] device: 0, cuDNN Version: 7.6.
Load pretraining parameters from ./output/step_13000.
2020-10-26 14:57:16,385-WARNING: Your reader has raised an exception!
Exception in thread Thread-1:
Traceback (most recent call last):
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/threading.py", line 926, in _bootstrap_inner
    self.run()
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/reader.py", line 1145, in __thread_main__
    six.reraise(*sys.exc_info())
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/six.py", line 703, in reraise
    raise value
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/reader.py", line 1125, in __thread_main__
    for tensors in self._tensor_reader():
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 204, in __wrapper__
    for batch in generator():
  File "/home/aistudio/ldk/Knover/readers/dialog_reader.py", line 380, in __wrapper__
    yield self._pad_batch_records(batch, is_infer)
  File "/home/aistudio/ldk/Knover/readers/nsp_reader.py", line 148, in _pad_batch_records
    batch_label = np.array(batch_label).astype("int64").reshape([-1, 1])
TypeError: int() argument must be a string, a bytes-like object or a number, not 'NoneType'

Traceback (most recent call last):
  File "./train.py", line 175, in <module>
    train(args)
  File "./train.py", line 98, in train
    for step, data in enumerate(train_generator(), args.start_step + 1):
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/reader.py", line 1104, in __next__
    return self._reader.read_next()
paddle.fluid.core_avx.EnforceNotMet: 

--------------------------------------------
C++ Call Stacks (More useful to developers):
--------------------------------------------
0   std::string paddle::platform::GetTraceBackString<std::string const&>(std::string const&, char const*, int)
1   paddle::platform::EnforceNotMet::EnforceNotMet(std::string const&, char const*, int)
2   paddle::operators::reader::BlockingQueue<std::vector<paddle::framework::LoDTensor, std::allocator<paddle::framework::LoDTensor> > >::Receive(std::vector<paddle::framework::LoDTensor, std::allocator<paddle::framework::LoDTensor> >*)
3   paddle::operators::reader::PyReader::ReadNext(std::vector<paddle::framework::LoDTensor, std::allocator<paddle::framework::LoDTensor> >*)
4   std::_Function_handler<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> (), std::__future_base::_Task_setter<std::unique_ptr<std::__future_base::_Result<unsigned long>, std::__future_base::_Result_base::_Deleter>, unsigned long> >::_M_invoke(std::_Any_data const&)
5   std::__future_base::_State_base::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>&, bool&)
6   ThreadPool::ThreadPool(unsigned long)::{lambda()#1}::operator()() const

----------------------
Error Message Summary:
----------------------
Error: Blocking queue is killed because the data reader raises an exception
  [Hint: Expected killed_ != true, but received killed_:1 == true:1.] at (/paddle/paddle/fluid/operators/reader/blocking_queue.h:141)

{
  "is_distributed": false,
  "save_path": "./nsp_model",
  "train_file": "data/train.txt",
  "valid_file": "data/valid.txt",
  "start_step": 0,
  "num_epochs": 20,
  "log_steps": 100,
  "validation_steps": 1000,
  "save_steps": 500,
  "Model": {
    "model": "NSPModel",
    "config_path": "./config/12L.json",
    "init_checkpoint": "",
    "init_pretraining_params": "./output/step_13000",
    "learning_rate": 1e-05,
    "warmup_steps": 1000,
    "weight_decay": 0.01,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": true,
    "amp_loss_scaling": 12800,
    "max_seq_len": 512,
    "weight_sharing": true,
    "mem_efficient": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "latent_type_size": 20,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": null,
    "topk": 10,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "NextSentencePrediction"
  },
  "Reader": {
    "max_src_len": 384,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": true,
    "batch_size": 8192,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "attention_style": "bidirectional",
    "mix_negative_sample": false,
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  }
}
W1026 14:59:38.810246  1459 device_context.cc:252] Please NOTE: device: 0, CUDA Capability: 70, Driver API Version: 10.1, Runtime API Version: 9.0
W1026 14:59:38.815362  1459 device_context.cc:260] device: 0, cuDNN Version: 7.6.
Load pretraining parameters from ./output/step_13000.
2020-10-26 14:59:45,193-WARNING: Your reader has raised an exception!
Exception in thread Thread-1:
Traceback (most recent call last):
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/threading.py", line 926, in _bootstrap_inner
    self.run()
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/reader.py", line 1145, in __thread_main__
    six.reraise(*sys.exc_info())
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/six.py", line 703, in reraise
    raise value
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/reader.py", line 1125, in __thread_main__
    for tensors in self._tensor_reader():
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 204, in __wrapper__
    for batch in generator():
  File "/home/aistudio/ldk/Knover/readers/dialog_reader.py", line 380, in __wrapper__
    yield self._pad_batch_records(batch, is_infer)
  File "/home/aistudio/ldk/Knover/readers/nsp_reader.py", line 148, in _pad_batch_records
    batch_label = np.array(batch_label).astype("int64").reshape([-1, 1])
TypeError: int() argument must be a string, a bytes-like object or a number, not 'NoneType'

Traceback (most recent call last):
  File "./train.py", line 175, in <module>
    train(args)
  File "./train.py", line 98, in train
    for step, data in enumerate(train_generator(), args.start_step + 1):
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/reader.py", line 1104, in __next__
    return self._reader.read_next()
paddle.fluid.core_avx.EnforceNotMet: 

--------------------------------------------
C++ Call Stacks (More useful to developers):
--------------------------------------------
0   std::string paddle::platform::GetTraceBackString<std::string const&>(std::string const&, char const*, int)
1   paddle::platform::EnforceNotMet::EnforceNotMet(std::string const&, char const*, int)
2   paddle::operators::reader::BlockingQueue<std::vector<paddle::framework::LoDTensor, std::allocator<paddle::framework::LoDTensor> > >::Receive(std::vector<paddle::framework::LoDTensor, std::allocator<paddle::framework::LoDTensor> >*)
3   paddle::operators::reader::PyReader::ReadNext(std::vector<paddle::framework::LoDTensor, std::allocator<paddle::framework::LoDTensor> >*)
4   std::_Function_handler<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> (), std::__future_base::_Task_setter<std::unique_ptr<std::__future_base::_Result<unsigned long>, std::__future_base::_Result_base::_Deleter>, unsigned long> >::_M_invoke(std::_Any_data const&)
5   std::__future_base::_State_base::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>&, bool&)
6   ThreadPool::ThreadPool(unsigned long)::{lambda()#1}::operator()() const

----------------------
Error Message Summary:
----------------------
Error: Blocking queue is killed because the data reader raises an exception
  [Hint: Expected killed_ != true, but received killed_:1 == true:1.] at (/paddle/paddle/fluid/operators/reader/blocking_queue.h:141)

{
  "is_distributed": false,
  "save_path": "./nsp_model",
  "train_file": "data/train.txt",
  "valid_file": "data/valid.txt",
  "start_step": 0,
  "num_epochs": 20,
  "log_steps": 100,
  "validation_steps": 1000,
  "save_steps": 500,
  "Model": {
    "model": "NSPModel",
    "config_path": "./config/12L.json",
    "init_checkpoint": "./output/step_13000",
    "init_pretraining_params": "./output/step_13000",
    "learning_rate": 1e-05,
    "warmup_steps": 1000,
    "weight_decay": 0.01,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": true,
    "amp_loss_scaling": 12800,
    "max_seq_len": 512,
    "weight_sharing": true,
    "mem_efficient": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "latent_type_size": 20,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": null,
    "topk": 10,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "NextSentencePrediction"
  },
  "Reader": {
    "max_src_len": 384,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": true,
    "batch_size": 8192,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "attention_style": "bidirectional",
    "mix_negative_sample": false,
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  }
}
W1026 15:02:34.968798  1644 device_context.cc:252] Please NOTE: device: 0, CUDA Capability: 70, Driver API Version: 10.1, Runtime API Version: 9.0
W1026 15:02:34.973748  1644 device_context.cc:260] device: 0, cuDNN Version: 7.6.
Load pretraining parameters from ./output/step_13000.
2020-10-26 15:02:41,320-WARNING: Your reader has raised an exception!
Exception in thread Thread-1:
Traceback (most recent call last):
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/threading.py", line 926, in _bootstrap_inner
    self.run()
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/reader.py", line 1145, in __thread_main__
    six.reraise(*sys.exc_info())
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/six.py", line 703, in reraise
    raise value
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/reader.py", line 1125, in __thread_main__
    for tensors in self._tensor_reader():
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 204, in __wrapper__
    for batch in generator():
  File "/home/aistudio/ldk/Knover/readers/dialog_reader.py", line 380, in __wrapper__
    yield self._pad_batch_records(batch, is_infer)
  File "/home/aistudio/ldk/Knover/readers/nsp_reader.py", line 148, in _pad_batch_records
    batch_label = np.array(batch_label).astype("int64").reshape([-1, 1])
TypeError: int() argument must be a string, a bytes-like object or a number, not 'NoneType'

Traceback (most recent call last):
  File "./train.py", line 175, in <module>
    train(args)
  File "./train.py", line 98, in train
    for step, data in enumerate(train_generator(), args.start_step + 1):
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/reader.py", line 1104, in __next__
    return self._reader.read_next()
paddle.fluid.core_avx.EnforceNotMet: 

--------------------------------------------
C++ Call Stacks (More useful to developers):
--------------------------------------------
0   std::string paddle::platform::GetTraceBackString<std::string const&>(std::string const&, char const*, int)
1   paddle::platform::EnforceNotMet::EnforceNotMet(std::string const&, char const*, int)
2   paddle::operators::reader::BlockingQueue<std::vector<paddle::framework::LoDTensor, std::allocator<paddle::framework::LoDTensor> > >::Receive(std::vector<paddle::framework::LoDTensor, std::allocator<paddle::framework::LoDTensor> >*)
3   paddle::operators::reader::PyReader::ReadNext(std::vector<paddle::framework::LoDTensor, std::allocator<paddle::framework::LoDTensor> >*)
4   std::_Function_handler<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> (), std::__future_base::_Task_setter<std::unique_ptr<std::__future_base::_Result<unsigned long>, std::__future_base::_Result_base::_Deleter>, unsigned long> >::_M_invoke(std::_Any_data const&)
5   std::__future_base::_State_base::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>&, bool&)
6   ThreadPool::ThreadPool(unsigned long)::{lambda()#1}::operator()() const

----------------------
Error Message Summary:
----------------------
Error: Blocking queue is killed because the data reader raises an exception
  [Hint: Expected killed_ != true, but received killed_:1 == true:1.] at (/paddle/paddle/fluid/operators/reader/blocking_queue.h:141)

{
  "is_distributed": false,
  "save_path": "./nsp_model",
  "train_file": "data/train.txt",
  "valid_file": "data/valid.txt",
  "start_step": 0,
  "num_epochs": 20,
  "log_steps": 100,
  "validation_steps": 1000,
  "save_steps": 500,
  "Model": {
    "model": "NSPModel",
    "config_path": "./config/12L.json",
    "init_checkpoint": "./output/step_13000",
    "init_pretraining_params": "./output/step_13000",
    "learning_rate": 1e-05,
    "warmup_steps": 1000,
    "weight_decay": 0.01,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": true,
    "amp_loss_scaling": 12800,
    "max_seq_len": 512,
    "weight_sharing": true,
    "mem_efficient": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "latent_type_size": 20,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": null,
    "topk": 10,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "NextSentencePrediction"
  },
  "Reader": {
    "max_src_len": 384,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": true,
    "batch_size": 8192,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "attention_style": "bidirectional",
    "mix_negative_sample": false,
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  }
}
Traceback (most recent call last):
  File "./train.py", line 175, in <module>
    train(args)
  File "./train.py", line 78, in train
    model = models.create_model(args, place)
  File "/home/aistudio/ldk/Knover/models/__init__.py", line 49, in create_model
    return MODEL_REGISTRY[args.model](args, place)
  File "/home/aistudio/ldk/Knover/models/unified_transformer.py", line 93, in __init__
    super(UnifiedTransformer, self).__init__(args, place)
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 74, in __init__
    self._build_programs()
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 124, in _build_programs
    metrics, statistics = self.get_metrics_and_statistics(inputs, outputs)
  File "/home/aistudio/ldk/Knover/models/unified_transformer.py", line 374, in get_metrics_and_statistics
    metrics = self._get_metrics(inputs, outputs)
  File "/home/aistudio/ldk/Knover/models/nsp_model.py", line 75, in _get_metrics
    fc_out = self._calc_logits(outputs["enc_out"], outputs["checkpoints"], inputs["tgt_pos"])
KeyError: 'checkpoints'
{
  "is_distributed": false,
  "save_path": "./nsp_model",
  "train_file": "data/train.txt",
  "valid_file": "data/valid.txt",
  "start_step": 0,
  "num_epochs": 20,
  "log_steps": 100,
  "validation_steps": 1000,
  "save_steps": 500,
  "Model": {
    "model": "NSPModel",
    "config_path": "./config/12L.json",
    "init_checkpoint": "./output/step_13000",
    "init_pretraining_params": "./output/step_13000",
    "learning_rate": 1e-05,
    "warmup_steps": 1000,
    "weight_decay": 0.01,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": true,
    "amp_loss_scaling": 12800,
    "max_seq_len": 512,
    "weight_sharing": true,
    "mem_efficient": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "latent_type_size": 20,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": null,
    "topk": 10,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "NextSentencePrediction"
  },
  "Reader": {
    "max_src_len": 384,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": true,
    "batch_size": 8192,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "attention_style": "bidirectional",
    "mix_negative_sample": false,
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  }
}
W1026 15:09:40.182540  2044 device_context.cc:252] Please NOTE: device: 0, CUDA Capability: 70, Driver API Version: 10.1, Runtime API Version: 9.0
W1026 15:09:40.187471  2044 device_context.cc:260] device: 0, cuDNN Version: 7.6.
Load pretraining parameters from ./output/step_13000.
2020-10-26 15:09:46,310-WARNING: Your reader has raised an exception!
Exception in thread Thread-1:
Traceback (most recent call last):
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/threading.py", line 926, in _bootstrap_inner
    self.run()
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/reader.py", line 1145, in __thread_main__
    six.reraise(*sys.exc_info())
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/six.py", line 703, in reraise
    raise value
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/reader.py", line 1125, in __thread_main__
    for tensors in self._tensor_reader():
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 204, in __wrapper__
    for batch in generator():
  File "/home/aistudio/ldk/Knover/readers/dialog_reader.py", line 380, in __wrapper__
    yield self._pad_batch_records(batch, is_infer)
  File "/home/aistudio/ldk/Knover/readers/nsp_reader.py", line 148, in _pad_batch_records
    batch_label = np.array(batch_label).astype("int64").reshape([-1, 1])
TypeError: int() argument must be a string, a bytes-like object or a number, not 'NoneType'

Traceback (most recent call last):
  File "./train.py", line 175, in <module>
    train(args)
  File "./train.py", line 98, in train
    for step, data in enumerate(train_generator(), args.start_step + 1):
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/reader.py", line 1104, in __next__
    return self._reader.read_next()
paddle.fluid.core_avx.EnforceNotMet: 

--------------------------------------------
C++ Call Stacks (More useful to developers):
--------------------------------------------
0   std::string paddle::platform::GetTraceBackString<std::string const&>(std::string const&, char const*, int)
1   paddle::platform::EnforceNotMet::EnforceNotMet(std::string const&, char const*, int)
2   paddle::operators::reader::BlockingQueue<std::vector<paddle::framework::LoDTensor, std::allocator<paddle::framework::LoDTensor> > >::Receive(std::vector<paddle::framework::LoDTensor, std::allocator<paddle::framework::LoDTensor> >*)
3   paddle::operators::reader::PyReader::ReadNext(std::vector<paddle::framework::LoDTensor, std::allocator<paddle::framework::LoDTensor> >*)
4   std::_Function_handler<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> (), std::__future_base::_Task_setter<std::unique_ptr<std::__future_base::_Result<unsigned long>, std::__future_base::_Result_base::_Deleter>, unsigned long> >::_M_invoke(std::_Any_data const&)
5   std::__future_base::_State_base::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>&, bool&)
6   ThreadPool::ThreadPool(unsigned long)::{lambda()#1}::operator()() const

----------------------
Error Message Summary:
----------------------
Error: Blocking queue is killed because the data reader raises an exception
  [Hint: Expected killed_ != true, but received killed_:1 == true:1.] at (/paddle/paddle/fluid/operators/reader/blocking_queue.h:141)

{
  "is_distributed": false,
  "save_path": "./nsp_model",
  "train_file": "data/train.txt",
  "valid_file": "data/valid.txt",
  "start_step": 0,
  "num_epochs": 20,
  "log_steps": 100,
  "validation_steps": 1000,
  "save_steps": 500,
  "Model": {
    "model": "NSPModel",
    "config_path": "./config/12L.json",
    "init_checkpoint": "./output/step_13000",
    "init_pretraining_params": "./output/step_13000",
    "learning_rate": 1e-05,
    "warmup_steps": 1000,
    "weight_decay": 0.01,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": true,
    "amp_loss_scaling": 12800,
    "max_seq_len": 512,
    "weight_sharing": true,
    "mem_efficient": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "latent_type_size": 20,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": null,
    "topk": 10,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "NextSentencePrediction"
  },
  "Reader": {
    "max_src_len": 384,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": true,
    "batch_size": 8192,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "attention_style": "bidirectional",
    "mix_negative_sample": false,
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  }
}
W1026 15:35:30.543105  3026 device_context.cc:252] Please NOTE: device: 0, CUDA Capability: 70, Driver API Version: 10.1, Runtime API Version: 9.0
W1026 15:35:30.548090  3026 device_context.cc:260] device: 0, cuDNN Version: 7.6.
Load pretraining parameters from ./output/step_13000.
2020-10-26 15:35:37,225-WARNING: Your reader has raised an exception!
Exception in thread Thread-1:
Traceback (most recent call last):
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/threading.py", line 926, in _bootstrap_inner
    self.run()
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/reader.py", line 1145, in __thread_main__
    six.reraise(*sys.exc_info())
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/six.py", line 703, in reraise
    raise value
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/reader.py", line 1125, in __thread_main__
    for tensors in self._tensor_reader():
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 204, in __wrapper__
    for batch in generator():
  File "/home/aistudio/ldk/Knover/readers/dialog_reader.py", line 380, in __wrapper__
    yield self._pad_batch_records(batch, is_infer)
  File "/home/aistudio/ldk/Knover/readers/nsp_reader.py", line 148, in _pad_batch_records
    batch_label = np.array(batch_label).astype("int64").reshape([-1, 1])
TypeError: int() argument must be a string, a bytes-like object or a number, not 'NoneType'

Traceback (most recent call last):
  File "./train.py", line 175, in <module>
    train(args)
  File "./train.py", line 98, in train
    for step, data in enumerate(train_generator(), args.start_step + 1):
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/reader.py", line 1104, in __next__
    return self._reader.read_next()
paddle.fluid.core_avx.EnforceNotMet: 

--------------------------------------------
C++ Call Stacks (More useful to developers):
--------------------------------------------
0   std::string paddle::platform::GetTraceBackString<std::string const&>(std::string const&, char const*, int)
1   paddle::platform::EnforceNotMet::EnforceNotMet(std::string const&, char const*, int)
2   paddle::operators::reader::BlockingQueue<std::vector<paddle::framework::LoDTensor, std::allocator<paddle::framework::LoDTensor> > >::Receive(std::vector<paddle::framework::LoDTensor, std::allocator<paddle::framework::LoDTensor> >*)
3   paddle::operators::reader::PyReader::ReadNext(std::vector<paddle::framework::LoDTensor, std::allocator<paddle::framework::LoDTensor> >*)
4   std::_Function_handler<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> (), std::__future_base::_Task_setter<std::unique_ptr<std::__future_base::_Result<unsigned long>, std::__future_base::_Result_base::_Deleter>, unsigned long> >::_M_invoke(std::_Any_data const&)
5   std::__future_base::_State_base::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>&, bool&)
6   ThreadPool::ThreadPool(unsigned long)::{lambda()#1}::operator()() const

----------------------
Error Message Summary:
----------------------
Error: Blocking queue is killed because the data reader raises an exception
  [Hint: Expected killed_ != true, but received killed_:1 == true:1.] at (/paddle/paddle/fluid/operators/reader/blocking_queue.h:141)

{
  "is_distributed": false,
  "save_path": "./nsp_model",
  "train_file": "data/train.txt",
  "valid_file": "data/valid.txt",
  "start_step": 0,
  "num_epochs": 20,
  "log_steps": 100,
  "validation_steps": 1000,
  "save_steps": 500,
  "Model": {
    "model": "NSPModel",
    "config_path": "./config/12L.json",
    "init_checkpoint": "./output/step_13000",
    "init_pretraining_params": "./output/step_13000",
    "learning_rate": 1e-05,
    "warmup_steps": 1000,
    "weight_decay": 0.01,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": true,
    "amp_loss_scaling": 12800,
    "max_seq_len": 512,
    "weight_sharing": true,
    "mem_efficient": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "latent_type_size": 20,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": null,
    "topk": 10,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "NextSentencePrediction"
  },
  "Reader": {
    "max_src_len": 384,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": true,
    "batch_size": 8192,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "attention_style": "bidirectional",
    "mix_negative_sample": true,
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  }
}
W1026 15:38:37.742060  3212 device_context.cc:252] Please NOTE: device: 0, CUDA Capability: 70, Driver API Version: 10.1, Runtime API Version: 9.0
W1026 15:38:37.747054  3212 device_context.cc:260] device: 0, cuDNN Version: 7.6.
Load pretraining parameters from ./output/step_13000.
2020-10-26 15:38:43,587-WARNING: Your reader has raised an exception!
Exception in thread Thread-1:
Traceback (most recent call last):
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/threading.py", line 926, in _bootstrap_inner
    self.run()
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/reader.py", line 1145, in __thread_main__
    six.reraise(*sys.exc_info())
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/six.py", line 703, in reraise
    raise value
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/reader.py", line 1125, in __thread_main__
    for tensors in self._tensor_reader():
  File "/home/aistudio/ldk/Knover/models/model_base.py", line 204, in __wrapper__
    for batch in generator():
  File "/home/aistudio/ldk/Knover/readers/dialog_reader.py", line 379, in __wrapper__
    for batch in batch_reader():
  File "/home/aistudio/ldk/Knover/readers/dialog_reader.py", line 330, in __wrapper__
    for batch in batch_reader():
  File "/home/aistudio/ldk/Knover/readers/dialog_reader.py", line 312, in __wrapper__
    for record in reader():
  File "/home/aistudio/ldk/Knover/readers/nsp_reader.py", line 87, in __wrapper__
    for record in gen_from_pool(pool):
  File "/home/aistudio/ldk/Knover/readers/nsp_reader.py", line 77, in gen_from_pool
    assert len(neg_record.token_ids) <= self.max_seq_len
AttributeError: 'NSPReader' object has no attribute 'max_seq_len'

Traceback (most recent call last):
  File "./train.py", line 175, in <module>
    train(args)
  File "./train.py", line 98, in train
    for step, data in enumerate(train_generator(), args.start_step + 1):
  File "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/reader.py", line 1104, in __next__
    return self._reader.read_next()
paddle.fluid.core_avx.EnforceNotMet: 

--------------------------------------------
C++ Call Stacks (More useful to developers):
--------------------------------------------
0   std::string paddle::platform::GetTraceBackString<std::string const&>(std::string const&, char const*, int)
1   paddle::platform::EnforceNotMet::EnforceNotMet(std::string const&, char const*, int)
2   paddle::operators::reader::BlockingQueue<std::vector<paddle::framework::LoDTensor, std::allocator<paddle::framework::LoDTensor> > >::Receive(std::vector<paddle::framework::LoDTensor, std::allocator<paddle::framework::LoDTensor> >*)
3   paddle::operators::reader::PyReader::ReadNext(std::vector<paddle::framework::LoDTensor, std::allocator<paddle::framework::LoDTensor> >*)
4   std::_Function_handler<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> (), std::__future_base::_Task_setter<std::unique_ptr<std::__future_base::_Result<unsigned long>, std::__future_base::_Result_base::_Deleter>, unsigned long> >::_M_invoke(std::_Any_data const&)
5   std::__future_base::_State_base::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>&, bool&)
6   ThreadPool::ThreadPool(unsigned long)::{lambda()#1}::operator()() const

----------------------
Error Message Summary:
----------------------
Error: Blocking queue is killed because the data reader raises an exception
  [Hint: Expected killed_ != true, but received killed_:1 == true:1.] at (/paddle/paddle/fluid/operators/reader/blocking_queue.h:141)

{
  "is_distributed": false,
  "save_path": "./nsp_model",
  "train_file": "data/train.txt",
  "valid_file": "data/valid.txt",
  "start_step": 0,
  "num_epochs": 20,
  "log_steps": 100,
  "validation_steps": 1000,
  "save_steps": 500,
  "Model": {
    "model": "NSPModel",
    "config_path": "./config/12L.json",
    "init_checkpoint": "./output/step_13000",
    "init_pretraining_params": "./output/step_13000",
    "learning_rate": 1e-05,
    "warmup_steps": 1000,
    "weight_decay": 0.01,
    "max_grad_norm": 0.1,
    "use_recompute": false,
    "use_amp": true,
    "amp_loss_scaling": 12800,
    "max_seq_len": 512,
    "weight_sharing": true,
    "mem_efficient": false,
    "pre_encoder_cmd": "d",
    "preprocess_cmd": "n",
    "postprocess_cmd": "da",
    "post_cls_cmd": "n",
    "cls_bias": true,
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "latent_type_size": 20,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "role_type_size": 32,
    "vocab_size": 30004
  },
  "Generator": {
    "min_dec_len": 1,
    "max_dec_len": 64,
    "decoding_strategy": "topk_sampling",
    "temperature": 1.0,
    "ignore_unk": true,
    "num_samples": null,
    "topk": 10,
    "topp": 0.9,
    "beam_size": 10,
    "length_average": true,
    "length_penalty": 0.0
  },
  "Task": {
    "task": "NextSentencePrediction"
  },
  "Reader": {
    "max_src_len": 384,
    "max_tgt_len": 128,
    "truncate_first_turn": false,
    "file_format": "file",
    "data_format": "numerical",
    "in_tokens": true,
    "batch_size": 8192,
    "continuous_position": true,
    "random_seed": 11,
    "sort_pool_size": 65536
  },
  "Tokenizer": {
    "tokenizer": "SentencePieceTokenizer",
    "attention_style": "bidirectional",
    "mix_negative_sample": true,
    "vocab_path": "./config/vocab.txt",
    "do_lower_case": false,
    "spm_model_file": "./config/spm.model"
  }
}
W1026 15:41:24.301679  3363 device_context.cc:252] Please NOTE: device: 0, CUDA Capability: 70, Driver API Version: 10.1, Runtime API Version: 9.0
W1026 15:41:24.306735  3363 device_context.cc:260] device: 0, cuDNN Version: 7.6.
Load pretraining parameters from ./output/step_13000.
[train][1] progress: 1/1 step: 100, time: 48.363, speed: 2.068 steps/s
	current lr: 0.0000010
	loss: 13.7086, lm_loss: 13.0175, nsp_loss: 0.6911, nsp_acc: 0.5172
[train][1] progress: 1/1 step: 200, time: 44.511, speed: 2.247 steps/s
	current lr: 0.0000020
	loss: 11.5150, lm_loss: 10.8239, nsp_loss: 0.6911, nsp_acc: 0.5479
[train][1] progress: 1/1 step: 300, time: 44.940, speed: 2.225 steps/s
	current lr: 0.0000030
	loss: 10.7325, lm_loss: 10.0394, nsp_loss: 0.6931, nsp_acc: 0.5078
[train][1] progress: 1/1 step: 400, time: 44.756, speed: 2.234 steps/s
	current lr: 0.0000040
	loss: 10.3843, lm_loss: 9.6931, nsp_loss: 0.6912, nsp_acc: 0.5281
[train][1] progress: 1/1 step: 500, time: 44.713, speed: 2.237 steps/s
	current lr: 0.0000050
	loss: 10.3199, lm_loss: 9.6281, nsp_loss: 0.6917, nsp_acc: 0.5828
[train][1] progress: 1/1 step: 600, time: 44.713, speed: 2.236 steps/s
	current lr: 0.0000060
	loss: 10.2512, lm_loss: 9.5567, nsp_loss: 0.6945, nsp_acc: 0.4434
[train][1] progress: 1/1 step: 700, time: 44.774, speed: 2.233 steps/s
	current lr: 0.0000070
	loss: 10.1103, lm_loss: 9.4162, nsp_loss: 0.6941, nsp_acc: 0.4471
[train][1] progress: 1/1 step: 800, time: 44.597, speed: 2.242 steps/s
	current lr: 0.0000080
	loss: 10.0446, lm_loss: 9.3480, nsp_loss: 0.6965, nsp_acc: 0.4359
[train][1] progress: 1/1 step: 900, time: 44.824, speed: 2.231 steps/s
	current lr: 0.0000090
	loss: 9.9292, lm_loss: 9.2347, nsp_loss: 0.6945, nsp_acc: 0.4652
[train][1] progress: 1/1 step: 1000, time: 44.787, speed: 2.233 steps/s
	current lr: 0.0000100
	loss: 9.8751, lm_loss: 9.1832, nsp_loss: 0.6919, nsp_acc: 0.5191
================================================================================
Evaluation:
	step 100:loss: 9.8821, lm_loss: 9.1892, nsp_loss: 0.6929, nsp_acc: 0.4982, tokens_num: 1079.8459
	step 200:loss: 9.8824, lm_loss: 9.1896, nsp_loss: 0.6927, nsp_acc: 0.5018, tokens_num: 1074.5392
	step 300:loss: 9.8830, lm_loss: 9.1903, nsp_loss: 0.6928, nsp_acc: 0.5008, tokens_num: 1075.7033
	step 400:loss: 9.8833, lm_loss: 9.1905, nsp_loss: 0.6929, nsp_acc: 0.4991, tokens_num: 1075.4909
	step 500:loss: 9.8836, lm_loss: 9.1908, nsp_loss: 0.6928, nsp_acc: 0.5003, tokens_num: 1077.3374
	step 600:loss: 9.8837, lm_loss: 9.1909, nsp_loss: 0.6928, nsp_acc: 0.4998, tokens_num: 1076.3317
	step 700:loss: 9.8834, lm_loss: 9.1906, nsp_loss: 0.6928, nsp_acc: 0.5000, tokens_num: 1076.3067
	step 800:loss: 9.8833, lm_loss: 9.1905, nsp_loss: 0.6928, nsp_acc: 0.5002, tokens_num: 1076.1125
	step 900:loss: 9.8834, lm_loss: 9.1906, nsp_loss: 0.6928, nsp_acc: 0.5004, tokens_num: 1076.6249
	step 1000:loss: 9.8834, lm_loss: 9.1906, nsp_loss: 0.6928, nsp_acc: 0.5000, tokens_num: 1077.1841
	step 1100:loss: 9.8837, lm_loss: 9.1909, nsp_loss: 0.6928, nsp_acc: 0.5001, tokens_num: 1076.6686
	step 1200:loss: 9.8836, lm_loss: 9.1908, nsp_loss: 0.6928, nsp_acc: 0.4997, tokens_num: 1076.8710
	step 1300:loss: 9.8837, lm_loss: 9.1909, nsp_loss: 0.6928, nsp_acc: 0.4998, tokens_num: 1076.9571
	step 1400:loss: 9.8838, lm_loss: 9.1910, nsp_loss: 0.6928, nsp_acc: 0.4999, tokens_num: 1076.8244
	step 1500:loss: 9.8838, lm_loss: 9.1910, nsp_loss: 0.6928, nsp_acc: 0.5002, tokens_num: 1077.1928
	step 1600:loss: 9.8837, lm_loss: 9.1909, nsp_loss: 0.6928, nsp_acc: 0.5001, tokens_num: 1077.0856
	step 1700:loss: 9.8839, lm_loss: 9.1911, nsp_loss: 0.6928, nsp_acc: 0.4999, tokens_num: 1076.6738
	step 1800:loss: 9.8840, lm_loss: 9.1912, nsp_loss: 0.6928, nsp_acc: 0.4999, tokens_num: 1076.5309
	step 1900:loss: 9.8840, lm_loss: 9.1912, nsp_loss: 0.6928, nsp_acc: 0.5001, tokens_num: 1076.2879
	step 2000:loss: 9.8837, lm_loss: 9.1909, nsp_loss: 0.6928, nsp_acc: 0.5001, tokens_num: 1076.3922
	step 2100:loss: 9.8837, lm_loss: 9.1909, nsp_loss: 0.6928, nsp_acc: 0.5000, tokens_num: 1076.1553
	step 2200:loss: 9.8838, lm_loss: 9.1910, nsp_loss: 0.6928, nsp_acc: 0.5000, tokens_num: 1075.9702
	step 2300:loss: 9.8838, lm_loss: 9.1910, nsp_loss: 0.6928, nsp_acc: 0.4999, tokens_num: 1075.8753
	step 2400:loss: 9.8839, lm_loss: 9.1911, nsp_loss: 0.6928, nsp_acc: 0.4997, tokens_num: 1076.0874
	step 2500:loss: 9.8838, lm_loss: 9.1910, nsp_loss: 0.6928, nsp_acc: 0.4997, tokens_num: 1076.0205
	step 2600:loss: 9.8838, lm_loss: 9.1910, nsp_loss: 0.6928, nsp_acc: 0.4998, tokens_num: 1076.1869
	step 2700:loss: 9.8838, lm_loss: 9.1910, nsp_loss: 0.6928, nsp_acc: 0.5000, tokens_num: 1076.3552
	step 2800:loss: 9.8838, lm_loss: 9.1909, nsp_loss: 0.6928, nsp_acc: 0.4999, tokens_num: 1076.3494
	step 2900:loss: 9.8838, lm_loss: 9.1910, nsp_loss: 0.6928, nsp_acc: 0.5000, tokens_num: 1076.3205
	step 3000:loss: 9.8837, lm_loss: 9.1909, nsp_loss: 0.6928, nsp_acc: 0.5000, tokens_num: 1076.3477
	step 3100:loss: 9.8837, lm_loss: 9.1909, nsp_loss: 0.6928, nsp_acc: 0.5001, tokens_num: 1076.2683
	step 3200:loss: 9.8838, lm_loss: 9.1909, nsp_loss: 0.6928, nsp_acc: 0.4999, tokens_num: 1076.0955
	step 3300:loss: 9.8837, lm_loss: 9.1909, nsp_loss: 0.6928, nsp_acc: 0.5000, tokens_num: 1076.1457
	step 3400:loss: 9.8837, lm_loss: 9.1909, nsp_loss: 0.6928, nsp_acc: 0.5000, tokens_num: 1076.2772
	step 3500:loss: 9.8838, lm_loss: 9.1910, nsp_loss: 0.6928, nsp_acc: 0.5000, tokens_num: 1076.6844
	step 3600:loss: 9.8839, lm_loss: 9.1911, nsp_loss: 0.6928, nsp_acc: 0.5000, tokens_num: 1076.8191
	step 3700:loss: 9.8839, lm_loss: 9.1911, nsp_loss: 0.6928, nsp_acc: 0.4998, tokens_num: 1076.9948
	step 3800:loss: 9.8840, lm_loss: 9.1912, nsp_loss: 0.6928, nsp_acc: 0.4999, tokens_num: 1077.3917
	step 3900:loss: 9.8840, lm_loss: 9.1912, nsp_loss: 0.6928, nsp_acc: 0.5000, tokens_num: 1077.8849
	step 4000:loss: 9.8841, lm_loss: 9.1912, nsp_loss: 0.6928, nsp_acc: 0.5001, tokens_num: 1078.1144
	step 4100:loss: 9.8841, lm_loss: 9.1913, nsp_loss: 0.6928, nsp_acc: 0.4999, tokens_num: 1078.3491
	step 4200:loss: 9.8842, lm_loss: 9.1914, nsp_loss: 0.6928, nsp_acc: 0.5000, tokens_num: 1078.4348
	step 4300:loss: 9.8842, lm_loss: 9.1914, nsp_loss: 0.6928, nsp_acc: 0.5001, tokens_num: 1078.0846
	step 4400:loss: 9.8843, lm_loss: 9.1915, nsp_loss: 0.6928, nsp_acc: 0.5000, tokens_num: 1077.6353
	step 4500:loss: 9.8845, lm_loss: 9.1917, nsp_loss: 0.6928, nsp_acc: 0.4999, tokens_num: 1077.0845
	step 4600:loss: 9.8846, lm_loss: 9.1917, nsp_loss: 0.6928, nsp_acc: 0.5000, tokens_num: 1076.3292
	step 4700:loss: 9.8846, lm_loss: 9.1918, nsp_loss: 0.6928, nsp_acc: 0.5001, tokens_num: 1075.1833
	step 4800:loss: 9.8847, lm_loss: 9.1919, nsp_loss: 0.6928, nsp_acc: 0.5002, tokens_num: 1074.3531
	step 4900:loss: 9.8848, lm_loss: 9.1920, nsp_loss: 0.6928, nsp_acc: 0.5001, tokens_num: 1073.6511
	step 5000:loss: 9.8850, lm_loss: 9.1921, nsp_loss: 0.6928, nsp_acc: 0.5000, tokens_num: 1073.1512
	step 5100:loss: 9.8850, lm_loss: 9.1922, nsp_loss: 0.6928, nsp_acc: 0.5000, tokens_num: 1072.9189
	step 5200:loss: 9.8851, lm_loss: 9.1923, nsp_loss: 0.6928, nsp_acc: 0.5000, tokens_num: 1073.0526
	step 5300:loss: 9.8852, lm_loss: 9.1923, nsp_loss: 0.6928, nsp_acc: 0.4999, tokens_num: 1073.1890
	step 5400:loss: 9.8852, lm_loss: 9.1924, nsp_loss: 0.6928, nsp_acc: 0.5000, tokens_num: 1073.3390
	step 5500:loss: 9.8853, lm_loss: 9.1925, nsp_loss: 0.6928, nsp_acc: 0.5000, tokens_num: 1073.5878
	step 5600:loss: 9.8854, lm_loss: 9.1926, nsp_loss: 0.6928, nsp_acc: 0.5001, tokens_num: 1073.7102
	step 5700:loss: 9.8854, lm_loss: 9.1926, nsp_loss: 0.6928, nsp_acc: 0.5000, tokens_num: 1073.8879
	step 5800:loss: 9.8855, lm_loss: 9.1927, nsp_loss: 0.6928, nsp_acc: 0.5001, tokens_num: 1073.9959
	step 5900:loss: 9.8855, lm_loss: 9.1927, nsp_loss: 0.6928, nsp_acc: 0.5000, tokens_num: 1074.1585
	step 6000:loss: 9.8856, lm_loss: 9.1928, nsp_loss: 0.6928, nsp_acc: 0.4999, tokens_num: 1074.3620
	step 6100:loss: 9.8856, lm_loss: 9.1928, nsp_loss: 0.6928, nsp_acc: 0.5000, tokens_num: 1074.5288
	step 6200:loss: 9.8856, lm_loss: 9.1928, nsp_loss: 0.6928, nsp_acc: 0.5000, tokens_num: 1074.7889
	step 6300:loss: 9.8857, lm_loss: 9.1929, nsp_loss: 0.6928, nsp_acc: 0.4999, tokens_num: 1074.8937
	step 6400:loss: 9.8857, lm_loss: 9.1930, nsp_loss: 0.6928, nsp_acc: 0.4999, tokens_num: 1075.0008
	step 6500:loss: 9.8858, lm_loss: 9.1930, nsp_loss: 0.6928, nsp_acc: 0.4999, tokens_num: 1075.2282
	step 6600:loss: 9.8858, lm_loss: 9.1930, nsp_loss: 0.6928, nsp_acc: 0.5000, tokens_num: 1075.2306
	step 6700:loss: 9.8859, lm_loss: 9.1931, nsp_loss: 0.6928, nsp_acc: 0.5000, tokens_num: 1075.3266
	step 6800:loss: 9.8859, lm_loss: 9.1932, nsp_loss: 0.6928, nsp_acc: 0.4999, tokens_num: 1075.3825
	step 6900:loss: 9.8859, lm_loss: 9.1932, nsp_loss: 0.6928, nsp_acc: 0.5000, tokens_num: 1075.4392
	step 7000:loss: 9.8860, lm_loss: 9.1932, nsp_loss: 0.6928, nsp_acc: 0.4999, tokens_num: 1075.6730
	step 7100:loss: 9.8860, lm_loss: 9.1933, nsp_loss: 0.6928, nsp_acc: 0.4999, tokens_num: 1075.7252
	step 7200:loss: 9.8861, lm_loss: 9.1933, nsp_loss: 0.6928, nsp_acc: 0.5000, tokens_num: 1075.7354
	step 7300:loss: 9.8862, lm_loss: 9.1934, nsp_loss: 0.6928, nsp_acc: 0.5000, tokens_num: 1075.6567
	step 7400:loss: 9.8862, lm_loss: 9.1935, nsp_loss: 0.6928, nsp_acc: 0.5001, tokens_num: 1075.6918
	step 7500:loss: 9.8863, lm_loss: 9.1935, nsp_loss: 0.6928, nsp_acc: 0.5000, tokens_num: 1075.8168
	step 7600:loss: 9.8863, lm_loss: 9.1936, nsp_loss: 0.6927, nsp_acc: 0.5000, tokens_num: 1075.8183
	step 7700:loss: 9.8864, lm_loss: 9.1936, nsp_loss: 0.6927, nsp_acc: 0.5000, tokens_num: 1075.6923
	step 7800:loss: 9.8864, lm_loss: 9.1936, nsp_loss: 0.6927, nsp_acc: 0.5001, tokens_num: 1075.7638
	step 7900:loss: 9.8864, lm_loss: 9.1937, nsp_loss: 0.6927, nsp_acc: 0.5000, tokens_num: 1075.8162
[Evaluation][1000]loss: 9.8864, lm_loss: 9.1937, nsp_loss: 0.6927, nsp_acc: 0.5000, tokens_num: 1075.8479
	time cost: 1646.696
================================================================================
[train][1] progress: 1/1 step: 1100, time: 45.826, speed: 2.182 steps/s
	current lr: 0.0000095
	loss: 9.8274, lm_loss: 9.1341, nsp_loss: 0.6934, nsp_acc: 0.4758
[train][1] progress: 1/1 step: 1200, time: 44.710, speed: 2.237 steps/s
	current lr: 0.0000091
	loss: 9.8210, lm_loss: 9.1286, nsp_loss: 0.6924, nsp_acc: 0.5035
